{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-3:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 192)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           3088        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 45,328\n",
      "Trainable params: 44,944\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 64)           0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 64)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 192)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           3088        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 66,128\n",
      "Trainable params: 65,616\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 64)           0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           4112        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 108,752\n",
      "Trainable params: 107,984\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 64)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 320)          0           global_average_pooling1d_9[0][0] \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 320)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           5136        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 192,336\n",
      "Trainable params: 191,312\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 128)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 128)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "                                                                 global_average_pooling1d_14[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 384)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           6160        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 275,920\n",
      "Trainable params: 274,640\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 128)          0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 128)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 384)          0           global_average_pooling1d_15[0][0]\n",
      "                                                                 global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 384)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           6160        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 358,480\n",
      "Trainable params: 356,944\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5301 - acc: 0.1920\n",
      "Epoch 00001: val_loss improved from inf to 2.43032, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/001-2.4303.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 2.5301 - acc: 0.1920 - val_loss: 2.4303 - val_acc: 0.3077\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2744 - acc: 0.2777\n",
      "Epoch 00002: val_loss improved from 2.43032 to 2.09515, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/002-2.0951.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.2744 - acc: 0.2777 - val_loss: 2.0951 - val_acc: 0.4198\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1483 - acc: 0.3198\n",
      "Epoch 00003: val_loss improved from 2.09515 to 1.98118, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/003-1.9812.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.1484 - acc: 0.3198 - val_loss: 1.9812 - val_acc: 0.4566\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0467 - acc: 0.3496\n",
      "Epoch 00004: val_loss improved from 1.98118 to 1.89785, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/004-1.8979.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.0467 - acc: 0.3496 - val_loss: 1.8979 - val_acc: 0.4817\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9628 - acc: 0.3755\n",
      "Epoch 00005: val_loss improved from 1.89785 to 1.80646, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/005-1.8065.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.9628 - acc: 0.3755 - val_loss: 1.8065 - val_acc: 0.5073\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8932 - acc: 0.3980\n",
      "Epoch 00006: val_loss improved from 1.80646 to 1.72764, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/006-1.7276.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.8932 - acc: 0.3979 - val_loss: 1.7276 - val_acc: 0.5344\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8331 - acc: 0.4173\n",
      "Epoch 00007: val_loss improved from 1.72764 to 1.67219, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/007-1.6722.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.8330 - acc: 0.4173 - val_loss: 1.6722 - val_acc: 0.5392\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7763 - acc: 0.4353\n",
      "Epoch 00008: val_loss improved from 1.67219 to 1.59759, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/008-1.5976.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.7763 - acc: 0.4353 - val_loss: 1.5976 - val_acc: 0.5688\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7324 - acc: 0.4515\n",
      "Epoch 00009: val_loss improved from 1.59759 to 1.56571, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/009-1.5657.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.7325 - acc: 0.4515 - val_loss: 1.5657 - val_acc: 0.5709\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6958 - acc: 0.4608\n",
      "Epoch 00010: val_loss improved from 1.56571 to 1.51431, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/010-1.5143.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.6958 - acc: 0.4609 - val_loss: 1.5143 - val_acc: 0.5823\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6570 - acc: 0.4749\n",
      "Epoch 00011: val_loss did not improve from 1.51431\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.6569 - acc: 0.4750 - val_loss: 1.5246 - val_acc: 0.5483\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6270 - acc: 0.4876\n",
      "Epoch 00012: val_loss improved from 1.51431 to 1.46168, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/012-1.4617.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.6270 - acc: 0.4876 - val_loss: 1.4617 - val_acc: 0.5856\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5978 - acc: 0.4993\n",
      "Epoch 00013: val_loss improved from 1.46168 to 1.42600, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/013-1.4260.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.5978 - acc: 0.4993 - val_loss: 1.4260 - val_acc: 0.5809\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5703 - acc: 0.5089\n",
      "Epoch 00014: val_loss did not improve from 1.42600\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.5703 - acc: 0.5089 - val_loss: 1.4820 - val_acc: 0.5437\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5499 - acc: 0.5124\n",
      "Epoch 00015: val_loss improved from 1.42600 to 1.35680, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/015-1.3568.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.5499 - acc: 0.5124 - val_loss: 1.3568 - val_acc: 0.6205\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5284 - acc: 0.5204\n",
      "Epoch 00016: val_loss improved from 1.35680 to 1.32227, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/016-1.3223.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.5284 - acc: 0.5204 - val_loss: 1.3223 - val_acc: 0.6273\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5030 - acc: 0.5292\n",
      "Epoch 00017: val_loss did not improve from 1.32227\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.5030 - acc: 0.5292 - val_loss: 1.3282 - val_acc: 0.6147\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4896 - acc: 0.5349\n",
      "Epoch 00018: val_loss improved from 1.32227 to 1.30607, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/018-1.3061.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4897 - acc: 0.5348 - val_loss: 1.3061 - val_acc: 0.6299\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4666 - acc: 0.5410\n",
      "Epoch 00019: val_loss improved from 1.30607 to 1.25517, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/019-1.2552.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4666 - acc: 0.5410 - val_loss: 1.2552 - val_acc: 0.6450\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4571 - acc: 0.5439\n",
      "Epoch 00020: val_loss did not improve from 1.25517\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4573 - acc: 0.5438 - val_loss: 1.3310 - val_acc: 0.5795\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4445 - acc: 0.5485\n",
      "Epoch 00021: val_loss did not improve from 1.25517\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4445 - acc: 0.5486 - val_loss: 1.4858 - val_acc: 0.5169\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4258 - acc: 0.5551\n",
      "Epoch 00022: val_loss improved from 1.25517 to 1.23705, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/022-1.2371.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4259 - acc: 0.5551 - val_loss: 1.2371 - val_acc: 0.6497\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4106 - acc: 0.5616\n",
      "Epoch 00023: val_loss did not improve from 1.23705\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4108 - acc: 0.5616 - val_loss: 1.3375 - val_acc: 0.5898\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4063 - acc: 0.5617\n",
      "Epoch 00024: val_loss did not improve from 1.23705\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4062 - acc: 0.5617 - val_loss: 1.2651 - val_acc: 0.6203\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3932 - acc: 0.5615\n",
      "Epoch 00025: val_loss did not improve from 1.23705\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3932 - acc: 0.5615 - val_loss: 1.2956 - val_acc: 0.5921\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3797 - acc: 0.5690\n",
      "Epoch 00026: val_loss did not improve from 1.23705\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3796 - acc: 0.5690 - val_loss: 1.2649 - val_acc: 0.6219\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3712 - acc: 0.5723\n",
      "Epoch 00027: val_loss did not improve from 1.23705\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3712 - acc: 0.5723 - val_loss: 1.4241 - val_acc: 0.5330\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3646 - acc: 0.5733\n",
      "Epoch 00028: val_loss improved from 1.23705 to 1.18173, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/028-1.1817.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3646 - acc: 0.5732 - val_loss: 1.1817 - val_acc: 0.6494\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3487 - acc: 0.5803\n",
      "Epoch 00029: val_loss did not improve from 1.18173\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3487 - acc: 0.5802 - val_loss: 1.2876 - val_acc: 0.5816\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3439 - acc: 0.5841\n",
      "Epoch 00030: val_loss improved from 1.18173 to 1.15643, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/030-1.1564.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3440 - acc: 0.5840 - val_loss: 1.1564 - val_acc: 0.6739\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3296 - acc: 0.5863\n",
      "Epoch 00031: val_loss improved from 1.15643 to 1.13209, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/031-1.1321.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3295 - acc: 0.5864 - val_loss: 1.1321 - val_acc: 0.6695\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3242 - acc: 0.5875\n",
      "Epoch 00032: val_loss did not improve from 1.13209\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3242 - acc: 0.5875 - val_loss: 1.1842 - val_acc: 0.6434\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3169 - acc: 0.5916\n",
      "Epoch 00033: val_loss did not improve from 1.13209\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3169 - acc: 0.5916 - val_loss: 1.2126 - val_acc: 0.6201\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3163 - acc: 0.5917\n",
      "Epoch 00034: val_loss did not improve from 1.13209\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3162 - acc: 0.5917 - val_loss: 1.1762 - val_acc: 0.6536\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2963 - acc: 0.5969\n",
      "Epoch 00035: val_loss did not improve from 1.13209\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2962 - acc: 0.5970 - val_loss: 1.1652 - val_acc: 0.6539\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2961 - acc: 0.5968\n",
      "Epoch 00036: val_loss did not improve from 1.13209\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2960 - acc: 0.5968 - val_loss: 1.1409 - val_acc: 0.6639\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2914 - acc: 0.6025\n",
      "Epoch 00037: val_loss did not improve from 1.13209\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2915 - acc: 0.6025 - val_loss: 1.1324 - val_acc: 0.6473\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2877 - acc: 0.6029\n",
      "Epoch 00038: val_loss improved from 1.13209 to 1.07753, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/038-1.0775.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2878 - acc: 0.6029 - val_loss: 1.0775 - val_acc: 0.6918\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2840 - acc: 0.6048\n",
      "Epoch 00039: val_loss did not improve from 1.07753\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2839 - acc: 0.6047 - val_loss: 1.2071 - val_acc: 0.6110\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2799 - acc: 0.6033\n",
      "Epoch 00040: val_loss did not improve from 1.07753\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2799 - acc: 0.6033 - val_loss: 1.0923 - val_acc: 0.6827\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2690 - acc: 0.6087\n",
      "Epoch 00041: val_loss did not improve from 1.07753\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2690 - acc: 0.6087 - val_loss: 1.3387 - val_acc: 0.5870\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2672 - acc: 0.6083\n",
      "Epoch 00042: val_loss improved from 1.07753 to 1.07540, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/042-1.0754.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2673 - acc: 0.6082 - val_loss: 1.0754 - val_acc: 0.6946\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2619 - acc: 0.6111\n",
      "Epoch 00043: val_loss did not improve from 1.07540\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2620 - acc: 0.6110 - val_loss: 1.2660 - val_acc: 0.5896\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2532 - acc: 0.6150\n",
      "Epoch 00044: val_loss did not improve from 1.07540\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2531 - acc: 0.6151 - val_loss: 1.1863 - val_acc: 0.6068\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2489 - acc: 0.6142\n",
      "Epoch 00045: val_loss improved from 1.07540 to 1.06532, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/045-1.0653.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2488 - acc: 0.6143 - val_loss: 1.0653 - val_acc: 0.6862\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2424 - acc: 0.6169\n",
      "Epoch 00046: val_loss did not improve from 1.06532\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2424 - acc: 0.6168 - val_loss: 1.0781 - val_acc: 0.6792\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2407 - acc: 0.6164\n",
      "Epoch 00047: val_loss did not improve from 1.06532\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2408 - acc: 0.6164 - val_loss: 1.2373 - val_acc: 0.5865\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2256 - acc: 0.6222\n",
      "Epoch 00048: val_loss did not improve from 1.06532\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2260 - acc: 0.6222 - val_loss: 1.1899 - val_acc: 0.6336\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2284 - acc: 0.6218\n",
      "Epoch 00049: val_loss did not improve from 1.06532\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2284 - acc: 0.6219 - val_loss: 3.0593 - val_acc: 0.3096\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2213 - acc: 0.6246\n",
      "Epoch 00050: val_loss did not improve from 1.06532\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2214 - acc: 0.6246 - val_loss: 1.3770 - val_acc: 0.5563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2184 - acc: 0.6259\n",
      "Epoch 00051: val_loss improved from 1.06532 to 1.03373, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/051-1.0337.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2184 - acc: 0.6259 - val_loss: 1.0337 - val_acc: 0.6983\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2094 - acc: 0.6264\n",
      "Epoch 00052: val_loss did not improve from 1.03373\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2094 - acc: 0.6263 - val_loss: 1.6631 - val_acc: 0.4703\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2096 - acc: 0.6307\n",
      "Epoch 00053: val_loss did not improve from 1.03373\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2095 - acc: 0.6307 - val_loss: 1.2220 - val_acc: 0.6056\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2048 - acc: 0.6304\n",
      "Epoch 00054: val_loss did not improve from 1.03373\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2048 - acc: 0.6303 - val_loss: 1.0988 - val_acc: 0.6515\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2052 - acc: 0.6292\n",
      "Epoch 00055: val_loss did not improve from 1.03373\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2053 - acc: 0.6292 - val_loss: 1.8404 - val_acc: 0.4055\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1959 - acc: 0.6333\n",
      "Epoch 00056: val_loss did not improve from 1.03373\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1960 - acc: 0.6333 - val_loss: 1.2556 - val_acc: 0.5763\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1970 - acc: 0.6338\n",
      "Epoch 00057: val_loss did not improve from 1.03373\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1972 - acc: 0.6337 - val_loss: 1.0559 - val_acc: 0.6942\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1971 - acc: 0.6340\n",
      "Epoch 00058: val_loss did not improve from 1.03373\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1973 - acc: 0.6340 - val_loss: 1.3547 - val_acc: 0.5528\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1794 - acc: 0.6383\n",
      "Epoch 00059: val_loss did not improve from 1.03373\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1794 - acc: 0.6383 - val_loss: 2.6290 - val_acc: 0.3142\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1838 - acc: 0.6346\n",
      "Epoch 00060: val_loss did not improve from 1.03373\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1838 - acc: 0.6346 - val_loss: 1.3608 - val_acc: 0.5604\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1778 - acc: 0.6382\n",
      "Epoch 00061: val_loss improved from 1.03373 to 1.02111, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/061-1.0211.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1779 - acc: 0.6382 - val_loss: 1.0211 - val_acc: 0.6858\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1777 - acc: 0.6383\n",
      "Epoch 00062: val_loss did not improve from 1.02111\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1777 - acc: 0.6383 - val_loss: 1.0500 - val_acc: 0.6599\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1698 - acc: 0.6435\n",
      "Epoch 00063: val_loss improved from 1.02111 to 1.00157, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/063-1.0016.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1702 - acc: 0.6434 - val_loss: 1.0016 - val_acc: 0.7018\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1672 - acc: 0.6438\n",
      "Epoch 00064: val_loss did not improve from 1.00157\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1675 - acc: 0.6438 - val_loss: 1.3458 - val_acc: 0.5872\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1668 - acc: 0.6417\n",
      "Epoch 00065: val_loss did not improve from 1.00157\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1671 - acc: 0.6417 - val_loss: 2.0732 - val_acc: 0.3687\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1666 - acc: 0.6451\n",
      "Epoch 00066: val_loss improved from 1.00157 to 0.99614, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/066-0.9961.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1666 - acc: 0.6451 - val_loss: 0.9961 - val_acc: 0.7112\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1556 - acc: 0.6466\n",
      "Epoch 00067: val_loss did not improve from 0.99614\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1557 - acc: 0.6466 - val_loss: 1.0017 - val_acc: 0.6935\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1545 - acc: 0.6468\n",
      "Epoch 00068: val_loss did not improve from 0.99614\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1546 - acc: 0.6468 - val_loss: 1.2745 - val_acc: 0.5665\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1539 - acc: 0.6486\n",
      "Epoch 00069: val_loss did not improve from 0.99614\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1543 - acc: 0.6485 - val_loss: 1.7770 - val_acc: 0.4223\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1549 - acc: 0.6487\n",
      "Epoch 00070: val_loss did not improve from 0.99614\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1550 - acc: 0.6486 - val_loss: 1.1567 - val_acc: 0.6278\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1483 - acc: 0.6467\n",
      "Epoch 00071: val_loss improved from 0.99614 to 0.93152, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/071-0.9315.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1487 - acc: 0.6467 - val_loss: 0.9315 - val_acc: 0.7321\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1537 - acc: 0.6473\n",
      "Epoch 00072: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1537 - acc: 0.6473 - val_loss: 0.9965 - val_acc: 0.7014\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1458 - acc: 0.6483\n",
      "Epoch 00073: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1459 - acc: 0.6483 - val_loss: 1.0421 - val_acc: 0.6771\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1356 - acc: 0.6532\n",
      "Epoch 00074: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1356 - acc: 0.6532 - val_loss: 3.0857 - val_acc: 0.3443\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1388 - acc: 0.6508\n",
      "Epoch 00075: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1388 - acc: 0.6508 - val_loss: 1.5256 - val_acc: 0.5269\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1317 - acc: 0.6558\n",
      "Epoch 00076: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1319 - acc: 0.6558 - val_loss: 0.9718 - val_acc: 0.7230\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1345 - acc: 0.6544\n",
      "Epoch 00077: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1344 - acc: 0.6544 - val_loss: 1.3692 - val_acc: 0.5455\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1290 - acc: 0.6542\n",
      "Epoch 00078: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1289 - acc: 0.6543 - val_loss: 1.1976 - val_acc: 0.6061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1275 - acc: 0.6577\n",
      "Epoch 00079: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1274 - acc: 0.6577 - val_loss: 1.0466 - val_acc: 0.6730\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1244 - acc: 0.6550\n",
      "Epoch 00080: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1244 - acc: 0.6550 - val_loss: 0.9901 - val_acc: 0.6995\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1335 - acc: 0.6546\n",
      "Epoch 00081: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1335 - acc: 0.6545 - val_loss: 1.0741 - val_acc: 0.6546\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1202 - acc: 0.6613\n",
      "Epoch 00082: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1202 - acc: 0.6613 - val_loss: 0.9947 - val_acc: 0.6951\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1182 - acc: 0.6577\n",
      "Epoch 00083: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1182 - acc: 0.6577 - val_loss: 1.1310 - val_acc: 0.6394\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1128 - acc: 0.6601\n",
      "Epoch 00084: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1128 - acc: 0.6600 - val_loss: 1.0769 - val_acc: 0.6406\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1119 - acc: 0.6605\n",
      "Epoch 00085: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1119 - acc: 0.6604 - val_loss: 1.1875 - val_acc: 0.6124\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1134 - acc: 0.6606\n",
      "Epoch 00086: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1135 - acc: 0.6606 - val_loss: 4.3057 - val_acc: 0.3347\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1121 - acc: 0.6630\n",
      "Epoch 00087: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1120 - acc: 0.6630 - val_loss: 1.2583 - val_acc: 0.5858\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1061 - acc: 0.6621\n",
      "Epoch 00088: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1062 - acc: 0.6621 - val_loss: 1.3116 - val_acc: 0.5835\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1050 - acc: 0.6621\n",
      "Epoch 00089: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1055 - acc: 0.6620 - val_loss: 0.9701 - val_acc: 0.7179\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0986 - acc: 0.6660\n",
      "Epoch 00090: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0987 - acc: 0.6660 - val_loss: 1.0293 - val_acc: 0.6797\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0965 - acc: 0.6682\n",
      "Epoch 00091: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0964 - acc: 0.6682 - val_loss: 1.0220 - val_acc: 0.6897\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0945 - acc: 0.6647\n",
      "Epoch 00092: val_loss did not improve from 0.93152\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0944 - acc: 0.6647 - val_loss: 1.0007 - val_acc: 0.7032\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0937 - acc: 0.6657\n",
      "Epoch 00093: val_loss improved from 0.93152 to 0.88624, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/093-0.8862.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0938 - acc: 0.6657 - val_loss: 0.8862 - val_acc: 0.7526\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0919 - acc: 0.6670\n",
      "Epoch 00094: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0919 - acc: 0.6670 - val_loss: 2.1076 - val_acc: 0.4407\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0913 - acc: 0.6709\n",
      "Epoch 00095: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0912 - acc: 0.6709 - val_loss: 0.9934 - val_acc: 0.6816\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0912 - acc: 0.6699\n",
      "Epoch 00096: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0912 - acc: 0.6699 - val_loss: 1.2585 - val_acc: 0.5896\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0896 - acc: 0.6687\n",
      "Epoch 00097: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0897 - acc: 0.6686 - val_loss: 4.0450 - val_acc: 0.3604\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0827 - acc: 0.6697\n",
      "Epoch 00098: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0827 - acc: 0.6697 - val_loss: 1.0282 - val_acc: 0.6690\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0848 - acc: 0.6682\n",
      "Epoch 00099: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0847 - acc: 0.6683 - val_loss: 2.0228 - val_acc: 0.4109\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0782 - acc: 0.6696\n",
      "Epoch 00100: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0782 - acc: 0.6696 - val_loss: 1.5087 - val_acc: 0.5241\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0808 - acc: 0.6712\n",
      "Epoch 00101: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0811 - acc: 0.6711 - val_loss: 2.1200 - val_acc: 0.4230\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0797 - acc: 0.6735\n",
      "Epoch 00102: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0798 - acc: 0.6735 - val_loss: 0.9288 - val_acc: 0.7214\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0744 - acc: 0.6713\n",
      "Epoch 00103: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0746 - acc: 0.6712 - val_loss: 3.0057 - val_acc: 0.3033\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0799 - acc: 0.6720\n",
      "Epoch 00104: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0799 - acc: 0.6720 - val_loss: 1.5442 - val_acc: 0.4866\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0690 - acc: 0.6762\n",
      "Epoch 00105: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0690 - acc: 0.6762 - val_loss: 1.0197 - val_acc: 0.6923\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0652 - acc: 0.6770\n",
      "Epoch 00106: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0652 - acc: 0.6770 - val_loss: 2.8138 - val_acc: 0.3361\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0631 - acc: 0.6792\n",
      "Epoch 00107: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0630 - acc: 0.6792 - val_loss: 0.9605 - val_acc: 0.6925\n",
      "Epoch 108/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0670 - acc: 0.6774\n",
      "Epoch 00108: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0669 - acc: 0.6774 - val_loss: 1.1560 - val_acc: 0.6131\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0547 - acc: 0.6802\n",
      "Epoch 00109: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0549 - acc: 0.6802 - val_loss: 2.8471 - val_acc: 0.3652\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0582 - acc: 0.6796\n",
      "Epoch 00110: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0583 - acc: 0.6796 - val_loss: 0.9751 - val_acc: 0.7088\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0639 - acc: 0.6773\n",
      "Epoch 00111: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0640 - acc: 0.6773 - val_loss: 1.0086 - val_acc: 0.6886\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0605 - acc: 0.6811\n",
      "Epoch 00112: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0606 - acc: 0.6810 - val_loss: 1.4266 - val_acc: 0.5754\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0568 - acc: 0.6794\n",
      "Epoch 00113: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0571 - acc: 0.6793 - val_loss: 1.1666 - val_acc: 0.6094\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0551 - acc: 0.6793\n",
      "Epoch 00114: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0550 - acc: 0.6793 - val_loss: 1.4862 - val_acc: 0.4892\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0571 - acc: 0.6784\n",
      "Epoch 00115: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0571 - acc: 0.6784 - val_loss: 2.3069 - val_acc: 0.4058\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0538 - acc: 0.6815\n",
      "Epoch 00116: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0538 - acc: 0.6814 - val_loss: 0.9613 - val_acc: 0.7079\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0508 - acc: 0.6812\n",
      "Epoch 00117: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0508 - acc: 0.6812 - val_loss: 0.8871 - val_acc: 0.7540\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0494 - acc: 0.6809\n",
      "Epoch 00118: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0494 - acc: 0.6809 - val_loss: 1.5661 - val_acc: 0.5148\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0467 - acc: 0.6822\n",
      "Epoch 00119: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0466 - acc: 0.6822 - val_loss: 2.4528 - val_acc: 0.3802\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0543 - acc: 0.6840\n",
      "Epoch 00120: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0543 - acc: 0.6840 - val_loss: 4.8153 - val_acc: 0.2772\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0462 - acc: 0.6840\n",
      "Epoch 00121: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0461 - acc: 0.6841 - val_loss: 1.4180 - val_acc: 0.5176\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0444 - acc: 0.6828\n",
      "Epoch 00122: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0444 - acc: 0.6828 - val_loss: 1.9002 - val_acc: 0.4184\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0423 - acc: 0.6825\n",
      "Epoch 00123: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0426 - acc: 0.6825 - val_loss: 1.3550 - val_acc: 0.5870\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0418 - acc: 0.6849\n",
      "Epoch 00124: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0418 - acc: 0.6849 - val_loss: 1.5446 - val_acc: 0.4766\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0331 - acc: 0.6877\n",
      "Epoch 00125: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0332 - acc: 0.6876 - val_loss: 1.3915 - val_acc: 0.5518\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0354 - acc: 0.6874\n",
      "Epoch 00126: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0355 - acc: 0.6873 - val_loss: 2.6288 - val_acc: 0.3604\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0330 - acc: 0.6892\n",
      "Epoch 00127: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0330 - acc: 0.6892 - val_loss: 0.9006 - val_acc: 0.7265\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0303 - acc: 0.6909\n",
      "Epoch 00128: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0306 - acc: 0.6908 - val_loss: 2.5324 - val_acc: 0.4577\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0337 - acc: 0.6886\n",
      "Epoch 00129: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0337 - acc: 0.6886 - val_loss: 3.0702 - val_acc: 0.4069\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0288 - acc: 0.6904\n",
      "Epoch 00130: val_loss did not improve from 0.88624\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0288 - acc: 0.6903 - val_loss: 0.9993 - val_acc: 0.6748\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0292 - acc: 0.6868\n",
      "Epoch 00131: val_loss improved from 0.88624 to 0.86713, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/131-0.8671.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0292 - acc: 0.6868 - val_loss: 0.8671 - val_acc: 0.7563\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0295 - acc: 0.6915\n",
      "Epoch 00132: val_loss did not improve from 0.86713\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0294 - acc: 0.6915 - val_loss: 1.3163 - val_acc: 0.5879\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0265 - acc: 0.6926\n",
      "Epoch 00133: val_loss did not improve from 0.86713\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0265 - acc: 0.6926 - val_loss: 0.9446 - val_acc: 0.7140\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0252 - acc: 0.6901\n",
      "Epoch 00134: val_loss did not improve from 0.86713\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0256 - acc: 0.6900 - val_loss: 2.2649 - val_acc: 0.4510\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0273 - acc: 0.6882\n",
      "Epoch 00135: val_loss did not improve from 0.86713\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0274 - acc: 0.6881 - val_loss: 1.5195 - val_acc: 0.5220\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0180 - acc: 0.6951\n",
      "Epoch 00136: val_loss improved from 0.86713 to 0.85687, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/136-0.8569.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0180 - acc: 0.6951 - val_loss: 0.8569 - val_acc: 0.7543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0250 - acc: 0.6905\n",
      "Epoch 00137: val_loss did not improve from 0.85687\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0251 - acc: 0.6904 - val_loss: 0.9253 - val_acc: 0.7354\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0193 - acc: 0.6945\n",
      "Epoch 00138: val_loss did not improve from 0.85687\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0193 - acc: 0.6945 - val_loss: 0.9762 - val_acc: 0.6848\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0152 - acc: 0.6939\n",
      "Epoch 00139: val_loss improved from 0.85687 to 0.85348, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/139-0.8535.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0152 - acc: 0.6939 - val_loss: 0.8535 - val_acc: 0.7515\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0159 - acc: 0.6921\n",
      "Epoch 00140: val_loss did not improve from 0.85348\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0162 - acc: 0.6921 - val_loss: 0.8686 - val_acc: 0.7484\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0164 - acc: 0.6916\n",
      "Epoch 00141: val_loss did not improve from 0.85348\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0164 - acc: 0.6916 - val_loss: 2.9888 - val_acc: 0.2965\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0108 - acc: 0.6948\n",
      "Epoch 00142: val_loss improved from 0.85348 to 0.81181, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/142-0.8118.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0108 - acc: 0.6947 - val_loss: 0.8118 - val_acc: 0.7668\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0087 - acc: 0.6947\n",
      "Epoch 00143: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0089 - acc: 0.6947 - val_loss: 0.8876 - val_acc: 0.7282\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0060 - acc: 0.6981\n",
      "Epoch 00144: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0060 - acc: 0.6981 - val_loss: 0.9119 - val_acc: 0.7319\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0103 - acc: 0.6934\n",
      "Epoch 00145: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0105 - acc: 0.6934 - val_loss: 2.6318 - val_acc: 0.4470\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0098 - acc: 0.6943\n",
      "Epoch 00146: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0098 - acc: 0.6943 - val_loss: 1.5351 - val_acc: 0.4962\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0087 - acc: 0.6959\n",
      "Epoch 00147: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0089 - acc: 0.6959 - val_loss: 1.1006 - val_acc: 0.6529\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0046 - acc: 0.6977\n",
      "Epoch 00148: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0046 - acc: 0.6977 - val_loss: 3.7470 - val_acc: 0.2791\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0027 - acc: 0.6972\n",
      "Epoch 00149: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0027 - acc: 0.6972 - val_loss: 1.1582 - val_acc: 0.6091\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0065 - acc: 0.6940\n",
      "Epoch 00150: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0066 - acc: 0.6939 - val_loss: 0.8609 - val_acc: 0.7431\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9964 - acc: 0.7021\n",
      "Epoch 00151: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9965 - acc: 0.7020 - val_loss: 2.1855 - val_acc: 0.4132\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0003 - acc: 0.6982\n",
      "Epoch 00152: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0004 - acc: 0.6982 - val_loss: 3.9999 - val_acc: 0.3373\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0014 - acc: 0.6988\n",
      "Epoch 00153: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0015 - acc: 0.6987 - val_loss: 0.8242 - val_acc: 0.7654\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9986 - acc: 0.6988\n",
      "Epoch 00154: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9987 - acc: 0.6988 - val_loss: 0.8481 - val_acc: 0.7540\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9955 - acc: 0.7009\n",
      "Epoch 00155: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9955 - acc: 0.7009 - val_loss: 2.1477 - val_acc: 0.4731\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9961 - acc: 0.7030\n",
      "Epoch 00156: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9962 - acc: 0.7029 - val_loss: 3.5040 - val_acc: 0.3191\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9939 - acc: 0.7021\n",
      "Epoch 00157: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9939 - acc: 0.7021 - val_loss: 1.0071 - val_acc: 0.6671\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9925 - acc: 0.7018\n",
      "Epoch 00158: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9924 - acc: 0.7018 - val_loss: 2.7254 - val_acc: 0.3892\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9932 - acc: 0.7008\n",
      "Epoch 00159: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9932 - acc: 0.7009 - val_loss: 2.5139 - val_acc: 0.4177\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9874 - acc: 0.7040\n",
      "Epoch 00160: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9875 - acc: 0.7040 - val_loss: 0.8553 - val_acc: 0.7389\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9956 - acc: 0.7017\n",
      "Epoch 00161: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9956 - acc: 0.7016 - val_loss: 0.9548 - val_acc: 0.6962\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9928 - acc: 0.7018\n",
      "Epoch 00162: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9928 - acc: 0.7017 - val_loss: 0.9220 - val_acc: 0.7207\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9943 - acc: 0.7035\n",
      "Epoch 00163: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9945 - acc: 0.7034 - val_loss: 0.9024 - val_acc: 0.7321\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9931 - acc: 0.7022\n",
      "Epoch 00164: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9931 - acc: 0.7022 - val_loss: 1.5379 - val_acc: 0.5476\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9847 - acc: 0.7055\n",
      "Epoch 00165: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9847 - acc: 0.7055 - val_loss: 0.9282 - val_acc: 0.7174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9899 - acc: 0.7016\n",
      "Epoch 00166: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9898 - acc: 0.7016 - val_loss: 0.9808 - val_acc: 0.6730\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9815 - acc: 0.7046\n",
      "Epoch 00167: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9816 - acc: 0.7046 - val_loss: 1.0572 - val_acc: 0.6413\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9858 - acc: 0.7063\n",
      "Epoch 00168: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9857 - acc: 0.7064 - val_loss: 1.6979 - val_acc: 0.5383\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9841 - acc: 0.7033\n",
      "Epoch 00169: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9848 - acc: 0.7032 - val_loss: 1.8531 - val_acc: 0.4708\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9921 - acc: 0.7034\n",
      "Epoch 00170: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9923 - acc: 0.7034 - val_loss: 4.1596 - val_acc: 0.3107\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9751 - acc: 0.7064\n",
      "Epoch 00171: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9754 - acc: 0.7063 - val_loss: 2.6412 - val_acc: 0.3678\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9842 - acc: 0.7055\n",
      "Epoch 00172: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9842 - acc: 0.7056 - val_loss: 0.9000 - val_acc: 0.7088\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9772 - acc: 0.7049\n",
      "Epoch 00173: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9773 - acc: 0.7049 - val_loss: 0.9038 - val_acc: 0.7270\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9766 - acc: 0.7071\n",
      "Epoch 00174: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9765 - acc: 0.7072 - val_loss: 2.6611 - val_acc: 0.4556\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9679 - acc: 0.7085\n",
      "Epoch 00175: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9681 - acc: 0.7085 - val_loss: 1.0618 - val_acc: 0.6553\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9804 - acc: 0.7089\n",
      "Epoch 00176: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9804 - acc: 0.7089 - val_loss: 1.0237 - val_acc: 0.6830\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9710 - acc: 0.7096\n",
      "Epoch 00177: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9710 - acc: 0.7096 - val_loss: 0.8321 - val_acc: 0.7605\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9717 - acc: 0.7089\n",
      "Epoch 00178: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9718 - acc: 0.7090 - val_loss: 1.0121 - val_acc: 0.6928\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9769 - acc: 0.7062\n",
      "Epoch 00179: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9772 - acc: 0.7061 - val_loss: 3.8963 - val_acc: 0.3657\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9704 - acc: 0.7087\n",
      "Epoch 00180: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9706 - acc: 0.7087 - val_loss: 3.6427 - val_acc: 0.3051\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9762 - acc: 0.7092\n",
      "Epoch 00181: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9763 - acc: 0.7091 - val_loss: 0.9803 - val_acc: 0.6830\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9663 - acc: 0.7077\n",
      "Epoch 00182: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9663 - acc: 0.7077 - val_loss: 1.4133 - val_acc: 0.5684\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9661 - acc: 0.7118\n",
      "Epoch 00183: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9660 - acc: 0.7118 - val_loss: 1.5806 - val_acc: 0.4913\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9696 - acc: 0.7101\n",
      "Epoch 00184: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9697 - acc: 0.7100 - val_loss: 1.2568 - val_acc: 0.5891\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9705 - acc: 0.7095\n",
      "Epoch 00185: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9706 - acc: 0.7094 - val_loss: 0.8278 - val_acc: 0.7531\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9682 - acc: 0.7089\n",
      "Epoch 00186: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9681 - acc: 0.7089 - val_loss: 1.2328 - val_acc: 0.6096\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9594 - acc: 0.7143\n",
      "Epoch 00187: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9594 - acc: 0.7143 - val_loss: 1.7622 - val_acc: 0.4850\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9688 - acc: 0.7112\n",
      "Epoch 00188: val_loss did not improve from 0.81181\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9689 - acc: 0.7112 - val_loss: 1.0505 - val_acc: 0.6394\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9678 - acc: 0.7091\n",
      "Epoch 00189: val_loss improved from 0.81181 to 0.80816, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/189-0.8082.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9679 - acc: 0.7091 - val_loss: 0.8082 - val_acc: 0.7633\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9559 - acc: 0.7130\n",
      "Epoch 00190: val_loss did not improve from 0.80816\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9560 - acc: 0.7129 - val_loss: 1.1722 - val_acc: 0.6131\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9536 - acc: 0.7149\n",
      "Epoch 00191: val_loss did not improve from 0.80816\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9536 - acc: 0.7148 - val_loss: 2.0449 - val_acc: 0.4305\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9600 - acc: 0.7141\n",
      "Epoch 00192: val_loss did not improve from 0.80816\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9599 - acc: 0.7141 - val_loss: 1.1638 - val_acc: 0.6462\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9532 - acc: 0.7148\n",
      "Epoch 00193: val_loss did not improve from 0.80816\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9531 - acc: 0.7148 - val_loss: 1.0085 - val_acc: 0.6713\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9532 - acc: 0.7153\n",
      "Epoch 00194: val_loss did not improve from 0.80816\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9533 - acc: 0.7153 - val_loss: 1.2236 - val_acc: 0.6129\n",
      "Epoch 195/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9567 - acc: 0.7142\n",
      "Epoch 00195: val_loss did not improve from 0.80816\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9566 - acc: 0.7143 - val_loss: 1.9169 - val_acc: 0.4754\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9607 - acc: 0.7113\n",
      "Epoch 00196: val_loss did not improve from 0.80816\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9607 - acc: 0.7112 - val_loss: 1.9258 - val_acc: 0.5181\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9578 - acc: 0.7141\n",
      "Epoch 00197: val_loss improved from 0.80816 to 0.79913, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/197-0.7991.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9578 - acc: 0.7141 - val_loss: 0.7991 - val_acc: 0.7803\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9568 - acc: 0.7143\n",
      "Epoch 00198: val_loss did not improve from 0.79913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9568 - acc: 0.7142 - val_loss: 1.8176 - val_acc: 0.5129\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9579 - acc: 0.7155\n",
      "Epoch 00199: val_loss did not improve from 0.79913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9579 - acc: 0.7155 - val_loss: 1.3115 - val_acc: 0.5830\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9579 - acc: 0.7154\n",
      "Epoch 00200: val_loss did not improve from 0.79913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9580 - acc: 0.7153 - val_loss: 2.1538 - val_acc: 0.4400\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9550 - acc: 0.7146\n",
      "Epoch 00201: val_loss did not improve from 0.79913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9549 - acc: 0.7146 - val_loss: 0.9760 - val_acc: 0.6983\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9528 - acc: 0.7138\n",
      "Epoch 00202: val_loss did not improve from 0.79913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9529 - acc: 0.7138 - val_loss: 6.8593 - val_acc: 0.2800\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9534 - acc: 0.7157\n",
      "Epoch 00203: val_loss did not improve from 0.79913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9534 - acc: 0.7157 - val_loss: 0.9035 - val_acc: 0.7109\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9452 - acc: 0.7181\n",
      "Epoch 00204: val_loss did not improve from 0.79913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9452 - acc: 0.7181 - val_loss: 1.8844 - val_acc: 0.5092\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9512 - acc: 0.7163\n",
      "Epoch 00205: val_loss did not improve from 0.79913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9514 - acc: 0.7163 - val_loss: 1.0718 - val_acc: 0.6459\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9504 - acc: 0.7188\n",
      "Epoch 00206: val_loss improved from 0.79913 to 0.79879, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_3_conv_checkpoint/206-0.7988.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9507 - acc: 0.7187 - val_loss: 0.7988 - val_acc: 0.7689\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9512 - acc: 0.7130\n",
      "Epoch 00207: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9511 - acc: 0.7130 - val_loss: 2.5529 - val_acc: 0.3890\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9468 - acc: 0.7174\n",
      "Epoch 00208: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9469 - acc: 0.7174 - val_loss: 0.8669 - val_acc: 0.7407\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9490 - acc: 0.7177\n",
      "Epoch 00209: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9489 - acc: 0.7177 - val_loss: 0.8229 - val_acc: 0.7657\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9440 - acc: 0.7172\n",
      "Epoch 00210: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9441 - acc: 0.7172 - val_loss: 0.8043 - val_acc: 0.7624\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9439 - acc: 0.7137\n",
      "Epoch 00211: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9440 - acc: 0.7137 - val_loss: 1.0517 - val_acc: 0.6317\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9454 - acc: 0.7178\n",
      "Epoch 00212: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9453 - acc: 0.7179 - val_loss: 4.5329 - val_acc: 0.1798\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9401 - acc: 0.7177\n",
      "Epoch 00213: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9400 - acc: 0.7178 - val_loss: 0.8590 - val_acc: 0.7412\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9382 - acc: 0.7190\n",
      "Epoch 00214: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9382 - acc: 0.7190 - val_loss: 0.8054 - val_acc: 0.7703\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9439 - acc: 0.7173\n",
      "Epoch 00215: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9442 - acc: 0.7172 - val_loss: 1.0768 - val_acc: 0.6362\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9393 - acc: 0.7194\n",
      "Epoch 00216: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9396 - acc: 0.7194 - val_loss: 3.3692 - val_acc: 0.3028\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9445 - acc: 0.7154\n",
      "Epoch 00217: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9447 - acc: 0.7153 - val_loss: 1.9512 - val_acc: 0.5174\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9399 - acc: 0.7186\n",
      "Epoch 00218: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9399 - acc: 0.7185 - val_loss: 0.9729 - val_acc: 0.6942\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9357 - acc: 0.7198\n",
      "Epoch 00219: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9357 - acc: 0.7197 - val_loss: 1.1469 - val_acc: 0.6320\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9338 - acc: 0.7208\n",
      "Epoch 00220: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9340 - acc: 0.7208 - val_loss: 2.1638 - val_acc: 0.5246\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9367 - acc: 0.7186\n",
      "Epoch 00221: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9368 - acc: 0.7186 - val_loss: 1.3109 - val_acc: 0.6019\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9403 - acc: 0.7200\n",
      "Epoch 00222: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9404 - acc: 0.7200 - val_loss: 6.2381 - val_acc: 0.3475\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9365 - acc: 0.7213\n",
      "Epoch 00223: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9365 - acc: 0.7213 - val_loss: 2.4785 - val_acc: 0.3245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9365 - acc: 0.7232\n",
      "Epoch 00224: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9365 - acc: 0.7232 - val_loss: 2.3391 - val_acc: 0.4286\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9298 - acc: 0.7204\n",
      "Epoch 00225: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9298 - acc: 0.7204 - val_loss: 1.0635 - val_acc: 0.6760\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9325 - acc: 0.7198\n",
      "Epoch 00226: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9327 - acc: 0.7198 - val_loss: 4.3351 - val_acc: 0.2371\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9355 - acc: 0.7218\n",
      "Epoch 00227: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9354 - acc: 0.7218 - val_loss: 0.9254 - val_acc: 0.7086\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9299 - acc: 0.7232\n",
      "Epoch 00228: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9298 - acc: 0.7232 - val_loss: 0.9162 - val_acc: 0.7074\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9374 - acc: 0.7200\n",
      "Epoch 00229: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9373 - acc: 0.7200 - val_loss: 0.8196 - val_acc: 0.7515\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9280 - acc: 0.7212\n",
      "Epoch 00230: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9279 - acc: 0.7213 - val_loss: 2.2840 - val_acc: 0.4465\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9290 - acc: 0.7223\n",
      "Epoch 00231: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9290 - acc: 0.7223 - val_loss: 2.1354 - val_acc: 0.4621\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9251 - acc: 0.7242\n",
      "Epoch 00232: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9251 - acc: 0.7242 - val_loss: 1.3968 - val_acc: 0.5462\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9308 - acc: 0.7222\n",
      "Epoch 00233: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9308 - acc: 0.7222 - val_loss: 2.5875 - val_acc: 0.4111\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9323 - acc: 0.7228\n",
      "Epoch 00234: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9323 - acc: 0.7228 - val_loss: 2.7423 - val_acc: 0.3734\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9284 - acc: 0.7246\n",
      "Epoch 00235: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9286 - acc: 0.7245 - val_loss: 4.8488 - val_acc: 0.2823\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9308 - acc: 0.7228\n",
      "Epoch 00236: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9308 - acc: 0.7228 - val_loss: 0.8101 - val_acc: 0.7664\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9225 - acc: 0.7239\n",
      "Epoch 00237: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9226 - acc: 0.7239 - val_loss: 3.0206 - val_acc: 0.3468\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9214 - acc: 0.7238\n",
      "Epoch 00238: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9215 - acc: 0.7238 - val_loss: 2.4693 - val_acc: 0.5376\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9237 - acc: 0.7246\n",
      "Epoch 00239: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9238 - acc: 0.7246 - val_loss: 1.6211 - val_acc: 0.5274\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9197 - acc: 0.7262\n",
      "Epoch 00240: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9197 - acc: 0.7262 - val_loss: 1.0714 - val_acc: 0.6797\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9252 - acc: 0.7260\n",
      "Epoch 00241: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9252 - acc: 0.7260 - val_loss: 0.8721 - val_acc: 0.7403\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9187 - acc: 0.7261\n",
      "Epoch 00242: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9187 - acc: 0.7261 - val_loss: 1.0971 - val_acc: 0.6431\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9242 - acc: 0.7247\n",
      "Epoch 00243: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9242 - acc: 0.7247 - val_loss: 1.9137 - val_acc: 0.4878\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9248 - acc: 0.7244\n",
      "Epoch 00244: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9247 - acc: 0.7245 - val_loss: 1.8736 - val_acc: 0.4999\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9163 - acc: 0.7281\n",
      "Epoch 00245: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9162 - acc: 0.7282 - val_loss: 5.0001 - val_acc: 0.2176\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9197 - acc: 0.7257\n",
      "Epoch 00246: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9197 - acc: 0.7257 - val_loss: 2.9518 - val_acc: 0.3864\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9121 - acc: 0.7276\n",
      "Epoch 00247: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9121 - acc: 0.7276 - val_loss: 2.7314 - val_acc: 0.3932\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9122 - acc: 0.7261\n",
      "Epoch 00248: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9121 - acc: 0.7262 - val_loss: 0.9698 - val_acc: 0.6671\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9192 - acc: 0.7274\n",
      "Epoch 00249: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9193 - acc: 0.7273 - val_loss: 2.3662 - val_acc: 0.4971\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9152 - acc: 0.7254\n",
      "Epoch 00250: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9153 - acc: 0.7254 - val_loss: 0.8978 - val_acc: 0.7317\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9220 - acc: 0.7260\n",
      "Epoch 00251: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9219 - acc: 0.7260 - val_loss: 0.9471 - val_acc: 0.7074\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9213 - acc: 0.7253\n",
      "Epoch 00252: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9212 - acc: 0.7253 - val_loss: 2.4534 - val_acc: 0.3785\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9146 - acc: 0.7277\n",
      "Epoch 00253: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9148 - acc: 0.7277 - val_loss: 2.6967 - val_acc: 0.4100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9184 - acc: 0.7261\n",
      "Epoch 00254: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9184 - acc: 0.7261 - val_loss: 4.5254 - val_acc: 0.3548\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9173 - acc: 0.7249\n",
      "Epoch 00255: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9175 - acc: 0.7249 - val_loss: 2.4006 - val_acc: 0.3457\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9127 - acc: 0.7255\n",
      "Epoch 00256: val_loss did not improve from 0.79879\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9127 - acc: 0.7254 - val_loss: 1.1704 - val_acc: 0.6238\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 559us/sample - loss: 0.8655 - acc: 0.7367\n",
      "Loss: 0.8654884642901078 Accuracy: 0.7366563\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4361 - acc: 0.2289\n",
      "Epoch 00001: val_loss improved from inf to 2.26315, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/001-2.2631.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 2.4361 - acc: 0.2289 - val_loss: 2.2631 - val_acc: 0.3317\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0735 - acc: 0.3422\n",
      "Epoch 00002: val_loss improved from 2.26315 to 1.81012, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/002-1.8101.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 2.0735 - acc: 0.3422 - val_loss: 1.8101 - val_acc: 0.5297\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8924 - acc: 0.4006\n",
      "Epoch 00003: val_loss improved from 1.81012 to 1.64385, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/003-1.6438.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.8924 - acc: 0.4006 - val_loss: 1.6438 - val_acc: 0.5570\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7638 - acc: 0.4452\n",
      "Epoch 00004: val_loss improved from 1.64385 to 1.51599, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/004-1.5160.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.7639 - acc: 0.4452 - val_loss: 1.5160 - val_acc: 0.5977\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6606 - acc: 0.4773\n",
      "Epoch 00005: val_loss did not improve from 1.51599\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.6607 - acc: 0.4773 - val_loss: 1.5538 - val_acc: 0.5143\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5703 - acc: 0.5110\n",
      "Epoch 00006: val_loss improved from 1.51599 to 1.34374, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/006-1.3437.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.5703 - acc: 0.5109 - val_loss: 1.3437 - val_acc: 0.6618\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5026 - acc: 0.5365\n",
      "Epoch 00007: val_loss improved from 1.34374 to 1.26616, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/007-1.2662.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.5025 - acc: 0.5366 - val_loss: 1.2662 - val_acc: 0.6667\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4455 - acc: 0.5565\n",
      "Epoch 00008: val_loss improved from 1.26616 to 1.26236, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/008-1.2624.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.4455 - acc: 0.5565 - val_loss: 1.2624 - val_acc: 0.6664\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3954 - acc: 0.5729\n",
      "Epoch 00009: val_loss improved from 1.26236 to 1.20265, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/009-1.2026.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.3955 - acc: 0.5729 - val_loss: 1.2026 - val_acc: 0.6606\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3570 - acc: 0.5860\n",
      "Epoch 00010: val_loss improved from 1.20265 to 1.15258, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/010-1.1526.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.3570 - acc: 0.5859 - val_loss: 1.1526 - val_acc: 0.6930\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3135 - acc: 0.5967\n",
      "Epoch 00011: val_loss improved from 1.15258 to 1.13231, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/011-1.1323.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3135 - acc: 0.5967 - val_loss: 1.1323 - val_acc: 0.6792\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2774 - acc: 0.6093\n",
      "Epoch 00012: val_loss did not improve from 1.13231\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2774 - acc: 0.6093 - val_loss: 1.4614 - val_acc: 0.4983\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2493 - acc: 0.6166\n",
      "Epoch 00013: val_loss improved from 1.13231 to 1.11376, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/013-1.1138.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2493 - acc: 0.6166 - val_loss: 1.1138 - val_acc: 0.6620\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2262 - acc: 0.6257\n",
      "Epoch 00014: val_loss did not improve from 1.11376\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2261 - acc: 0.6257 - val_loss: 1.1144 - val_acc: 0.6550\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1996 - acc: 0.6339\n",
      "Epoch 00015: val_loss improved from 1.11376 to 1.03526, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/015-1.0353.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1996 - acc: 0.6339 - val_loss: 1.0353 - val_acc: 0.7070\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1831 - acc: 0.6411\n",
      "Epoch 00016: val_loss improved from 1.03526 to 1.00583, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/016-1.0058.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1831 - acc: 0.6411 - val_loss: 1.0058 - val_acc: 0.7282\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1599 - acc: 0.6473\n",
      "Epoch 00017: val_loss improved from 1.00583 to 0.97539, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/017-0.9754.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1600 - acc: 0.6472 - val_loss: 0.9754 - val_acc: 0.7333\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1409 - acc: 0.6518\n",
      "Epoch 00018: val_loss improved from 0.97539 to 0.93155, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/018-0.9315.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1411 - acc: 0.6518 - val_loss: 0.9315 - val_acc: 0.7477\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1243 - acc: 0.6585\n",
      "Epoch 00019: val_loss improved from 0.93155 to 0.92843, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/019-0.9284.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1243 - acc: 0.6585 - val_loss: 0.9284 - val_acc: 0.7480\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1071 - acc: 0.6630\n",
      "Epoch 00020: val_loss did not improve from 0.92843\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1071 - acc: 0.6630 - val_loss: 0.9816 - val_acc: 0.7014\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1001 - acc: 0.6642\n",
      "Epoch 00021: val_loss improved from 0.92843 to 0.92469, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/021-0.9247.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1000 - acc: 0.6642 - val_loss: 0.9247 - val_acc: 0.7484\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0728 - acc: 0.6736\n",
      "Epoch 00022: val_loss improved from 0.92469 to 0.89565, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/022-0.8956.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0728 - acc: 0.6737 - val_loss: 0.8956 - val_acc: 0.7598\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.6760\n",
      "Epoch 00023: val_loss did not improve from 0.89565\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0663 - acc: 0.6759 - val_loss: 0.9130 - val_acc: 0.7477\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0542 - acc: 0.6805\n",
      "Epoch 00024: val_loss improved from 0.89565 to 0.87672, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/024-0.8767.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0543 - acc: 0.6805 - val_loss: 0.8767 - val_acc: 0.7536\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0414 - acc: 0.6843\n",
      "Epoch 00025: val_loss did not improve from 0.87672\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0414 - acc: 0.6843 - val_loss: 0.9385 - val_acc: 0.7328\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0356 - acc: 0.6878\n",
      "Epoch 00026: val_loss did not improve from 0.87672\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0355 - acc: 0.6878 - val_loss: 0.9553 - val_acc: 0.7039\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0221 - acc: 0.6907\n",
      "Epoch 00027: val_loss did not improve from 0.87672\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0222 - acc: 0.6907 - val_loss: 0.8983 - val_acc: 0.7368\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0113 - acc: 0.6946\n",
      "Epoch 00028: val_loss improved from 0.87672 to 0.79568, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/028-0.7957.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0117 - acc: 0.6946 - val_loss: 0.7957 - val_acc: 0.7920\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0058 - acc: 0.6971\n",
      "Epoch 00029: val_loss did not improve from 0.79568\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0058 - acc: 0.6971 - val_loss: 0.8533 - val_acc: 0.7512\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9927 - acc: 0.7016\n",
      "Epoch 00030: val_loss improved from 0.79568 to 0.78185, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/030-0.7819.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9926 - acc: 0.7016 - val_loss: 0.7819 - val_acc: 0.7848\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9883 - acc: 0.7010\n",
      "Epoch 00031: val_loss did not improve from 0.78185\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9883 - acc: 0.7010 - val_loss: 0.8181 - val_acc: 0.7759\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9785 - acc: 0.7060\n",
      "Epoch 00032: val_loss did not improve from 0.78185\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9785 - acc: 0.7060 - val_loss: 0.8082 - val_acc: 0.7675\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9773 - acc: 0.7055\n",
      "Epoch 00033: val_loss did not improve from 0.78185\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9773 - acc: 0.7055 - val_loss: 0.8305 - val_acc: 0.7580\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9677 - acc: 0.7124\n",
      "Epoch 00034: val_loss did not improve from 0.78185\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9676 - acc: 0.7125 - val_loss: 0.7895 - val_acc: 0.7792\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9582 - acc: 0.7123\n",
      "Epoch 00035: val_loss did not improve from 0.78185\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9582 - acc: 0.7123 - val_loss: 0.8186 - val_acc: 0.7631\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9500 - acc: 0.7124\n",
      "Epoch 00036: val_loss did not improve from 0.78185\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9500 - acc: 0.7124 - val_loss: 0.8097 - val_acc: 0.7633\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9471 - acc: 0.7154\n",
      "Epoch 00037: val_loss improved from 0.78185 to 0.77337, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/037-0.7734.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9470 - acc: 0.7154 - val_loss: 0.7734 - val_acc: 0.7738\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9392 - acc: 0.7167\n",
      "Epoch 00038: val_loss did not improve from 0.77337\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9392 - acc: 0.7167 - val_loss: 1.0271 - val_acc: 0.6692\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9320 - acc: 0.7201\n",
      "Epoch 00039: val_loss improved from 0.77337 to 0.73628, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/039-0.7363.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9319 - acc: 0.7201 - val_loss: 0.7363 - val_acc: 0.8020\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9211 - acc: 0.7248\n",
      "Epoch 00040: val_loss did not improve from 0.73628\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9211 - acc: 0.7248 - val_loss: 0.7390 - val_acc: 0.8004\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9238 - acc: 0.7227\n",
      "Epoch 00041: val_loss did not improve from 0.73628\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9238 - acc: 0.7227 - val_loss: 0.9188 - val_acc: 0.7261\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9185 - acc: 0.7248\n",
      "Epoch 00042: val_loss did not improve from 0.73628\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9185 - acc: 0.7249 - val_loss: 0.7483 - val_acc: 0.7866\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9161 - acc: 0.7257\n",
      "Epoch 00043: val_loss did not improve from 0.73628\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9161 - acc: 0.7257 - val_loss: 0.8455 - val_acc: 0.7398\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9087 - acc: 0.7268\n",
      "Epoch 00044: val_loss did not improve from 0.73628\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9086 - acc: 0.7268 - val_loss: 0.9537 - val_acc: 0.6939\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9024 - acc: 0.7291\n",
      "Epoch 00045: val_loss did not improve from 0.73628\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9024 - acc: 0.7291 - val_loss: 0.8928 - val_acc: 0.7167\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8971 - acc: 0.7320\n",
      "Epoch 00046: val_loss did not improve from 0.73628\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8971 - acc: 0.7319 - val_loss: 0.8349 - val_acc: 0.7498\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8905 - acc: 0.7329\n",
      "Epoch 00047: val_loss improved from 0.73628 to 0.73581, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/047-0.7358.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8905 - acc: 0.7329 - val_loss: 0.7358 - val_acc: 0.8062\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8892 - acc: 0.7360\n",
      "Epoch 00048: val_loss improved from 0.73581 to 0.69420, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/048-0.6942.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8894 - acc: 0.7360 - val_loss: 0.6942 - val_acc: 0.8146\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8815 - acc: 0.7358\n",
      "Epoch 00049: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8815 - acc: 0.7358 - val_loss: 0.7131 - val_acc: 0.8041\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8791 - acc: 0.7357\n",
      "Epoch 00050: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8790 - acc: 0.7358 - val_loss: 0.7236 - val_acc: 0.7941\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8672 - acc: 0.7412\n",
      "Epoch 00051: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8672 - acc: 0.7412 - val_loss: 0.7430 - val_acc: 0.7834\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8724 - acc: 0.7360\n",
      "Epoch 00052: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8723 - acc: 0.7361 - val_loss: 0.9882 - val_acc: 0.6767\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8685 - acc: 0.7411\n",
      "Epoch 00053: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8687 - acc: 0.7411 - val_loss: 0.6960 - val_acc: 0.8062\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8640 - acc: 0.7387\n",
      "Epoch 00054: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8641 - acc: 0.7386 - val_loss: 0.9034 - val_acc: 0.7207\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8504 - acc: 0.7435\n",
      "Epoch 00055: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8504 - acc: 0.7435 - val_loss: 0.7757 - val_acc: 0.7724\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8593 - acc: 0.7443\n",
      "Epoch 00056: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8592 - acc: 0.7443 - val_loss: 0.6967 - val_acc: 0.8102\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8494 - acc: 0.7432\n",
      "Epoch 00057: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8495 - acc: 0.7431 - val_loss: 0.8165 - val_acc: 0.7603\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8513 - acc: 0.7444\n",
      "Epoch 00058: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8513 - acc: 0.7445 - val_loss: 0.7025 - val_acc: 0.8046\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8426 - acc: 0.7455\n",
      "Epoch 00059: val_loss did not improve from 0.69420\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8425 - acc: 0.7456 - val_loss: 0.7780 - val_acc: 0.7752\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8355 - acc: 0.7467\n",
      "Epoch 00060: val_loss improved from 0.69420 to 0.65027, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/060-0.6503.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8355 - acc: 0.7467 - val_loss: 0.6503 - val_acc: 0.8227\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8332 - acc: 0.7473\n",
      "Epoch 00061: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8332 - acc: 0.7473 - val_loss: 0.7139 - val_acc: 0.7957\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8390 - acc: 0.7494\n",
      "Epoch 00062: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8391 - acc: 0.7494 - val_loss: 0.7297 - val_acc: 0.7869\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8273 - acc: 0.7531\n",
      "Epoch 00063: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8272 - acc: 0.7531 - val_loss: 1.0452 - val_acc: 0.6490\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8233 - acc: 0.7531\n",
      "Epoch 00064: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8234 - acc: 0.7530 - val_loss: 0.6530 - val_acc: 0.8190\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8199 - acc: 0.7529\n",
      "Epoch 00065: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8198 - acc: 0.7530 - val_loss: 0.6888 - val_acc: 0.8118\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8196 - acc: 0.7540\n",
      "Epoch 00066: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8196 - acc: 0.7540 - val_loss: 0.7142 - val_acc: 0.8090\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8203 - acc: 0.7550\n",
      "Epoch 00067: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8203 - acc: 0.7550 - val_loss: 0.7342 - val_acc: 0.7927\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8094 - acc: 0.7580\n",
      "Epoch 00068: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8095 - acc: 0.7580 - val_loss: 1.0585 - val_acc: 0.6497\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8110 - acc: 0.7560\n",
      "Epoch 00069: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8109 - acc: 0.7560 - val_loss: 0.6989 - val_acc: 0.7980\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8047 - acc: 0.7611\n",
      "Epoch 00070: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8050 - acc: 0.7610 - val_loss: 2.3578 - val_acc: 0.4272\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8064 - acc: 0.7592\n",
      "Epoch 00071: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8064 - acc: 0.7592 - val_loss: 0.7090 - val_acc: 0.7892\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8027 - acc: 0.7607\n",
      "Epoch 00072: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8028 - acc: 0.7606 - val_loss: 0.6733 - val_acc: 0.8111\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8005 - acc: 0.7600\n",
      "Epoch 00073: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8005 - acc: 0.7600 - val_loss: 0.8236 - val_acc: 0.7466\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7944 - acc: 0.7606\n",
      "Epoch 00074: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7946 - acc: 0.7606 - val_loss: 0.6506 - val_acc: 0.8209\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7941 - acc: 0.7617\n",
      "Epoch 00075: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7940 - acc: 0.7617 - val_loss: 0.6626 - val_acc: 0.8181\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7857 - acc: 0.7638\n",
      "Epoch 00076: val_loss did not improve from 0.65027\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7857 - acc: 0.7638 - val_loss: 0.6686 - val_acc: 0.8148\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7903 - acc: 0.7642\n",
      "Epoch 00077: val_loss improved from 0.65027 to 0.63247, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/077-0.6325.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7904 - acc: 0.7642 - val_loss: 0.6325 - val_acc: 0.8260\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7831 - acc: 0.7653\n",
      "Epoch 00078: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7837 - acc: 0.7652 - val_loss: 0.7245 - val_acc: 0.8004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7876 - acc: 0.7637\n",
      "Epoch 00079: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7876 - acc: 0.7637 - val_loss: 0.8155 - val_acc: 0.7543\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7848 - acc: 0.7640\n",
      "Epoch 00080: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7848 - acc: 0.7640 - val_loss: 0.6350 - val_acc: 0.8321\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7780 - acc: 0.7689\n",
      "Epoch 00081: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7780 - acc: 0.7688 - val_loss: 0.6661 - val_acc: 0.8213\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7748 - acc: 0.7681\n",
      "Epoch 00082: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7750 - acc: 0.7680 - val_loss: 1.0292 - val_acc: 0.6671\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7711 - acc: 0.7699\n",
      "Epoch 00083: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7712 - acc: 0.7698 - val_loss: 0.7343 - val_acc: 0.7983\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7691 - acc: 0.7703\n",
      "Epoch 00084: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7691 - acc: 0.7702 - val_loss: 0.7039 - val_acc: 0.8041\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7714 - acc: 0.7693\n",
      "Epoch 00085: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7714 - acc: 0.7693 - val_loss: 0.6992 - val_acc: 0.7925\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7702 - acc: 0.7686\n",
      "Epoch 00086: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7704 - acc: 0.7685 - val_loss: 0.7042 - val_acc: 0.7962\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7637 - acc: 0.7713\n",
      "Epoch 00087: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7640 - acc: 0.7712 - val_loss: 2.1606 - val_acc: 0.4288\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7610 - acc: 0.7723\n",
      "Epoch 00088: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7610 - acc: 0.7723 - val_loss: 0.7164 - val_acc: 0.7925\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7631 - acc: 0.7716\n",
      "Epoch 00089: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7632 - acc: 0.7716 - val_loss: 0.7481 - val_acc: 0.7754\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7594 - acc: 0.7725\n",
      "Epoch 00090: val_loss did not improve from 0.63247\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7593 - acc: 0.7725 - val_loss: 0.7307 - val_acc: 0.7906\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7566 - acc: 0.7735\n",
      "Epoch 00091: val_loss improved from 0.63247 to 0.60126, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/091-0.6013.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7565 - acc: 0.7736 - val_loss: 0.6013 - val_acc: 0.8416\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7514 - acc: 0.7753\n",
      "Epoch 00092: val_loss did not improve from 0.60126\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7514 - acc: 0.7754 - val_loss: 0.9033 - val_acc: 0.7338\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7498 - acc: 0.7744\n",
      "Epoch 00093: val_loss did not improve from 0.60126\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7499 - acc: 0.7744 - val_loss: 0.6508 - val_acc: 0.8146\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7496 - acc: 0.7743\n",
      "Epoch 00094: val_loss improved from 0.60126 to 0.59894, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/094-0.5989.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7495 - acc: 0.7743 - val_loss: 0.5989 - val_acc: 0.8362\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7482 - acc: 0.7772\n",
      "Epoch 00095: val_loss did not improve from 0.59894\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7481 - acc: 0.7772 - val_loss: 0.6285 - val_acc: 0.8239\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7461 - acc: 0.7772\n",
      "Epoch 00096: val_loss did not improve from 0.59894\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7460 - acc: 0.7772 - val_loss: 0.8475 - val_acc: 0.7412\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7382 - acc: 0.7783\n",
      "Epoch 00097: val_loss did not improve from 0.59894\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7382 - acc: 0.7783 - val_loss: 1.3853 - val_acc: 0.6098\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7426 - acc: 0.7779\n",
      "Epoch 00098: val_loss did not improve from 0.59894\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7427 - acc: 0.7779 - val_loss: 0.6328 - val_acc: 0.8314\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7451 - acc: 0.7794\n",
      "Epoch 00099: val_loss did not improve from 0.59894\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7451 - acc: 0.7794 - val_loss: 0.6043 - val_acc: 0.8367\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7343 - acc: 0.7830\n",
      "Epoch 00100: val_loss did not improve from 0.59894\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7343 - acc: 0.7830 - val_loss: 0.6236 - val_acc: 0.8344\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.7798\n",
      "Epoch 00101: val_loss did not improve from 0.59894\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7360 - acc: 0.7797 - val_loss: 0.6500 - val_acc: 0.8232\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.7798\n",
      "Epoch 00102: val_loss did not improve from 0.59894\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7358 - acc: 0.7798 - val_loss: 0.6490 - val_acc: 0.8148\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7320 - acc: 0.7795\n",
      "Epoch 00103: val_loss improved from 0.59894 to 0.58309, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/103-0.5831.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7320 - acc: 0.7795 - val_loss: 0.5831 - val_acc: 0.8446\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7302 - acc: 0.7805\n",
      "Epoch 00104: val_loss did not improve from 0.58309\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7301 - acc: 0.7805 - val_loss: 0.6382 - val_acc: 0.8155\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7279 - acc: 0.7832\n",
      "Epoch 00105: val_loss did not improve from 0.58309\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7278 - acc: 0.7832 - val_loss: 0.7054 - val_acc: 0.7959\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7230 - acc: 0.7830\n",
      "Epoch 00106: val_loss did not improve from 0.58309\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7230 - acc: 0.7830 - val_loss: 0.6378 - val_acc: 0.8169\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7283 - acc: 0.7832\n",
      "Epoch 00107: val_loss did not improve from 0.58309\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7283 - acc: 0.7832 - val_loss: 0.6238 - val_acc: 0.8286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7250 - acc: 0.7839\n",
      "Epoch 00108: val_loss did not improve from 0.58309\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7251 - acc: 0.7839 - val_loss: 0.6139 - val_acc: 0.8341\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7201 - acc: 0.7857\n",
      "Epoch 00109: val_loss did not improve from 0.58309\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7200 - acc: 0.7857 - val_loss: 0.9108 - val_acc: 0.7216\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7120 - acc: 0.7873\n",
      "Epoch 00110: val_loss improved from 0.58309 to 0.58132, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/110-0.5813.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7121 - acc: 0.7872 - val_loss: 0.5813 - val_acc: 0.8435\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7124 - acc: 0.7894\n",
      "Epoch 00111: val_loss did not improve from 0.58132\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7125 - acc: 0.7894 - val_loss: 1.2787 - val_acc: 0.6329\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7179 - acc: 0.7855\n",
      "Epoch 00112: val_loss did not improve from 0.58132\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7179 - acc: 0.7854 - val_loss: 2.9485 - val_acc: 0.4095\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7144 - acc: 0.7861\n",
      "Epoch 00113: val_loss improved from 0.58132 to 0.56652, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/113-0.5665.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7145 - acc: 0.7861 - val_loss: 0.5665 - val_acc: 0.8493\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7118 - acc: 0.7893\n",
      "Epoch 00114: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7118 - acc: 0.7893 - val_loss: 0.6749 - val_acc: 0.8064\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7048 - acc: 0.7892\n",
      "Epoch 00115: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7051 - acc: 0.7891 - val_loss: 0.6963 - val_acc: 0.8025\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7175 - acc: 0.7872\n",
      "Epoch 00116: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7175 - acc: 0.7872 - val_loss: 0.6045 - val_acc: 0.8318\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7139 - acc: 0.7895\n",
      "Epoch 00117: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7139 - acc: 0.7895 - val_loss: 0.8142 - val_acc: 0.7582\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7014 - acc: 0.7916\n",
      "Epoch 00118: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7015 - acc: 0.7916 - val_loss: 1.8031 - val_acc: 0.5276\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6984 - acc: 0.7880\n",
      "Epoch 00119: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6984 - acc: 0.7880 - val_loss: 0.6193 - val_acc: 0.8283\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7070 - acc: 0.7882\n",
      "Epoch 00120: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7070 - acc: 0.7882 - val_loss: 0.5917 - val_acc: 0.8451\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6974 - acc: 0.7913\n",
      "Epoch 00121: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6973 - acc: 0.7913 - val_loss: 0.6426 - val_acc: 0.8216\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6979 - acc: 0.7907\n",
      "Epoch 00122: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6980 - acc: 0.7906 - val_loss: 0.6720 - val_acc: 0.8143\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6967 - acc: 0.7909\n",
      "Epoch 00123: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6966 - acc: 0.7909 - val_loss: 0.7388 - val_acc: 0.7652\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6980 - acc: 0.7901\n",
      "Epoch 00124: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6980 - acc: 0.7901 - val_loss: 0.6248 - val_acc: 0.8279\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6978 - acc: 0.7915\n",
      "Epoch 00125: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6979 - acc: 0.7914 - val_loss: 0.7072 - val_acc: 0.7955\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6937 - acc: 0.7934\n",
      "Epoch 00126: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6937 - acc: 0.7934 - val_loss: 0.5782 - val_acc: 0.8421\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6971 - acc: 0.7916\n",
      "Epoch 00127: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6970 - acc: 0.7916 - val_loss: 0.8817 - val_acc: 0.7291\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6911 - acc: 0.7930\n",
      "Epoch 00128: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6912 - acc: 0.7930 - val_loss: 1.4382 - val_acc: 0.6010\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.7931\n",
      "Epoch 00129: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6924 - acc: 0.7931 - val_loss: 1.4002 - val_acc: 0.6089\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6874 - acc: 0.7932\n",
      "Epoch 00130: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6875 - acc: 0.7932 - val_loss: 0.7228 - val_acc: 0.7966\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6893 - acc: 0.7920\n",
      "Epoch 00131: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6893 - acc: 0.7919 - val_loss: 1.0230 - val_acc: 0.6720\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6842 - acc: 0.7951\n",
      "Epoch 00132: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6843 - acc: 0.7951 - val_loss: 0.6548 - val_acc: 0.8015\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.7963\n",
      "Epoch 00133: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6807 - acc: 0.7963 - val_loss: 0.8095 - val_acc: 0.7466\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6835 - acc: 0.7971\n",
      "Epoch 00134: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6836 - acc: 0.7971 - val_loss: 0.6608 - val_acc: 0.8102\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6834 - acc: 0.7956\n",
      "Epoch 00135: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6836 - acc: 0.7955 - val_loss: 0.7532 - val_acc: 0.7519\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6812 - acc: 0.7980\n",
      "Epoch 00136: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6812 - acc: 0.7980 - val_loss: 0.5894 - val_acc: 0.8402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6830 - acc: 0.7970\n",
      "Epoch 00137: val_loss did not improve from 0.56652\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6831 - acc: 0.7970 - val_loss: 0.7859 - val_acc: 0.7547\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6793 - acc: 0.7981\n",
      "Epoch 00138: val_loss improved from 0.56652 to 0.55344, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/138-0.5534.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6794 - acc: 0.7980 - val_loss: 0.5534 - val_acc: 0.8579\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6710 - acc: 0.7988\n",
      "Epoch 00139: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6710 - acc: 0.7988 - val_loss: 0.6589 - val_acc: 0.8123\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6782 - acc: 0.7990\n",
      "Epoch 00140: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6782 - acc: 0.7989 - val_loss: 0.5600 - val_acc: 0.8507\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6744 - acc: 0.7985\n",
      "Epoch 00141: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6744 - acc: 0.7985 - val_loss: 3.0645 - val_acc: 0.4512\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6717 - acc: 0.7985\n",
      "Epoch 00142: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6718 - acc: 0.7985 - val_loss: 0.6408 - val_acc: 0.8211\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6795 - acc: 0.7968\n",
      "Epoch 00143: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6794 - acc: 0.7968 - val_loss: 0.5924 - val_acc: 0.8404\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6677 - acc: 0.8000\n",
      "Epoch 00144: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6678 - acc: 0.8000 - val_loss: 0.6125 - val_acc: 0.8321\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6700 - acc: 0.8005\n",
      "Epoch 00145: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6702 - acc: 0.8005 - val_loss: 0.5739 - val_acc: 0.8451\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6697 - acc: 0.7993\n",
      "Epoch 00146: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6698 - acc: 0.7992 - val_loss: 0.5816 - val_acc: 0.8409\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6702 - acc: 0.7997\n",
      "Epoch 00147: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6702 - acc: 0.7998 - val_loss: 0.5665 - val_acc: 0.8479\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6694 - acc: 0.8000\n",
      "Epoch 00148: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6696 - acc: 0.8000 - val_loss: 1.1396 - val_acc: 0.6422\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.8018\n",
      "Epoch 00149: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6660 - acc: 0.8018 - val_loss: 0.6778 - val_acc: 0.7990\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6658 - acc: 0.8029\n",
      "Epoch 00150: val_loss did not improve from 0.55344\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6658 - acc: 0.8029 - val_loss: 0.6492 - val_acc: 0.8148\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6693 - acc: 0.8006\n",
      "Epoch 00151: val_loss improved from 0.55344 to 0.53409, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/151-0.5341.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6693 - acc: 0.8006 - val_loss: 0.5341 - val_acc: 0.8584\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6667 - acc: 0.8015\n",
      "Epoch 00152: val_loss did not improve from 0.53409\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6668 - acc: 0.8014 - val_loss: 0.7306 - val_acc: 0.7883\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6658 - acc: 0.8014\n",
      "Epoch 00153: val_loss did not improve from 0.53409\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6660 - acc: 0.8014 - val_loss: 0.8039 - val_acc: 0.7552\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6627 - acc: 0.8009\n",
      "Epoch 00154: val_loss did not improve from 0.53409\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6626 - acc: 0.8009 - val_loss: 0.6105 - val_acc: 0.8358\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6526 - acc: 0.8065\n",
      "Epoch 00155: val_loss improved from 0.53409 to 0.52271, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/155-0.5227.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6527 - acc: 0.8065 - val_loss: 0.5227 - val_acc: 0.8584\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6589 - acc: 0.8047\n",
      "Epoch 00156: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6590 - acc: 0.8046 - val_loss: 0.5240 - val_acc: 0.8577\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.8046\n",
      "Epoch 00157: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6598 - acc: 0.8046 - val_loss: 0.5472 - val_acc: 0.8521\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6589 - acc: 0.8033\n",
      "Epoch 00158: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6589 - acc: 0.8032 - val_loss: 0.5602 - val_acc: 0.8493\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6593 - acc: 0.8010\n",
      "Epoch 00159: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6592 - acc: 0.8009 - val_loss: 0.8478 - val_acc: 0.7372\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6495 - acc: 0.8076\n",
      "Epoch 00160: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6494 - acc: 0.8076 - val_loss: 0.5476 - val_acc: 0.8509\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6550 - acc: 0.8048\n",
      "Epoch 00161: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6554 - acc: 0.8047 - val_loss: 0.5574 - val_acc: 0.8495\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6581 - acc: 0.8042\n",
      "Epoch 00162: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6581 - acc: 0.8042 - val_loss: 2.3403 - val_acc: 0.4969\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6490 - acc: 0.8073\n",
      "Epoch 00163: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6490 - acc: 0.8073 - val_loss: 0.7171 - val_acc: 0.7594\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6504 - acc: 0.8067\n",
      "Epoch 00164: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6504 - acc: 0.8067 - val_loss: 0.7886 - val_acc: 0.7591\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.8072\n",
      "Epoch 00165: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6490 - acc: 0.8072 - val_loss: 0.5882 - val_acc: 0.8297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6504 - acc: 0.8057\n",
      "Epoch 00166: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6503 - acc: 0.8057 - val_loss: 1.6535 - val_acc: 0.5723\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6503 - acc: 0.8063\n",
      "Epoch 00167: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6503 - acc: 0.8063 - val_loss: 0.8006 - val_acc: 0.7570\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6432 - acc: 0.8081\n",
      "Epoch 00168: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6433 - acc: 0.8081 - val_loss: 5.4309 - val_acc: 0.3261\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6424 - acc: 0.8076\n",
      "Epoch 00169: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6423 - acc: 0.8076 - val_loss: 0.5359 - val_acc: 0.8574\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6455 - acc: 0.8069\n",
      "Epoch 00170: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6455 - acc: 0.8070 - val_loss: 0.5237 - val_acc: 0.8621\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6417 - acc: 0.8082\n",
      "Epoch 00171: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6417 - acc: 0.8083 - val_loss: 0.5759 - val_acc: 0.8362\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6372 - acc: 0.8099\n",
      "Epoch 00172: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6372 - acc: 0.8099 - val_loss: 0.6642 - val_acc: 0.7994\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6445 - acc: 0.8086\n",
      "Epoch 00173: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6446 - acc: 0.8086 - val_loss: 3.1392 - val_acc: 0.4794\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6412 - acc: 0.8078\n",
      "Epoch 00174: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6411 - acc: 0.8078 - val_loss: 0.5330 - val_acc: 0.8560\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6378 - acc: 0.8090\n",
      "Epoch 00175: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6381 - acc: 0.8090 - val_loss: 0.5548 - val_acc: 0.8495\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6421 - acc: 0.8054\n",
      "Epoch 00176: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6420 - acc: 0.8054 - val_loss: 0.5517 - val_acc: 0.8526\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6338 - acc: 0.8107\n",
      "Epoch 00177: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6337 - acc: 0.8107 - val_loss: 0.6834 - val_acc: 0.8116\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6326 - acc: 0.8115\n",
      "Epoch 00178: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6326 - acc: 0.8115 - val_loss: 0.6432 - val_acc: 0.8251\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6329 - acc: 0.8115\n",
      "Epoch 00179: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6329 - acc: 0.8115 - val_loss: 1.0285 - val_acc: 0.6785\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6279 - acc: 0.8111\n",
      "Epoch 00180: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6280 - acc: 0.8111 - val_loss: 1.0737 - val_acc: 0.6944\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6336 - acc: 0.8111\n",
      "Epoch 00181: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6338 - acc: 0.8110 - val_loss: 2.0850 - val_acc: 0.4764\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6360 - acc: 0.8086\n",
      "Epoch 00182: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6361 - acc: 0.8086 - val_loss: 0.6306 - val_acc: 0.8209\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6313 - acc: 0.8121\n",
      "Epoch 00183: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6312 - acc: 0.8122 - val_loss: 0.6629 - val_acc: 0.7906\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6323 - acc: 0.8096\n",
      "Epoch 00184: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6324 - acc: 0.8096 - val_loss: 0.5790 - val_acc: 0.8453\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6287 - acc: 0.8112\n",
      "Epoch 00185: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6290 - acc: 0.8112 - val_loss: 0.6035 - val_acc: 0.8348\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6314 - acc: 0.8109\n",
      "Epoch 00186: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6314 - acc: 0.8109 - val_loss: 1.3996 - val_acc: 0.5970\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6236 - acc: 0.8142\n",
      "Epoch 00187: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6236 - acc: 0.8142 - val_loss: 0.5920 - val_acc: 0.8407\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6271 - acc: 0.8132\n",
      "Epoch 00188: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6271 - acc: 0.8132 - val_loss: 0.6767 - val_acc: 0.8060\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6195 - acc: 0.8165\n",
      "Epoch 00189: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6198 - acc: 0.8164 - val_loss: 0.8139 - val_acc: 0.7652\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6272 - acc: 0.8121\n",
      "Epoch 00190: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6271 - acc: 0.8121 - val_loss: 0.7726 - val_acc: 0.7568\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6212 - acc: 0.8128\n",
      "Epoch 00191: val_loss did not improve from 0.52271\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6216 - acc: 0.8127 - val_loss: 0.7179 - val_acc: 0.7906\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6275 - acc: 0.8118\n",
      "Epoch 00192: val_loss improved from 0.52271 to 0.50648, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/192-0.5065.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6277 - acc: 0.8118 - val_loss: 0.5065 - val_acc: 0.8714\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6206 - acc: 0.8140\n",
      "Epoch 00193: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6205 - acc: 0.8141 - val_loss: 0.5952 - val_acc: 0.8279\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6101 - acc: 0.8157\n",
      "Epoch 00194: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6101 - acc: 0.8157 - val_loss: 0.5234 - val_acc: 0.8591\n",
      "Epoch 195/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6189 - acc: 0.8149\n",
      "Epoch 00195: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6189 - acc: 0.8149 - val_loss: 2.4455 - val_acc: 0.4596\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6215 - acc: 0.8143\n",
      "Epoch 00196: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6215 - acc: 0.8143 - val_loss: 0.5478 - val_acc: 0.8579\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6179 - acc: 0.8183\n",
      "Epoch 00197: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6180 - acc: 0.8183 - val_loss: 1.4832 - val_acc: 0.6222\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6200 - acc: 0.8164\n",
      "Epoch 00198: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6201 - acc: 0.8164 - val_loss: 0.5897 - val_acc: 0.8283\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6148 - acc: 0.8157\n",
      "Epoch 00199: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6147 - acc: 0.8157 - val_loss: 3.5661 - val_acc: 0.4156\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6181 - acc: 0.8139\n",
      "Epoch 00200: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6183 - acc: 0.8139 - val_loss: 0.5294 - val_acc: 0.8563\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6144 - acc: 0.8146\n",
      "Epoch 00201: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6144 - acc: 0.8146 - val_loss: 0.7655 - val_acc: 0.7566\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6136 - acc: 0.8184\n",
      "Epoch 00202: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6136 - acc: 0.8184 - val_loss: 0.6789 - val_acc: 0.8113\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6109 - acc: 0.8183\n",
      "Epoch 00203: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6108 - acc: 0.8184 - val_loss: 0.6598 - val_acc: 0.8043\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6083 - acc: 0.8192\n",
      "Epoch 00204: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6085 - acc: 0.8192 - val_loss: 0.6135 - val_acc: 0.8148\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6151 - acc: 0.8176\n",
      "Epoch 00205: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6151 - acc: 0.8176 - val_loss: 0.5505 - val_acc: 0.8467\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6109 - acc: 0.8174\n",
      "Epoch 00206: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6109 - acc: 0.8174 - val_loss: 0.6203 - val_acc: 0.8227\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6104 - acc: 0.8162\n",
      "Epoch 00207: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6105 - acc: 0.8162 - val_loss: 0.5710 - val_acc: 0.8397\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6069 - acc: 0.8196\n",
      "Epoch 00208: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6069 - acc: 0.8196 - val_loss: 1.4791 - val_acc: 0.5737\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6078 - acc: 0.8195\n",
      "Epoch 00209: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6078 - acc: 0.8195 - val_loss: 0.5942 - val_acc: 0.8321\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6071 - acc: 0.8175\n",
      "Epoch 00210: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6072 - acc: 0.8174 - val_loss: 0.9933 - val_acc: 0.6930\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6053 - acc: 0.8158\n",
      "Epoch 00211: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6052 - acc: 0.8158 - val_loss: 1.0317 - val_acc: 0.6853\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6013 - acc: 0.8191\n",
      "Epoch 00212: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6013 - acc: 0.8190 - val_loss: 1.1838 - val_acc: 0.6501\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6108 - acc: 0.8169\n",
      "Epoch 00213: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6109 - acc: 0.8169 - val_loss: 0.5914 - val_acc: 0.8300\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6074 - acc: 0.8163\n",
      "Epoch 00214: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6073 - acc: 0.8163 - val_loss: 0.8333 - val_acc: 0.7065\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5966 - acc: 0.8200\n",
      "Epoch 00215: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5966 - acc: 0.8200 - val_loss: 0.8693 - val_acc: 0.7342\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5972 - acc: 0.8214\n",
      "Epoch 00216: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5972 - acc: 0.8214 - val_loss: 0.7040 - val_acc: 0.7799\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5977 - acc: 0.8209\n",
      "Epoch 00217: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5977 - acc: 0.8209 - val_loss: 0.5470 - val_acc: 0.8502\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5939 - acc: 0.8233\n",
      "Epoch 00218: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5940 - acc: 0.8233 - val_loss: 0.9690 - val_acc: 0.7237\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5968 - acc: 0.8219\n",
      "Epoch 00219: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5968 - acc: 0.8218 - val_loss: 0.5497 - val_acc: 0.8437\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5965 - acc: 0.8183\n",
      "Epoch 00220: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5965 - acc: 0.8183 - val_loss: 0.5122 - val_acc: 0.8675\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5976 - acc: 0.8210\n",
      "Epoch 00221: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5978 - acc: 0.8209 - val_loss: 0.5581 - val_acc: 0.8472\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5978 - acc: 0.8211\n",
      "Epoch 00222: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5980 - acc: 0.8211 - val_loss: 0.5168 - val_acc: 0.8605\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5938 - acc: 0.8220\n",
      "Epoch 00223: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5938 - acc: 0.8220 - val_loss: 0.5305 - val_acc: 0.8507\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5899 - acc: 0.8227\n",
      "Epoch 00224: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5898 - acc: 0.8227 - val_loss: 0.8299 - val_acc: 0.7421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5912 - acc: 0.8233\n",
      "Epoch 00225: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5913 - acc: 0.8233 - val_loss: 1.4705 - val_acc: 0.6075\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5965 - acc: 0.8212\n",
      "Epoch 00226: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5965 - acc: 0.8211 - val_loss: 0.6317 - val_acc: 0.8195\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5922 - acc: 0.8231\n",
      "Epoch 00227: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5921 - acc: 0.8231 - val_loss: 2.0851 - val_acc: 0.5872\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5876 - acc: 0.8230\n",
      "Epoch 00228: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5876 - acc: 0.8230 - val_loss: 0.6080 - val_acc: 0.8295\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5891 - acc: 0.8225\n",
      "Epoch 00229: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5892 - acc: 0.8225 - val_loss: 0.5169 - val_acc: 0.8574\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5844 - acc: 0.8257\n",
      "Epoch 00230: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5845 - acc: 0.8256 - val_loss: 3.2348 - val_acc: 0.4433\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5895 - acc: 0.8236\n",
      "Epoch 00231: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5895 - acc: 0.8236 - val_loss: 0.5238 - val_acc: 0.8528\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5877 - acc: 0.8226\n",
      "Epoch 00232: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5877 - acc: 0.8226 - val_loss: 0.5694 - val_acc: 0.8498\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5791 - acc: 0.8254\n",
      "Epoch 00233: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5794 - acc: 0.8254 - val_loss: 0.5583 - val_acc: 0.8439\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5825 - acc: 0.8253\n",
      "Epoch 00234: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5825 - acc: 0.8253 - val_loss: 0.7326 - val_acc: 0.7731\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5850 - acc: 0.8253\n",
      "Epoch 00235: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5849 - acc: 0.8253 - val_loss: 1.4185 - val_acc: 0.6080\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5781 - acc: 0.8265\n",
      "Epoch 00236: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5781 - acc: 0.8265 - val_loss: 2.2993 - val_acc: 0.5160\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5840 - acc: 0.8246\n",
      "Epoch 00237: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5841 - acc: 0.8246 - val_loss: 0.5670 - val_acc: 0.8376\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5868 - acc: 0.8242\n",
      "Epoch 00238: val_loss did not improve from 0.50648\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5867 - acc: 0.8243 - val_loss: 0.7115 - val_acc: 0.7659\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5871 - acc: 0.8241\n",
      "Epoch 00239: val_loss improved from 0.50648 to 0.50207, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/239-0.5021.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5872 - acc: 0.8241 - val_loss: 0.5021 - val_acc: 0.8644\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5812 - acc: 0.8254\n",
      "Epoch 00240: val_loss did not improve from 0.50207\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5812 - acc: 0.8254 - val_loss: 0.6359 - val_acc: 0.8060\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5763 - acc: 0.8288\n",
      "Epoch 00241: val_loss did not improve from 0.50207\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5764 - acc: 0.8287 - val_loss: 2.4652 - val_acc: 0.4859\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5794 - acc: 0.8285\n",
      "Epoch 00242: val_loss did not improve from 0.50207\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5794 - acc: 0.8284 - val_loss: 0.5301 - val_acc: 0.8572\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5853 - acc: 0.8246\n",
      "Epoch 00243: val_loss did not improve from 0.50207\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5853 - acc: 0.8246 - val_loss: 2.2537 - val_acc: 0.5094\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.8246\n",
      "Epoch 00244: val_loss did not improve from 0.50207\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5889 - acc: 0.8246 - val_loss: 1.5176 - val_acc: 0.6094\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5761 - acc: 0.8265\n",
      "Epoch 00245: val_loss did not improve from 0.50207\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5761 - acc: 0.8265 - val_loss: 0.5896 - val_acc: 0.8314\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5827 - acc: 0.8278\n",
      "Epoch 00246: val_loss did not improve from 0.50207\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5827 - acc: 0.8278 - val_loss: 0.8560 - val_acc: 0.7160\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5771 - acc: 0.8266\n",
      "Epoch 00247: val_loss did not improve from 0.50207\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5773 - acc: 0.8266 - val_loss: 0.8205 - val_acc: 0.7198\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5856 - acc: 0.8241\n",
      "Epoch 00248: val_loss did not improve from 0.50207\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5856 - acc: 0.8240 - val_loss: 0.5248 - val_acc: 0.8546\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5771 - acc: 0.8269\n",
      "Epoch 00249: val_loss improved from 0.50207 to 0.49791, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/249-0.4979.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5772 - acc: 0.8269 - val_loss: 0.4979 - val_acc: 0.8656\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5689 - acc: 0.8295\n",
      "Epoch 00250: val_loss did not improve from 0.49791\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5690 - acc: 0.8295 - val_loss: 0.5439 - val_acc: 0.8486\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.8260\n",
      "Epoch 00251: val_loss did not improve from 0.49791\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5735 - acc: 0.8260 - val_loss: 2.5078 - val_acc: 0.4621\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5709 - acc: 0.8293\n",
      "Epoch 00252: val_loss improved from 0.49791 to 0.49062, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/252-0.4906.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5710 - acc: 0.8293 - val_loss: 0.4906 - val_acc: 0.8654\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5731 - acc: 0.8264\n",
      "Epoch 00253: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5730 - acc: 0.8265 - val_loss: 0.5691 - val_acc: 0.8458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5703 - acc: 0.8271\n",
      "Epoch 00254: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5702 - acc: 0.8271 - val_loss: 1.3826 - val_acc: 0.6024\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5669 - acc: 0.8304\n",
      "Epoch 00255: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5668 - acc: 0.8304 - val_loss: 2.0919 - val_acc: 0.5099\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5691 - acc: 0.8300\n",
      "Epoch 00256: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5691 - acc: 0.8300 - val_loss: 2.0212 - val_acc: 0.5257\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.8284\n",
      "Epoch 00257: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5734 - acc: 0.8284 - val_loss: 0.6835 - val_acc: 0.7855\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5686 - acc: 0.8296\n",
      "Epoch 00258: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5689 - acc: 0.8296 - val_loss: 0.8168 - val_acc: 0.7519\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.8279\n",
      "Epoch 00259: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5749 - acc: 0.8279 - val_loss: 1.3428 - val_acc: 0.6338\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8290\n",
      "Epoch 00260: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5733 - acc: 0.8290 - val_loss: 0.5013 - val_acc: 0.8654\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5680 - acc: 0.8297\n",
      "Epoch 00261: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5681 - acc: 0.8297 - val_loss: 2.1526 - val_acc: 0.5029\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.8278\n",
      "Epoch 00262: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5702 - acc: 0.8277 - val_loss: 0.6911 - val_acc: 0.7694\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5714 - acc: 0.8310\n",
      "Epoch 00263: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5714 - acc: 0.8310 - val_loss: 6.0291 - val_acc: 0.3480\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.8317\n",
      "Epoch 00264: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5651 - acc: 0.8317 - val_loss: 2.6974 - val_acc: 0.5215\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5687 - acc: 0.8295\n",
      "Epoch 00265: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5687 - acc: 0.8295 - val_loss: 0.5158 - val_acc: 0.8588\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.8293\n",
      "Epoch 00266: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5663 - acc: 0.8292 - val_loss: 0.5322 - val_acc: 0.8600\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5691 - acc: 0.8308\n",
      "Epoch 00267: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5691 - acc: 0.8308 - val_loss: 0.5497 - val_acc: 0.8528\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5612 - acc: 0.8319\n",
      "Epoch 00268: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5612 - acc: 0.8319 - val_loss: 1.0918 - val_acc: 0.7065\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5661 - acc: 0.8279\n",
      "Epoch 00269: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5661 - acc: 0.8278 - val_loss: 0.4922 - val_acc: 0.8684\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.8321\n",
      "Epoch 00270: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5624 - acc: 0.8321 - val_loss: 0.7273 - val_acc: 0.7573\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5634 - acc: 0.8327\n",
      "Epoch 00271: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5633 - acc: 0.8327 - val_loss: 0.4983 - val_acc: 0.8626\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5629 - acc: 0.8292\n",
      "Epoch 00272: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5629 - acc: 0.8292 - val_loss: 0.5435 - val_acc: 0.8460\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.8325\n",
      "Epoch 00273: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5617 - acc: 0.8325 - val_loss: 0.8356 - val_acc: 0.7333\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5649 - acc: 0.8308\n",
      "Epoch 00274: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5649 - acc: 0.8308 - val_loss: 0.8126 - val_acc: 0.7291\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5645 - acc: 0.8318\n",
      "Epoch 00275: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5646 - acc: 0.8317 - val_loss: 0.6156 - val_acc: 0.8239\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5632 - acc: 0.8325\n",
      "Epoch 00276: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5631 - acc: 0.8325 - val_loss: 0.5061 - val_acc: 0.8588\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.8327\n",
      "Epoch 00277: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5555 - acc: 0.8327 - val_loss: 0.7477 - val_acc: 0.7948\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.8337\n",
      "Epoch 00278: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5547 - acc: 0.8336 - val_loss: 0.5825 - val_acc: 0.8383\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5587 - acc: 0.8323\n",
      "Epoch 00279: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5591 - acc: 0.8323 - val_loss: 0.6218 - val_acc: 0.8043\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5660 - acc: 0.8295\n",
      "Epoch 00280: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5660 - acc: 0.8295 - val_loss: 0.7902 - val_acc: 0.7608\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5560 - acc: 0.8336\n",
      "Epoch 00281: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5561 - acc: 0.8336 - val_loss: 0.8714 - val_acc: 0.7191\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.8343\n",
      "Epoch 00282: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5496 - acc: 0.8343 - val_loss: 0.5313 - val_acc: 0.8539\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.8313\n",
      "Epoch 00283: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5536 - acc: 0.8313 - val_loss: 0.7526 - val_acc: 0.7794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.8314\n",
      "Epoch 00284: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5563 - acc: 0.8314 - val_loss: 0.5038 - val_acc: 0.8668\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5574 - acc: 0.8338\n",
      "Epoch 00285: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5574 - acc: 0.8338 - val_loss: 0.6877 - val_acc: 0.7957\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.8329\n",
      "Epoch 00286: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5534 - acc: 0.8329 - val_loss: 0.4944 - val_acc: 0.8700\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.8332\n",
      "Epoch 00287: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5557 - acc: 0.8331 - val_loss: 2.8731 - val_acc: 0.5017\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.8349\n",
      "Epoch 00288: val_loss did not improve from 0.49062\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5493 - acc: 0.8349 - val_loss: 1.1188 - val_acc: 0.6837\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.8352\n",
      "Epoch 00289: val_loss improved from 0.49062 to 0.47692, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/289-0.4769.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5463 - acc: 0.8352 - val_loss: 0.4769 - val_acc: 0.8721\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.8360\n",
      "Epoch 00290: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5467 - acc: 0.8359 - val_loss: 0.5716 - val_acc: 0.8393\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.8339\n",
      "Epoch 00291: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5503 - acc: 0.8339 - val_loss: 1.0938 - val_acc: 0.7119\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.8341\n",
      "Epoch 00292: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5520 - acc: 0.8341 - val_loss: 2.6787 - val_acc: 0.4708\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5475 - acc: 0.8355\n",
      "Epoch 00293: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5475 - acc: 0.8355 - val_loss: 0.6818 - val_acc: 0.7976\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.8350\n",
      "Epoch 00294: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5467 - acc: 0.8350 - val_loss: 0.6215 - val_acc: 0.8050\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5471 - acc: 0.8358\n",
      "Epoch 00295: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5471 - acc: 0.8358 - val_loss: 2.7378 - val_acc: 0.4575\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.8369\n",
      "Epoch 00296: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5456 - acc: 0.8369 - val_loss: 0.4924 - val_acc: 0.8682\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5489 - acc: 0.8342\n",
      "Epoch 00297: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5488 - acc: 0.8343 - val_loss: 0.9743 - val_acc: 0.6806\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8367\n",
      "Epoch 00298: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5448 - acc: 0.8367 - val_loss: 0.9433 - val_acc: 0.6706\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.8361\n",
      "Epoch 00299: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5440 - acc: 0.8361 - val_loss: 3.0240 - val_acc: 0.4561\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5542 - acc: 0.8320\n",
      "Epoch 00300: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5542 - acc: 0.8320 - val_loss: 0.5044 - val_acc: 0.8658\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.8386\n",
      "Epoch 00301: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5421 - acc: 0.8386 - val_loss: 0.5582 - val_acc: 0.8430\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.8371\n",
      "Epoch 00302: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5423 - acc: 0.8370 - val_loss: 0.5268 - val_acc: 0.8542\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.8382\n",
      "Epoch 00303: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5449 - acc: 0.8382 - val_loss: 1.0061 - val_acc: 0.7188\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.8362\n",
      "Epoch 00304: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5438 - acc: 0.8362 - val_loss: 1.1255 - val_acc: 0.6753\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.8357\n",
      "Epoch 00305: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5483 - acc: 0.8358 - val_loss: 0.5267 - val_acc: 0.8574\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.8360\n",
      "Epoch 00306: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5410 - acc: 0.8360 - val_loss: 0.9424 - val_acc: 0.7333\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.8371\n",
      "Epoch 00307: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5449 - acc: 0.8371 - val_loss: 4.4260 - val_acc: 0.4137\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.8361\n",
      "Epoch 00308: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5392 - acc: 0.8360 - val_loss: 0.5220 - val_acc: 0.8556\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.8392\n",
      "Epoch 00309: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5383 - acc: 0.8392 - val_loss: 1.0030 - val_acc: 0.6846\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.8405\n",
      "Epoch 00310: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5351 - acc: 0.8406 - val_loss: 0.5320 - val_acc: 0.8500\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.8384\n",
      "Epoch 00311: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5396 - acc: 0.8384 - val_loss: 3.7234 - val_acc: 0.3678\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5432 - acc: 0.8366\n",
      "Epoch 00312: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5434 - acc: 0.8365 - val_loss: 0.5569 - val_acc: 0.8416\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.8365\n",
      "Epoch 00313: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5378 - acc: 0.8365 - val_loss: 0.4787 - val_acc: 0.8737\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.8374\n",
      "Epoch 00314: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5416 - acc: 0.8374 - val_loss: 0.5427 - val_acc: 0.8612\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.8381\n",
      "Epoch 00315: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5355 - acc: 0.8381 - val_loss: 2.7656 - val_acc: 0.4857\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.8407\n",
      "Epoch 00316: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5314 - acc: 0.8407 - val_loss: 3.0899 - val_acc: 0.4971\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.8363\n",
      "Epoch 00317: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5410 - acc: 0.8363 - val_loss: 3.2312 - val_acc: 0.4463\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.8390\n",
      "Epoch 00318: val_loss did not improve from 0.47692\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5394 - acc: 0.8390 - val_loss: 4.1338 - val_acc: 0.4137\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.8390\n",
      "Epoch 00319: val_loss improved from 0.47692 to 0.46854, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_4_conv_checkpoint/319-0.4685.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5374 - acc: 0.8390 - val_loss: 0.4685 - val_acc: 0.8770\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.8421\n",
      "Epoch 00320: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5301 - acc: 0.8421 - val_loss: 1.3050 - val_acc: 0.6548\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.8383\n",
      "Epoch 00321: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5356 - acc: 0.8383 - val_loss: 0.5750 - val_acc: 0.8372\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.8391\n",
      "Epoch 00322: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5317 - acc: 0.8392 - val_loss: 0.6254 - val_acc: 0.8092\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.8411\n",
      "Epoch 00323: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5284 - acc: 0.8411 - val_loss: 1.4837 - val_acc: 0.6462\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.8376\n",
      "Epoch 00324: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5356 - acc: 0.8375 - val_loss: 0.5857 - val_acc: 0.8253\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.8410\n",
      "Epoch 00325: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5283 - acc: 0.8410 - val_loss: 0.5468 - val_acc: 0.8495\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.8401\n",
      "Epoch 00326: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5296 - acc: 0.8400 - val_loss: 0.5041 - val_acc: 0.8581\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.8412\n",
      "Epoch 00327: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5299 - acc: 0.8411 - val_loss: 0.5877 - val_acc: 0.8220\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.8412\n",
      "Epoch 00328: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5310 - acc: 0.8412 - val_loss: 1.4310 - val_acc: 0.6280\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.8384\n",
      "Epoch 00329: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5295 - acc: 0.8384 - val_loss: 0.5371 - val_acc: 0.8598\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.8424\n",
      "Epoch 00330: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5301 - acc: 0.8424 - val_loss: 0.7311 - val_acc: 0.7876\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.8415\n",
      "Epoch 00331: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5275 - acc: 0.8415 - val_loss: 0.8759 - val_acc: 0.7356\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.8401\n",
      "Epoch 00332: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5306 - acc: 0.8401 - val_loss: 7.5899 - val_acc: 0.3156\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.8439\n",
      "Epoch 00333: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5209 - acc: 0.8439 - val_loss: 1.1195 - val_acc: 0.6634\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5225 - acc: 0.8414\n",
      "Epoch 00334: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5225 - acc: 0.8414 - val_loss: 0.5006 - val_acc: 0.8649\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5300 - acc: 0.8392\n",
      "Epoch 00335: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5300 - acc: 0.8392 - val_loss: 0.8265 - val_acc: 0.7608\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8426\n",
      "Epoch 00336: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5202 - acc: 0.8426 - val_loss: 0.8193 - val_acc: 0.7559\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.8430\n",
      "Epoch 00337: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5222 - acc: 0.8429 - val_loss: 0.5611 - val_acc: 0.8449\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5205 - acc: 0.8431\n",
      "Epoch 00338: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5205 - acc: 0.8431 - val_loss: 0.5523 - val_acc: 0.8453\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.8416\n",
      "Epoch 00339: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5223 - acc: 0.8416 - val_loss: 0.5857 - val_acc: 0.8358\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.8438\n",
      "Epoch 00340: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5201 - acc: 0.8437 - val_loss: 6.7711 - val_acc: 0.3555\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.8402\n",
      "Epoch 00341: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5252 - acc: 0.8402 - val_loss: 0.4770 - val_acc: 0.8712\n",
      "Epoch 342/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.8421\n",
      "Epoch 00342: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5234 - acc: 0.8421 - val_loss: 2.0162 - val_acc: 0.5302\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.8421\n",
      "Epoch 00343: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5242 - acc: 0.8421 - val_loss: 0.5782 - val_acc: 0.8337\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.8456\n",
      "Epoch 00344: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5180 - acc: 0.8456 - val_loss: 0.9319 - val_acc: 0.7032\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5158 - acc: 0.8451\n",
      "Epoch 00345: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5158 - acc: 0.8450 - val_loss: 1.0278 - val_acc: 0.7230\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.8414\n",
      "Epoch 00346: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5229 - acc: 0.8414 - val_loss: 1.1529 - val_acc: 0.7035\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.8458\n",
      "Epoch 00347: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5181 - acc: 0.8457 - val_loss: 1.2753 - val_acc: 0.6292\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.8435\n",
      "Epoch 00348: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5237 - acc: 0.8435 - val_loss: 1.2605 - val_acc: 0.6177\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.8438\n",
      "Epoch 00349: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5191 - acc: 0.8437 - val_loss: 0.7454 - val_acc: 0.7678\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.8456\n",
      "Epoch 00350: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5156 - acc: 0.8456 - val_loss: 0.5843 - val_acc: 0.8232\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.8454\n",
      "Epoch 00351: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5141 - acc: 0.8453 - val_loss: 7.1406 - val_acc: 0.3499\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.8452\n",
      "Epoch 00352: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5151 - acc: 0.8452 - val_loss: 2.1153 - val_acc: 0.5013\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.8453\n",
      "Epoch 00353: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5158 - acc: 0.8453 - val_loss: 0.5251 - val_acc: 0.8560\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.8446\n",
      "Epoch 00354: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5144 - acc: 0.8446 - val_loss: 1.4088 - val_acc: 0.6343\n",
      "Epoch 355/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.8451\n",
      "Epoch 00355: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5149 - acc: 0.8450 - val_loss: 0.8461 - val_acc: 0.7289\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.8463\n",
      "Epoch 00356: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5157 - acc: 0.8462 - val_loss: 0.5470 - val_acc: 0.8609\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5087 - acc: 0.8458\n",
      "Epoch 00357: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5087 - acc: 0.8458 - val_loss: 0.5559 - val_acc: 0.8395\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5128 - acc: 0.8441\n",
      "Epoch 00358: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5127 - acc: 0.8442 - val_loss: 0.5120 - val_acc: 0.8581\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5135 - acc: 0.8452\n",
      "Epoch 00359: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5135 - acc: 0.8453 - val_loss: 0.6501 - val_acc: 0.8067\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.8457\n",
      "Epoch 00360: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5129 - acc: 0.8457 - val_loss: 1.1407 - val_acc: 0.6725\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.8485\n",
      "Epoch 00361: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5045 - acc: 0.8485 - val_loss: 4.2906 - val_acc: 0.3923\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.8480\n",
      "Epoch 00362: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5052 - acc: 0.8480 - val_loss: 0.6636 - val_acc: 0.7976\n",
      "Epoch 363/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5093 - acc: 0.8455\n",
      "Epoch 00363: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5096 - acc: 0.8455 - val_loss: 0.5051 - val_acc: 0.8605\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5178 - acc: 0.8432\n",
      "Epoch 00364: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5179 - acc: 0.8431 - val_loss: 2.2023 - val_acc: 0.5788\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.8458\n",
      "Epoch 00365: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5089 - acc: 0.8458 - val_loss: 0.6872 - val_acc: 0.7824\n",
      "Epoch 366/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.8464\n",
      "Epoch 00366: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5107 - acc: 0.8465 - val_loss: 0.5408 - val_acc: 0.8474\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8481\n",
      "Epoch 00367: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5086 - acc: 0.8481 - val_loss: 1.6827 - val_acc: 0.6154\n",
      "Epoch 368/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5091 - acc: 0.8477\n",
      "Epoch 00368: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5091 - acc: 0.8477 - val_loss: 0.5980 - val_acc: 0.8244\n",
      "Epoch 369/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5085 - acc: 0.8459\n",
      "Epoch 00369: val_loss did not improve from 0.46854\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5085 - acc: 0.8459 - val_loss: 0.6485 - val_acc: 0.8067\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 598us/sample - loss: 0.5369 - acc: 0.8451\n",
      "Loss: 0.536925614486726 Accuracy: 0.8450675\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2068 - acc: 0.2992\n",
      "Epoch 00001: val_loss improved from inf to 1.96978, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/001-1.9698.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 2.2068 - acc: 0.2992 - val_loss: 1.9698 - val_acc: 0.4160\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7189 - acc: 0.4602\n",
      "Epoch 00002: val_loss improved from 1.96978 to 1.39162, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/002-1.3916.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.7189 - acc: 0.4602 - val_loss: 1.3916 - val_acc: 0.6271\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4992 - acc: 0.5350\n",
      "Epoch 00003: val_loss improved from 1.39162 to 1.24696, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/003-1.2470.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.4992 - acc: 0.5350 - val_loss: 1.2470 - val_acc: 0.6608\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3524 - acc: 0.5873\n",
      "Epoch 00004: val_loss improved from 1.24696 to 1.16151, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/004-1.1615.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.3524 - acc: 0.5873 - val_loss: 1.1615 - val_acc: 0.6676\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2481 - acc: 0.6215\n",
      "Epoch 00005: val_loss improved from 1.16151 to 1.02755, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/005-1.0276.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.2481 - acc: 0.6215 - val_loss: 1.0276 - val_acc: 0.7352\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1616 - acc: 0.6532\n",
      "Epoch 00006: val_loss improved from 1.02755 to 0.97529, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/006-0.9753.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.1615 - acc: 0.6532 - val_loss: 0.9753 - val_acc: 0.7233\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1009 - acc: 0.6730\n",
      "Epoch 00007: val_loss did not improve from 0.97529\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.1010 - acc: 0.6730 - val_loss: 1.2813 - val_acc: 0.5726\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0453 - acc: 0.6895\n",
      "Epoch 00008: val_loss improved from 0.97529 to 0.93454, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/008-0.9345.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.0453 - acc: 0.6895 - val_loss: 0.9345 - val_acc: 0.7240\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9968 - acc: 0.7046\n",
      "Epoch 00009: val_loss improved from 0.93454 to 0.79840, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/009-0.7984.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.9969 - acc: 0.7046 - val_loss: 0.7984 - val_acc: 0.7980\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9561 - acc: 0.7165\n",
      "Epoch 00010: val_loss improved from 0.79840 to 0.78615, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/010-0.7861.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.9562 - acc: 0.7165 - val_loss: 0.7861 - val_acc: 0.7887\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9250 - acc: 0.7274\n",
      "Epoch 00011: val_loss did not improve from 0.78615\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.9249 - acc: 0.7275 - val_loss: 1.0126 - val_acc: 0.6781\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8922 - acc: 0.7370\n",
      "Epoch 00012: val_loss improved from 0.78615 to 0.70718, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/012-0.7072.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8922 - acc: 0.7370 - val_loss: 0.7072 - val_acc: 0.8209\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8720 - acc: 0.7431\n",
      "Epoch 00013: val_loss did not improve from 0.70718\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8721 - acc: 0.7431 - val_loss: 0.7796 - val_acc: 0.7685\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8468 - acc: 0.7520\n",
      "Epoch 00014: val_loss improved from 0.70718 to 0.68210, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/014-0.6821.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8469 - acc: 0.7520 - val_loss: 0.6821 - val_acc: 0.8286\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8290 - acc: 0.7583\n",
      "Epoch 00015: val_loss did not improve from 0.68210\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8291 - acc: 0.7583 - val_loss: 0.7326 - val_acc: 0.8041\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8054 - acc: 0.7668\n",
      "Epoch 00016: val_loss did not improve from 0.68210\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8054 - acc: 0.7669 - val_loss: 0.7577 - val_acc: 0.7757\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7816 - acc: 0.7720\n",
      "Epoch 00017: val_loss improved from 0.68210 to 0.63109, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/017-0.6311.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7816 - acc: 0.7720 - val_loss: 0.6311 - val_acc: 0.8330\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7650 - acc: 0.7755\n",
      "Epoch 00018: val_loss did not improve from 0.63109\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7649 - acc: 0.7756 - val_loss: 1.0219 - val_acc: 0.6713\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7522 - acc: 0.7821\n",
      "Epoch 00019: val_loss improved from 0.63109 to 0.61209, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/019-0.6121.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7521 - acc: 0.7821 - val_loss: 0.6121 - val_acc: 0.8481\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7395 - acc: 0.7858\n",
      "Epoch 00020: val_loss did not improve from 0.61209\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7395 - acc: 0.7858 - val_loss: 1.0135 - val_acc: 0.6669\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7295 - acc: 0.7868\n",
      "Epoch 00021: val_loss did not improve from 0.61209\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7295 - acc: 0.7868 - val_loss: 0.8697 - val_acc: 0.7167\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7147 - acc: 0.7936\n",
      "Epoch 00022: val_loss did not improve from 0.61209\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7148 - acc: 0.7936 - val_loss: 0.8179 - val_acc: 0.7454\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7009 - acc: 0.7960\n",
      "Epoch 00023: val_loss did not improve from 0.61209\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7008 - acc: 0.7960 - val_loss: 0.8588 - val_acc: 0.7289\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6886 - acc: 0.7996\n",
      "Epoch 00024: val_loss did not improve from 0.61209\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6886 - acc: 0.7996 - val_loss: 0.6642 - val_acc: 0.8192\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6793 - acc: 0.8034\n",
      "Epoch 00025: val_loss improved from 0.61209 to 0.55747, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/025-0.5575.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6795 - acc: 0.8034 - val_loss: 0.5575 - val_acc: 0.8553\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6668 - acc: 0.8064\n",
      "Epoch 00026: val_loss did not improve from 0.55747\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6669 - acc: 0.8064 - val_loss: 0.5836 - val_acc: 0.8479\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6564 - acc: 0.8090\n",
      "Epoch 00027: val_loss did not improve from 0.55747\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6564 - acc: 0.8090 - val_loss: 0.6725 - val_acc: 0.7992\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6483 - acc: 0.8131\n",
      "Epoch 00028: val_loss did not improve from 0.55747\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6483 - acc: 0.8131 - val_loss: 1.0277 - val_acc: 0.6848\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6351 - acc: 0.8174\n",
      "Epoch 00029: val_loss improved from 0.55747 to 0.53774, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/029-0.5377.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6352 - acc: 0.8173 - val_loss: 0.5377 - val_acc: 0.8677\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6337 - acc: 0.8155\n",
      "Epoch 00030: val_loss did not improve from 0.53774\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6337 - acc: 0.8155 - val_loss: 1.0018 - val_acc: 0.7002\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6189 - acc: 0.8192\n",
      "Epoch 00031: val_loss improved from 0.53774 to 0.49625, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/031-0.4963.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6189 - acc: 0.8192 - val_loss: 0.4963 - val_acc: 0.8665\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6172 - acc: 0.8203\n",
      "Epoch 00032: val_loss did not improve from 0.49625\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6172 - acc: 0.8203 - val_loss: 0.5552 - val_acc: 0.8477\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5995 - acc: 0.8250\n",
      "Epoch 00033: val_loss did not improve from 0.49625\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5997 - acc: 0.8250 - val_loss: 0.5817 - val_acc: 0.8362\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5984 - acc: 0.8257\n",
      "Epoch 00034: val_loss did not improve from 0.49625\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5985 - acc: 0.8257 - val_loss: 0.5480 - val_acc: 0.8491\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5865 - acc: 0.8287\n",
      "Epoch 00035: val_loss improved from 0.49625 to 0.49238, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/035-0.4924.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5869 - acc: 0.8287 - val_loss: 0.4924 - val_acc: 0.8663\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5810 - acc: 0.8321\n",
      "Epoch 00036: val_loss did not improve from 0.49238\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5811 - acc: 0.8320 - val_loss: 0.5095 - val_acc: 0.8654\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5798 - acc: 0.8322\n",
      "Epoch 00037: val_loss did not improve from 0.49238\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5798 - acc: 0.8322 - val_loss: 0.4937 - val_acc: 0.8710\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5634 - acc: 0.8361\n",
      "Epoch 00038: val_loss did not improve from 0.49238\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5634 - acc: 0.8362 - val_loss: 0.6121 - val_acc: 0.8162\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.8388\n",
      "Epoch 00039: val_loss did not improve from 0.49238\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5601 - acc: 0.8388 - val_loss: 0.5046 - val_acc: 0.8710\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.8407\n",
      "Epoch 00040: val_loss improved from 0.49238 to 0.48143, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/040-0.4814.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5550 - acc: 0.8408 - val_loss: 0.4814 - val_acc: 0.8665\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5447 - acc: 0.8401\n",
      "Epoch 00041: val_loss improved from 0.48143 to 0.46335, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/041-0.4634.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5447 - acc: 0.8400 - val_loss: 0.4634 - val_acc: 0.8833\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5433 - acc: 0.8424\n",
      "Epoch 00042: val_loss did not improve from 0.46335\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5434 - acc: 0.8423 - val_loss: 0.5747 - val_acc: 0.8314\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.8426\n",
      "Epoch 00043: val_loss did not improve from 0.46335\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5404 - acc: 0.8426 - val_loss: 0.5616 - val_acc: 0.8449\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.8474\n",
      "Epoch 00044: val_loss improved from 0.46335 to 0.43679, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/044-0.4368.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5271 - acc: 0.8474 - val_loss: 0.4368 - val_acc: 0.8866\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.8480\n",
      "Epoch 00045: val_loss did not improve from 0.43679\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5219 - acc: 0.8480 - val_loss: 0.4914 - val_acc: 0.8567\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.8469\n",
      "Epoch 00046: val_loss did not improve from 0.43679\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5179 - acc: 0.8469 - val_loss: 0.4604 - val_acc: 0.8761\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.8508\n",
      "Epoch 00047: val_loss did not improve from 0.43679\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5099 - acc: 0.8507 - val_loss: 0.4737 - val_acc: 0.8710\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5119 - acc: 0.8516\n",
      "Epoch 00048: val_loss did not improve from 0.43679\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5120 - acc: 0.8515 - val_loss: 0.5553 - val_acc: 0.8479\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.8522\n",
      "Epoch 00049: val_loss did not improve from 0.43679\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5036 - acc: 0.8521 - val_loss: 0.4554 - val_acc: 0.8775\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8568\n",
      "Epoch 00050: val_loss did not improve from 0.43679\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4909 - acc: 0.8568 - val_loss: 1.9708 - val_acc: 0.5821\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4904 - acc: 0.8586\n",
      "Epoch 00051: val_loss did not improve from 0.43679\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4904 - acc: 0.8586 - val_loss: 0.6082 - val_acc: 0.8143\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4827 - acc: 0.8598\n",
      "Epoch 00052: val_loss improved from 0.43679 to 0.40915, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/052-0.4091.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4827 - acc: 0.8598 - val_loss: 0.4091 - val_acc: 0.8915\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8605\n",
      "Epoch 00053: val_loss improved from 0.40915 to 0.39432, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/053-0.3943.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4805 - acc: 0.8605 - val_loss: 0.3943 - val_acc: 0.8959\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4764 - acc: 0.8621\n",
      "Epoch 00054: val_loss did not improve from 0.39432\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4764 - acc: 0.8621 - val_loss: 0.4023 - val_acc: 0.8917\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4671 - acc: 0.8649\n",
      "Epoch 00055: val_loss did not improve from 0.39432\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4671 - acc: 0.8649 - val_loss: 0.4038 - val_acc: 0.8956\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4683 - acc: 0.8644\n",
      "Epoch 00056: val_loss did not improve from 0.39432\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4685 - acc: 0.8644 - val_loss: 1.1670 - val_acc: 0.6685\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8659\n",
      "Epoch 00057: val_loss did not improve from 0.39432\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4617 - acc: 0.8659 - val_loss: 0.8955 - val_acc: 0.7312\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8680\n",
      "Epoch 00058: val_loss did not improve from 0.39432\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4571 - acc: 0.8681 - val_loss: 0.3970 - val_acc: 0.9001\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8704\n",
      "Epoch 00059: val_loss improved from 0.39432 to 0.38642, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/059-0.3864.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4496 - acc: 0.8705 - val_loss: 0.3864 - val_acc: 0.9008\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4465 - acc: 0.8703\n",
      "Epoch 00060: val_loss did not improve from 0.38642\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4465 - acc: 0.8703 - val_loss: 0.5650 - val_acc: 0.8339\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8702\n",
      "Epoch 00061: val_loss did not improve from 0.38642\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4476 - acc: 0.8701 - val_loss: 0.4480 - val_acc: 0.8800\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4452 - acc: 0.8708\n",
      "Epoch 00062: val_loss did not improve from 0.38642\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4452 - acc: 0.8708 - val_loss: 0.5519 - val_acc: 0.8423\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4294 - acc: 0.8740\n",
      "Epoch 00063: val_loss did not improve from 0.38642\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4294 - acc: 0.8740 - val_loss: 0.4463 - val_acc: 0.8845\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.8748\n",
      "Epoch 00064: val_loss did not improve from 0.38642\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4312 - acc: 0.8747 - val_loss: 0.7479 - val_acc: 0.7682\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.8758\n",
      "Epoch 00065: val_loss did not improve from 0.38642\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4294 - acc: 0.8758 - val_loss: 0.4082 - val_acc: 0.8933\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8767\n",
      "Epoch 00066: val_loss did not improve from 0.38642\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4223 - acc: 0.8767 - val_loss: 0.4852 - val_acc: 0.8751\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4206 - acc: 0.8771\n",
      "Epoch 00067: val_loss did not improve from 0.38642\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4208 - acc: 0.8771 - val_loss: 0.4709 - val_acc: 0.8737\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4167 - acc: 0.8786\n",
      "Epoch 00068: val_loss did not improve from 0.38642\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4167 - acc: 0.8785 - val_loss: 0.4003 - val_acc: 0.8924\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8792\n",
      "Epoch 00069: val_loss improved from 0.38642 to 0.37235, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/069-0.3723.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4126 - acc: 0.8792 - val_loss: 0.3723 - val_acc: 0.9047\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8815\n",
      "Epoch 00070: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4071 - acc: 0.8814 - val_loss: 0.5634 - val_acc: 0.8388\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8814\n",
      "Epoch 00071: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4083 - acc: 0.8814 - val_loss: 0.5732 - val_acc: 0.8453\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.8812\n",
      "Epoch 00072: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4059 - acc: 0.8812 - val_loss: 0.3756 - val_acc: 0.8975\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8834\n",
      "Epoch 00073: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3981 - acc: 0.8834 - val_loss: 0.9568 - val_acc: 0.7512\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8863\n",
      "Epoch 00074: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3935 - acc: 0.8863 - val_loss: 0.4534 - val_acc: 0.8728\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8832\n",
      "Epoch 00075: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3942 - acc: 0.8832 - val_loss: 0.3839 - val_acc: 0.8975\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3880 - acc: 0.8858\n",
      "Epoch 00076: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3880 - acc: 0.8857 - val_loss: 0.4406 - val_acc: 0.8824\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.8861\n",
      "Epoch 00077: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3901 - acc: 0.8860 - val_loss: 0.4090 - val_acc: 0.8870\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.8860\n",
      "Epoch 00078: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3882 - acc: 0.8860 - val_loss: 0.4092 - val_acc: 0.8933\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8867\n",
      "Epoch 00079: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3864 - acc: 0.8867 - val_loss: 0.3907 - val_acc: 0.8980\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3787 - acc: 0.8896\n",
      "Epoch 00080: val_loss did not improve from 0.37235\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3788 - acc: 0.8895 - val_loss: 0.3820 - val_acc: 0.8994\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8929\n",
      "Epoch 00081: val_loss improved from 0.37235 to 0.37118, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/081-0.3712.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3697 - acc: 0.8928 - val_loss: 0.3712 - val_acc: 0.9024\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8902\n",
      "Epoch 00082: val_loss improved from 0.37118 to 0.36008, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/082-0.3601.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3727 - acc: 0.8902 - val_loss: 0.3601 - val_acc: 0.9036\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8906\n",
      "Epoch 00083: val_loss improved from 0.36008 to 0.35348, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/083-0.3535.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3749 - acc: 0.8906 - val_loss: 0.3535 - val_acc: 0.9073\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8905\n",
      "Epoch 00084: val_loss did not improve from 0.35348\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3692 - acc: 0.8906 - val_loss: 0.4129 - val_acc: 0.8849\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3644 - acc: 0.8938\n",
      "Epoch 00085: val_loss improved from 0.35348 to 0.33820, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/085-0.3382.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3646 - acc: 0.8937 - val_loss: 0.3382 - val_acc: 0.9108\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3623 - acc: 0.8941\n",
      "Epoch 00086: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3623 - acc: 0.8941 - val_loss: 1.0527 - val_acc: 0.7084\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8931\n",
      "Epoch 00087: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3613 - acc: 0.8931 - val_loss: 0.3751 - val_acc: 0.8987\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3572 - acc: 0.8940\n",
      "Epoch 00088: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3572 - acc: 0.8940 - val_loss: 0.3612 - val_acc: 0.9061\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8963\n",
      "Epoch 00089: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3520 - acc: 0.8963 - val_loss: 0.3673 - val_acc: 0.9003\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8970\n",
      "Epoch 00090: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3553 - acc: 0.8970 - val_loss: 1.3413 - val_acc: 0.6811\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8971\n",
      "Epoch 00091: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3516 - acc: 0.8971 - val_loss: 0.3868 - val_acc: 0.8915\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3453 - acc: 0.8996\n",
      "Epoch 00092: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3454 - acc: 0.8996 - val_loss: 0.3852 - val_acc: 0.8966\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8959\n",
      "Epoch 00093: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3497 - acc: 0.8958 - val_loss: 0.3633 - val_acc: 0.9022\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3440 - acc: 0.8988\n",
      "Epoch 00094: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3440 - acc: 0.8988 - val_loss: 0.3449 - val_acc: 0.9108\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.9000\n",
      "Epoch 00095: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3411 - acc: 0.9000 - val_loss: 0.4665 - val_acc: 0.8672\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3428 - acc: 0.8991\n",
      "Epoch 00096: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3427 - acc: 0.8991 - val_loss: 0.3727 - val_acc: 0.9005\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.9001\n",
      "Epoch 00097: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3364 - acc: 0.9001 - val_loss: 1.6291 - val_acc: 0.6357\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.9028\n",
      "Epoch 00098: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3316 - acc: 0.9028 - val_loss: 0.3681 - val_acc: 0.9099\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.9026\n",
      "Epoch 00099: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3315 - acc: 0.9026 - val_loss: 0.3889 - val_acc: 0.8961\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.9028\n",
      "Epoch 00100: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3299 - acc: 0.9028 - val_loss: 0.4167 - val_acc: 0.8873\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.9029\n",
      "Epoch 00101: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3300 - acc: 0.9029 - val_loss: 0.3393 - val_acc: 0.9099\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.9039\n",
      "Epoch 00102: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3222 - acc: 0.9039 - val_loss: 0.3958 - val_acc: 0.8933\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.9048\n",
      "Epoch 00103: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3230 - acc: 0.9048 - val_loss: 0.3501 - val_acc: 0.9113\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.9056\n",
      "Epoch 00104: val_loss did not improve from 0.33820\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3189 - acc: 0.9056 - val_loss: 0.3404 - val_acc: 0.9089\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.9058\n",
      "Epoch 00105: val_loss improved from 0.33820 to 0.33717, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/105-0.3372.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3164 - acc: 0.9058 - val_loss: 0.3372 - val_acc: 0.9061\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.9075\n",
      "Epoch 00106: val_loss improved from 0.33717 to 0.31962, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/106-0.3196.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3141 - acc: 0.9075 - val_loss: 0.3196 - val_acc: 0.9150\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.9080\n",
      "Epoch 00107: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3127 - acc: 0.9079 - val_loss: 0.8733 - val_acc: 0.7584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9089\n",
      "Epoch 00108: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3107 - acc: 0.9089 - val_loss: 0.3386 - val_acc: 0.9115\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9099\n",
      "Epoch 00109: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3093 - acc: 0.9099 - val_loss: 0.3817 - val_acc: 0.8959\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9089\n",
      "Epoch 00110: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3109 - acc: 0.9089 - val_loss: 0.3640 - val_acc: 0.9066\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.9120\n",
      "Epoch 00111: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2979 - acc: 0.9119 - val_loss: 0.3993 - val_acc: 0.8933\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9108\n",
      "Epoch 00112: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3057 - acc: 0.9108 - val_loss: 0.3303 - val_acc: 0.9164\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.9110\n",
      "Epoch 00113: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3046 - acc: 0.9110 - val_loss: 0.4515 - val_acc: 0.8789\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2959 - acc: 0.9130\n",
      "Epoch 00114: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2959 - acc: 0.9130 - val_loss: 0.9210 - val_acc: 0.7608\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9089\n",
      "Epoch 00115: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3036 - acc: 0.9089 - val_loss: 0.5019 - val_acc: 0.8539\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.9115\n",
      "Epoch 00116: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2994 - acc: 0.9115 - val_loss: 0.3739 - val_acc: 0.9061\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9126\n",
      "Epoch 00117: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2968 - acc: 0.9126 - val_loss: 0.3547 - val_acc: 0.9087\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9123\n",
      "Epoch 00118: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2951 - acc: 0.9123 - val_loss: 0.3506 - val_acc: 0.9094\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2909 - acc: 0.9138\n",
      "Epoch 00119: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2910 - acc: 0.9138 - val_loss: 0.3706 - val_acc: 0.8980\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9135\n",
      "Epoch 00120: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2928 - acc: 0.9135 - val_loss: 0.3268 - val_acc: 0.9143\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.9154\n",
      "Epoch 00121: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2856 - acc: 0.9154 - val_loss: 0.3820 - val_acc: 0.8940\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9157\n",
      "Epoch 00122: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2829 - acc: 0.9157 - val_loss: 0.3361 - val_acc: 0.9180\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.9148\n",
      "Epoch 00123: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2879 - acc: 0.9148 - val_loss: 0.3661 - val_acc: 0.9066\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9154\n",
      "Epoch 00124: val_loss did not improve from 0.31962\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2825 - acc: 0.9153 - val_loss: 0.5510 - val_acc: 0.8334\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9173\n",
      "Epoch 00125: val_loss improved from 0.31962 to 0.31342, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_5_conv_checkpoint/125-0.3134.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2842 - acc: 0.9173 - val_loss: 0.3134 - val_acc: 0.9196\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9165\n",
      "Epoch 00126: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2754 - acc: 0.9165 - val_loss: 0.3277 - val_acc: 0.9113\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9179\n",
      "Epoch 00127: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2755 - acc: 0.9179 - val_loss: 0.3783 - val_acc: 0.8905\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.9179\n",
      "Epoch 00128: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2756 - acc: 0.9179 - val_loss: 0.3493 - val_acc: 0.9133\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.9211\n",
      "Epoch 00129: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2694 - acc: 0.9210 - val_loss: 0.5671 - val_acc: 0.8437\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9179\n",
      "Epoch 00130: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2741 - acc: 0.9179 - val_loss: 0.3792 - val_acc: 0.9047\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9187\n",
      "Epoch 00131: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2744 - acc: 0.9186 - val_loss: 0.3676 - val_acc: 0.9031\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2711 - acc: 0.9182\n",
      "Epoch 00132: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2712 - acc: 0.9182 - val_loss: 0.4219 - val_acc: 0.8789\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9208\n",
      "Epoch 00133: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2656 - acc: 0.9208 - val_loss: 0.5475 - val_acc: 0.8542\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9202\n",
      "Epoch 00134: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2644 - acc: 0.9202 - val_loss: 0.3389 - val_acc: 0.9106\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9194\n",
      "Epoch 00135: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2681 - acc: 0.9194 - val_loss: 0.5457 - val_acc: 0.8449\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9217\n",
      "Epoch 00136: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2619 - acc: 0.9216 - val_loss: 0.3624 - val_acc: 0.9066\n",
      "Epoch 137/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9169\n",
      "Epoch 00137: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2753 - acc: 0.9169 - val_loss: 0.3605 - val_acc: 0.9059\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9210\n",
      "Epoch 00138: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2637 - acc: 0.9210 - val_loss: 0.3247 - val_acc: 0.9180\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9223\n",
      "Epoch 00139: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2628 - acc: 0.9223 - val_loss: 0.3171 - val_acc: 0.9224\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.9222\n",
      "Epoch 00140: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2606 - acc: 0.9222 - val_loss: 0.4967 - val_acc: 0.8649\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9249\n",
      "Epoch 00141: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2537 - acc: 0.9249 - val_loss: 0.9121 - val_acc: 0.7633\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9248\n",
      "Epoch 00142: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2567 - acc: 0.9247 - val_loss: 0.3883 - val_acc: 0.8949\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9249\n",
      "Epoch 00143: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2534 - acc: 0.9249 - val_loss: 0.3961 - val_acc: 0.9017\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.9254\n",
      "Epoch 00144: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2509 - acc: 0.9254 - val_loss: 0.3540 - val_acc: 0.9145\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9246\n",
      "Epoch 00145: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2548 - acc: 0.9245 - val_loss: 0.4258 - val_acc: 0.8926\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9240\n",
      "Epoch 00146: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2565 - acc: 0.9240 - val_loss: 0.9983 - val_acc: 0.7582\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9255\n",
      "Epoch 00147: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2473 - acc: 0.9255 - val_loss: 0.3501 - val_acc: 0.9096\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9275\n",
      "Epoch 00148: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2438 - acc: 0.9275 - val_loss: 0.5165 - val_acc: 0.8572\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9269\n",
      "Epoch 00149: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2457 - acc: 0.9269 - val_loss: 1.5204 - val_acc: 0.6695\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9271\n",
      "Epoch 00150: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2441 - acc: 0.9271 - val_loss: 0.3734 - val_acc: 0.9019\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9267\n",
      "Epoch 00151: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2403 - acc: 0.9267 - val_loss: 0.4346 - val_acc: 0.8831\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9282\n",
      "Epoch 00152: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2420 - acc: 0.9281 - val_loss: 0.3253 - val_acc: 0.9220\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9287\n",
      "Epoch 00153: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2377 - acc: 0.9287 - val_loss: 0.3312 - val_acc: 0.9099\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9284\n",
      "Epoch 00154: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2404 - acc: 0.9284 - val_loss: 0.5812 - val_acc: 0.8449\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9277\n",
      "Epoch 00155: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2421 - acc: 0.9277 - val_loss: 0.3711 - val_acc: 0.9066\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9270\n",
      "Epoch 00156: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2434 - acc: 0.9270 - val_loss: 0.6033 - val_acc: 0.8451\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9301\n",
      "Epoch 00157: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2335 - acc: 0.9300 - val_loss: 0.3294 - val_acc: 0.9182\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9301\n",
      "Epoch 00158: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2336 - acc: 0.9301 - val_loss: 0.3537 - val_acc: 0.9094\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9309\n",
      "Epoch 00159: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2306 - acc: 0.9309 - val_loss: 0.5705 - val_acc: 0.8507\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9297\n",
      "Epoch 00160: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2320 - acc: 0.9297 - val_loss: 0.3681 - val_acc: 0.9085\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9311\n",
      "Epoch 00161: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2284 - acc: 0.9311 - val_loss: 0.3750 - val_acc: 0.9026\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9308\n",
      "Epoch 00162: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2322 - acc: 0.9308 - val_loss: 0.5937 - val_acc: 0.8148\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9296\n",
      "Epoch 00163: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2284 - acc: 0.9296 - val_loss: 0.3811 - val_acc: 0.9026\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9301\n",
      "Epoch 00164: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2306 - acc: 0.9301 - val_loss: 0.3297 - val_acc: 0.9173\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9317\n",
      "Epoch 00165: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2261 - acc: 0.9317 - val_loss: 0.3258 - val_acc: 0.9171\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9332\n",
      "Epoch 00166: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2243 - acc: 0.9331 - val_loss: 0.3646 - val_acc: 0.9101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9324\n",
      "Epoch 00167: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2261 - acc: 0.9324 - val_loss: 0.3262 - val_acc: 0.9178\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9337\n",
      "Epoch 00168: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2201 - acc: 0.9337 - val_loss: 0.3239 - val_acc: 0.9187\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9308\n",
      "Epoch 00169: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2269 - acc: 0.9308 - val_loss: 0.3156 - val_acc: 0.9194\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9354\n",
      "Epoch 00170: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2190 - acc: 0.9353 - val_loss: 0.5582 - val_acc: 0.8470\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9332\n",
      "Epoch 00171: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2192 - acc: 0.9332 - val_loss: 0.3577 - val_acc: 0.9061\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9328\n",
      "Epoch 00172: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2222 - acc: 0.9328 - val_loss: 0.5458 - val_acc: 0.8491\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9330\n",
      "Epoch 00173: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2189 - acc: 0.9330 - val_loss: 0.4010 - val_acc: 0.8966\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9351\n",
      "Epoch 00174: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2132 - acc: 0.9350 - val_loss: 0.5773 - val_acc: 0.8321\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9328\n",
      "Epoch 00175: val_loss did not improve from 0.31342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2215 - acc: 0.9328 - val_loss: 0.3840 - val_acc: 0.9033\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 615us/sample - loss: 0.3902 - acc: 0.8874\n",
      "Loss: 0.39020791636820523 Accuracy: 0.8874351\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9960 - acc: 0.3733\n",
      "Epoch 00001: val_loss improved from inf to 1.73286, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/001-1.7329.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.9960 - acc: 0.3733 - val_loss: 1.7329 - val_acc: 0.4936\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3813 - acc: 0.5874\n",
      "Epoch 00002: val_loss improved from 1.73286 to 1.04779, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/002-1.0478.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 1.3813 - acc: 0.5875 - val_loss: 1.0478 - val_acc: 0.7333\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1366 - acc: 0.6711\n",
      "Epoch 00003: val_loss improved from 1.04779 to 0.88855, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/003-0.8885.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 1.1367 - acc: 0.6711 - val_loss: 0.8885 - val_acc: 0.7782\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9787 - acc: 0.7228\n",
      "Epoch 00004: val_loss improved from 0.88855 to 0.77009, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/004-0.7701.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.9787 - acc: 0.7228 - val_loss: 0.7701 - val_acc: 0.8088\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8659 - acc: 0.7579\n",
      "Epoch 00005: val_loss improved from 0.77009 to 0.68636, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/005-0.6864.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.8659 - acc: 0.7579 - val_loss: 0.6864 - val_acc: 0.8258\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7733 - acc: 0.7866\n",
      "Epoch 00006: val_loss improved from 0.68636 to 0.60418, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/006-0.6042.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.7733 - acc: 0.7866 - val_loss: 0.6042 - val_acc: 0.8577\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7075 - acc: 0.8055\n",
      "Epoch 00007: val_loss improved from 0.60418 to 0.57801, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/007-0.5780.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.7074 - acc: 0.8055 - val_loss: 0.5780 - val_acc: 0.8546\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6525 - acc: 0.8188\n",
      "Epoch 00008: val_loss improved from 0.57801 to 0.47980, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/008-0.4798.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.6526 - acc: 0.8187 - val_loss: 0.4798 - val_acc: 0.8831\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6102 - acc: 0.8309\n",
      "Epoch 00009: val_loss improved from 0.47980 to 0.47173, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/009-0.4717.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.6104 - acc: 0.8308 - val_loss: 0.4717 - val_acc: 0.8859\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5736 - acc: 0.8405\n",
      "Epoch 00010: val_loss improved from 0.47173 to 0.41954, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/010-0.4195.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5736 - acc: 0.8405 - val_loss: 0.4195 - val_acc: 0.8963\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.8501\n",
      "Epoch 00011: val_loss did not improve from 0.41954\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5358 - acc: 0.8500 - val_loss: 0.4302 - val_acc: 0.8863\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5109 - acc: 0.8574\n",
      "Epoch 00012: val_loss improved from 0.41954 to 0.41461, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/012-0.4146.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5109 - acc: 0.8574 - val_loss: 0.4146 - val_acc: 0.8947\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.8616\n",
      "Epoch 00013: val_loss improved from 0.41461 to 0.41228, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/013-0.4123.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4918 - acc: 0.8615 - val_loss: 0.4123 - val_acc: 0.8779\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4703 - acc: 0.8677\n",
      "Epoch 00014: val_loss improved from 0.41228 to 0.36146, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/014-0.3615.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4703 - acc: 0.8677 - val_loss: 0.3615 - val_acc: 0.9066\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8736\n",
      "Epoch 00015: val_loss improved from 0.36146 to 0.35280, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/015-0.3528.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4495 - acc: 0.8736 - val_loss: 0.3528 - val_acc: 0.9040\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4340 - acc: 0.8782\n",
      "Epoch 00016: val_loss improved from 0.35280 to 0.33583, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/016-0.3358.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4340 - acc: 0.8782 - val_loss: 0.3358 - val_acc: 0.9122\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8804\n",
      "Epoch 00017: val_loss improved from 0.33583 to 0.31851, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/017-0.3185.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4180 - acc: 0.8803 - val_loss: 0.3185 - val_acc: 0.9206\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8860\n",
      "Epoch 00018: val_loss did not improve from 0.31851\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3994 - acc: 0.8860 - val_loss: 0.3337 - val_acc: 0.9131\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8892\n",
      "Epoch 00019: val_loss improved from 0.31851 to 0.30692, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/019-0.3069.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3892 - acc: 0.8892 - val_loss: 0.3069 - val_acc: 0.9182\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3811 - acc: 0.8920\n",
      "Epoch 00020: val_loss improved from 0.30692 to 0.28752, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/020-0.2875.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3811 - acc: 0.8920 - val_loss: 0.2875 - val_acc: 0.9264\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8929\n",
      "Epoch 00021: val_loss improved from 0.28752 to 0.27564, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/021-0.2756.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3678 - acc: 0.8929 - val_loss: 0.2756 - val_acc: 0.9301\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8974\n",
      "Epoch 00022: val_loss did not improve from 0.27564\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3566 - acc: 0.8974 - val_loss: 0.3508 - val_acc: 0.9019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3447 - acc: 0.9006\n",
      "Epoch 00023: val_loss did not improve from 0.27564\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3448 - acc: 0.9006 - val_loss: 0.3009 - val_acc: 0.9173\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.9030\n",
      "Epoch 00024: val_loss did not improve from 0.27564\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3346 - acc: 0.9029 - val_loss: 0.2902 - val_acc: 0.9243\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.9075\n",
      "Epoch 00025: val_loss improved from 0.27564 to 0.26436, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/025-0.2644.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3224 - acc: 0.9075 - val_loss: 0.2644 - val_acc: 0.9259\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.9077\n",
      "Epoch 00026: val_loss improved from 0.26436 to 0.25309, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/026-0.2531.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3205 - acc: 0.9077 - val_loss: 0.2531 - val_acc: 0.9324\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.9095\n",
      "Epoch 00027: val_loss did not improve from 0.25309\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3140 - acc: 0.9095 - val_loss: 0.3093 - val_acc: 0.9108\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.9116\n",
      "Epoch 00028: val_loss did not improve from 0.25309\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3056 - acc: 0.9116 - val_loss: 0.2721 - val_acc: 0.9297\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.9142\n",
      "Epoch 00029: val_loss improved from 0.25309 to 0.25087, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/029-0.2509.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3005 - acc: 0.9141 - val_loss: 0.2509 - val_acc: 0.9343\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9165\n",
      "Epoch 00030: val_loss did not improve from 0.25087\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2901 - acc: 0.9165 - val_loss: 0.2642 - val_acc: 0.9320\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9167\n",
      "Epoch 00031: val_loss did not improve from 0.25087\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2839 - acc: 0.9167 - val_loss: 0.2605 - val_acc: 0.9292\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2796 - acc: 0.9185\n",
      "Epoch 00032: val_loss improved from 0.25087 to 0.24499, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/032-0.2450.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2796 - acc: 0.9185 - val_loss: 0.2450 - val_acc: 0.9359\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9188\n",
      "Epoch 00033: val_loss did not improve from 0.24499\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2725 - acc: 0.9188 - val_loss: 0.3418 - val_acc: 0.9038\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9233\n",
      "Epoch 00034: val_loss improved from 0.24499 to 0.23593, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/034-0.2359.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2595 - acc: 0.9233 - val_loss: 0.2359 - val_acc: 0.9336\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9250\n",
      "Epoch 00035: val_loss did not improve from 0.23593\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2595 - acc: 0.9250 - val_loss: 0.2504 - val_acc: 0.9355\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9254\n",
      "Epoch 00036: val_loss improved from 0.23593 to 0.22216, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/036-0.2222.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2552 - acc: 0.9254 - val_loss: 0.2222 - val_acc: 0.9385\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9267\n",
      "Epoch 00037: val_loss did not improve from 0.22216\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2510 - acc: 0.9267 - val_loss: 0.2693 - val_acc: 0.9257\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9279\n",
      "Epoch 00038: val_loss did not improve from 0.22216\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2454 - acc: 0.9279 - val_loss: 0.2300 - val_acc: 0.9383\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9312\n",
      "Epoch 00039: val_loss improved from 0.22216 to 0.21899, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/039-0.2190.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2348 - acc: 0.9312 - val_loss: 0.2190 - val_acc: 0.9408\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9301\n",
      "Epoch 00040: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2336 - acc: 0.9301 - val_loss: 0.2359 - val_acc: 0.9350\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9310\n",
      "Epoch 00041: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2345 - acc: 0.9310 - val_loss: 0.2550 - val_acc: 0.9329\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9333\n",
      "Epoch 00042: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2221 - acc: 0.9333 - val_loss: 0.2243 - val_acc: 0.9345\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9321\n",
      "Epoch 00043: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2286 - acc: 0.9321 - val_loss: 0.6263 - val_acc: 0.8293\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9348\n",
      "Epoch 00044: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2215 - acc: 0.9348 - val_loss: 0.4718 - val_acc: 0.8647\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9357\n",
      "Epoch 00045: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2167 - acc: 0.9357 - val_loss: 0.4091 - val_acc: 0.8833\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9374\n",
      "Epoch 00046: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2114 - acc: 0.9374 - val_loss: 0.2705 - val_acc: 0.9196\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9383\n",
      "Epoch 00047: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2083 - acc: 0.9382 - val_loss: 0.3028 - val_acc: 0.9180\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9370\n",
      "Epoch 00048: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2112 - acc: 0.9370 - val_loss: 0.2318 - val_acc: 0.9369\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9405\n",
      "Epoch 00049: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2016 - acc: 0.9405 - val_loss: 0.2715 - val_acc: 0.9231\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.9403\n",
      "Epoch 00050: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1999 - acc: 0.9403 - val_loss: 0.2287 - val_acc: 0.9369\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9411\n",
      "Epoch 00051: val_loss did not improve from 0.21899\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1983 - acc: 0.9411 - val_loss: 0.2269 - val_acc: 0.9418\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9433\n",
      "Epoch 00052: val_loss improved from 0.21899 to 0.21022, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/052-0.2102.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1932 - acc: 0.9433 - val_loss: 0.2102 - val_acc: 0.9443\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9433\n",
      "Epoch 00053: val_loss did not improve from 0.21022\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1894 - acc: 0.9433 - val_loss: 0.2231 - val_acc: 0.9404\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9431\n",
      "Epoch 00054: val_loss did not improve from 0.21022\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1881 - acc: 0.9431 - val_loss: 0.2665 - val_acc: 0.9287\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9471\n",
      "Epoch 00055: val_loss did not improve from 0.21022\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1830 - acc: 0.9471 - val_loss: 0.2296 - val_acc: 0.9366\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9461\n",
      "Epoch 00056: val_loss did not improve from 0.21022\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1818 - acc: 0.9461 - val_loss: 0.2741 - val_acc: 0.9257\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9457\n",
      "Epoch 00057: val_loss did not improve from 0.21022\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1828 - acc: 0.9457 - val_loss: 0.2312 - val_acc: 0.9357\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9474\n",
      "Epoch 00058: val_loss did not improve from 0.21022\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1777 - acc: 0.9474 - val_loss: 0.2335 - val_acc: 0.9413\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9482\n",
      "Epoch 00059: val_loss did not improve from 0.21022\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1739 - acc: 0.9482 - val_loss: 0.2360 - val_acc: 0.9348\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9515\n",
      "Epoch 00060: val_loss did not improve from 0.21022\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1632 - acc: 0.9515 - val_loss: 0.2338 - val_acc: 0.9362\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9508\n",
      "Epoch 00061: val_loss did not improve from 0.21022\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1691 - acc: 0.9507 - val_loss: 0.2331 - val_acc: 0.9352\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9495\n",
      "Epoch 00062: val_loss did not improve from 0.21022\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1688 - acc: 0.9495 - val_loss: 0.2162 - val_acc: 0.9401\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9509\n",
      "Epoch 00063: val_loss improved from 0.21022 to 0.19980, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/063-0.1998.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1626 - acc: 0.9509 - val_loss: 0.1998 - val_acc: 0.9439\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9527\n",
      "Epoch 00064: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1631 - acc: 0.9527 - val_loss: 0.2105 - val_acc: 0.9455\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9526\n",
      "Epoch 00065: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1590 - acc: 0.9526 - val_loss: 0.2194 - val_acc: 0.9390\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9547\n",
      "Epoch 00066: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1518 - acc: 0.9547 - val_loss: 0.2378 - val_acc: 0.9355\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9535\n",
      "Epoch 00067: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1521 - acc: 0.9535 - val_loss: 0.2015 - val_acc: 0.9457\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9545\n",
      "Epoch 00068: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1516 - acc: 0.9545 - val_loss: 0.3013 - val_acc: 0.9222\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9515\n",
      "Epoch 00069: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1595 - acc: 0.9515 - val_loss: 0.2053 - val_acc: 0.9478\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9548\n",
      "Epoch 00070: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1484 - acc: 0.9548 - val_loss: 0.2548 - val_acc: 0.9297\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9561\n",
      "Epoch 00071: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1459 - acc: 0.9560 - val_loss: 0.2225 - val_acc: 0.9401\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9557\n",
      "Epoch 00072: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1480 - acc: 0.9557 - val_loss: 0.3064 - val_acc: 0.9250\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9587\n",
      "Epoch 00073: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1352 - acc: 0.9587 - val_loss: 0.2096 - val_acc: 0.9429\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9587\n",
      "Epoch 00074: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1417 - acc: 0.9586 - val_loss: 0.2307 - val_acc: 0.9413\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9569\n",
      "Epoch 00075: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1421 - acc: 0.9569 - val_loss: 0.2208 - val_acc: 0.9432\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9581\n",
      "Epoch 00076: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1401 - acc: 0.9581 - val_loss: 0.2155 - val_acc: 0.9425\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9598\n",
      "Epoch 00077: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1335 - acc: 0.9598 - val_loss: 0.2453 - val_acc: 0.9306\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9613\n",
      "Epoch 00078: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1301 - acc: 0.9612 - val_loss: 0.2530 - val_acc: 0.9331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9603\n",
      "Epoch 00079: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1324 - acc: 0.9602 - val_loss: 0.2108 - val_acc: 0.9413\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9584\n",
      "Epoch 00080: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1353 - acc: 0.9584 - val_loss: 0.2437 - val_acc: 0.9317\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9599\n",
      "Epoch 00081: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1325 - acc: 0.9598 - val_loss: 0.2249 - val_acc: 0.9434\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9610\n",
      "Epoch 00082: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1294 - acc: 0.9610 - val_loss: 0.2233 - val_acc: 0.9418\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9622\n",
      "Epoch 00083: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1279 - acc: 0.9622 - val_loss: 0.2157 - val_acc: 0.9422\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9629\n",
      "Epoch 00084: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1218 - acc: 0.9629 - val_loss: 0.3974 - val_acc: 0.8919\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9636\n",
      "Epoch 00085: val_loss did not improve from 0.19980\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1203 - acc: 0.9636 - val_loss: 0.2446 - val_acc: 0.9362\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9638\n",
      "Epoch 00086: val_loss improved from 0.19980 to 0.19425, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/086-0.1942.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1209 - acc: 0.9637 - val_loss: 0.1942 - val_acc: 0.9478\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9649\n",
      "Epoch 00087: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1180 - acc: 0.9648 - val_loss: 0.2380 - val_acc: 0.9378\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9607\n",
      "Epoch 00088: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1298 - acc: 0.9607 - val_loss: 0.2093 - val_acc: 0.9453\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9665\n",
      "Epoch 00089: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1130 - acc: 0.9664 - val_loss: 0.2357 - val_acc: 0.9378\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9663\n",
      "Epoch 00090: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1113 - acc: 0.9663 - val_loss: 0.2056 - val_acc: 0.9448\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9661\n",
      "Epoch 00091: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1110 - acc: 0.9661 - val_loss: 0.1953 - val_acc: 0.9446\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9672\n",
      "Epoch 00092: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1086 - acc: 0.9672 - val_loss: 0.2094 - val_acc: 0.9462\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9663\n",
      "Epoch 00093: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1097 - acc: 0.9663 - val_loss: 0.2043 - val_acc: 0.9483\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9670\n",
      "Epoch 00094: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1106 - acc: 0.9669 - val_loss: 0.2746 - val_acc: 0.9292\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9646\n",
      "Epoch 00095: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1189 - acc: 0.9646 - val_loss: 0.2046 - val_acc: 0.9483\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9688\n",
      "Epoch 00096: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1043 - acc: 0.9688 - val_loss: 0.2177 - val_acc: 0.9415\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9691\n",
      "Epoch 00097: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1033 - acc: 0.9691 - val_loss: 0.2169 - val_acc: 0.9390\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9685\n",
      "Epoch 00098: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1031 - acc: 0.9685 - val_loss: 0.2039 - val_acc: 0.9483\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9688\n",
      "Epoch 00099: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1033 - acc: 0.9688 - val_loss: 0.2366 - val_acc: 0.9429\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9689\n",
      "Epoch 00100: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1019 - acc: 0.9689 - val_loss: 0.2386 - val_acc: 0.9418\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9703\n",
      "Epoch 00101: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1000 - acc: 0.9703 - val_loss: 0.2760 - val_acc: 0.9285\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9713\n",
      "Epoch 00102: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0969 - acc: 0.9713 - val_loss: 0.2096 - val_acc: 0.9453\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9714\n",
      "Epoch 00103: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0950 - acc: 0.9714 - val_loss: 0.2548 - val_acc: 0.9369\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9710\n",
      "Epoch 00104: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0968 - acc: 0.9710 - val_loss: 0.2919 - val_acc: 0.9290\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9722\n",
      "Epoch 00105: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0947 - acc: 0.9722 - val_loss: 0.2832 - val_acc: 0.9297\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9722\n",
      "Epoch 00106: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0917 - acc: 0.9722 - val_loss: 0.1972 - val_acc: 0.9471\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9710\n",
      "Epoch 00107: val_loss did not improve from 0.19425\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0948 - acc: 0.9710 - val_loss: 0.2188 - val_acc: 0.9404\n",
      "Epoch 108/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9730\n",
      "Epoch 00108: val_loss improved from 0.19425 to 0.18820, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_6_conv_checkpoint/108-0.1882.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0916 - acc: 0.9730 - val_loss: 0.1882 - val_acc: 0.9476\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9732\n",
      "Epoch 00109: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0892 - acc: 0.9732 - val_loss: 0.1966 - val_acc: 0.9460\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9746\n",
      "Epoch 00110: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0845 - acc: 0.9746 - val_loss: 0.2533 - val_acc: 0.9348\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9702\n",
      "Epoch 00111: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0947 - acc: 0.9702 - val_loss: 0.2355 - val_acc: 0.9415\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9739\n",
      "Epoch 00112: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0855 - acc: 0.9739 - val_loss: 0.3136 - val_acc: 0.9280\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9740\n",
      "Epoch 00113: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0863 - acc: 0.9740 - val_loss: 0.2155 - val_acc: 0.9450\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9754\n",
      "Epoch 00114: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0831 - acc: 0.9754 - val_loss: 0.2132 - val_acc: 0.9432\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9760\n",
      "Epoch 00115: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0823 - acc: 0.9759 - val_loss: 0.2297 - val_acc: 0.9399\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9712\n",
      "Epoch 00116: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0955 - acc: 0.9713 - val_loss: 0.2015 - val_acc: 0.9504\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9768\n",
      "Epoch 00117: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0793 - acc: 0.9768 - val_loss: 0.2458 - val_acc: 0.9380\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9758\n",
      "Epoch 00118: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0836 - acc: 0.9758 - val_loss: 0.2381 - val_acc: 0.9411\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9762\n",
      "Epoch 00119: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0801 - acc: 0.9762 - val_loss: 0.2091 - val_acc: 0.9506\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9775\n",
      "Epoch 00120: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0786 - acc: 0.9775 - val_loss: 0.2993 - val_acc: 0.9297\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9732\n",
      "Epoch 00121: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0912 - acc: 0.9732 - val_loss: 0.2196 - val_acc: 0.9418\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9727\n",
      "Epoch 00122: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0931 - acc: 0.9726 - val_loss: 0.2095 - val_acc: 0.9462\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9766\n",
      "Epoch 00123: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0800 - acc: 0.9766 - val_loss: 0.2217 - val_acc: 0.9420\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9770\n",
      "Epoch 00124: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0740 - acc: 0.9770 - val_loss: 0.4103 - val_acc: 0.9024\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9774\n",
      "Epoch 00125: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0748 - acc: 0.9774 - val_loss: 0.2488 - val_acc: 0.9443\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9781\n",
      "Epoch 00126: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0751 - acc: 0.9781 - val_loss: 0.2205 - val_acc: 0.9425\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9784\n",
      "Epoch 00127: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0721 - acc: 0.9784 - val_loss: 0.2370 - val_acc: 0.9455\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9777\n",
      "Epoch 00128: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0736 - acc: 0.9777 - val_loss: 0.2316 - val_acc: 0.9450\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9789\n",
      "Epoch 00129: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0714 - acc: 0.9788 - val_loss: 0.2570 - val_acc: 0.9404\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9726\n",
      "Epoch 00130: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0920 - acc: 0.9726 - val_loss: 0.2444 - val_acc: 0.9387\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9805\n",
      "Epoch 00131: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0679 - acc: 0.9805 - val_loss: 0.2525 - val_acc: 0.9366\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9790\n",
      "Epoch 00132: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0700 - acc: 0.9789 - val_loss: 0.2123 - val_acc: 0.9481\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9791\n",
      "Epoch 00133: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0701 - acc: 0.9791 - val_loss: 0.2296 - val_acc: 0.9453\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9784\n",
      "Epoch 00134: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0722 - acc: 0.9784 - val_loss: 0.2286 - val_acc: 0.9467\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9803\n",
      "Epoch 00135: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0653 - acc: 0.9802 - val_loss: 0.2189 - val_acc: 0.9427\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9783\n",
      "Epoch 00136: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0719 - acc: 0.9783 - val_loss: 0.2050 - val_acc: 0.9483\n",
      "Epoch 137/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9806\n",
      "Epoch 00137: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0653 - acc: 0.9806 - val_loss: 0.2243 - val_acc: 0.9441\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9804\n",
      "Epoch 00138: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0673 - acc: 0.9803 - val_loss: 0.2007 - val_acc: 0.9506\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9809\n",
      "Epoch 00139: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0653 - acc: 0.9809 - val_loss: 0.2059 - val_acc: 0.9522\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9811\n",
      "Epoch 00140: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0647 - acc: 0.9811 - val_loss: 0.2201 - val_acc: 0.9446\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9797\n",
      "Epoch 00141: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0684 - acc: 0.9797 - val_loss: 0.2031 - val_acc: 0.9502\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9822\n",
      "Epoch 00142: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0610 - acc: 0.9822 - val_loss: 0.2158 - val_acc: 0.9488\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9799\n",
      "Epoch 00143: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0677 - acc: 0.9799 - val_loss: 0.2472 - val_acc: 0.9411\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9813\n",
      "Epoch 00144: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0636 - acc: 0.9813 - val_loss: 0.2264 - val_acc: 0.9460\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9798\n",
      "Epoch 00145: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0664 - acc: 0.9798 - val_loss: 0.2174 - val_acc: 0.9474\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9816\n",
      "Epoch 00146: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0598 - acc: 0.9816 - val_loss: 0.2096 - val_acc: 0.9504\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9822\n",
      "Epoch 00147: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0625 - acc: 0.9822 - val_loss: 0.2432 - val_acc: 0.9352\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9832\n",
      "Epoch 00148: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0571 - acc: 0.9832 - val_loss: 0.2470 - val_acc: 0.9441\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9825\n",
      "Epoch 00149: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0580 - acc: 0.9825 - val_loss: 0.2121 - val_acc: 0.9488\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9838\n",
      "Epoch 00150: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0570 - acc: 0.9838 - val_loss: 0.2109 - val_acc: 0.9443\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9814\n",
      "Epoch 00151: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0606 - acc: 0.9814 - val_loss: 0.2064 - val_acc: 0.9504\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9810\n",
      "Epoch 00152: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0622 - acc: 0.9810 - val_loss: 0.2258 - val_acc: 0.9455\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9832\n",
      "Epoch 00153: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0568 - acc: 0.9832 - val_loss: 0.2350 - val_acc: 0.9460\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9827\n",
      "Epoch 00154: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0575 - acc: 0.9826 - val_loss: 0.2027 - val_acc: 0.9504\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9806\n",
      "Epoch 00155: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0651 - acc: 0.9805 - val_loss: 0.2581 - val_acc: 0.9387\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9837\n",
      "Epoch 00156: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0546 - acc: 0.9837 - val_loss: 0.2334 - val_acc: 0.9450\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9837\n",
      "Epoch 00157: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0529 - acc: 0.9837 - val_loss: 0.2463 - val_acc: 0.9420\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9845\n",
      "Epoch 00158: val_loss did not improve from 0.18820\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0543 - acc: 0.9845 - val_loss: 0.3237 - val_acc: 0.9271\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 673us/sample - loss: 0.2353 - acc: 0.9350\n",
      "Loss: 0.23532529091414137 Accuracy: 0.9349948\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7952 - acc: 0.4438\n",
      "Epoch 00001: val_loss improved from inf to 1.39866, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/001-1.3987.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.7952 - acc: 0.4438 - val_loss: 1.3987 - val_acc: 0.6175\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9922 - acc: 0.7145\n",
      "Epoch 00002: val_loss improved from 1.39866 to 0.64716, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/002-0.6472.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.9924 - acc: 0.7145 - val_loss: 0.6472 - val_acc: 0.8505\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7080 - acc: 0.8080\n",
      "Epoch 00003: val_loss improved from 0.64716 to 0.47852, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/003-0.4785.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.7080 - acc: 0.8080 - val_loss: 0.4785 - val_acc: 0.8877\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.8460\n",
      "Epoch 00004: val_loss improved from 0.47852 to 0.39740, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/004-0.3974.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.5673 - acc: 0.8460 - val_loss: 0.3974 - val_acc: 0.8984\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4858 - acc: 0.8680\n",
      "Epoch 00005: val_loss improved from 0.39740 to 0.32355, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/005-0.3235.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4858 - acc: 0.8680 - val_loss: 0.3235 - val_acc: 0.9231\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4240 - acc: 0.8836\n",
      "Epoch 00006: val_loss did not improve from 0.32355\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4240 - acc: 0.8836 - val_loss: 0.3409 - val_acc: 0.9078\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8959\n",
      "Epoch 00007: val_loss improved from 0.32355 to 0.27021, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/007-0.2702.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3801 - acc: 0.8959 - val_loss: 0.2702 - val_acc: 0.9315\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.9024\n",
      "Epoch 00008: val_loss did not improve from 0.27021\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3507 - acc: 0.9024 - val_loss: 0.3495 - val_acc: 0.9080\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.9105\n",
      "Epoch 00009: val_loss improved from 0.27021 to 0.27007, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/009-0.2701.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3177 - acc: 0.9105 - val_loss: 0.2701 - val_acc: 0.9234\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9179\n",
      "Epoch 00010: val_loss improved from 0.27007 to 0.25512, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/010-0.2551.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2956 - acc: 0.9179 - val_loss: 0.2551 - val_acc: 0.9292\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9215\n",
      "Epoch 00011: val_loss improved from 0.25512 to 0.22691, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/011-0.2269.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2784 - acc: 0.9215 - val_loss: 0.2269 - val_acc: 0.9406\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9270\n",
      "Epoch 00012: val_loss improved from 0.22691 to 0.19734, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/012-0.1973.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2587 - acc: 0.9270 - val_loss: 0.1973 - val_acc: 0.9502\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9313\n",
      "Epoch 00013: val_loss improved from 0.19734 to 0.18636, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/013-0.1864.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2408 - acc: 0.9313 - val_loss: 0.1864 - val_acc: 0.9532\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9341\n",
      "Epoch 00014: val_loss did not improve from 0.18636\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2311 - acc: 0.9341 - val_loss: 0.2175 - val_acc: 0.9401\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9383\n",
      "Epoch 00015: val_loss improved from 0.18636 to 0.16912, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/015-0.1691.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2152 - acc: 0.9384 - val_loss: 0.1691 - val_acc: 0.9555\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9409\n",
      "Epoch 00016: val_loss did not improve from 0.16912\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2077 - acc: 0.9409 - val_loss: 0.2343 - val_acc: 0.9311\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9423\n",
      "Epoch 00017: val_loss did not improve from 0.16912\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1981 - acc: 0.9422 - val_loss: 0.1799 - val_acc: 0.9525\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9435\n",
      "Epoch 00018: val_loss did not improve from 0.16912\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1944 - acc: 0.9435 - val_loss: 0.2937 - val_acc: 0.9145\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9480\n",
      "Epoch 00019: val_loss did not improve from 0.16912\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1801 - acc: 0.9479 - val_loss: 0.2047 - val_acc: 0.9418\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9510\n",
      "Epoch 00020: val_loss did not improve from 0.16912\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1696 - acc: 0.9510 - val_loss: 0.2025 - val_acc: 0.9422\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9513\n",
      "Epoch 00021: val_loss improved from 0.16912 to 0.14488, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/021-0.1449.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1701 - acc: 0.9513 - val_loss: 0.1449 - val_acc: 0.9590\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.9526\n",
      "Epoch 00022: val_loss did not improve from 0.14488\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1597 - acc: 0.9526 - val_loss: 0.1644 - val_acc: 0.9525\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9553\n",
      "Epoch 00023: val_loss did not improve from 0.14488\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1528 - acc: 0.9553 - val_loss: 0.1879 - val_acc: 0.9460\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9567\n",
      "Epoch 00024: val_loss did not improve from 0.14488\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1467 - acc: 0.9567 - val_loss: 0.1594 - val_acc: 0.9564\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9593\n",
      "Epoch 00025: val_loss did not improve from 0.14488\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1411 - acc: 0.9593 - val_loss: 0.1959 - val_acc: 0.9453\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9602\n",
      "Epoch 00026: val_loss did not improve from 0.14488\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1352 - acc: 0.9602 - val_loss: 0.1888 - val_acc: 0.9476\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9618\n",
      "Epoch 00027: val_loss did not improve from 0.14488\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1298 - acc: 0.9618 - val_loss: 0.1613 - val_acc: 0.9509\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9625\n",
      "Epoch 00028: val_loss did not improve from 0.14488\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1290 - acc: 0.9625 - val_loss: 0.1585 - val_acc: 0.9541\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9652\n",
      "Epoch 00029: val_loss did not improve from 0.14488\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1184 - acc: 0.9652 - val_loss: 0.1545 - val_acc: 0.9532\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9668\n",
      "Epoch 00030: val_loss did not improve from 0.14488\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1150 - acc: 0.9668 - val_loss: 0.1731 - val_acc: 0.9543\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9673\n",
      "Epoch 00031: val_loss did not improve from 0.14488\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1124 - acc: 0.9673 - val_loss: 0.1650 - val_acc: 0.9518\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9677\n",
      "Epoch 00032: val_loss improved from 0.14488 to 0.14429, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/032-0.1443.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1104 - acc: 0.9677 - val_loss: 0.1443 - val_acc: 0.9555\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9680\n",
      "Epoch 00033: val_loss did not improve from 0.14429\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1086 - acc: 0.9680 - val_loss: 0.1718 - val_acc: 0.9485\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9712\n",
      "Epoch 00034: val_loss did not improve from 0.14429\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1008 - acc: 0.9713 - val_loss: 0.1620 - val_acc: 0.9532\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9706\n",
      "Epoch 00035: val_loss did not improve from 0.14429\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0997 - acc: 0.9706 - val_loss: 0.1863 - val_acc: 0.9453\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9723\n",
      "Epoch 00036: val_loss did not improve from 0.14429\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0941 - acc: 0.9723 - val_loss: 0.1625 - val_acc: 0.9557\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9721\n",
      "Epoch 00037: val_loss did not improve from 0.14429\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0946 - acc: 0.9720 - val_loss: 0.1777 - val_acc: 0.9474\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9714\n",
      "Epoch 00038: val_loss did not improve from 0.14429\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0973 - acc: 0.9714 - val_loss: 0.1526 - val_acc: 0.9578\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9763\n",
      "Epoch 00039: val_loss improved from 0.14429 to 0.14011, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_7_conv_checkpoint/039-0.1401.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0836 - acc: 0.9763 - val_loss: 0.1401 - val_acc: 0.9585\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9756\n",
      "Epoch 00040: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0818 - acc: 0.9755 - val_loss: 0.1581 - val_acc: 0.9539\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9752\n",
      "Epoch 00041: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0847 - acc: 0.9752 - val_loss: 0.1472 - val_acc: 0.9581\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9767\n",
      "Epoch 00042: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0769 - acc: 0.9767 - val_loss: 0.1687 - val_acc: 0.9476\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9780\n",
      "Epoch 00043: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0761 - acc: 0.9780 - val_loss: 0.1594 - val_acc: 0.9569\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9787\n",
      "Epoch 00044: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0736 - acc: 0.9787 - val_loss: 0.1517 - val_acc: 0.9583\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9792\n",
      "Epoch 00045: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0706 - acc: 0.9792 - val_loss: 0.1479 - val_acc: 0.9609\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9798\n",
      "Epoch 00046: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0693 - acc: 0.9798 - val_loss: 0.1462 - val_acc: 0.9550\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9805\n",
      "Epoch 00047: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0674 - acc: 0.9805 - val_loss: 0.1544 - val_acc: 0.9574\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9793\n",
      "Epoch 00048: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0688 - acc: 0.9793 - val_loss: 0.1439 - val_acc: 0.9583\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9818\n",
      "Epoch 00049: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0619 - acc: 0.9818 - val_loss: 0.1493 - val_acc: 0.9585\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9827\n",
      "Epoch 00050: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0587 - acc: 0.9827 - val_loss: 0.1551 - val_acc: 0.9564\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9833\n",
      "Epoch 00051: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0585 - acc: 0.9832 - val_loss: 0.1681 - val_acc: 0.9541\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9798\n",
      "Epoch 00052: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0676 - acc: 0.9798 - val_loss: 0.1856 - val_acc: 0.9478\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9837\n",
      "Epoch 00053: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0545 - acc: 0.9837 - val_loss: 0.1561 - val_acc: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9830\n",
      "Epoch 00054: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0568 - acc: 0.9830 - val_loss: 0.1430 - val_acc: 0.9609\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9855\n",
      "Epoch 00055: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0497 - acc: 0.9855 - val_loss: 0.1443 - val_acc: 0.9599\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9849\n",
      "Epoch 00056: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0498 - acc: 0.9849 - val_loss: 0.2107 - val_acc: 0.9411\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9852\n",
      "Epoch 00057: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0523 - acc: 0.9852 - val_loss: 0.1551 - val_acc: 0.9606\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9868\n",
      "Epoch 00058: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0473 - acc: 0.9867 - val_loss: 0.2825 - val_acc: 0.9304\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9811\n",
      "Epoch 00059: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0631 - acc: 0.9811 - val_loss: 0.1567 - val_acc: 0.9569\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9877\n",
      "Epoch 00060: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0443 - acc: 0.9877 - val_loss: 0.1594 - val_acc: 0.9590\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9893\n",
      "Epoch 00061: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0394 - acc: 0.9893 - val_loss: 0.1746 - val_acc: 0.9557\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9872\n",
      "Epoch 00062: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0442 - acc: 0.9872 - val_loss: 0.1709 - val_acc: 0.9520\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9878\n",
      "Epoch 00063: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0438 - acc: 0.9878 - val_loss: 0.1487 - val_acc: 0.9634\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9866\n",
      "Epoch 00064: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0478 - acc: 0.9866 - val_loss: 0.2103 - val_acc: 0.9497\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9880\n",
      "Epoch 00065: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0409 - acc: 0.9880 - val_loss: 0.1533 - val_acc: 0.9609\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9890\n",
      "Epoch 00066: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0395 - acc: 0.9891 - val_loss: 0.1940 - val_acc: 0.9467\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9886\n",
      "Epoch 00067: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0400 - acc: 0.9886 - val_loss: 0.2057 - val_acc: 0.9518\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9881\n",
      "Epoch 00068: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0391 - acc: 0.9881 - val_loss: 0.1620 - val_acc: 0.9571\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9905\n",
      "Epoch 00069: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0350 - acc: 0.9905 - val_loss: 0.1451 - val_acc: 0.9618\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9907\n",
      "Epoch 00070: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0340 - acc: 0.9907 - val_loss: 0.1647 - val_acc: 0.9588\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9872\n",
      "Epoch 00071: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0455 - acc: 0.9872 - val_loss: 0.1483 - val_acc: 0.9634\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9902\n",
      "Epoch 00072: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0347 - acc: 0.9902 - val_loss: 0.1758 - val_acc: 0.9539\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9873\n",
      "Epoch 00073: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0440 - acc: 0.9873 - val_loss: 0.1530 - val_acc: 0.9578\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9916\n",
      "Epoch 00074: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0298 - acc: 0.9916 - val_loss: 0.1653 - val_acc: 0.9553\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9918\n",
      "Epoch 00075: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0304 - acc: 0.9918 - val_loss: 0.1542 - val_acc: 0.9623\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9901\n",
      "Epoch 00076: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0350 - acc: 0.9901 - val_loss: 0.1715 - val_acc: 0.9548\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9897\n",
      "Epoch 00077: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0362 - acc: 0.9897 - val_loss: 0.1588 - val_acc: 0.9616\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9917\n",
      "Epoch 00078: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0304 - acc: 0.9917 - val_loss: 0.1621 - val_acc: 0.9616\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9920\n",
      "Epoch 00079: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0286 - acc: 0.9920 - val_loss: 0.1867 - val_acc: 0.9569\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9915\n",
      "Epoch 00080: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0297 - acc: 0.9916 - val_loss: 0.1600 - val_acc: 0.9581\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9917\n",
      "Epoch 00081: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0293 - acc: 0.9917 - val_loss: 0.1676 - val_acc: 0.9567\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9889\n",
      "Epoch 00082: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0387 - acc: 0.9889 - val_loss: 0.1711 - val_acc: 0.9609\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9929\n",
      "Epoch 00083: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0267 - acc: 0.9929 - val_loss: 0.1552 - val_acc: 0.9602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9913\n",
      "Epoch 00084: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0298 - acc: 0.9913 - val_loss: 0.1919 - val_acc: 0.9555\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9927\n",
      "Epoch 00085: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0270 - acc: 0.9927 - val_loss: 0.1670 - val_acc: 0.9632\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9935\n",
      "Epoch 00086: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0246 - acc: 0.9935 - val_loss: 0.1825 - val_acc: 0.9581\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9925\n",
      "Epoch 00087: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0262 - acc: 0.9925 - val_loss: 0.1691 - val_acc: 0.9592\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9933\n",
      "Epoch 00088: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0238 - acc: 0.9933 - val_loss: 0.2027 - val_acc: 0.9536\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9929\n",
      "Epoch 00089: val_loss did not improve from 0.14011\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0262 - acc: 0.9928 - val_loss: 0.1732 - val_acc: 0.9571\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 660us/sample - loss: 0.1713 - acc: 0.9489\n",
      "Loss: 0.17125582505609388 Accuracy: 0.94890964\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6859 - acc: 0.4783\n",
      "Epoch 00001: val_loss improved from inf to 1.00839, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/001-1.0084.hdf5\n",
      "36805/36805 [==============================] - 69s 2ms/sample - loss: 1.6858 - acc: 0.4784 - val_loss: 1.0084 - val_acc: 0.7421\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7639 - acc: 0.7734\n",
      "Epoch 00002: val_loss improved from 1.00839 to 0.41513, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/002-0.4151.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.7639 - acc: 0.7734 - val_loss: 0.4151 - val_acc: 0.8919\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5263 - acc: 0.8449\n",
      "Epoch 00003: val_loss improved from 0.41513 to 0.37331, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/003-0.3733.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.5263 - acc: 0.8449 - val_loss: 0.3733 - val_acc: 0.8894\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8779\n",
      "Epoch 00004: val_loss improved from 0.37331 to 0.27906, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/004-0.2791.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4173 - acc: 0.8779 - val_loss: 0.2791 - val_acc: 0.9287\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8985\n",
      "Epoch 00005: val_loss improved from 0.27906 to 0.27074, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/005-0.2707.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3516 - acc: 0.8985 - val_loss: 0.2707 - val_acc: 0.9241\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9104\n",
      "Epoch 00006: val_loss improved from 0.27074 to 0.24292, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/006-0.2429.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3066 - acc: 0.9104 - val_loss: 0.2429 - val_acc: 0.9336\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9214\n",
      "Epoch 00007: val_loss improved from 0.24292 to 0.21863, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/007-0.2186.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2675 - acc: 0.9214 - val_loss: 0.2186 - val_acc: 0.9378\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9295\n",
      "Epoch 00008: val_loss improved from 0.21863 to 0.20216, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/008-0.2022.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2413 - acc: 0.9295 - val_loss: 0.2022 - val_acc: 0.9408\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9356\n",
      "Epoch 00009: val_loss improved from 0.20216 to 0.18231, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/009-0.1823.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2238 - acc: 0.9356 - val_loss: 0.1823 - val_acc: 0.9462\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9425\n",
      "Epoch 00010: val_loss improved from 0.18231 to 0.18134, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/010-0.1813.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1974 - acc: 0.9425 - val_loss: 0.1813 - val_acc: 0.9483\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9457\n",
      "Epoch 00011: val_loss improved from 0.18134 to 0.15462, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/011-0.1546.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1847 - acc: 0.9456 - val_loss: 0.1546 - val_acc: 0.9567\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9490\n",
      "Epoch 00012: val_loss did not improve from 0.15462\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1742 - acc: 0.9490 - val_loss: 0.1622 - val_acc: 0.9553\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9510\n",
      "Epoch 00013: val_loss did not improve from 0.15462\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1625 - acc: 0.9509 - val_loss: 0.1604 - val_acc: 0.9560\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9566\n",
      "Epoch 00014: val_loss improved from 0.15462 to 0.13825, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/014-0.1383.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1460 - acc: 0.9566 - val_loss: 0.1383 - val_acc: 0.9604\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9597\n",
      "Epoch 00015: val_loss did not improve from 0.13825\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1336 - acc: 0.9597 - val_loss: 0.1489 - val_acc: 0.9562\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9608\n",
      "Epoch 00016: val_loss improved from 0.13825 to 0.13580, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/016-0.1358.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1330 - acc: 0.9608 - val_loss: 0.1358 - val_acc: 0.9602\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9638\n",
      "Epoch 00017: val_loss did not improve from 0.13580\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1210 - acc: 0.9638 - val_loss: 0.1364 - val_acc: 0.9592\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9661\n",
      "Epoch 00018: val_loss improved from 0.13580 to 0.12061, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_DO_BN_8_conv_checkpoint/018-0.1206.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1139 - acc: 0.9661 - val_loss: 0.1206 - val_acc: 0.9637\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9670\n",
      "Epoch 00019: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1072 - acc: 0.9670 - val_loss: 0.1261 - val_acc: 0.9644\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9705\n",
      "Epoch 00020: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1022 - acc: 0.9705 - val_loss: 0.1366 - val_acc: 0.9569\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9729\n",
      "Epoch 00021: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0913 - acc: 0.9729 - val_loss: 0.1369 - val_acc: 0.9592\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9730\n",
      "Epoch 00022: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0894 - acc: 0.9729 - val_loss: 0.1439 - val_acc: 0.9571\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9728\n",
      "Epoch 00023: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0904 - acc: 0.9728 - val_loss: 0.1537 - val_acc: 0.9536\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9767\n",
      "Epoch 00024: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0788 - acc: 0.9767 - val_loss: 0.1418 - val_acc: 0.9597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9788\n",
      "Epoch 00025: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0733 - acc: 0.9788 - val_loss: 0.1524 - val_acc: 0.9553\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9755\n",
      "Epoch 00026: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0820 - acc: 0.9755 - val_loss: 0.1366 - val_acc: 0.9627\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9808\n",
      "Epoch 00027: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0648 - acc: 0.9808 - val_loss: 0.1352 - val_acc: 0.9578\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9804\n",
      "Epoch 00028: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0652 - acc: 0.9804 - val_loss: 0.1339 - val_acc: 0.9611\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9818\n",
      "Epoch 00029: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0610 - acc: 0.9818 - val_loss: 0.1370 - val_acc: 0.9588\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9817\n",
      "Epoch 00030: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0615 - acc: 0.9817 - val_loss: 0.1309 - val_acc: 0.9611\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9832\n",
      "Epoch 00031: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0557 - acc: 0.9832 - val_loss: 0.1235 - val_acc: 0.9667\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9819\n",
      "Epoch 00032: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0594 - acc: 0.9819 - val_loss: 0.1301 - val_acc: 0.9620\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9860\n",
      "Epoch 00033: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0490 - acc: 0.9860 - val_loss: 0.1230 - val_acc: 0.9660\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9866\n",
      "Epoch 00034: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0476 - acc: 0.9866 - val_loss: 0.1569 - val_acc: 0.9564\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9872\n",
      "Epoch 00035: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0449 - acc: 0.9872 - val_loss: 0.1524 - val_acc: 0.9574\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9870\n",
      "Epoch 00036: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0441 - acc: 0.9870 - val_loss: 0.1589 - val_acc: 0.9576\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9879\n",
      "Epoch 00037: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0426 - acc: 0.9879 - val_loss: 0.1361 - val_acc: 0.9660\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9877\n",
      "Epoch 00038: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0418 - acc: 0.9877 - val_loss: 0.1290 - val_acc: 0.9665\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9885\n",
      "Epoch 00039: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0396 - acc: 0.9885 - val_loss: 0.1532 - val_acc: 0.9581\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9889\n",
      "Epoch 00040: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0378 - acc: 0.9889 - val_loss: 0.1487 - val_acc: 0.9585\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9904\n",
      "Epoch 00041: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0345 - acc: 0.9904 - val_loss: 0.1352 - val_acc: 0.9590\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9905\n",
      "Epoch 00042: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0340 - acc: 0.9905 - val_loss: 0.1480 - val_acc: 0.9597\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9873\n",
      "Epoch 00043: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0413 - acc: 0.9873 - val_loss: 0.1429 - val_acc: 0.9625\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9913\n",
      "Epoch 00044: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0311 - acc: 0.9913 - val_loss: 0.1538 - val_acc: 0.9592\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9915\n",
      "Epoch 00045: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0306 - acc: 0.9915 - val_loss: 0.1320 - val_acc: 0.9637\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9928\n",
      "Epoch 00046: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0263 - acc: 0.9928 - val_loss: 0.1824 - val_acc: 0.9525\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9903\n",
      "Epoch 00047: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0326 - acc: 0.9903 - val_loss: 0.1343 - val_acc: 0.9623\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9921\n",
      "Epoch 00048: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0265 - acc: 0.9921 - val_loss: 0.1657 - val_acc: 0.9569\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9925\n",
      "Epoch 00049: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0273 - acc: 0.9925 - val_loss: 0.1579 - val_acc: 0.9585\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9925\n",
      "Epoch 00050: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0264 - acc: 0.9925 - val_loss: 0.1758 - val_acc: 0.9592\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9929\n",
      "Epoch 00051: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0257 - acc: 0.9929 - val_loss: 0.1676 - val_acc: 0.9602\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9939\n",
      "Epoch 00052: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0220 - acc: 0.9939 - val_loss: 0.1929 - val_acc: 0.9513\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9942\n",
      "Epoch 00053: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0222 - acc: 0.9942 - val_loss: 0.1413 - val_acc: 0.9637\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9908\n",
      "Epoch 00054: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0307 - acc: 0.9908 - val_loss: 0.1454 - val_acc: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9943\n",
      "Epoch 00055: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0203 - acc: 0.9943 - val_loss: 0.1747 - val_acc: 0.9574\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9931\n",
      "Epoch 00056: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0234 - acc: 0.9931 - val_loss: 0.1708 - val_acc: 0.9597\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9897\n",
      "Epoch 00057: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0339 - acc: 0.9897 - val_loss: 0.1397 - val_acc: 0.9634\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9962\n",
      "Epoch 00058: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0150 - acc: 0.9962 - val_loss: 0.1753 - val_acc: 0.9590\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9952\n",
      "Epoch 00059: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0188 - acc: 0.9951 - val_loss: 0.1492 - val_acc: 0.9639\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9908\n",
      "Epoch 00060: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0303 - acc: 0.9907 - val_loss: 0.1661 - val_acc: 0.9583\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9846\n",
      "Epoch 00061: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0515 - acc: 0.9846 - val_loss: 0.1430 - val_acc: 0.9646\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9930\n",
      "Epoch 00062: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0242 - acc: 0.9930 - val_loss: 0.1428 - val_acc: 0.9660\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9974\n",
      "Epoch 00063: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0120 - acc: 0.9974 - val_loss: 0.1412 - val_acc: 0.9697\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9966\n",
      "Epoch 00064: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0143 - acc: 0.9966 - val_loss: 0.1531 - val_acc: 0.9588\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9959\n",
      "Epoch 00065: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0146 - acc: 0.9959 - val_loss: 0.1384 - val_acc: 0.9672\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9938\n",
      "Epoch 00066: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0214 - acc: 0.9938 - val_loss: 0.1514 - val_acc: 0.9637\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9962\n",
      "Epoch 00067: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0156 - acc: 0.9962 - val_loss: 0.2291 - val_acc: 0.9513\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9950\n",
      "Epoch 00068: val_loss did not improve from 0.12061\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0171 - acc: 0.9950 - val_loss: 0.2137 - val_acc: 0.9543\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 702us/sample - loss: 0.1721 - acc: 0.9508\n",
      "Loss: 0.17213183499694737 Accuracy: 0.95077884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_GAP_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 64)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 192)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3088        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 45,328\n",
      "Trainable params: 44,944\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 638us/sample - loss: 0.8655 - acc: 0.7367\n",
      "Loss: 0.8654884642901078 Accuracy: 0.7366563\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 64)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 192)          0           global_average_pooling1d_21[0][0]\n",
      "                                                                 global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 192)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           3088        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 66,128\n",
      "Trainable params: 65,616\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 674us/sample - loss: 0.5369 - acc: 0.8451\n",
      "Loss: 0.536925614486726 Accuracy: 0.8450675\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 64)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 global_average_pooling1d_25[0][0]\n",
      "                                                                 global_average_pooling1d_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 256)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           4112        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 108,752\n",
      "Trainable params: 107,984\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 712us/sample - loss: 0.3902 - acc: 0.8874\n",
      "Loss: 0.39020791636820523 Accuracy: 0.8874351\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 64)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 320)          0           global_average_pooling1d_27[0][0]\n",
      "                                                                 global_average_pooling1d_28[0][0]\n",
      "                                                                 global_average_pooling1d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 320)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           5136        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 192,336\n",
      "Trainable params: 191,312\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 744us/sample - loss: 0.2353 - acc: 0.9350\n",
      "Loss: 0.23532529091414137 Accuracy: 0.9349948\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 128)          0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Gl (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 384)          0           global_average_pooling1d_30[0][0]\n",
      "                                                                 global_average_pooling1d_31[0][0]\n",
      "                                                                 global_average_pooling1d_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 384)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           6160        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 275,920\n",
      "Trainable params: 274,640\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 762us/sample - loss: 0.1713 - acc: 0.9489\n",
      "Loss: 0.17125582505609388 Accuracy: 0.94890964\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Gl (None, 128)          0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_35 (Gl (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 384)          0           global_average_pooling1d_33[0][0]\n",
      "                                                                 global_average_pooling1d_34[0][0]\n",
      "                                                                 global_average_pooling1d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 384)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           6160        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 358,480\n",
      "Trainable params: 356,944\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 781us/sample - loss: 0.1721 - acc: 0.9508\n",
      "Loss: 0.17213183499694737 Accuracy: 0.95077884\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_GAP_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 64)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 192)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3088        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 45,328\n",
      "Trainable params: 44,944\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 760us/sample - loss: 1.2384 - acc: 0.5969\n",
      "Loss: 1.2383866399495649 Accuracy: 0.5968847\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 64)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 192)          0           global_average_pooling1d_21[0][0]\n",
      "                                                                 global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 192)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           3088        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 66,128\n",
      "Trainable params: 65,616\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 780us/sample - loss: 0.7202 - acc: 0.7796\n",
      "Loss: 0.7201832166713347 Accuracy: 0.77964693\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 64)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 global_average_pooling1d_25[0][0]\n",
      "                                                                 global_average_pooling1d_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 256)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           4112        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 108,752\n",
      "Trainable params: 107,984\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 792us/sample - loss: 0.4692 - acc: 0.8658\n",
      "Loss: 0.46918815632475497 Accuracy: 0.8658359\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 64)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 320)          0           global_average_pooling1d_27[0][0]\n",
      "                                                                 global_average_pooling1d_28[0][0]\n",
      "                                                                 global_average_pooling1d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 320)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           5136        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 192,336\n",
      "Trainable params: 191,312\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 825us/sample - loss: 0.3673 - acc: 0.9080\n",
      "Loss: 0.36726190790085284 Accuracy: 0.9079958\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 128)          0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Gl (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 384)          0           global_average_pooling1d_30[0][0]\n",
      "                                                                 global_average_pooling1d_31[0][0]\n",
      "                                                                 global_average_pooling1d_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 384)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           6160        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 275,920\n",
      "Trainable params: 274,640\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 866us/sample - loss: 0.2252 - acc: 0.9423\n",
      "Loss: 0.22522118056767454 Accuracy: 0.9422638\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Gl (None, 128)          0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_35 (Gl (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 384)          0           global_average_pooling1d_33[0][0]\n",
      "                                                                 global_average_pooling1d_34[0][0]\n",
      "                                                                 global_average_pooling1d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 384)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           6160        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 358,480\n",
      "Trainable params: 356,944\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 896us/sample - loss: 0.2710 - acc: 0.9379\n",
      "Loss: 0.2710006074556309 Accuracy: 0.9379024\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
