{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_only_conv_conv_5_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                682512    \n",
      "=================================================================\n",
      "Total params: 682,560\n",
      "Trainable params: 682,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                454672    \n",
      "=================================================================\n",
      "Total params: 455,376\n",
      "Trainable params: 455,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                302608    \n",
      "=================================================================\n",
      "Total params: 305,904\n",
      "Trainable params: 305,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                200720    \n",
      "=================================================================\n",
      "Total params: 214,320\n",
      "Trainable params: 214,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                131088    \n",
      "=================================================================\n",
      "Total params: 185,776\n",
      "Trainable params: 185,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                81936     \n",
      "=================================================================\n",
      "Total params: 300,720\n",
      "Trainable params: 300,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                49168     \n",
      "=================================================================\n",
      "Total params: 923,824\n",
      "Trainable params: 923,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 2, 1024)           2622464   \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 3,513,520\n",
      "Trainable params: 3,513,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    model = build_1d_cnn_only_conv_conv_5_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.5447 - acc: 0.1967\n",
      "Epoch 00001: val_loss improved from inf to 2.40603, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/001-2.4060.hdf5\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 2.5442 - acc: 0.1970 - val_loss: 2.4060 - val_acc: 0.2835\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.2986 - acc: 0.3091\n",
      "Epoch 00002: val_loss improved from 2.40603 to 2.26586, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/002-2.2659.hdf5\n",
      "36805/36805 [==============================] - 7s 178us/sample - loss: 2.2987 - acc: 0.3092 - val_loss: 2.2659 - val_acc: 0.3282\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1618 - acc: 0.3527\n",
      "Epoch 00003: val_loss improved from 2.26586 to 2.20813, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/003-2.2081.hdf5\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 2.1618 - acc: 0.3527 - val_loss: 2.2081 - val_acc: 0.3364\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.0672 - acc: 0.3846\n",
      "Epoch 00004: val_loss improved from 2.20813 to 2.16437, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/004-2.1644.hdf5\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 2.0673 - acc: 0.3847 - val_loss: 2.1644 - val_acc: 0.3522\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.9897 - acc: 0.4087\n",
      "Epoch 00005: val_loss improved from 2.16437 to 2.14022, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/005-2.1402.hdf5\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.9892 - acc: 0.4091 - val_loss: 2.1402 - val_acc: 0.3541\n",
      "Epoch 6/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.9261 - acc: 0.4229\n",
      "Epoch 00006: val_loss improved from 2.14022 to 2.13050, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/006-2.1305.hdf5\n",
      "36805/36805 [==============================] - 7s 178us/sample - loss: 1.9259 - acc: 0.4233 - val_loss: 2.1305 - val_acc: 0.3464\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8690 - acc: 0.4465\n",
      "Epoch 00007: val_loss improved from 2.13050 to 2.12119, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/007-2.1212.hdf5\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.8690 - acc: 0.4465 - val_loss: 2.1212 - val_acc: 0.3550\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.8171 - acc: 0.4592\n",
      "Epoch 00008: val_loss improved from 2.12119 to 2.11714, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/008-2.1171.hdf5\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.8169 - acc: 0.4592 - val_loss: 2.1171 - val_acc: 0.3531\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7708 - acc: 0.4719\n",
      "Epoch 00009: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.7711 - acc: 0.4717 - val_loss: 2.1180 - val_acc: 0.3508\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.7332 - acc: 0.4818\n",
      "Epoch 00010: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.7341 - acc: 0.4815 - val_loss: 2.1199 - val_acc: 0.3529\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6936 - acc: 0.4930\n",
      "Epoch 00011: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 177us/sample - loss: 1.6934 - acc: 0.4929 - val_loss: 2.1305 - val_acc: 0.3438\n",
      "Epoch 12/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.6607 - acc: 0.5019\n",
      "Epoch 00012: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 1.6606 - acc: 0.5019 - val_loss: 2.1405 - val_acc: 0.3352\n",
      "Epoch 13/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.6328 - acc: 0.5079\n",
      "Epoch 00013: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 177us/sample - loss: 1.6321 - acc: 0.5081 - val_loss: 2.1368 - val_acc: 0.3422\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6018 - acc: 0.5172\n",
      "Epoch 00014: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 1.6018 - acc: 0.5172 - val_loss: 2.1478 - val_acc: 0.3336\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5794 - acc: 0.5244\n",
      "Epoch 00015: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 1.5794 - acc: 0.5244 - val_loss: 2.1473 - val_acc: 0.3392\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5524 - acc: 0.5309\n",
      "Epoch 00016: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 173us/sample - loss: 1.5519 - acc: 0.5308 - val_loss: 2.1624 - val_acc: 0.3399\n",
      "Epoch 17/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.5295 - acc: 0.5376\n",
      "Epoch 00017: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 1.5292 - acc: 0.5378 - val_loss: 2.1598 - val_acc: 0.3406\n",
      "Epoch 18/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.5092 - acc: 0.5420\n",
      "Epoch 00018: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 177us/sample - loss: 1.5093 - acc: 0.5420 - val_loss: 2.1652 - val_acc: 0.3357\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4907 - acc: 0.5486\n",
      "Epoch 00019: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.4906 - acc: 0.5486 - val_loss: 2.1759 - val_acc: 0.3366\n",
      "Epoch 20/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.4680 - acc: 0.5539\n",
      "Epoch 00020: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 1.4678 - acc: 0.5540 - val_loss: 2.1738 - val_acc: 0.3385\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4503 - acc: 0.5605\n",
      "Epoch 00021: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.4503 - acc: 0.5605 - val_loss: 2.1788 - val_acc: 0.3359\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4319 - acc: 0.5626\n",
      "Epoch 00022: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 178us/sample - loss: 1.4318 - acc: 0.5627 - val_loss: 2.2010 - val_acc: 0.3308\n",
      "Epoch 23/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.4184 - acc: 0.5704\n",
      "Epoch 00023: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 178us/sample - loss: 1.4190 - acc: 0.5704 - val_loss: 2.1973 - val_acc: 0.3347\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3963 - acc: 0.5729\n",
      "Epoch 00024: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 1.3958 - acc: 0.5731 - val_loss: 2.1990 - val_acc: 0.3345\n",
      "Epoch 25/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.3869 - acc: 0.5771\n",
      "Epoch 00025: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.3869 - acc: 0.5773 - val_loss: 2.2045 - val_acc: 0.3340\n",
      "Epoch 26/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.3753 - acc: 0.5775\n",
      "Epoch 00026: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 177us/sample - loss: 1.3748 - acc: 0.5775 - val_loss: 2.2095 - val_acc: 0.3382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.3600 - acc: 0.5865\n",
      "Epoch 00027: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.3601 - acc: 0.5866 - val_loss: 2.2208 - val_acc: 0.3364\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3464 - acc: 0.5883\n",
      "Epoch 00028: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 1.3463 - acc: 0.5883 - val_loss: 2.2244 - val_acc: 0.3364\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3325 - acc: 0.5895\n",
      "Epoch 00029: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 178us/sample - loss: 1.3327 - acc: 0.5893 - val_loss: 2.2221 - val_acc: 0.3322\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3205 - acc: 0.5939\n",
      "Epoch 00030: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.3207 - acc: 0.5938 - val_loss: 2.2403 - val_acc: 0.3280\n",
      "Epoch 31/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.3097 - acc: 0.5965\n",
      "Epoch 00031: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 177us/sample - loss: 1.3098 - acc: 0.5964 - val_loss: 2.2312 - val_acc: 0.3350\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2932 - acc: 0.6029\n",
      "Epoch 00032: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.2929 - acc: 0.6029 - val_loss: 2.2407 - val_acc: 0.3282\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2825 - acc: 0.6065\n",
      "Epoch 00033: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 171us/sample - loss: 1.2827 - acc: 0.6063 - val_loss: 2.2416 - val_acc: 0.3305\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2768 - acc: 0.6099\n",
      "Epoch 00034: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 173us/sample - loss: 1.2768 - acc: 0.6099 - val_loss: 2.2448 - val_acc: 0.3289\n",
      "Epoch 35/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.2637 - acc: 0.6111\n",
      "Epoch 00035: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.2633 - acc: 0.6110 - val_loss: 2.2453 - val_acc: 0.3347\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2553 - acc: 0.6146\n",
      "Epoch 00036: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 173us/sample - loss: 1.2553 - acc: 0.6146 - val_loss: 2.2516 - val_acc: 0.3380\n",
      "Epoch 37/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2454 - acc: 0.6182\n",
      "Epoch 00037: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.2461 - acc: 0.6179 - val_loss: 2.2685 - val_acc: 0.3340\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2312 - acc: 0.6206\n",
      "Epoch 00038: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.2313 - acc: 0.6206 - val_loss: 2.2676 - val_acc: 0.3291\n",
      "Epoch 39/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2258 - acc: 0.6204\n",
      "Epoch 00039: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 170us/sample - loss: 1.2258 - acc: 0.6204 - val_loss: 2.2622 - val_acc: 0.3273\n",
      "Epoch 40/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2041 - acc: 0.6306\n",
      "Epoch 00040: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 173us/sample - loss: 1.2050 - acc: 0.6304 - val_loss: 2.2782 - val_acc: 0.3336\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2016 - acc: 0.6289\n",
      "Epoch 00041: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 178us/sample - loss: 1.2015 - acc: 0.6288 - val_loss: 2.2707 - val_acc: 0.3317\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1896 - acc: 0.6345\n",
      "Epoch 00042: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 1.1896 - acc: 0.6343 - val_loss: 2.2718 - val_acc: 0.3422\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1768 - acc: 0.6363\n",
      "Epoch 00043: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 178us/sample - loss: 1.1768 - acc: 0.6363 - val_loss: 2.2739 - val_acc: 0.3385\n",
      "Epoch 44/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.1609 - acc: 0.6442\n",
      "Epoch 00044: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 171us/sample - loss: 1.1625 - acc: 0.6436 - val_loss: 2.2745 - val_acc: 0.3385\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1582 - acc: 0.6436\n",
      "Epoch 00045: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 171us/sample - loss: 1.1581 - acc: 0.6436 - val_loss: 2.2786 - val_acc: 0.3427\n",
      "Epoch 46/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.1578 - acc: 0.6433\n",
      "Epoch 00046: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.1581 - acc: 0.6433 - val_loss: 2.2796 - val_acc: 0.3403\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1417 - acc: 0.6498\n",
      "Epoch 00047: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 180us/sample - loss: 1.1417 - acc: 0.6496 - val_loss: 2.2823 - val_acc: 0.3478\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1309 - acc: 0.6509\n",
      "Epoch 00048: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.1309 - acc: 0.6510 - val_loss: 2.2729 - val_acc: 0.3438\n",
      "Epoch 49/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.1238 - acc: 0.6512\n",
      "Epoch 00049: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 179us/sample - loss: 1.1237 - acc: 0.6512 - val_loss: 2.2911 - val_acc: 0.3482\n",
      "Epoch 50/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1120 - acc: 0.6566\n",
      "Epoch 00050: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 177us/sample - loss: 1.1118 - acc: 0.6567 - val_loss: 2.2896 - val_acc: 0.3438\n",
      "Epoch 51/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1102 - acc: 0.6555\n",
      "Epoch 00051: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 1.1097 - acc: 0.6554 - val_loss: 2.3029 - val_acc: 0.3431\n",
      "Epoch 52/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1005 - acc: 0.6594\n",
      "Epoch 00052: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 171us/sample - loss: 1.1010 - acc: 0.6593 - val_loss: 2.3020 - val_acc: 0.3447\n",
      "Epoch 53/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0959 - acc: 0.6626\n",
      "Epoch 00053: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 1.0962 - acc: 0.6624 - val_loss: 2.2978 - val_acc: 0.3487\n",
      "Epoch 54/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0809 - acc: 0.6649\n",
      "Epoch 00054: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.0811 - acc: 0.6647 - val_loss: 2.3034 - val_acc: 0.3475\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0746 - acc: 0.6657\n",
      "Epoch 00055: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.0743 - acc: 0.6659 - val_loss: 2.3053 - val_acc: 0.3501\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0695 - acc: 0.6690\n",
      "Epoch 00056: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.0695 - acc: 0.6690 - val_loss: 2.3199 - val_acc: 0.3464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.0602 - acc: 0.6703\n",
      "Epoch 00057: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 169us/sample - loss: 1.0613 - acc: 0.6699 - val_loss: 2.3128 - val_acc: 0.3543\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0571 - acc: 0.6712\n",
      "Epoch 00058: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.0575 - acc: 0.6710 - val_loss: 2.3194 - val_acc: 0.3508\n",
      "Epoch 59/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0416 - acc: 0.6750\n",
      "Epoch 00059: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 1.0421 - acc: 0.6749 - val_loss: 2.3266 - val_acc: 0.3445\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0417 - acc: 0.6746\n",
      "Epoch 00060: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.0415 - acc: 0.6746 - val_loss: 2.3353 - val_acc: 0.3438\n",
      "Epoch 61/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0261 - acc: 0.6809\n",
      "Epoch 00061: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 1.0267 - acc: 0.6807 - val_loss: 2.3390 - val_acc: 0.3508\n",
      "Epoch 62/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.0249 - acc: 0.6806\n",
      "Epoch 00062: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 1.0259 - acc: 0.6802 - val_loss: 2.3353 - val_acc: 0.3459\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0210 - acc: 0.6817\n",
      "Epoch 00063: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 1.0211 - acc: 0.6815 - val_loss: 2.3461 - val_acc: 0.3471\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0168 - acc: 0.6816\n",
      "Epoch 00064: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.0177 - acc: 0.6814 - val_loss: 2.3425 - val_acc: 0.3445\n",
      "Epoch 65/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.0093 - acc: 0.6844\n",
      "Epoch 00065: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 170us/sample - loss: 1.0098 - acc: 0.6842 - val_loss: 2.3530 - val_acc: 0.3506\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0050 - acc: 0.6868\n",
      "Epoch 00066: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 170us/sample - loss: 1.0050 - acc: 0.6868 - val_loss: 2.3473 - val_acc: 0.3545\n",
      "Epoch 67/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0008 - acc: 0.6873\n",
      "Epoch 00067: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 1.0011 - acc: 0.6872 - val_loss: 2.3559 - val_acc: 0.3485\n",
      "Epoch 68/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9950 - acc: 0.6864\n",
      "Epoch 00068: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 0.9951 - acc: 0.6863 - val_loss: 2.3640 - val_acc: 0.3536\n",
      "Epoch 69/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9855 - acc: 0.6903\n",
      "Epoch 00069: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 0.9852 - acc: 0.6906 - val_loss: 2.3629 - val_acc: 0.3487\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9813 - acc: 0.6931\n",
      "Epoch 00070: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 177us/sample - loss: 0.9814 - acc: 0.6931 - val_loss: 2.3631 - val_acc: 0.3520\n",
      "Epoch 71/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9813 - acc: 0.6924\n",
      "Epoch 00071: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 0.9819 - acc: 0.6923 - val_loss: 2.3724 - val_acc: 0.3552\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9726 - acc: 0.6943\n",
      "Epoch 00072: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 178us/sample - loss: 0.9723 - acc: 0.6944 - val_loss: 2.3718 - val_acc: 0.3536\n",
      "Epoch 73/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9640 - acc: 0.6957\n",
      "Epoch 00073: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 173us/sample - loss: 0.9638 - acc: 0.6958 - val_loss: 2.3745 - val_acc: 0.3499\n",
      "Epoch 74/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9644 - acc: 0.6943\n",
      "Epoch 00074: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 0.9646 - acc: 0.6944 - val_loss: 2.3859 - val_acc: 0.3550\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9618 - acc: 0.6965\n",
      "Epoch 00075: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 176us/sample - loss: 0.9617 - acc: 0.6965 - val_loss: 2.3849 - val_acc: 0.3571\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9558 - acc: 0.7003\n",
      "Epoch 00076: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 0.9558 - acc: 0.7003 - val_loss: 2.3942 - val_acc: 0.3569\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9480 - acc: 0.7032\n",
      "Epoch 00077: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 0.9481 - acc: 0.7032 - val_loss: 2.4012 - val_acc: 0.3494\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9492 - acc: 0.7011\n",
      "Epoch 00078: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 0.9495 - acc: 0.7011 - val_loss: 2.3998 - val_acc: 0.3513\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9379 - acc: 0.7034\n",
      "Epoch 00079: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 0.9381 - acc: 0.7033 - val_loss: 2.3979 - val_acc: 0.3527\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9395 - acc: 0.7061\n",
      "Epoch 00080: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 178us/sample - loss: 0.9393 - acc: 0.7062 - val_loss: 2.4089 - val_acc: 0.3517\n",
      "Epoch 81/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9332 - acc: 0.7081\n",
      "Epoch 00081: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 7s 178us/sample - loss: 0.9338 - acc: 0.7078 - val_loss: 2.4164 - val_acc: 0.3524\n",
      "Epoch 82/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9306 - acc: 0.7062\n",
      "Epoch 00082: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 0.9310 - acc: 0.7059 - val_loss: 2.4146 - val_acc: 0.3559\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9263 - acc: 0.7107\n",
      "Epoch 00083: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 0.9264 - acc: 0.7107 - val_loss: 2.4119 - val_acc: 0.3573\n",
      "Epoch 84/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9250 - acc: 0.7069\n",
      "Epoch 00084: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 0.9251 - acc: 0.7066 - val_loss: 2.4305 - val_acc: 0.3529\n",
      "Epoch 85/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9171 - acc: 0.7110\n",
      "Epoch 00085: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 171us/sample - loss: 0.9166 - acc: 0.7109 - val_loss: 2.4223 - val_acc: 0.3594\n",
      "Epoch 86/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9131 - acc: 0.7102\n",
      "Epoch 00086: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 0.9132 - acc: 0.7100 - val_loss: 2.4216 - val_acc: 0.3580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9105 - acc: 0.7132\n",
      "Epoch 00087: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 173us/sample - loss: 0.9102 - acc: 0.7134 - val_loss: 2.4264 - val_acc: 0.3611\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9123 - acc: 0.7126\n",
      "Epoch 00088: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 0.9121 - acc: 0.7126 - val_loss: 2.4405 - val_acc: 0.3557\n",
      "Epoch 89/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9017 - acc: 0.7127\n",
      "Epoch 00089: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 173us/sample - loss: 0.9016 - acc: 0.7128 - val_loss: 2.4465 - val_acc: 0.3545\n",
      "Epoch 90/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9037 - acc: 0.7149\n",
      "Epoch 00090: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 0.9034 - acc: 0.7149 - val_loss: 2.4565 - val_acc: 0.3545\n",
      "Epoch 91/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.8972 - acc: 0.7144\n",
      "Epoch 00091: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 0.8960 - acc: 0.7149 - val_loss: 2.4505 - val_acc: 0.3552\n",
      "Epoch 92/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 0.8963 - acc: 0.7145\n",
      "Epoch 00092: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 173us/sample - loss: 0.8965 - acc: 0.7144 - val_loss: 2.4598 - val_acc: 0.3457\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8916 - acc: 0.7142\n",
      "Epoch 00093: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 171us/sample - loss: 0.8916 - acc: 0.7142 - val_loss: 2.4616 - val_acc: 0.3527\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8853 - acc: 0.7165\n",
      "Epoch 00094: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 173us/sample - loss: 0.8853 - acc: 0.7164 - val_loss: 2.4538 - val_acc: 0.3571\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8821 - acc: 0.7201\n",
      "Epoch 00095: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 0.8822 - acc: 0.7201 - val_loss: 2.4637 - val_acc: 0.3520\n",
      "Epoch 96/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.8805 - acc: 0.7206\n",
      "Epoch 00096: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 0.8808 - acc: 0.7207 - val_loss: 2.4827 - val_acc: 0.3531\n",
      "Epoch 97/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8773 - acc: 0.7184\n",
      "Epoch 00097: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 173us/sample - loss: 0.8772 - acc: 0.7185 - val_loss: 2.4844 - val_acc: 0.3485\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8773 - acc: 0.7172\n",
      "Epoch 00098: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 0.8773 - acc: 0.7172 - val_loss: 2.4780 - val_acc: 0.3531\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8712 - acc: 0.7217\n",
      "Epoch 00099: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 168us/sample - loss: 0.8709 - acc: 0.7219 - val_loss: 2.4856 - val_acc: 0.3548\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8718 - acc: 0.7225\n",
      "Epoch 00100: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 175us/sample - loss: 0.8717 - acc: 0.7226 - val_loss: 2.4846 - val_acc: 0.3534\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8614 - acc: 0.7254\n",
      "Epoch 00101: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 168us/sample - loss: 0.8612 - acc: 0.7256 - val_loss: 2.5012 - val_acc: 0.3531\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8661 - acc: 0.7222\n",
      "Epoch 00102: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 171us/sample - loss: 0.8658 - acc: 0.7222 - val_loss: 2.4904 - val_acc: 0.3543\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8536 - acc: 0.7242\n",
      "Epoch 00103: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 168us/sample - loss: 0.8538 - acc: 0.7241 - val_loss: 2.4955 - val_acc: 0.3562\n",
      "Epoch 104/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.8576 - acc: 0.7250\n",
      "Epoch 00104: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 171us/sample - loss: 0.8578 - acc: 0.7249 - val_loss: 2.4999 - val_acc: 0.3541\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8562 - acc: 0.7259\n",
      "Epoch 00105: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 171us/sample - loss: 0.8563 - acc: 0.7259 - val_loss: 2.5083 - val_acc: 0.3583\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8508 - acc: 0.7265\n",
      "Epoch 00106: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 174us/sample - loss: 0.8509 - acc: 0.7265 - val_loss: 2.5018 - val_acc: 0.3545\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8534 - acc: 0.7256\n",
      "Epoch 00107: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 0.8532 - acc: 0.7256 - val_loss: 2.5072 - val_acc: 0.3538\n",
      "Epoch 108/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8439 - acc: 0.7315\n",
      "Epoch 00108: val_loss did not improve from 2.11714\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 0.8440 - acc: 0.7315 - val_loss: 2.5165 - val_acc: 0.3566\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 149us/sample - loss: 2.1209 - acc: 0.3283\n",
      "Loss: 2.1209438459647902 Accuracy: 0.3283489\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.5330 - acc: 0.2090\n",
      "Epoch 00001: val_loss improved from inf to 2.29690, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/001-2.2969.hdf5\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 2.5322 - acc: 0.2095 - val_loss: 2.2969 - val_acc: 0.3119\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.1500 - acc: 0.3507\n",
      "Epoch 00002: val_loss improved from 2.29690 to 2.06733, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/002-2.0673.hdf5\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 2.1498 - acc: 0.3509 - val_loss: 2.0673 - val_acc: 0.4041\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.9620 - acc: 0.4085\n",
      "Epoch 00003: val_loss improved from 2.06733 to 1.93849, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/003-1.9385.hdf5\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.9624 - acc: 0.4083 - val_loss: 1.9385 - val_acc: 0.4179\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.8397 - acc: 0.4470\n",
      "Epoch 00004: val_loss improved from 1.93849 to 1.85646, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/004-1.8565.hdf5\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.8394 - acc: 0.4470 - val_loss: 1.8565 - val_acc: 0.4507\n",
      "Epoch 5/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.7463 - acc: 0.4747\n",
      "Epoch 00005: val_loss improved from 1.85646 to 1.81936, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/005-1.8194.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.7464 - acc: 0.4747 - val_loss: 1.8194 - val_acc: 0.4458\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6731 - acc: 0.4947\n",
      "Epoch 00006: val_loss improved from 1.81936 to 1.77963, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/006-1.7796.hdf5\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.6729 - acc: 0.4947 - val_loss: 1.7796 - val_acc: 0.4603\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6120 - acc: 0.5125\n",
      "Epoch 00007: val_loss improved from 1.77963 to 1.74066, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/007-1.7407.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.6121 - acc: 0.5126 - val_loss: 1.7407 - val_acc: 0.4743\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5593 - acc: 0.5247\n",
      "Epoch 00008: val_loss improved from 1.74066 to 1.72620, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/008-1.7262.hdf5\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 1.5589 - acc: 0.5248 - val_loss: 1.7262 - val_acc: 0.4696\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5148 - acc: 0.5362\n",
      "Epoch 00009: val_loss improved from 1.72620 to 1.70600, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/009-1.7060.hdf5\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.5147 - acc: 0.5362 - val_loss: 1.7060 - val_acc: 0.4738\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4744 - acc: 0.5506\n",
      "Epoch 00010: val_loss improved from 1.70600 to 1.69515, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/010-1.6952.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.4744 - acc: 0.5507 - val_loss: 1.6952 - val_acc: 0.4759\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4430 - acc: 0.5583\n",
      "Epoch 00011: val_loss improved from 1.69515 to 1.67962, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/011-1.6796.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.4434 - acc: 0.5582 - val_loss: 1.6796 - val_acc: 0.4738\n",
      "Epoch 12/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.4064 - acc: 0.5686\n",
      "Epoch 00012: val_loss improved from 1.67962 to 1.67123, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/012-1.6712.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.4059 - acc: 0.5688 - val_loss: 1.6712 - val_acc: 0.4850\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3794 - acc: 0.5763\n",
      "Epoch 00013: val_loss improved from 1.67123 to 1.66990, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/013-1.6699.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.3794 - acc: 0.5763 - val_loss: 1.6699 - val_acc: 0.4852\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3448 - acc: 0.5844\n",
      "Epoch 00014: val_loss improved from 1.66990 to 1.66299, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/014-1.6630.hdf5\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.3450 - acc: 0.5842 - val_loss: 1.6630 - val_acc: 0.4801\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3287 - acc: 0.5883\n",
      "Epoch 00015: val_loss did not improve from 1.66299\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.3287 - acc: 0.5883 - val_loss: 1.6631 - val_acc: 0.4840\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3028 - acc: 0.5950\n",
      "Epoch 00016: val_loss improved from 1.66299 to 1.64713, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/016-1.6471.hdf5\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 1.3030 - acc: 0.5949 - val_loss: 1.6471 - val_acc: 0.4859\n",
      "Epoch 17/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2807 - acc: 0.6070\n",
      "Epoch 00017: val_loss improved from 1.64713 to 1.63796, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/017-1.6380.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.2808 - acc: 0.6068 - val_loss: 1.6380 - val_acc: 0.4917\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2628 - acc: 0.6077\n",
      "Epoch 00018: val_loss did not improve from 1.63796\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.2626 - acc: 0.6076 - val_loss: 1.6418 - val_acc: 0.4903\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2380 - acc: 0.6191\n",
      "Epoch 00019: val_loss did not improve from 1.63796\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.2378 - acc: 0.6190 - val_loss: 1.6473 - val_acc: 0.4875\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2285 - acc: 0.6192\n",
      "Epoch 00020: val_loss improved from 1.63796 to 1.63276, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/020-1.6328.hdf5\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 1.2287 - acc: 0.6192 - val_loss: 1.6328 - val_acc: 0.4861\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2096 - acc: 0.6264\n",
      "Epoch 00021: val_loss improved from 1.63276 to 1.62797, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/021-1.6280.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.2098 - acc: 0.6263 - val_loss: 1.6280 - val_acc: 0.4910\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1913 - acc: 0.6296\n",
      "Epoch 00022: val_loss did not improve from 1.62797\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.1912 - acc: 0.6295 - val_loss: 1.6295 - val_acc: 0.4978\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1768 - acc: 0.6346\n",
      "Epoch 00023: val_loss improved from 1.62797 to 1.62491, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/023-1.6249.hdf5\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.1768 - acc: 0.6346 - val_loss: 1.6249 - val_acc: 0.4908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1575 - acc: 0.6379\n",
      "Epoch 00024: val_loss did not improve from 1.62491\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.1580 - acc: 0.6379 - val_loss: 1.6269 - val_acc: 0.4976\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1458 - acc: 0.6413\n",
      "Epoch 00025: val_loss did not improve from 1.62491\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.1456 - acc: 0.6411 - val_loss: 1.6368 - val_acc: 0.4910\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1284 - acc: 0.6481\n",
      "Epoch 00026: val_loss improved from 1.62491 to 1.61861, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/026-1.6186.hdf5\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.1285 - acc: 0.6480 - val_loss: 1.6186 - val_acc: 0.5010\n",
      "Epoch 27/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1121 - acc: 0.6527\n",
      "Epoch 00027: val_loss improved from 1.61861 to 1.60628, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/027-1.6063.hdf5\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.1119 - acc: 0.6527 - val_loss: 1.6063 - val_acc: 0.5008\n",
      "Epoch 28/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1019 - acc: 0.6571\n",
      "Epoch 00028: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.1014 - acc: 0.6573 - val_loss: 1.6188 - val_acc: 0.5020\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0892 - acc: 0.6583\n",
      "Epoch 00029: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.0891 - acc: 0.6584 - val_loss: 1.6169 - val_acc: 0.5038\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0758 - acc: 0.6641\n",
      "Epoch 00030: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.0762 - acc: 0.6641 - val_loss: 1.6206 - val_acc: 0.5003\n",
      "Epoch 31/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0654 - acc: 0.6668\n",
      "Epoch 00031: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.0658 - acc: 0.6666 - val_loss: 1.6069 - val_acc: 0.5090\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0547 - acc: 0.6701\n",
      "Epoch 00032: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.0545 - acc: 0.6702 - val_loss: 1.6178 - val_acc: 0.5062\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0393 - acc: 0.6734\n",
      "Epoch 00033: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.0398 - acc: 0.6732 - val_loss: 1.6203 - val_acc: 0.5059\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0347 - acc: 0.6756\n",
      "Epoch 00034: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.0344 - acc: 0.6756 - val_loss: 1.6323 - val_acc: 0.4976\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0177 - acc: 0.6799\n",
      "Epoch 00035: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 1.0179 - acc: 0.6798 - val_loss: 1.6236 - val_acc: 0.5048\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0096 - acc: 0.6819\n",
      "Epoch 00036: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.0099 - acc: 0.6819 - val_loss: 1.6130 - val_acc: 0.5125\n",
      "Epoch 37/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9959 - acc: 0.6843\n",
      "Epoch 00037: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.9961 - acc: 0.6841 - val_loss: 1.6137 - val_acc: 0.5148\n",
      "Epoch 38/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9876 - acc: 0.6873\n",
      "Epoch 00038: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.9869 - acc: 0.6873 - val_loss: 1.6065 - val_acc: 0.5101\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9786 - acc: 0.6944\n",
      "Epoch 00039: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.9788 - acc: 0.6944 - val_loss: 1.6088 - val_acc: 0.5106\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9686 - acc: 0.6912\n",
      "Epoch 00040: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.9683 - acc: 0.6914 - val_loss: 1.6180 - val_acc: 0.5113\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9576 - acc: 0.6999\n",
      "Epoch 00041: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.9575 - acc: 0.7000 - val_loss: 1.6152 - val_acc: 0.5097\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9569 - acc: 0.6950\n",
      "Epoch 00042: val_loss did not improve from 1.60628\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.9568 - acc: 0.6949 - val_loss: 1.6085 - val_acc: 0.5176\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9451 - acc: 0.7008\n",
      "Epoch 00043: val_loss improved from 1.60628 to 1.60045, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/043-1.6004.hdf5\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.9447 - acc: 0.7009 - val_loss: 1.6004 - val_acc: 0.5185\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9358 - acc: 0.7048\n",
      "Epoch 00044: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.9354 - acc: 0.7049 - val_loss: 1.6093 - val_acc: 0.5176\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9238 - acc: 0.7055\n",
      "Epoch 00045: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.9240 - acc: 0.7053 - val_loss: 1.6042 - val_acc: 0.5208\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9146 - acc: 0.7089\n",
      "Epoch 00046: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.9145 - acc: 0.7089 - val_loss: 1.6129 - val_acc: 0.5148\n",
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9078 - acc: 0.7113\n",
      "Epoch 00047: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.9079 - acc: 0.7113 - val_loss: 1.6039 - val_acc: 0.5278\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8940 - acc: 0.7184\n",
      "Epoch 00048: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.8942 - acc: 0.7184 - val_loss: 1.6158 - val_acc: 0.5197\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8896 - acc: 0.7155\n",
      "Epoch 00049: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.8895 - acc: 0.7155 - val_loss: 1.6094 - val_acc: 0.5262\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8811 - acc: 0.7216\n",
      "Epoch 00050: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.8810 - acc: 0.7217 - val_loss: 1.6093 - val_acc: 0.5276\n",
      "Epoch 51/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8788 - acc: 0.7182\n",
      "Epoch 00051: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.8793 - acc: 0.7178 - val_loss: 1.6138 - val_acc: 0.5232\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8723 - acc: 0.7220\n",
      "Epoch 00052: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.8722 - acc: 0.7221 - val_loss: 1.6141 - val_acc: 0.5311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8586 - acc: 0.7249\n",
      "Epoch 00053: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.8585 - acc: 0.7250 - val_loss: 1.6074 - val_acc: 0.5260\n",
      "Epoch 54/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8622 - acc: 0.7251\n",
      "Epoch 00054: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.8629 - acc: 0.7251 - val_loss: 1.6173 - val_acc: 0.5278\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8510 - acc: 0.7256\n",
      "Epoch 00055: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.8511 - acc: 0.7255 - val_loss: 1.6103 - val_acc: 0.5320\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8401 - acc: 0.7345\n",
      "Epoch 00056: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.8401 - acc: 0.7345 - val_loss: 1.6314 - val_acc: 0.5232\n",
      "Epoch 57/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8355 - acc: 0.7340\n",
      "Epoch 00057: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8352 - acc: 0.7341 - val_loss: 1.6015 - val_acc: 0.5323\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8262 - acc: 0.7360\n",
      "Epoch 00058: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.8260 - acc: 0.7360 - val_loss: 1.6263 - val_acc: 0.5355\n",
      "Epoch 59/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8245 - acc: 0.7375\n",
      "Epoch 00059: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.8246 - acc: 0.7375 - val_loss: 1.6114 - val_acc: 0.5411\n",
      "Epoch 60/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8172 - acc: 0.7388\n",
      "Epoch 00060: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.8169 - acc: 0.7389 - val_loss: 1.6156 - val_acc: 0.5306\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8115 - acc: 0.7395\n",
      "Epoch 00061: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.8119 - acc: 0.7394 - val_loss: 1.6177 - val_acc: 0.5344\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8085 - acc: 0.7414\n",
      "Epoch 00062: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.8082 - acc: 0.7416 - val_loss: 1.6235 - val_acc: 0.5264\n",
      "Epoch 63/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7970 - acc: 0.7443\n",
      "Epoch 00063: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.7969 - acc: 0.7442 - val_loss: 1.6238 - val_acc: 0.5330\n",
      "Epoch 64/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7914 - acc: 0.7468\n",
      "Epoch 00064: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7914 - acc: 0.7467 - val_loss: 1.6124 - val_acc: 0.5455\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7879 - acc: 0.7482\n",
      "Epoch 00065: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.7880 - acc: 0.7482 - val_loss: 1.6178 - val_acc: 0.5404\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7842 - acc: 0.7472\n",
      "Epoch 00066: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.7841 - acc: 0.7472 - val_loss: 1.6187 - val_acc: 0.5362\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7799 - acc: 0.7510\n",
      "Epoch 00067: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.7799 - acc: 0.7510 - val_loss: 1.6366 - val_acc: 0.5332\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7702 - acc: 0.7533\n",
      "Epoch 00068: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.7704 - acc: 0.7533 - val_loss: 1.6160 - val_acc: 0.5416\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7649 - acc: 0.7540\n",
      "Epoch 00069: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.7652 - acc: 0.7540 - val_loss: 1.6156 - val_acc: 0.5458\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7612 - acc: 0.7573\n",
      "Epoch 00070: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.7612 - acc: 0.7573 - val_loss: 1.6154 - val_acc: 0.5369\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7517 - acc: 0.7562\n",
      "Epoch 00071: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.7517 - acc: 0.7563 - val_loss: 1.6242 - val_acc: 0.5411\n",
      "Epoch 72/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7516 - acc: 0.7567\n",
      "Epoch 00072: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.7511 - acc: 0.7568 - val_loss: 1.6163 - val_acc: 0.5444\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7465 - acc: 0.7577\n",
      "Epoch 00073: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.7467 - acc: 0.7576 - val_loss: 1.6140 - val_acc: 0.5479\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7412 - acc: 0.7598\n",
      "Epoch 00074: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.7411 - acc: 0.7598 - val_loss: 1.6152 - val_acc: 0.5423\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7323 - acc: 0.7640\n",
      "Epoch 00075: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.7323 - acc: 0.7641 - val_loss: 1.6323 - val_acc: 0.5381\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7339 - acc: 0.7611\n",
      "Epoch 00076: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.7339 - acc: 0.7611 - val_loss: 1.6236 - val_acc: 0.5413\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7297 - acc: 0.7656\n",
      "Epoch 00077: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7300 - acc: 0.7656 - val_loss: 1.6266 - val_acc: 0.5448\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7249 - acc: 0.7672\n",
      "Epoch 00078: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.7249 - acc: 0.7672 - val_loss: 1.6267 - val_acc: 0.5437\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7156 - acc: 0.7704\n",
      "Epoch 00079: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.7160 - acc: 0.7702 - val_loss: 1.6206 - val_acc: 0.5425\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7088 - acc: 0.7726\n",
      "Epoch 00080: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.7088 - acc: 0.7726 - val_loss: 1.6281 - val_acc: 0.5460\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7022 - acc: 0.7740\n",
      "Epoch 00081: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.7022 - acc: 0.7740 - val_loss: 1.6226 - val_acc: 0.5474\n",
      "Epoch 82/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.7774\n",
      "Epoch 00082: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.6924 - acc: 0.7774 - val_loss: 1.6529 - val_acc: 0.5372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7064 - acc: 0.7709\n",
      "Epoch 00083: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7063 - acc: 0.7709 - val_loss: 1.6412 - val_acc: 0.5458\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6910 - acc: 0.7771\n",
      "Epoch 00084: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.6916 - acc: 0.7767 - val_loss: 1.6383 - val_acc: 0.5444\n",
      "Epoch 85/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6906 - acc: 0.7755\n",
      "Epoch 00085: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.6903 - acc: 0.7757 - val_loss: 1.6412 - val_acc: 0.5476\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6875 - acc: 0.7771\n",
      "Epoch 00086: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.6876 - acc: 0.7771 - val_loss: 1.6327 - val_acc: 0.5460\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6790 - acc: 0.7807\n",
      "Epoch 00087: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.6788 - acc: 0.7808 - val_loss: 1.6391 - val_acc: 0.5455\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6818 - acc: 0.7795\n",
      "Epoch 00088: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.6817 - acc: 0.7795 - val_loss: 1.6269 - val_acc: 0.5563\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6759 - acc: 0.7809\n",
      "Epoch 00089: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.6761 - acc: 0.7809 - val_loss: 1.6300 - val_acc: 0.5453\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6681 - acc: 0.7815\n",
      "Epoch 00090: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.6679 - acc: 0.7816 - val_loss: 1.6277 - val_acc: 0.5486\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6599 - acc: 0.7863\n",
      "Epoch 00091: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.6597 - acc: 0.7865 - val_loss: 1.6316 - val_acc: 0.5481\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6646 - acc: 0.7856\n",
      "Epoch 00092: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.6645 - acc: 0.7857 - val_loss: 1.6408 - val_acc: 0.5530\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6564 - acc: 0.7881\n",
      "Epoch 00093: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.6564 - acc: 0.7880 - val_loss: 1.6457 - val_acc: 0.5532\n",
      "Epoch 94/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6539 - acc: 0.7862\n",
      "Epoch 00094: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.6536 - acc: 0.7864 - val_loss: 1.6354 - val_acc: 0.5549\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6534 - acc: 0.7842\n",
      "Epoch 00095: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.6543 - acc: 0.7842 - val_loss: 1.6374 - val_acc: 0.5525\n",
      "Epoch 96/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6405 - acc: 0.7936\n",
      "Epoch 00096: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.6412 - acc: 0.7933 - val_loss: 1.6404 - val_acc: 0.5502\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6474 - acc: 0.7908\n",
      "Epoch 00097: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6472 - acc: 0.7909 - val_loss: 1.6568 - val_acc: 0.5495\n",
      "Epoch 98/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6401 - acc: 0.7913\n",
      "Epoch 00098: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6401 - acc: 0.7913 - val_loss: 1.6343 - val_acc: 0.5574\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6293 - acc: 0.7960\n",
      "Epoch 00099: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.6293 - acc: 0.7960 - val_loss: 1.6438 - val_acc: 0.5535\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6338 - acc: 0.7951\n",
      "Epoch 00100: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.6336 - acc: 0.7950 - val_loss: 1.6462 - val_acc: 0.5551\n",
      "Epoch 101/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6240 - acc: 0.7982\n",
      "Epoch 00101: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.6236 - acc: 0.7983 - val_loss: 1.6525 - val_acc: 0.5525\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6207 - acc: 0.7989\n",
      "Epoch 00102: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.6207 - acc: 0.7989 - val_loss: 1.6531 - val_acc: 0.5490\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6211 - acc: 0.7972\n",
      "Epoch 00103: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6212 - acc: 0.7973 - val_loss: 1.6502 - val_acc: 0.5558\n",
      "Epoch 104/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6152 - acc: 0.8002\n",
      "Epoch 00104: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.6149 - acc: 0.8003 - val_loss: 1.6582 - val_acc: 0.5525\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6064 - acc: 0.8013\n",
      "Epoch 00105: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6066 - acc: 0.8010 - val_loss: 1.6480 - val_acc: 0.5593\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6095 - acc: 0.8003\n",
      "Epoch 00106: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.6095 - acc: 0.8003 - val_loss: 1.6558 - val_acc: 0.5532\n",
      "Epoch 107/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.8025\n",
      "Epoch 00107: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6103 - acc: 0.8026 - val_loss: 1.6621 - val_acc: 0.5570\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.8019\n",
      "Epoch 00108: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.6033 - acc: 0.8019 - val_loss: 1.6691 - val_acc: 0.5539\n",
      "Epoch 109/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5985 - acc: 0.8052\n",
      "Epoch 00109: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5984 - acc: 0.8052 - val_loss: 1.6428 - val_acc: 0.5602\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5972 - acc: 0.8062\n",
      "Epoch 00110: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5974 - acc: 0.8061 - val_loss: 1.6540 - val_acc: 0.5588\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5908 - acc: 0.8076\n",
      "Epoch 00111: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.5908 - acc: 0.8076 - val_loss: 1.6483 - val_acc: 0.5602\n",
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.8087\n",
      "Epoch 00112: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.5889 - acc: 0.8087 - val_loss: 1.6651 - val_acc: 0.5586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5866 - acc: 0.8083\n",
      "Epoch 00113: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.5866 - acc: 0.8084 - val_loss: 1.6500 - val_acc: 0.5614\n",
      "Epoch 114/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5816 - acc: 0.8100\n",
      "Epoch 00114: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.5819 - acc: 0.8098 - val_loss: 1.6641 - val_acc: 0.5590\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.8119\n",
      "Epoch 00115: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5774 - acc: 0.8121 - val_loss: 1.6702 - val_acc: 0.5567\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.8102\n",
      "Epoch 00116: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5785 - acc: 0.8103 - val_loss: 1.6628 - val_acc: 0.5609\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5678 - acc: 0.8159\n",
      "Epoch 00117: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.5682 - acc: 0.8158 - val_loss: 1.6682 - val_acc: 0.5604\n",
      "Epoch 118/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.8141\n",
      "Epoch 00118: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5657 - acc: 0.8142 - val_loss: 1.6782 - val_acc: 0.5584\n",
      "Epoch 119/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5646 - acc: 0.8142\n",
      "Epoch 00119: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5652 - acc: 0.8140 - val_loss: 1.6739 - val_acc: 0.5625\n",
      "Epoch 120/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5612 - acc: 0.8173\n",
      "Epoch 00120: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.5612 - acc: 0.8172 - val_loss: 1.6693 - val_acc: 0.5604\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5627 - acc: 0.8155\n",
      "Epoch 00121: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5627 - acc: 0.8155 - val_loss: 1.6962 - val_acc: 0.5518\n",
      "Epoch 122/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5564 - acc: 0.8177\n",
      "Epoch 00122: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5564 - acc: 0.8177 - val_loss: 1.6639 - val_acc: 0.5632\n",
      "Epoch 123/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.8176\n",
      "Epoch 00123: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5542 - acc: 0.8174 - val_loss: 1.6797 - val_acc: 0.5597\n",
      "Epoch 124/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.8196\n",
      "Epoch 00124: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5494 - acc: 0.8195 - val_loss: 1.6831 - val_acc: 0.5581\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5464 - acc: 0.8210\n",
      "Epoch 00125: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5465 - acc: 0.8209 - val_loss: 1.6724 - val_acc: 0.5639\n",
      "Epoch 126/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.8210\n",
      "Epoch 00126: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5477 - acc: 0.8210 - val_loss: 1.6846 - val_acc: 0.5584\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5419 - acc: 0.8199\n",
      "Epoch 00127: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.5420 - acc: 0.8199 - val_loss: 1.6881 - val_acc: 0.5628\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.8215\n",
      "Epoch 00128: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.5429 - acc: 0.8214 - val_loss: 1.6874 - val_acc: 0.5609\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8263\n",
      "Epoch 00129: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.5347 - acc: 0.8262 - val_loss: 1.6802 - val_acc: 0.5632\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.8262\n",
      "Epoch 00130: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.5359 - acc: 0.8262 - val_loss: 1.6769 - val_acc: 0.5646\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5357 - acc: 0.8282\n",
      "Epoch 00131: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.5359 - acc: 0.8281 - val_loss: 1.6839 - val_acc: 0.5618\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.8253\n",
      "Epoch 00132: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.5351 - acc: 0.8253 - val_loss: 1.6801 - val_acc: 0.5660\n",
      "Epoch 133/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.8276\n",
      "Epoch 00133: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.5305 - acc: 0.8277 - val_loss: 1.6886 - val_acc: 0.5677\n",
      "Epoch 134/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.8288\n",
      "Epoch 00134: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.5237 - acc: 0.8289 - val_loss: 1.6923 - val_acc: 0.5656\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5206 - acc: 0.8298\n",
      "Epoch 00135: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.5206 - acc: 0.8298 - val_loss: 1.6812 - val_acc: 0.5616\n",
      "Epoch 136/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.8342\n",
      "Epoch 00136: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5162 - acc: 0.8341 - val_loss: 1.6940 - val_acc: 0.5667\n",
      "Epoch 137/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.8304\n",
      "Epoch 00137: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5188 - acc: 0.8303 - val_loss: 1.6935 - val_acc: 0.5616\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5185 - acc: 0.8324\n",
      "Epoch 00138: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.5185 - acc: 0.8324 - val_loss: 1.6869 - val_acc: 0.5663\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5073 - acc: 0.8346\n",
      "Epoch 00139: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5073 - acc: 0.8346 - val_loss: 1.7165 - val_acc: 0.5607\n",
      "Epoch 140/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.8332\n",
      "Epoch 00140: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5063 - acc: 0.8335 - val_loss: 1.6902 - val_acc: 0.5691\n",
      "Epoch 141/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.8363\n",
      "Epoch 00141: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.5045 - acc: 0.8360 - val_loss: 1.6999 - val_acc: 0.5649\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.8348\n",
      "Epoch 00142: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5040 - acc: 0.8350 - val_loss: 1.6989 - val_acc: 0.5667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4981 - acc: 0.8366\n",
      "Epoch 00143: val_loss did not improve from 1.60045\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.4977 - acc: 0.8368 - val_loss: 1.6981 - val_acc: 0.5672\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 187us/sample - loss: 1.6641 - acc: 0.5016\n",
      "Loss: 1.6641123400546556 Accuracy: 0.50155765\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 2.4590 - acc: 0.2213\n",
      "Epoch 00001: val_loss improved from inf to 2.11647, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/001-2.1165.hdf5\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 2.4575 - acc: 0.2220 - val_loss: 2.1165 - val_acc: 0.3776\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.9595 - acc: 0.3946\n",
      "Epoch 00002: val_loss improved from 2.11647 to 1.81496, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/002-1.8150.hdf5\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 1.9590 - acc: 0.3950 - val_loss: 1.8150 - val_acc: 0.4568\n",
      "Epoch 3/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.7542 - acc: 0.4573\n",
      "Epoch 00003: val_loss improved from 1.81496 to 1.68593, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/003-1.6859.hdf5\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 1.7541 - acc: 0.4572 - val_loss: 1.6859 - val_acc: 0.4994\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6461 - acc: 0.4870\n",
      "Epoch 00004: val_loss improved from 1.68593 to 1.60142, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/004-1.6014.hdf5\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 1.6465 - acc: 0.4867 - val_loss: 1.6014 - val_acc: 0.5192\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5722 - acc: 0.5143\n",
      "Epoch 00005: val_loss improved from 1.60142 to 1.55387, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/005-1.5539.hdf5\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 1.5725 - acc: 0.5141 - val_loss: 1.5539 - val_acc: 0.5236\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5134 - acc: 0.5341\n",
      "Epoch 00006: val_loss improved from 1.55387 to 1.52640, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/006-1.5264.hdf5\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 1.5132 - acc: 0.5342 - val_loss: 1.5264 - val_acc: 0.5327\n",
      "Epoch 7/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.4623 - acc: 0.5465\n",
      "Epoch 00007: val_loss improved from 1.52640 to 1.47050, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/007-1.4705.hdf5\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 1.4625 - acc: 0.5465 - val_loss: 1.4705 - val_acc: 0.5523\n",
      "Epoch 8/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4189 - acc: 0.5637\n",
      "Epoch 00008: val_loss improved from 1.47050 to 1.45741, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/008-1.4574.hdf5\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 1.4186 - acc: 0.5638 - val_loss: 1.4574 - val_acc: 0.5570\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3846 - acc: 0.5726\n",
      "Epoch 00009: val_loss improved from 1.45741 to 1.42999, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/009-1.4300.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 1.3844 - acc: 0.5727 - val_loss: 1.4300 - val_acc: 0.5702\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3506 - acc: 0.5860\n",
      "Epoch 00010: val_loss improved from 1.42999 to 1.40839, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/010-1.4084.hdf5\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 1.3500 - acc: 0.5862 - val_loss: 1.4084 - val_acc: 0.5763\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3139 - acc: 0.5960\n",
      "Epoch 00011: val_loss improved from 1.40839 to 1.38578, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/011-1.3858.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 1.3143 - acc: 0.5958 - val_loss: 1.3858 - val_acc: 0.5761\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2834 - acc: 0.6085\n",
      "Epoch 00012: val_loss did not improve from 1.38578\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 1.2830 - acc: 0.6086 - val_loss: 1.3898 - val_acc: 0.5823\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2549 - acc: 0.6180\n",
      "Epoch 00013: val_loss improved from 1.38578 to 1.35084, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/013-1.3508.hdf5\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 1.2545 - acc: 0.6181 - val_loss: 1.3508 - val_acc: 0.5921\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2243 - acc: 0.6277\n",
      "Epoch 00014: val_loss improved from 1.35084 to 1.33820, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/014-1.3382.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 1.2241 - acc: 0.6278 - val_loss: 1.3382 - val_acc: 0.5914\n",
      "Epoch 15/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1965 - acc: 0.6345\n",
      "Epoch 00015: val_loss improved from 1.33820 to 1.31321, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/015-1.3132.hdf5\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 1.1965 - acc: 0.6346 - val_loss: 1.3132 - val_acc: 0.5982\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1746 - acc: 0.6411\n",
      "Epoch 00016: val_loss did not improve from 1.31321\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 1.1752 - acc: 0.6409 - val_loss: 1.3192 - val_acc: 0.5980\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1531 - acc: 0.6512\n",
      "Epoch 00017: val_loss did not improve from 1.31321\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 1.1530 - acc: 0.6514 - val_loss: 1.3212 - val_acc: 0.5917\n",
      "Epoch 18/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1267 - acc: 0.6580\n",
      "Epoch 00018: val_loss improved from 1.31321 to 1.29196, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/018-1.2920.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 1.1276 - acc: 0.6576 - val_loss: 1.2920 - val_acc: 0.6059\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1036 - acc: 0.6646\n",
      "Epoch 00019: val_loss improved from 1.29196 to 1.27666, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/019-1.2767.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 1.1039 - acc: 0.6644 - val_loss: 1.2767 - val_acc: 0.6101\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0807 - acc: 0.6696\n",
      "Epoch 00020: val_loss did not improve from 1.27666\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 1.0808 - acc: 0.6698 - val_loss: 1.2827 - val_acc: 0.6070\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0613 - acc: 0.6770\n",
      "Epoch 00021: val_loss improved from 1.27666 to 1.26043, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/021-1.2604.hdf5\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 1.0615 - acc: 0.6770 - val_loss: 1.2604 - val_acc: 0.6124\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0437 - acc: 0.6835\n",
      "Epoch 00022: val_loss improved from 1.26043 to 1.25980, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/022-1.2598.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 1.0436 - acc: 0.6833 - val_loss: 1.2598 - val_acc: 0.6129\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0211 - acc: 0.6901\n",
      "Epoch 00023: val_loss improved from 1.25980 to 1.24081, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/023-1.2408.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 1.0216 - acc: 0.6900 - val_loss: 1.2408 - val_acc: 0.6171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0018 - acc: 0.6938\n",
      "Epoch 00024: val_loss improved from 1.24081 to 1.23514, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/024-1.2351.hdf5\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 1.0019 - acc: 0.6938 - val_loss: 1.2351 - val_acc: 0.6203\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9899 - acc: 0.6990\n",
      "Epoch 00025: val_loss did not improve from 1.23514\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.9897 - acc: 0.6991 - val_loss: 1.2427 - val_acc: 0.6189\n",
      "Epoch 26/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9649 - acc: 0.7053\n",
      "Epoch 00026: val_loss improved from 1.23514 to 1.22588, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/026-1.2259.hdf5\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.9652 - acc: 0.7053 - val_loss: 1.2259 - val_acc: 0.6224\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9504 - acc: 0.7110\n",
      "Epoch 00027: val_loss did not improve from 1.22588\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.9508 - acc: 0.7109 - val_loss: 1.2259 - val_acc: 0.6257\n",
      "Epoch 28/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9364 - acc: 0.7132\n",
      "Epoch 00028: val_loss did not improve from 1.22588\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.9362 - acc: 0.7133 - val_loss: 1.2290 - val_acc: 0.6226\n",
      "Epoch 29/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9150 - acc: 0.7239\n",
      "Epoch 00029: val_loss improved from 1.22588 to 1.21259, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/029-1.2126.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.9152 - acc: 0.7237 - val_loss: 1.2126 - val_acc: 0.6362\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9052 - acc: 0.7239\n",
      "Epoch 00030: val_loss improved from 1.21259 to 1.21241, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/030-1.2124.hdf5\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.9052 - acc: 0.7238 - val_loss: 1.2124 - val_acc: 0.6334\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8867 - acc: 0.7295\n",
      "Epoch 00031: val_loss improved from 1.21241 to 1.21133, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/031-1.2113.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.8861 - acc: 0.7296 - val_loss: 1.2113 - val_acc: 0.6331\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8741 - acc: 0.7353\n",
      "Epoch 00032: val_loss did not improve from 1.21133\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.8737 - acc: 0.7352 - val_loss: 1.2168 - val_acc: 0.6327\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8588 - acc: 0.7356\n",
      "Epoch 00033: val_loss improved from 1.21133 to 1.20758, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/033-1.2076.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.8590 - acc: 0.7354 - val_loss: 1.2076 - val_acc: 0.6362\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8484 - acc: 0.7402\n",
      "Epoch 00034: val_loss did not improve from 1.20758\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.8482 - acc: 0.7402 - val_loss: 1.2157 - val_acc: 0.6254\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8323 - acc: 0.7465\n",
      "Epoch 00035: val_loss improved from 1.20758 to 1.18385, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/035-1.1838.hdf5\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.8325 - acc: 0.7463 - val_loss: 1.1838 - val_acc: 0.6403\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8221 - acc: 0.7465\n",
      "Epoch 00036: val_loss did not improve from 1.18385\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.8224 - acc: 0.7463 - val_loss: 1.1950 - val_acc: 0.6350\n",
      "Epoch 37/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8087 - acc: 0.7539\n",
      "Epoch 00037: val_loss improved from 1.18385 to 1.17490, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/037-1.1749.hdf5\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.8089 - acc: 0.7536 - val_loss: 1.1749 - val_acc: 0.6396\n",
      "Epoch 38/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7928 - acc: 0.7549\n",
      "Epoch 00038: val_loss did not improve from 1.17490\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.7929 - acc: 0.7549 - val_loss: 1.1833 - val_acc: 0.6420\n",
      "Epoch 39/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7814 - acc: 0.7585\n",
      "Epoch 00039: val_loss did not improve from 1.17490\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.7814 - acc: 0.7583 - val_loss: 1.1813 - val_acc: 0.6378\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7687 - acc: 0.7628\n",
      "Epoch 00040: val_loss improved from 1.17490 to 1.17267, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/040-1.1727.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.7686 - acc: 0.7627 - val_loss: 1.1727 - val_acc: 0.6436\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7582 - acc: 0.7656\n",
      "Epoch 00041: val_loss improved from 1.17267 to 1.16741, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/041-1.1674.hdf5\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.7580 - acc: 0.7657 - val_loss: 1.1674 - val_acc: 0.6462\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7439 - acc: 0.7694\n",
      "Epoch 00042: val_loss did not improve from 1.16741\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.7439 - acc: 0.7694 - val_loss: 1.1757 - val_acc: 0.6441\n",
      "Epoch 43/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7415 - acc: 0.7696\n",
      "Epoch 00043: val_loss did not improve from 1.16741\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.7415 - acc: 0.7696 - val_loss: 1.1792 - val_acc: 0.6406\n",
      "Epoch 44/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7234 - acc: 0.7769\n",
      "Epoch 00044: val_loss did not improve from 1.16741\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.7240 - acc: 0.7768 - val_loss: 1.1739 - val_acc: 0.6473\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7168 - acc: 0.7782\n",
      "Epoch 00045: val_loss improved from 1.16741 to 1.16311, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/045-1.1631.hdf5\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.7167 - acc: 0.7782 - val_loss: 1.1631 - val_acc: 0.6513\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7013 - acc: 0.7836\n",
      "Epoch 00046: val_loss did not improve from 1.16311\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.7013 - acc: 0.7836 - val_loss: 1.1723 - val_acc: 0.6497\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6974 - acc: 0.7833\n",
      "Epoch 00047: val_loss improved from 1.16311 to 1.15119, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/047-1.1512.hdf5\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.6972 - acc: 0.7833 - val_loss: 1.1512 - val_acc: 0.6580\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6830 - acc: 0.7877\n",
      "Epoch 00048: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.6830 - acc: 0.7877 - val_loss: 1.1579 - val_acc: 0.6527\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6700 - acc: 0.7919\n",
      "Epoch 00049: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.6700 - acc: 0.7920 - val_loss: 1.1623 - val_acc: 0.6553\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6731 - acc: 0.7897\n",
      "Epoch 00050: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.6732 - acc: 0.7896 - val_loss: 1.1602 - val_acc: 0.6515\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6530 - acc: 0.7967\n",
      "Epoch 00051: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.6531 - acc: 0.7966 - val_loss: 1.1520 - val_acc: 0.6562\n",
      "Epoch 52/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6452 - acc: 0.7987\n",
      "Epoch 00052: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.6461 - acc: 0.7983 - val_loss: 1.1578 - val_acc: 0.6569\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6416 - acc: 0.8001\n",
      "Epoch 00053: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.6413 - acc: 0.8001 - val_loss: 1.1536 - val_acc: 0.6585\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6313 - acc: 0.8018\n",
      "Epoch 00054: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.6309 - acc: 0.8019 - val_loss: 1.1648 - val_acc: 0.6536\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6220 - acc: 0.8049\n",
      "Epoch 00055: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.6224 - acc: 0.8049 - val_loss: 1.1730 - val_acc: 0.6550\n",
      "Epoch 56/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6165 - acc: 0.8053\n",
      "Epoch 00056: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.6162 - acc: 0.8055 - val_loss: 1.1574 - val_acc: 0.6543\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6040 - acc: 0.8099\n",
      "Epoch 00057: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.6035 - acc: 0.8100 - val_loss: 1.1654 - val_acc: 0.6522\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6047 - acc: 0.8074\n",
      "Epoch 00058: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.6046 - acc: 0.8075 - val_loss: 1.1672 - val_acc: 0.6573\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5937 - acc: 0.8123\n",
      "Epoch 00059: val_loss did not improve from 1.15119\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.5940 - acc: 0.8122 - val_loss: 1.1553 - val_acc: 0.6601\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.8163\n",
      "Epoch 00060: val_loss improved from 1.15119 to 1.14809, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/060-1.1481.hdf5\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.5784 - acc: 0.8165 - val_loss: 1.1481 - val_acc: 0.6650\n",
      "Epoch 61/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5793 - acc: 0.8185\n",
      "Epoch 00061: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.5791 - acc: 0.8186 - val_loss: 1.1622 - val_acc: 0.6636\n",
      "Epoch 62/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.8194\n",
      "Epoch 00062: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.5700 - acc: 0.8195 - val_loss: 1.1714 - val_acc: 0.6608\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.8242\n",
      "Epoch 00063: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.5596 - acc: 0.8242 - val_loss: 1.1509 - val_acc: 0.6674\n",
      "Epoch 64/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.8232\n",
      "Epoch 00064: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.5572 - acc: 0.8233 - val_loss: 1.1571 - val_acc: 0.6667\n",
      "Epoch 65/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.8225\n",
      "Epoch 00065: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.5547 - acc: 0.8226 - val_loss: 1.1631 - val_acc: 0.6636\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.8295\n",
      "Epoch 00066: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.5430 - acc: 0.8294 - val_loss: 1.1616 - val_acc: 0.6655\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.8294\n",
      "Epoch 00067: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.5388 - acc: 0.8293 - val_loss: 1.1526 - val_acc: 0.6711\n",
      "Epoch 68/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5285 - acc: 0.8333\n",
      "Epoch 00068: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.5281 - acc: 0.8335 - val_loss: 1.1710 - val_acc: 0.6655\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.8336\n",
      "Epoch 00069: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.5223 - acc: 0.8336 - val_loss: 1.1676 - val_acc: 0.6639\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.8318\n",
      "Epoch 00070: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.5270 - acc: 0.8318 - val_loss: 1.1606 - val_acc: 0.6706\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5166 - acc: 0.8366\n",
      "Epoch 00071: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.5165 - acc: 0.8367 - val_loss: 1.1555 - val_acc: 0.6737\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5119 - acc: 0.8381\n",
      "Epoch 00072: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.5122 - acc: 0.8379 - val_loss: 1.1621 - val_acc: 0.6671\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.8409\n",
      "Epoch 00073: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.5011 - acc: 0.8406 - val_loss: 1.1662 - val_acc: 0.6727\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4979 - acc: 0.8398\n",
      "Epoch 00074: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.4982 - acc: 0.8397 - val_loss: 1.1704 - val_acc: 0.6674\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4975 - acc: 0.8412\n",
      "Epoch 00075: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.4977 - acc: 0.8411 - val_loss: 1.1610 - val_acc: 0.6730\n",
      "Epoch 76/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4897 - acc: 0.8435\n",
      "Epoch 00076: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.4897 - acc: 0.8435 - val_loss: 1.1747 - val_acc: 0.6746\n",
      "Epoch 77/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4896 - acc: 0.8441\n",
      "Epoch 00077: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.4895 - acc: 0.8441 - val_loss: 1.1717 - val_acc: 0.6758\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4780 - acc: 0.8468\n",
      "Epoch 00078: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.4782 - acc: 0.8467 - val_loss: 1.1603 - val_acc: 0.6748\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4716 - acc: 0.8485\n",
      "Epoch 00079: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.4715 - acc: 0.8486 - val_loss: 1.1825 - val_acc: 0.6697\n",
      "Epoch 80/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4688 - acc: 0.8482\n",
      "Epoch 00080: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.4686 - acc: 0.8483 - val_loss: 1.1633 - val_acc: 0.6720\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4606 - acc: 0.8531\n",
      "Epoch 00081: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.4608 - acc: 0.8530 - val_loss: 1.1724 - val_acc: 0.6744\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4623 - acc: 0.8510\n",
      "Epoch 00082: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.4621 - acc: 0.8511 - val_loss: 1.1695 - val_acc: 0.6799\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4529 - acc: 0.8559\n",
      "Epoch 00083: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.4525 - acc: 0.8560 - val_loss: 1.1746 - val_acc: 0.6732\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.8577\n",
      "Epoch 00084: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.4435 - acc: 0.8578 - val_loss: 1.1917 - val_acc: 0.6720\n",
      "Epoch 85/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4485 - acc: 0.8565\n",
      "Epoch 00085: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.4483 - acc: 0.8565 - val_loss: 1.1745 - val_acc: 0.6755\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4447 - acc: 0.8583\n",
      "Epoch 00086: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.4449 - acc: 0.8583 - val_loss: 1.1699 - val_acc: 0.6746\n",
      "Epoch 87/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4388 - acc: 0.8583\n",
      "Epoch 00087: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.4387 - acc: 0.8583 - val_loss: 1.1741 - val_acc: 0.6725\n",
      "Epoch 88/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4329 - acc: 0.8585\n",
      "Epoch 00088: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.4329 - acc: 0.8584 - val_loss: 1.1618 - val_acc: 0.6781\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8604\n",
      "Epoch 00089: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.4322 - acc: 0.8603 - val_loss: 1.1930 - val_acc: 0.6732\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.8631\n",
      "Epoch 00090: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.4239 - acc: 0.8631 - val_loss: 1.1824 - val_acc: 0.6769\n",
      "Epoch 91/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.8640\n",
      "Epoch 00091: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.4228 - acc: 0.8641 - val_loss: 1.1820 - val_acc: 0.6790\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4196 - acc: 0.8637\n",
      "Epoch 00092: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.4196 - acc: 0.8636 - val_loss: 1.1781 - val_acc: 0.6774\n",
      "Epoch 93/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4199 - acc: 0.8634\n",
      "Epoch 00093: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.4199 - acc: 0.8634 - val_loss: 1.1905 - val_acc: 0.6776\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8686\n",
      "Epoch 00094: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.4098 - acc: 0.8688 - val_loss: 1.1773 - val_acc: 0.6783\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8696\n",
      "Epoch 00095: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.4049 - acc: 0.8697 - val_loss: 1.1787 - val_acc: 0.6855\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8711\n",
      "Epoch 00096: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.4011 - acc: 0.8710 - val_loss: 1.1851 - val_acc: 0.6839\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.8729\n",
      "Epoch 00097: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3939 - acc: 0.8729 - val_loss: 1.1830 - val_acc: 0.6841\n",
      "Epoch 98/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8719\n",
      "Epoch 00098: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3954 - acc: 0.8721 - val_loss: 1.1948 - val_acc: 0.6776\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8741\n",
      "Epoch 00099: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3900 - acc: 0.8741 - val_loss: 1.1823 - val_acc: 0.6869\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3897 - acc: 0.8732\n",
      "Epoch 00100: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.3893 - acc: 0.8732 - val_loss: 1.1964 - val_acc: 0.6832\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3882 - acc: 0.8741\n",
      "Epoch 00101: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3882 - acc: 0.8741 - val_loss: 1.1881 - val_acc: 0.6848\n",
      "Epoch 102/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8768\n",
      "Epoch 00102: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3802 - acc: 0.8768 - val_loss: 1.2151 - val_acc: 0.6804\n",
      "Epoch 103/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8767\n",
      "Epoch 00103: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.3778 - acc: 0.8767 - val_loss: 1.2298 - val_acc: 0.6778\n",
      "Epoch 104/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8801\n",
      "Epoch 00104: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3728 - acc: 0.8801 - val_loss: 1.1904 - val_acc: 0.6848\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8776\n",
      "Epoch 00105: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.3712 - acc: 0.8779 - val_loss: 1.2200 - val_acc: 0.6820\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8761\n",
      "Epoch 00106: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.3729 - acc: 0.8760 - val_loss: 1.1898 - val_acc: 0.6862\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3654 - acc: 0.8804\n",
      "Epoch 00107: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3651 - acc: 0.8804 - val_loss: 1.2175 - val_acc: 0.6820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3659 - acc: 0.8840\n",
      "Epoch 00108: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.3654 - acc: 0.8841 - val_loss: 1.1981 - val_acc: 0.6837\n",
      "Epoch 109/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8813\n",
      "Epoch 00109: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.3645 - acc: 0.8814 - val_loss: 1.1910 - val_acc: 0.6846\n",
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8825\n",
      "Epoch 00110: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3647 - acc: 0.8827 - val_loss: 1.2033 - val_acc: 0.6883\n",
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8851\n",
      "Epoch 00111: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.3575 - acc: 0.8850 - val_loss: 1.2066 - val_acc: 0.6858\n",
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.8862\n",
      "Epoch 00112: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.3518 - acc: 0.8860 - val_loss: 1.2051 - val_acc: 0.6841\n",
      "Epoch 113/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8862\n",
      "Epoch 00113: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3486 - acc: 0.8862 - val_loss: 1.2069 - val_acc: 0.6895\n",
      "Epoch 114/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3447 - acc: 0.8881\n",
      "Epoch 00114: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.3451 - acc: 0.8880 - val_loss: 1.2000 - val_acc: 0.6895\n",
      "Epoch 115/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3468 - acc: 0.8879\n",
      "Epoch 00115: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3465 - acc: 0.8881 - val_loss: 1.1943 - val_acc: 0.6893\n",
      "Epoch 116/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8881\n",
      "Epoch 00116: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3445 - acc: 0.8882 - val_loss: 1.2032 - val_acc: 0.6911\n",
      "Epoch 117/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8887\n",
      "Epoch 00117: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3401 - acc: 0.8887 - val_loss: 1.2047 - val_acc: 0.6886\n",
      "Epoch 118/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.8913\n",
      "Epoch 00118: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3338 - acc: 0.8913 - val_loss: 1.2057 - val_acc: 0.6946\n",
      "Epoch 119/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8882\n",
      "Epoch 00119: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3412 - acc: 0.8881 - val_loss: 1.2163 - val_acc: 0.6902\n",
      "Epoch 120/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8917\n",
      "Epoch 00120: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3289 - acc: 0.8918 - val_loss: 1.2164 - val_acc: 0.6918\n",
      "Epoch 121/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.8903\n",
      "Epoch 00121: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.3338 - acc: 0.8904 - val_loss: 1.2073 - val_acc: 0.6911\n",
      "Epoch 122/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8925\n",
      "Epoch 00122: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3286 - acc: 0.8925 - val_loss: 1.2185 - val_acc: 0.6930\n",
      "Epoch 123/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8948\n",
      "Epoch 00123: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3249 - acc: 0.8948 - val_loss: 1.2047 - val_acc: 0.6911\n",
      "Epoch 124/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8943\n",
      "Epoch 00124: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.3237 - acc: 0.8943 - val_loss: 1.2179 - val_acc: 0.6858\n",
      "Epoch 125/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.8959\n",
      "Epoch 00125: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3180 - acc: 0.8960 - val_loss: 1.2147 - val_acc: 0.6942\n",
      "Epoch 126/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.8968\n",
      "Epoch 00126: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3150 - acc: 0.8969 - val_loss: 1.2339 - val_acc: 0.6867\n",
      "Epoch 127/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.8962\n",
      "Epoch 00127: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3192 - acc: 0.8963 - val_loss: 1.2082 - val_acc: 0.6918\n",
      "Epoch 128/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.8981\n",
      "Epoch 00128: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3133 - acc: 0.8980 - val_loss: 1.2159 - val_acc: 0.6921\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.8977\n",
      "Epoch 00129: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.3150 - acc: 0.8976 - val_loss: 1.2077 - val_acc: 0.6944\n",
      "Epoch 130/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.8974\n",
      "Epoch 00130: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3144 - acc: 0.8973 - val_loss: 1.2249 - val_acc: 0.6904\n",
      "Epoch 131/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.8985\n",
      "Epoch 00131: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3086 - acc: 0.8984 - val_loss: 1.2183 - val_acc: 0.6893\n",
      "Epoch 132/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.8969\n",
      "Epoch 00132: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3107 - acc: 0.8969 - val_loss: 1.2181 - val_acc: 0.6956\n",
      "Epoch 133/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9002\n",
      "Epoch 00133: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.3066 - acc: 0.9001 - val_loss: 1.2023 - val_acc: 0.7002\n",
      "Epoch 134/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.8990\n",
      "Epoch 00134: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3067 - acc: 0.8989 - val_loss: 1.2205 - val_acc: 0.6946\n",
      "Epoch 135/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.8993\n",
      "Epoch 00135: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.3043 - acc: 0.8992 - val_loss: 1.2167 - val_acc: 0.6979\n",
      "Epoch 136/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.8984\n",
      "Epoch 00136: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3088 - acc: 0.8984 - val_loss: 1.2148 - val_acc: 0.6958\n",
      "Epoch 137/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9028\n",
      "Epoch 00137: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.2966 - acc: 0.9027 - val_loss: 1.2308 - val_acc: 0.6976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9053\n",
      "Epoch 00138: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.2901 - acc: 0.9054 - val_loss: 1.2199 - val_acc: 0.6976\n",
      "Epoch 139/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9057\n",
      "Epoch 00139: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.2952 - acc: 0.9056 - val_loss: 1.2186 - val_acc: 0.6986\n",
      "Epoch 140/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9034\n",
      "Epoch 00140: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.2940 - acc: 0.9035 - val_loss: 1.2228 - val_acc: 0.7016\n",
      "Epoch 141/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9043\n",
      "Epoch 00141: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.2909 - acc: 0.9044 - val_loss: 1.2307 - val_acc: 0.6956\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.9045\n",
      "Epoch 00142: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.2921 - acc: 0.9044 - val_loss: 1.2324 - val_acc: 0.6951\n",
      "Epoch 143/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9074\n",
      "Epoch 00143: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.2831 - acc: 0.9073 - val_loss: 1.2279 - val_acc: 0.6981\n",
      "Epoch 144/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9065\n",
      "Epoch 00144: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.2851 - acc: 0.9066 - val_loss: 1.2315 - val_acc: 0.6960\n",
      "Epoch 145/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9078\n",
      "Epoch 00145: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.2826 - acc: 0.9078 - val_loss: 1.2298 - val_acc: 0.6965\n",
      "Epoch 146/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2838 - acc: 0.9061\n",
      "Epoch 00146: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.2838 - acc: 0.9060 - val_loss: 1.2343 - val_acc: 0.6986\n",
      "Epoch 147/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9068\n",
      "Epoch 00147: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.2805 - acc: 0.9069 - val_loss: 1.2246 - val_acc: 0.6986\n",
      "Epoch 148/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9079\n",
      "Epoch 00148: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.2827 - acc: 0.9077 - val_loss: 1.2493 - val_acc: 0.6965\n",
      "Epoch 149/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9125\n",
      "Epoch 00149: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.2728 - acc: 0.9125 - val_loss: 1.2258 - val_acc: 0.7030\n",
      "Epoch 150/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2762 - acc: 0.9097\n",
      "Epoch 00150: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.2766 - acc: 0.9096 - val_loss: 1.2515 - val_acc: 0.6967\n",
      "Epoch 151/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9113\n",
      "Epoch 00151: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.2743 - acc: 0.9113 - val_loss: 1.2308 - val_acc: 0.6997\n",
      "Epoch 152/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9118\n",
      "Epoch 00152: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.2748 - acc: 0.9118 - val_loss: 1.2323 - val_acc: 0.7007\n",
      "Epoch 153/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.9137\n",
      "Epoch 00153: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.2695 - acc: 0.9137 - val_loss: 1.2304 - val_acc: 0.7030\n",
      "Epoch 154/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9115\n",
      "Epoch 00154: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.2732 - acc: 0.9115 - val_loss: 1.2440 - val_acc: 0.6986\n",
      "Epoch 155/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9127\n",
      "Epoch 00155: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.2704 - acc: 0.9128 - val_loss: 1.2597 - val_acc: 0.6956\n",
      "Epoch 156/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9118\n",
      "Epoch 00156: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.2660 - acc: 0.9118 - val_loss: 1.2314 - val_acc: 0.7042\n",
      "Epoch 157/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2648 - acc: 0.9145\n",
      "Epoch 00157: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.2648 - acc: 0.9145 - val_loss: 1.2375 - val_acc: 0.7018\n",
      "Epoch 158/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9120\n",
      "Epoch 00158: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.2668 - acc: 0.9121 - val_loss: 1.2325 - val_acc: 0.7037\n",
      "Epoch 159/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2594 - acc: 0.9149\n",
      "Epoch 00159: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.2603 - acc: 0.9148 - val_loss: 1.2324 - val_acc: 0.7039\n",
      "Epoch 160/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9161\n",
      "Epoch 00160: val_loss did not improve from 1.14809\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.2596 - acc: 0.9160 - val_loss: 1.2469 - val_acc: 0.7046\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 198us/sample - loss: 1.2467 - acc: 0.6280\n",
      "Loss: 1.2466799556033015 Accuracy: 0.6280374\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4345 - acc: 0.2155\n",
      "Epoch 00001: val_loss improved from inf to 1.94394, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/001-1.9439.hdf5\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 2.4344 - acc: 0.2155 - val_loss: 1.9439 - val_acc: 0.4039\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8159 - acc: 0.4255\n",
      "Epoch 00002: val_loss improved from 1.94394 to 1.65649, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/002-1.6565.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 1.8159 - acc: 0.4255 - val_loss: 1.6565 - val_acc: 0.5003\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6485 - acc: 0.4830\n",
      "Epoch 00003: val_loss improved from 1.65649 to 1.55378, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/003-1.5538.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 1.6484 - acc: 0.4830 - val_loss: 1.5538 - val_acc: 0.5176\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5426 - acc: 0.5172\n",
      "Epoch 00004: val_loss improved from 1.55378 to 1.44429, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/004-1.4443.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 1.5426 - acc: 0.5172 - val_loss: 1.4443 - val_acc: 0.5679\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4644 - acc: 0.5452\n",
      "Epoch 00005: val_loss improved from 1.44429 to 1.38352, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/005-1.3835.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 1.4640 - acc: 0.5453 - val_loss: 1.3835 - val_acc: 0.5747\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3980 - acc: 0.5673\n",
      "Epoch 00006: val_loss improved from 1.38352 to 1.32287, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/006-1.3229.hdf5\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 1.3980 - acc: 0.5673 - val_loss: 1.3229 - val_acc: 0.6159\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3345 - acc: 0.5910\n",
      "Epoch 00007: val_loss improved from 1.32287 to 1.28233, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/007-1.2823.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 1.3345 - acc: 0.5911 - val_loss: 1.2823 - val_acc: 0.6182\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2787 - acc: 0.6082\n",
      "Epoch 00008: val_loss improved from 1.28233 to 1.23344, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/008-1.2334.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 1.2804 - acc: 0.6080 - val_loss: 1.2334 - val_acc: 0.6396\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2338 - acc: 0.6221\n",
      "Epoch 00009: val_loss improved from 1.23344 to 1.19931, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/009-1.1993.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 1.2340 - acc: 0.6221 - val_loss: 1.1993 - val_acc: 0.6522\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1911 - acc: 0.6394\n",
      "Epoch 00010: val_loss improved from 1.19931 to 1.15162, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/010-1.1516.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 1.1911 - acc: 0.6394 - val_loss: 1.1516 - val_acc: 0.6560\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1558 - acc: 0.6503\n",
      "Epoch 00011: val_loss improved from 1.15162 to 1.13308, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/011-1.1331.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 1.1557 - acc: 0.6503 - val_loss: 1.1331 - val_acc: 0.6662\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1186 - acc: 0.6626\n",
      "Epoch 00012: val_loss improved from 1.13308 to 1.09556, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/012-1.0956.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 1.1187 - acc: 0.6625 - val_loss: 1.0956 - val_acc: 0.6737\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0888 - acc: 0.6709\n",
      "Epoch 00013: val_loss improved from 1.09556 to 1.06833, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/013-1.0683.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 1.0885 - acc: 0.6710 - val_loss: 1.0683 - val_acc: 0.6902\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0592 - acc: 0.6808\n",
      "Epoch 00014: val_loss improved from 1.06833 to 1.05900, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/014-1.0590.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 1.0592 - acc: 0.6808 - val_loss: 1.0590 - val_acc: 0.6923\n",
      "Epoch 15/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0298 - acc: 0.6916\n",
      "Epoch 00015: val_loss improved from 1.05900 to 1.01977, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/015-1.0198.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 1.0302 - acc: 0.6914 - val_loss: 1.0198 - val_acc: 0.7016\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0030 - acc: 0.6980\n",
      "Epoch 00016: val_loss improved from 1.01977 to 1.00444, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/016-1.0044.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 1.0029 - acc: 0.6981 - val_loss: 1.0044 - val_acc: 0.7095\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9804 - acc: 0.7051\n",
      "Epoch 00017: val_loss improved from 1.00444 to 0.98159, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/017-0.9816.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.9797 - acc: 0.7054 - val_loss: 0.9816 - val_acc: 0.7130\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9568 - acc: 0.7108\n",
      "Epoch 00018: val_loss did not improve from 0.98159\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.9569 - acc: 0.7108 - val_loss: 0.9903 - val_acc: 0.7123\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9345 - acc: 0.7204\n",
      "Epoch 00019: val_loss improved from 0.98159 to 0.95754, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/019-0.9575.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.9345 - acc: 0.7204 - val_loss: 0.9575 - val_acc: 0.7174\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9091 - acc: 0.7277\n",
      "Epoch 00020: val_loss improved from 0.95754 to 0.94626, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/020-0.9463.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.9095 - acc: 0.7274 - val_loss: 0.9463 - val_acc: 0.7237\n",
      "Epoch 21/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8931 - acc: 0.7318\n",
      "Epoch 00021: val_loss improved from 0.94626 to 0.92482, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/021-0.9248.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.8924 - acc: 0.7320 - val_loss: 0.9248 - val_acc: 0.7272\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8684 - acc: 0.7388\n",
      "Epoch 00022: val_loss improved from 0.92482 to 0.92192, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/022-0.9219.hdf5\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.8684 - acc: 0.7388 - val_loss: 0.9219 - val_acc: 0.7261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8512 - acc: 0.7446\n",
      "Epoch 00023: val_loss improved from 0.92192 to 0.90401, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/023-0.9040.hdf5\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.8511 - acc: 0.7446 - val_loss: 0.9040 - val_acc: 0.7372\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8295 - acc: 0.7513\n",
      "Epoch 00024: val_loss improved from 0.90401 to 0.89460, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/024-0.8946.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.8296 - acc: 0.7513 - val_loss: 0.8946 - val_acc: 0.7384\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8156 - acc: 0.7555\n",
      "Epoch 00025: val_loss improved from 0.89460 to 0.88909, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/025-0.8891.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.8157 - acc: 0.7554 - val_loss: 0.8891 - val_acc: 0.7382\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7989 - acc: 0.7616\n",
      "Epoch 00026: val_loss improved from 0.88909 to 0.88447, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/026-0.8845.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.7989 - acc: 0.7617 - val_loss: 0.8845 - val_acc: 0.7417\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7786 - acc: 0.7654\n",
      "Epoch 00027: val_loss improved from 0.88447 to 0.87096, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/027-0.8710.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.7786 - acc: 0.7654 - val_loss: 0.8710 - val_acc: 0.7498\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7651 - acc: 0.7706\n",
      "Epoch 00028: val_loss improved from 0.87096 to 0.85312, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/028-0.8531.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.7651 - acc: 0.7705 - val_loss: 0.8531 - val_acc: 0.7496\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7489 - acc: 0.7739\n",
      "Epoch 00029: val_loss improved from 0.85312 to 0.83734, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/029-0.8373.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.7489 - acc: 0.7740 - val_loss: 0.8373 - val_acc: 0.7505\n",
      "Epoch 30/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7371 - acc: 0.7783\n",
      "Epoch 00030: val_loss did not improve from 0.83734\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.7367 - acc: 0.7784 - val_loss: 0.8581 - val_acc: 0.7543\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7258 - acc: 0.7807\n",
      "Epoch 00031: val_loss improved from 0.83734 to 0.82689, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/031-0.8269.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.7256 - acc: 0.7808 - val_loss: 0.8269 - val_acc: 0.7638\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7081 - acc: 0.7854\n",
      "Epoch 00032: val_loss did not improve from 0.82689\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.7077 - acc: 0.7855 - val_loss: 0.8563 - val_acc: 0.7505\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6953 - acc: 0.7917\n",
      "Epoch 00033: val_loss improved from 0.82689 to 0.81758, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/033-0.8176.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.6952 - acc: 0.7917 - val_loss: 0.8176 - val_acc: 0.7622\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6776 - acc: 0.7960\n",
      "Epoch 00034: val_loss did not improve from 0.81758\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.6775 - acc: 0.7960 - val_loss: 0.8228 - val_acc: 0.7563\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6692 - acc: 0.8010\n",
      "Epoch 00035: val_loss improved from 0.81758 to 0.81702, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/035-0.8170.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.6692 - acc: 0.8010 - val_loss: 0.8170 - val_acc: 0.7587\n",
      "Epoch 36/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6592 - acc: 0.8020\n",
      "Epoch 00036: val_loss improved from 0.81702 to 0.79542, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/036-0.7954.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.6590 - acc: 0.8020 - val_loss: 0.7954 - val_acc: 0.7680\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6424 - acc: 0.8070\n",
      "Epoch 00037: val_loss improved from 0.79542 to 0.78551, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/037-0.7855.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.6424 - acc: 0.8070 - val_loss: 0.7855 - val_acc: 0.7710\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6329 - acc: 0.8100\n",
      "Epoch 00038: val_loss did not improve from 0.78551\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.6331 - acc: 0.8100 - val_loss: 0.8275 - val_acc: 0.7549\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6235 - acc: 0.8147\n",
      "Epoch 00039: val_loss did not improve from 0.78551\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.6228 - acc: 0.8149 - val_loss: 0.7875 - val_acc: 0.7713\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6130 - acc: 0.8176\n",
      "Epoch 00040: val_loss improved from 0.78551 to 0.78138, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/040-0.7814.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.6132 - acc: 0.8174 - val_loss: 0.7814 - val_acc: 0.7729\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.8175\n",
      "Epoch 00041: val_loss improved from 0.78138 to 0.78107, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/041-0.7811.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.6017 - acc: 0.8175 - val_loss: 0.7811 - val_acc: 0.7738\n",
      "Epoch 42/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5890 - acc: 0.8230\n",
      "Epoch 00042: val_loss did not improve from 0.78107\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.5883 - acc: 0.8232 - val_loss: 0.7886 - val_acc: 0.7680\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5824 - acc: 0.8252\n",
      "Epoch 00043: val_loss improved from 0.78107 to 0.77260, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/043-0.7726.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.5824 - acc: 0.8252 - val_loss: 0.7726 - val_acc: 0.7731\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.8272\n",
      "Epoch 00044: val_loss improved from 0.77260 to 0.76951, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/044-0.7695.hdf5\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.5725 - acc: 0.8272 - val_loss: 0.7695 - val_acc: 0.7724\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.8303\n",
      "Epoch 00045: val_loss did not improve from 0.76951\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.5600 - acc: 0.8303 - val_loss: 0.7718 - val_acc: 0.7747\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.8322\n",
      "Epoch 00046: val_loss improved from 0.76951 to 0.76285, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/046-0.7628.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.5505 - acc: 0.8321 - val_loss: 0.7628 - val_acc: 0.7764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.8358\n",
      "Epoch 00047: val_loss did not improve from 0.76285\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.5451 - acc: 0.8358 - val_loss: 0.7648 - val_acc: 0.7817\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.8375\n",
      "Epoch 00048: val_loss did not improve from 0.76285\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.5355 - acc: 0.8376 - val_loss: 0.7712 - val_acc: 0.7734\n",
      "Epoch 49/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5275 - acc: 0.8378\n",
      "Epoch 00049: val_loss improved from 0.76285 to 0.75668, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/049-0.7567.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.5271 - acc: 0.8379 - val_loss: 0.7567 - val_acc: 0.7813\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.8408\n",
      "Epoch 00050: val_loss did not improve from 0.75668\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.5197 - acc: 0.8408 - val_loss: 0.7646 - val_acc: 0.7771\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8430\n",
      "Epoch 00051: val_loss did not improve from 0.75668\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.5116 - acc: 0.8430 - val_loss: 0.7574 - val_acc: 0.7752\n",
      "Epoch 52/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8466\n",
      "Epoch 00052: val_loss did not improve from 0.75668\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.5031 - acc: 0.8465 - val_loss: 0.7594 - val_acc: 0.7775\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8495\n",
      "Epoch 00053: val_loss improved from 0.75668 to 0.75303, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/053-0.7530.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.4943 - acc: 0.8494 - val_loss: 0.7530 - val_acc: 0.7817\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4888 - acc: 0.8523\n",
      "Epoch 00054: val_loss did not improve from 0.75303\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.4884 - acc: 0.8523 - val_loss: 0.7611 - val_acc: 0.7815\n",
      "Epoch 55/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4808 - acc: 0.8540\n",
      "Epoch 00055: val_loss did not improve from 0.75303\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4810 - acc: 0.8540 - val_loss: 0.7616 - val_acc: 0.7775\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.8528\n",
      "Epoch 00056: val_loss did not improve from 0.75303\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.4766 - acc: 0.8528 - val_loss: 0.7580 - val_acc: 0.7754\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4670 - acc: 0.8577\n",
      "Epoch 00057: val_loss improved from 0.75303 to 0.74433, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/057-0.7443.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4670 - acc: 0.8577 - val_loss: 0.7443 - val_acc: 0.7838\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.8588\n",
      "Epoch 00058: val_loss improved from 0.74433 to 0.74233, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/058-0.7423.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.4550 - acc: 0.8588 - val_loss: 0.7423 - val_acc: 0.7838\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.8590\n",
      "Epoch 00059: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.4543 - acc: 0.8591 - val_loss: 0.7630 - val_acc: 0.7785\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4504 - acc: 0.8606\n",
      "Epoch 00060: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4504 - acc: 0.8606 - val_loss: 0.7494 - val_acc: 0.7871\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8634\n",
      "Epoch 00061: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.4411 - acc: 0.8635 - val_loss: 0.7728 - val_acc: 0.7794\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.8663\n",
      "Epoch 00062: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.4325 - acc: 0.8663 - val_loss: 0.7529 - val_acc: 0.7850\n",
      "Epoch 63/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.8674\n",
      "Epoch 00063: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.4275 - acc: 0.8673 - val_loss: 0.7488 - val_acc: 0.7836\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8698\n",
      "Epoch 00064: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4242 - acc: 0.8697 - val_loss: 0.7429 - val_acc: 0.7901\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8720\n",
      "Epoch 00065: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4124 - acc: 0.8720 - val_loss: 0.7529 - val_acc: 0.7873\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8724\n",
      "Epoch 00066: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.4062 - acc: 0.8724 - val_loss: 0.7482 - val_acc: 0.7822\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8767\n",
      "Epoch 00067: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.4026 - acc: 0.8768 - val_loss: 0.7713 - val_acc: 0.7782\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8751\n",
      "Epoch 00068: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.4011 - acc: 0.8753 - val_loss: 0.7456 - val_acc: 0.7906\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3989 - acc: 0.8772\n",
      "Epoch 00069: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.3989 - acc: 0.8772 - val_loss: 0.7573 - val_acc: 0.7838\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3890 - acc: 0.8777\n",
      "Epoch 00070: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3890 - acc: 0.8777 - val_loss: 0.7513 - val_acc: 0.7869\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8798\n",
      "Epoch 00071: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3833 - acc: 0.8798 - val_loss: 0.7641 - val_acc: 0.7848\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8828\n",
      "Epoch 00072: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3790 - acc: 0.8830 - val_loss: 0.7599 - val_acc: 0.7824\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8825\n",
      "Epoch 00073: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3775 - acc: 0.8825 - val_loss: 0.7567 - val_acc: 0.7848\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8847\n",
      "Epoch 00074: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3691 - acc: 0.8847 - val_loss: 0.7776 - val_acc: 0.7806\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3679 - acc: 0.8855\n",
      "Epoch 00075: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.3679 - acc: 0.8855 - val_loss: 0.7506 - val_acc: 0.7866\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8862\n",
      "Epoch 00076: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3624 - acc: 0.8863 - val_loss: 0.7628 - val_acc: 0.7827\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8886\n",
      "Epoch 00077: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3581 - acc: 0.8886 - val_loss: 0.7648 - val_acc: 0.7864\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8897\n",
      "Epoch 00078: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3519 - acc: 0.8897 - val_loss: 0.7550 - val_acc: 0.7890\n",
      "Epoch 79/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8896\n",
      "Epoch 00079: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3481 - acc: 0.8897 - val_loss: 0.7522 - val_acc: 0.7871\n",
      "Epoch 80/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8916\n",
      "Epoch 00080: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3492 - acc: 0.8916 - val_loss: 0.7512 - val_acc: 0.7948\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8931\n",
      "Epoch 00081: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.3395 - acc: 0.8930 - val_loss: 0.7524 - val_acc: 0.7897\n",
      "Epoch 82/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3376 - acc: 0.8914\n",
      "Epoch 00082: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3375 - acc: 0.8915 - val_loss: 0.7767 - val_acc: 0.7899\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.8942\n",
      "Epoch 00083: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.3319 - acc: 0.8942 - val_loss: 0.7547 - val_acc: 0.7890\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8945\n",
      "Epoch 00084: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3345 - acc: 0.8945 - val_loss: 0.7808 - val_acc: 0.7906\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3307 - acc: 0.8947\n",
      "Epoch 00085: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.3306 - acc: 0.8947 - val_loss: 0.7596 - val_acc: 0.7904\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.8996\n",
      "Epoch 00086: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3173 - acc: 0.8996 - val_loss: 0.7834 - val_acc: 0.7911\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.8967\n",
      "Epoch 00087: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3207 - acc: 0.8965 - val_loss: 0.7640 - val_acc: 0.7966\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9002\n",
      "Epoch 00088: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.3148 - acc: 0.9003 - val_loss: 0.7610 - val_acc: 0.7955\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.8999\n",
      "Epoch 00089: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3130 - acc: 0.9000 - val_loss: 0.7550 - val_acc: 0.7964\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.9031\n",
      "Epoch 00090: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3097 - acc: 0.9030 - val_loss: 0.7875 - val_acc: 0.7890\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.9015\n",
      "Epoch 00091: val_loss did not improve from 0.74233\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3117 - acc: 0.9013 - val_loss: 0.7666 - val_acc: 0.7945\n",
      "Epoch 92/500\n",
      "32576/36805 [=========================>....] - ETA: 1s - loss: 0.3012 - acc: 0.9008"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    base = '1D_CNN_only_conv_conv_5_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_only_conv_conv_5_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_only_conv_conv_5_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, accuracy, loss])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
