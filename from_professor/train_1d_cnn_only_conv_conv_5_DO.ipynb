{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_only_conv_conv_5_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                682512    \n",
      "=================================================================\n",
      "Total params: 682,560\n",
      "Trainable params: 682,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                454672    \n",
      "=================================================================\n",
      "Total params: 455,376\n",
      "Trainable params: 455,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                302608    \n",
      "=================================================================\n",
      "Total params: 305,904\n",
      "Trainable params: 305,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                200720    \n",
      "=================================================================\n",
      "Total params: 214,320\n",
      "Trainable params: 214,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                131088    \n",
      "=================================================================\n",
      "Total params: 185,776\n",
      "Trainable params: 185,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                81936     \n",
      "=================================================================\n",
      "Total params: 300,720\n",
      "Trainable params: 300,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                49168     \n",
      "=================================================================\n",
      "Total params: 923,824\n",
      "Trainable params: 923,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 2, 1024)           2622464   \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 3,513,520\n",
      "Trainable params: 3,513,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    model = build_1d_cnn_only_conv_conv_5_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.5786 - acc: 0.1698\n",
      "Epoch 00001: val_loss improved from inf to 2.45625, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/001-2.4563.hdf5\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 2.5785 - acc: 0.1699 - val_loss: 2.4563 - val_acc: 0.2176\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3829 - acc: 0.2533\n",
      "Epoch 00002: val_loss improved from 2.45625 to 2.37448, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/002-2.3745.hdf5\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 2.3830 - acc: 0.2533 - val_loss: 2.3745 - val_acc: 0.2539\n",
      "Epoch 3/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 2.2653 - acc: 0.3031\n",
      "Epoch 00003: val_loss improved from 2.37448 to 2.31661, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/003-2.3166.hdf5\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 2.2652 - acc: 0.3035 - val_loss: 2.3166 - val_acc: 0.2791\n",
      "Epoch 4/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 2.1697 - acc: 0.3402\n",
      "Epoch 00004: val_loss improved from 2.31661 to 2.28353, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/004-2.2835.hdf5\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 2.1696 - acc: 0.3404 - val_loss: 2.2835 - val_acc: 0.2812\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0837 - acc: 0.3730\n",
      "Epoch 00005: val_loss improved from 2.28353 to 2.26127, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/005-2.2613.hdf5\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 2.0839 - acc: 0.3729 - val_loss: 2.2613 - val_acc: 0.2919\n",
      "Epoch 6/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 2.0123 - acc: 0.3962\n",
      "Epoch 00006: val_loss improved from 2.26127 to 2.24894, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/006-2.2489.hdf5\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 2.0126 - acc: 0.3958 - val_loss: 2.2489 - val_acc: 0.2944\n",
      "Epoch 7/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.9510 - acc: 0.4151\n",
      "Epoch 00007: val_loss improved from 2.24894 to 2.23156, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/007-2.2316.hdf5\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.9502 - acc: 0.4153 - val_loss: 2.2316 - val_acc: 0.2942\n",
      "Epoch 8/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 1.8960 - acc: 0.4311\n",
      "Epoch 00008: val_loss improved from 2.23156 to 2.22531, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/008-2.2253.hdf5\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 1.8962 - acc: 0.4312 - val_loss: 2.2253 - val_acc: 0.2926\n",
      "Epoch 9/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.8459 - acc: 0.4452\n",
      "Epoch 00009: val_loss improved from 2.22531 to 2.22208, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/009-2.2221.hdf5\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.8456 - acc: 0.4456 - val_loss: 2.2221 - val_acc: 0.2958\n",
      "Epoch 10/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.8013 - acc: 0.4619\n",
      "Epoch 00010: val_loss improved from 2.22208 to 2.21459, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_1_conv_checkpoint/010-2.2146.hdf5\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 1.8013 - acc: 0.4618 - val_loss: 2.2146 - val_acc: 0.3061\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.7578 - acc: 0.4716\n",
      "Epoch 00011: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.7576 - acc: 0.4717 - val_loss: 2.2254 - val_acc: 0.3024\n",
      "Epoch 12/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.7214 - acc: 0.4798\n",
      "Epoch 00012: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 1.7218 - acc: 0.4794 - val_loss: 2.2213 - val_acc: 0.3024\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6877 - acc: 0.4921\n",
      "Epoch 00013: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.6880 - acc: 0.4920 - val_loss: 2.2258 - val_acc: 0.3049\n",
      "Epoch 14/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.6577 - acc: 0.5029\n",
      "Epoch 00014: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 1.6577 - acc: 0.5031 - val_loss: 2.2212 - val_acc: 0.3098\n",
      "Epoch 15/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6272 - acc: 0.5097\n",
      "Epoch 00015: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.6274 - acc: 0.5096 - val_loss: 2.2305 - val_acc: 0.3126\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6031 - acc: 0.5139\n",
      "Epoch 00016: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.6023 - acc: 0.5140 - val_loss: 2.2282 - val_acc: 0.3140\n",
      "Epoch 17/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 1.5770 - acc: 0.5222\n",
      "Epoch 00017: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 1.5766 - acc: 0.5219 - val_loss: 2.2428 - val_acc: 0.3156\n",
      "Epoch 18/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.5576 - acc: 0.5270\n",
      "Epoch 00018: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 1.5572 - acc: 0.5272 - val_loss: 2.2378 - val_acc: 0.3079\n",
      "Epoch 19/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.5297 - acc: 0.5362\n",
      "Epoch 00019: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 1.5297 - acc: 0.5363 - val_loss: 2.2358 - val_acc: 0.3142\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5102 - acc: 0.5395\n",
      "Epoch 00020: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 1.5104 - acc: 0.5392 - val_loss: 2.2428 - val_acc: 0.3149\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4889 - acc: 0.5483\n",
      "Epoch 00021: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 1.4888 - acc: 0.5481 - val_loss: 2.2406 - val_acc: 0.3121\n",
      "Epoch 22/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.4688 - acc: 0.5513\n",
      "Epoch 00022: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 160us/sample - loss: 1.4688 - acc: 0.5514 - val_loss: 2.2484 - val_acc: 0.3117\n",
      "Epoch 23/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.4569 - acc: 0.5563\n",
      "Epoch 00023: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 1.4574 - acc: 0.5556 - val_loss: 2.2506 - val_acc: 0.3126\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4345 - acc: 0.5608\n",
      "Epoch 00024: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 1.4346 - acc: 0.5606 - val_loss: 2.2476 - val_acc: 0.3212\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4215 - acc: 0.5655\n",
      "Epoch 00025: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.4220 - acc: 0.5653 - val_loss: 2.2549 - val_acc: 0.3173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.4113 - acc: 0.5646\n",
      "Epoch 00026: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 1.4116 - acc: 0.5642 - val_loss: 2.2617 - val_acc: 0.3205\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3890 - acc: 0.5731\n",
      "Epoch 00027: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.3887 - acc: 0.5732 - val_loss: 2.2691 - val_acc: 0.3196\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3749 - acc: 0.5769\n",
      "Epoch 00028: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 160us/sample - loss: 1.3750 - acc: 0.5768 - val_loss: 2.2637 - val_acc: 0.3215\n",
      "Epoch 29/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.3586 - acc: 0.5823\n",
      "Epoch 00029: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 1.3587 - acc: 0.5821 - val_loss: 2.2742 - val_acc: 0.3177\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3501 - acc: 0.5837\n",
      "Epoch 00030: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 1.3507 - acc: 0.5835 - val_loss: 2.2765 - val_acc: 0.3201\n",
      "Epoch 31/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.3339 - acc: 0.5909\n",
      "Epoch 00031: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.3340 - acc: 0.5908 - val_loss: 2.2861 - val_acc: 0.3203\n",
      "Epoch 32/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 1.3268 - acc: 0.5921\n",
      "Epoch 00032: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 1.3276 - acc: 0.5920 - val_loss: 2.2823 - val_acc: 0.3184\n",
      "Epoch 33/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.3200 - acc: 0.5954\n",
      "Epoch 00033: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 1.3200 - acc: 0.5952 - val_loss: 2.2873 - val_acc: 0.3203\n",
      "Epoch 34/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.3092 - acc: 0.5960\n",
      "Epoch 00034: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 1.3091 - acc: 0.5961 - val_loss: 2.2926 - val_acc: 0.3252\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2926 - acc: 0.6027\n",
      "Epoch 00035: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.2926 - acc: 0.6027 - val_loss: 2.2980 - val_acc: 0.3247\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2816 - acc: 0.6048\n",
      "Epoch 00036: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 1.2815 - acc: 0.6049 - val_loss: 2.3054 - val_acc: 0.3208\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2726 - acc: 0.6071\n",
      "Epoch 00037: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.2726 - acc: 0.6071 - val_loss: 2.3167 - val_acc: 0.3226\n",
      "Epoch 38/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2592 - acc: 0.6095\n",
      "Epoch 00038: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.2595 - acc: 0.6096 - val_loss: 2.3151 - val_acc: 0.3277\n",
      "Epoch 39/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.2496 - acc: 0.6142\n",
      "Epoch 00039: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 1.2493 - acc: 0.6143 - val_loss: 2.3168 - val_acc: 0.3210\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2428 - acc: 0.6161\n",
      "Epoch 00040: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.2429 - acc: 0.6161 - val_loss: 2.3269 - val_acc: 0.3315\n",
      "Epoch 41/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.2330 - acc: 0.6194\n",
      "Epoch 00041: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.2333 - acc: 0.6192 - val_loss: 2.3331 - val_acc: 0.3212\n",
      "Epoch 42/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.2240 - acc: 0.6231\n",
      "Epoch 00042: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.2242 - acc: 0.6231 - val_loss: 2.3295 - val_acc: 0.3240\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2189 - acc: 0.6217\n",
      "Epoch 00043: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.2189 - acc: 0.6217 - val_loss: 2.3280 - val_acc: 0.3289\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2157 - acc: 0.6217\n",
      "Epoch 00044: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 1.2156 - acc: 0.6218 - val_loss: 2.3217 - val_acc: 0.3333\n",
      "Epoch 45/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2003 - acc: 0.6265\n",
      "Epoch 00045: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.1997 - acc: 0.6267 - val_loss: 2.3414 - val_acc: 0.3266\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1907 - acc: 0.6286\n",
      "Epoch 00046: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 1.1905 - acc: 0.6286 - val_loss: 2.3414 - val_acc: 0.3249\n",
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1843 - acc: 0.6348\n",
      "Epoch 00047: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 1.1841 - acc: 0.6348 - val_loss: 2.3412 - val_acc: 0.3333\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1794 - acc: 0.6355\n",
      "Epoch 00048: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.1791 - acc: 0.6356 - val_loss: 2.3504 - val_acc: 0.3357\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1683 - acc: 0.6375\n",
      "Epoch 00049: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 1.1686 - acc: 0.6373 - val_loss: 2.3545 - val_acc: 0.3268\n",
      "Epoch 50/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 1.1600 - acc: 0.6374\n",
      "Epoch 00050: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.1596 - acc: 0.6375 - val_loss: 2.3499 - val_acc: 0.3312\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1587 - acc: 0.6389\n",
      "Epoch 00051: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 1.1587 - acc: 0.6389 - val_loss: 2.3552 - val_acc: 0.3354\n",
      "Epoch 52/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1505 - acc: 0.6409\n",
      "Epoch 00052: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.1513 - acc: 0.6408 - val_loss: 2.3706 - val_acc: 0.3331\n",
      "Epoch 53/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.1504 - acc: 0.6404\n",
      "Epoch 00053: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.1505 - acc: 0.6405 - val_loss: 2.3646 - val_acc: 0.3301\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1338 - acc: 0.6479\n",
      "Epoch 00054: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.1338 - acc: 0.6479 - val_loss: 2.3676 - val_acc: 0.3340\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1246 - acc: 0.6519\n",
      "Epoch 00055: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.1245 - acc: 0.6519 - val_loss: 2.3720 - val_acc: 0.3338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1211 - acc: 0.6500\n",
      "Epoch 00056: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.1211 - acc: 0.6500 - val_loss: 2.3565 - val_acc: 0.3375\n",
      "Epoch 57/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.1200 - acc: 0.6482\n",
      "Epoch 00057: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.1207 - acc: 0.6478 - val_loss: 2.3723 - val_acc: 0.3331\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1143 - acc: 0.6522\n",
      "Epoch 00058: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.1150 - acc: 0.6521 - val_loss: 2.3726 - val_acc: 0.3322\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1032 - acc: 0.6530\n",
      "Epoch 00059: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 169us/sample - loss: 1.1033 - acc: 0.6530 - val_loss: 2.3689 - val_acc: 0.3373\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0959 - acc: 0.6570\n",
      "Epoch 00060: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.0956 - acc: 0.6571 - val_loss: 2.3820 - val_acc: 0.3357\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0861 - acc: 0.6619\n",
      "Epoch 00061: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.0861 - acc: 0.6619 - val_loss: 2.3886 - val_acc: 0.3354\n",
      "Epoch 62/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0865 - acc: 0.6603\n",
      "Epoch 00062: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 1.0865 - acc: 0.6603 - val_loss: 2.3863 - val_acc: 0.3417\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0763 - acc: 0.6631\n",
      "Epoch 00063: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.0761 - acc: 0.6633 - val_loss: 2.3853 - val_acc: 0.3392\n",
      "Epoch 64/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.0713 - acc: 0.6636\n",
      "Epoch 00064: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 172us/sample - loss: 1.0712 - acc: 0.6636 - val_loss: 2.3913 - val_acc: 0.3368\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0653 - acc: 0.6669\n",
      "Epoch 00065: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 1.0654 - acc: 0.6669 - val_loss: 2.3958 - val_acc: 0.3385\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0609 - acc: 0.6647\n",
      "Epoch 00066: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 1.0615 - acc: 0.6645 - val_loss: 2.4004 - val_acc: 0.3413\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0515 - acc: 0.6699\n",
      "Epoch 00067: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 168us/sample - loss: 1.0514 - acc: 0.6699 - val_loss: 2.3941 - val_acc: 0.3452\n",
      "Epoch 68/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0474 - acc: 0.6702\n",
      "Epoch 00068: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 1.0468 - acc: 0.6703 - val_loss: 2.4054 - val_acc: 0.3464\n",
      "Epoch 69/500\n",
      "36480/36805 [============================>.] - ETA: 0s - loss: 1.0411 - acc: 0.6725\n",
      "Epoch 00069: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 1.0412 - acc: 0.6725 - val_loss: 2.4027 - val_acc: 0.3385\n",
      "Epoch 70/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.0370 - acc: 0.6751\n",
      "Epoch 00070: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.0377 - acc: 0.6748 - val_loss: 2.4077 - val_acc: 0.3473\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0363 - acc: 0.6745\n",
      "Epoch 00071: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.0365 - acc: 0.6745 - val_loss: 2.4148 - val_acc: 0.3445\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0317 - acc: 0.6761\n",
      "Epoch 00072: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 1.0321 - acc: 0.6759 - val_loss: 2.4185 - val_acc: 0.3431\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0296 - acc: 0.6736\n",
      "Epoch 00073: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 1.0290 - acc: 0.6737 - val_loss: 2.4204 - val_acc: 0.3359\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0188 - acc: 0.6777\n",
      "Epoch 00074: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 159us/sample - loss: 1.0188 - acc: 0.6777 - val_loss: 2.4280 - val_acc: 0.3424\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0109 - acc: 0.6837\n",
      "Epoch 00075: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 1.0109 - acc: 0.6837 - val_loss: 2.4332 - val_acc: 0.3401\n",
      "Epoch 76/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.0079 - acc: 0.6832\n",
      "Epoch 00076: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 1.0084 - acc: 0.6830 - val_loss: 2.4322 - val_acc: 0.3415\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0050 - acc: 0.6830\n",
      "Epoch 00077: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 1.0053 - acc: 0.6830 - val_loss: 2.4352 - val_acc: 0.3431\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9988 - acc: 0.6847\n",
      "Epoch 00078: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 170us/sample - loss: 0.9990 - acc: 0.6846 - val_loss: 2.4382 - val_acc: 0.3392\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9923 - acc: 0.6891\n",
      "Epoch 00079: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 167us/sample - loss: 0.9919 - acc: 0.6891 - val_loss: 2.4412 - val_acc: 0.3454\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9969 - acc: 0.6856\n",
      "Epoch 00080: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.9968 - acc: 0.6856 - val_loss: 2.4410 - val_acc: 0.3454\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9917 - acc: 0.6876\n",
      "Epoch 00081: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 168us/sample - loss: 0.9922 - acc: 0.6875 - val_loss: 2.4479 - val_acc: 0.3413\n",
      "Epoch 82/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9851 - acc: 0.6891\n",
      "Epoch 00082: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 0.9848 - acc: 0.6891 - val_loss: 2.4469 - val_acc: 0.3482\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9781 - acc: 0.6911\n",
      "Epoch 00083: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 0.9781 - acc: 0.6911 - val_loss: 2.4493 - val_acc: 0.3485\n",
      "Epoch 84/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9724 - acc: 0.6925\n",
      "Epoch 00084: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 165us/sample - loss: 0.9722 - acc: 0.6925 - val_loss: 2.4590 - val_acc: 0.3445\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9750 - acc: 0.6912\n",
      "Epoch 00085: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.9748 - acc: 0.6913 - val_loss: 2.4693 - val_acc: 0.3375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9673 - acc: 0.6950\n",
      "Epoch 00086: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 169us/sample - loss: 0.9665 - acc: 0.6953 - val_loss: 2.4724 - val_acc: 0.3440\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9616 - acc: 0.6948\n",
      "Epoch 00087: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 0.9616 - acc: 0.6949 - val_loss: 2.4635 - val_acc: 0.3468\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9676 - acc: 0.6935\n",
      "Epoch 00088: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.9676 - acc: 0.6936 - val_loss: 2.4729 - val_acc: 0.3487\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9563 - acc: 0.6985\n",
      "Epoch 00089: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 169us/sample - loss: 0.9562 - acc: 0.6983 - val_loss: 2.4703 - val_acc: 0.3471\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9539 - acc: 0.6961\n",
      "Epoch 00090: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.9538 - acc: 0.6960 - val_loss: 2.4806 - val_acc: 0.3464\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9450 - acc: 0.6980\n",
      "Epoch 00091: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 0.9447 - acc: 0.6981 - val_loss: 2.4901 - val_acc: 0.3480\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9466 - acc: 0.6998\n",
      "Epoch 00092: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 160us/sample - loss: 0.9467 - acc: 0.6998 - val_loss: 2.4805 - val_acc: 0.3485\n",
      "Epoch 93/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9478 - acc: 0.6967\n",
      "Epoch 00093: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.9476 - acc: 0.6968 - val_loss: 2.4800 - val_acc: 0.3485\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9411 - acc: 0.7003\n",
      "Epoch 00094: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.9406 - acc: 0.7004 - val_loss: 2.4908 - val_acc: 0.3515\n",
      "Epoch 95/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9372 - acc: 0.7016\n",
      "Epoch 00095: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 0.9374 - acc: 0.7015 - val_loss: 2.5017 - val_acc: 0.3457\n",
      "Epoch 96/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9354 - acc: 0.7012\n",
      "Epoch 00096: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.9354 - acc: 0.7013 - val_loss: 2.4914 - val_acc: 0.3427\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9326 - acc: 0.7027\n",
      "Epoch 00097: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.9324 - acc: 0.7028 - val_loss: 2.5032 - val_acc: 0.3527\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9250 - acc: 0.7057\n",
      "Epoch 00098: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.9250 - acc: 0.7057 - val_loss: 2.5035 - val_acc: 0.3478\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9220 - acc: 0.7054\n",
      "Epoch 00099: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.9220 - acc: 0.7054 - val_loss: 2.5028 - val_acc: 0.3513\n",
      "Epoch 100/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9256 - acc: 0.7044\n",
      "Epoch 00100: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 158us/sample - loss: 0.9248 - acc: 0.7047 - val_loss: 2.5070 - val_acc: 0.3522\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9231 - acc: 0.7052\n",
      "Epoch 00101: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.9229 - acc: 0.7052 - val_loss: 2.5126 - val_acc: 0.3510\n",
      "Epoch 102/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9203 - acc: 0.7058\n",
      "Epoch 00102: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.9207 - acc: 0.7056 - val_loss: 2.5138 - val_acc: 0.3457\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9204 - acc: 0.7036\n",
      "Epoch 00103: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 161us/sample - loss: 0.9204 - acc: 0.7036 - val_loss: 2.5180 - val_acc: 0.3536\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9096 - acc: 0.7084\n",
      "Epoch 00104: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 166us/sample - loss: 0.9095 - acc: 0.7084 - val_loss: 2.5230 - val_acc: 0.3510\n",
      "Epoch 105/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9095 - acc: 0.7076\n",
      "Epoch 00105: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 162us/sample - loss: 0.9092 - acc: 0.7076 - val_loss: 2.5285 - val_acc: 0.3492\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8957 - acc: 0.7134\n",
      "Epoch 00106: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.8960 - acc: 0.7132 - val_loss: 2.5197 - val_acc: 0.3520\n",
      "Epoch 107/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.9037 - acc: 0.7111\n",
      "Epoch 00107: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 160us/sample - loss: 0.9047 - acc: 0.7107 - val_loss: 2.5305 - val_acc: 0.3492\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8915 - acc: 0.7153\n",
      "Epoch 00108: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 163us/sample - loss: 0.8914 - acc: 0.7153 - val_loss: 2.5313 - val_acc: 0.3494\n",
      "Epoch 109/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8962 - acc: 0.7129\n",
      "Epoch 00109: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.8966 - acc: 0.7129 - val_loss: 2.5452 - val_acc: 0.3494\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8975 - acc: 0.7103\n",
      "Epoch 00110: val_loss did not improve from 2.21459\n",
      "36805/36805 [==============================] - 6s 164us/sample - loss: 0.8975 - acc: 0.7103 - val_loss: 2.5428 - val_acc: 0.3475\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 140us/sample - loss: 2.2452 - acc: 0.2818\n",
      "Loss: 2.2452293619807513 Accuracy: 0.28182763\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.5109 - acc: 0.2158\n",
      "Epoch 00001: val_loss improved from inf to 2.29192, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/001-2.2919.hdf5\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 2.5101 - acc: 0.2162 - val_loss: 2.2919 - val_acc: 0.3091\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1601 - acc: 0.3418\n",
      "Epoch 00002: val_loss improved from 2.29192 to 2.06798, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/002-2.0680.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 2.1600 - acc: 0.3419 - val_loss: 2.0680 - val_acc: 0.3790\n",
      "Epoch 3/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.9657 - acc: 0.4038\n",
      "Epoch 00003: val_loss improved from 2.06798 to 1.95115, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/003-1.9512.hdf5\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.9653 - acc: 0.4036 - val_loss: 1.9512 - val_acc: 0.4004\n",
      "Epoch 4/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.8325 - acc: 0.4448\n",
      "Epoch 00004: val_loss improved from 1.95115 to 1.86753, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/004-1.8675.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.8323 - acc: 0.4449 - val_loss: 1.8675 - val_acc: 0.4321\n",
      "Epoch 5/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.7334 - acc: 0.4749\n",
      "Epoch 00005: val_loss improved from 1.86753 to 1.81848, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/005-1.8185.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.7333 - acc: 0.4749 - val_loss: 1.8185 - val_acc: 0.4300\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6572 - acc: 0.4965\n",
      "Epoch 00006: val_loss improved from 1.81848 to 1.77957, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/006-1.7796.hdf5\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 1.6566 - acc: 0.4968 - val_loss: 1.7796 - val_acc: 0.4475\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5903 - acc: 0.5147\n",
      "Epoch 00007: val_loss improved from 1.77957 to 1.75498, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/007-1.7550.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.5899 - acc: 0.5148 - val_loss: 1.7550 - val_acc: 0.4433\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5358 - acc: 0.5282\n",
      "Epoch 00008: val_loss improved from 1.75498 to 1.75430, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/008-1.7543.hdf5\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 1.5355 - acc: 0.5283 - val_loss: 1.7543 - val_acc: 0.4433\n",
      "Epoch 9/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.4905 - acc: 0.5407\n",
      "Epoch 00009: val_loss improved from 1.75430 to 1.72247, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/009-1.7225.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.4903 - acc: 0.5407 - val_loss: 1.7225 - val_acc: 0.4589\n",
      "Epoch 10/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.4436 - acc: 0.5547\n",
      "Epoch 00010: val_loss improved from 1.72247 to 1.71126, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/010-1.7113.hdf5\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.4438 - acc: 0.5544 - val_loss: 1.7113 - val_acc: 0.4598\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4107 - acc: 0.5620\n",
      "Epoch 00011: val_loss improved from 1.71126 to 1.70687, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/011-1.7069.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.4105 - acc: 0.5621 - val_loss: 1.7069 - val_acc: 0.4612\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3774 - acc: 0.5716\n",
      "Epoch 00012: val_loss improved from 1.70687 to 1.69954, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/012-1.6995.hdf5\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.3779 - acc: 0.5714 - val_loss: 1.6995 - val_acc: 0.4601\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3517 - acc: 0.5796\n",
      "Epoch 00013: val_loss did not improve from 1.69954\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.3516 - acc: 0.5796 - val_loss: 1.7027 - val_acc: 0.4612\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3278 - acc: 0.5857\n",
      "Epoch 00014: val_loss improved from 1.69954 to 1.69397, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/014-1.6940.hdf5\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.3274 - acc: 0.5858 - val_loss: 1.6940 - val_acc: 0.4696\n",
      "Epoch 15/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.3059 - acc: 0.5919\n",
      "Epoch 00015: val_loss improved from 1.69397 to 1.68782, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/015-1.6878.hdf5\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.3065 - acc: 0.5917 - val_loss: 1.6878 - val_acc: 0.4715\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2806 - acc: 0.6023\n",
      "Epoch 00016: val_loss improved from 1.68782 to 1.68350, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/016-1.6835.hdf5\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.2807 - acc: 0.6023 - val_loss: 1.6835 - val_acc: 0.4663\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2604 - acc: 0.6051\n",
      "Epoch 00017: val_loss improved from 1.68350 to 1.67698, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/017-1.6770.hdf5\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.2607 - acc: 0.6050 - val_loss: 1.6770 - val_acc: 0.4682\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2476 - acc: 0.6075\n",
      "Epoch 00018: val_loss did not improve from 1.67698\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.2474 - acc: 0.6076 - val_loss: 1.6786 - val_acc: 0.4759\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2272 - acc: 0.6161\n",
      "Epoch 00019: val_loss improved from 1.67698 to 1.67386, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/019-1.6739.hdf5\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.2273 - acc: 0.6159 - val_loss: 1.6739 - val_acc: 0.4729\n",
      "Epoch 20/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2114 - acc: 0.6212\n",
      "Epoch 00020: val_loss did not improve from 1.67386\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.2114 - acc: 0.6215 - val_loss: 1.6930 - val_acc: 0.4710\n",
      "Epoch 21/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1958 - acc: 0.6240\n",
      "Epoch 00021: val_loss did not improve from 1.67386\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.1949 - acc: 0.6242 - val_loss: 1.6834 - val_acc: 0.4759\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1782 - acc: 0.6304\n",
      "Epoch 00022: val_loss improved from 1.67386 to 1.66728, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/022-1.6673.hdf5\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.1783 - acc: 0.6304 - val_loss: 1.6673 - val_acc: 0.4796\n",
      "Epoch 23/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1637 - acc: 0.6342\n",
      "Epoch 00023: val_loss did not improve from 1.66728\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.1631 - acc: 0.6344 - val_loss: 1.6693 - val_acc: 0.4764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1515 - acc: 0.6368\n",
      "Epoch 00024: val_loss did not improve from 1.66728\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 1.1518 - acc: 0.6368 - val_loss: 1.6732 - val_acc: 0.4864\n",
      "Epoch 25/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1365 - acc: 0.6426\n",
      "Epoch 00025: val_loss improved from 1.66728 to 1.66179, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/025-1.6618.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.1381 - acc: 0.6424 - val_loss: 1.6618 - val_acc: 0.4908\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1229 - acc: 0.6460\n",
      "Epoch 00026: val_loss did not improve from 1.66179\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.1227 - acc: 0.6460 - val_loss: 1.6655 - val_acc: 0.4854\n",
      "Epoch 27/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1061 - acc: 0.6509\n",
      "Epoch 00027: val_loss improved from 1.66179 to 1.66021, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/027-1.6602.hdf5\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 1.1060 - acc: 0.6512 - val_loss: 1.6602 - val_acc: 0.4906\n",
      "Epoch 28/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0982 - acc: 0.6556\n",
      "Epoch 00028: val_loss did not improve from 1.66021\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.0983 - acc: 0.6556 - val_loss: 1.6670 - val_acc: 0.4873\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0826 - acc: 0.6593\n",
      "Epoch 00029: val_loss did not improve from 1.66021\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 1.0826 - acc: 0.6593 - val_loss: 1.6671 - val_acc: 0.4924\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0706 - acc: 0.6627\n",
      "Epoch 00030: val_loss did not improve from 1.66021\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 1.0702 - acc: 0.6629 - val_loss: 1.6695 - val_acc: 0.4908\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0582 - acc: 0.6659\n",
      "Epoch 00031: val_loss improved from 1.66021 to 1.65337, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/031-1.6534.hdf5\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.0578 - acc: 0.6659 - val_loss: 1.6534 - val_acc: 0.4952\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0490 - acc: 0.6656\n",
      "Epoch 00032: val_loss did not improve from 1.65337\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.0490 - acc: 0.6656 - val_loss: 1.6572 - val_acc: 0.4969\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0331 - acc: 0.6752\n",
      "Epoch 00033: val_loss improved from 1.65337 to 1.65082, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_2_conv_checkpoint/033-1.6508.hdf5\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 1.0331 - acc: 0.6752 - val_loss: 1.6508 - val_acc: 0.4976\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0284 - acc: 0.6743\n",
      "Epoch 00034: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 1.0284 - acc: 0.6743 - val_loss: 1.6564 - val_acc: 0.4983\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0115 - acc: 0.6823\n",
      "Epoch 00035: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 1.0118 - acc: 0.6820 - val_loss: 1.6572 - val_acc: 0.5013\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0077 - acc: 0.6797\n",
      "Epoch 00036: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 1.0081 - acc: 0.6795 - val_loss: 1.6664 - val_acc: 0.4962\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9965 - acc: 0.6853\n",
      "Epoch 00037: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.9965 - acc: 0.6853 - val_loss: 1.6582 - val_acc: 0.4997\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9856 - acc: 0.6884\n",
      "Epoch 00038: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.9855 - acc: 0.6884 - val_loss: 1.6634 - val_acc: 0.4980\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9805 - acc: 0.6885\n",
      "Epoch 00039: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.9806 - acc: 0.6885 - val_loss: 1.6652 - val_acc: 0.4966\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9715 - acc: 0.6905\n",
      "Epoch 00040: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.9714 - acc: 0.6905 - val_loss: 1.6531 - val_acc: 0.5022\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9594 - acc: 0.6978\n",
      "Epoch 00041: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.9591 - acc: 0.6978 - val_loss: 1.6669 - val_acc: 0.5017\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9482 - acc: 0.6985\n",
      "Epoch 00042: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.9482 - acc: 0.6984 - val_loss: 1.6665 - val_acc: 0.5090\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9409 - acc: 0.6997\n",
      "Epoch 00043: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.9408 - acc: 0.6999 - val_loss: 1.6704 - val_acc: 0.5001\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9319 - acc: 0.7046\n",
      "Epoch 00044: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.9318 - acc: 0.7045 - val_loss: 1.6649 - val_acc: 0.5076\n",
      "Epoch 45/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9203 - acc: 0.7073\n",
      "Epoch 00045: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.9206 - acc: 0.7070 - val_loss: 1.6596 - val_acc: 0.5125\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9154 - acc: 0.7090\n",
      "Epoch 00046: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.9157 - acc: 0.7088 - val_loss: 1.6749 - val_acc: 0.5085\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9123 - acc: 0.7081\n",
      "Epoch 00047: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.9124 - acc: 0.7081 - val_loss: 1.6668 - val_acc: 0.5092\n",
      "Epoch 48/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8989 - acc: 0.7117\n",
      "Epoch 00048: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.8989 - acc: 0.7118 - val_loss: 1.6636 - val_acc: 0.5125\n",
      "Epoch 49/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8948 - acc: 0.7154\n",
      "Epoch 00049: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 0.8949 - acc: 0.7154 - val_loss: 1.6553 - val_acc: 0.5164\n",
      "Epoch 50/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8853 - acc: 0.7183\n",
      "Epoch 00050: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.8855 - acc: 0.7182 - val_loss: 1.6727 - val_acc: 0.5127\n",
      "Epoch 51/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8755 - acc: 0.7221\n",
      "Epoch 00051: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.8759 - acc: 0.7220 - val_loss: 1.6671 - val_acc: 0.5122\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8679 - acc: 0.7227\n",
      "Epoch 00052: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.8679 - acc: 0.7227 - val_loss: 1.6741 - val_acc: 0.5111\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8547 - acc: 0.7261\n",
      "Epoch 00053: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.8553 - acc: 0.7259 - val_loss: 1.6696 - val_acc: 0.5167\n",
      "Epoch 54/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8557 - acc: 0.7250\n",
      "Epoch 00054: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 204us/sample - loss: 0.8554 - acc: 0.7251 - val_loss: 1.6659 - val_acc: 0.5174\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8481 - acc: 0.7303\n",
      "Epoch 00055: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.8477 - acc: 0.7303 - val_loss: 1.6731 - val_acc: 0.5153\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8451 - acc: 0.7296\n",
      "Epoch 00056: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.8452 - acc: 0.7295 - val_loss: 1.6674 - val_acc: 0.5204\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8335 - acc: 0.7321\n",
      "Epoch 00057: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.8339 - acc: 0.7320 - val_loss: 1.6676 - val_acc: 0.5174\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8298 - acc: 0.7326\n",
      "Epoch 00058: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.8300 - acc: 0.7326 - val_loss: 1.6589 - val_acc: 0.5222\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8228 - acc: 0.7381\n",
      "Epoch 00059: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.8229 - acc: 0.7381 - val_loss: 1.6882 - val_acc: 0.5136\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8129 - acc: 0.7368\n",
      "Epoch 00060: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.8129 - acc: 0.7368 - val_loss: 1.6688 - val_acc: 0.5218\n",
      "Epoch 61/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8080 - acc: 0.7405\n",
      "Epoch 00061: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.8079 - acc: 0.7405 - val_loss: 1.6710 - val_acc: 0.5250\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8034 - acc: 0.7431\n",
      "Epoch 00062: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.8031 - acc: 0.7433 - val_loss: 1.6780 - val_acc: 0.5260\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7958 - acc: 0.7433\n",
      "Epoch 00063: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.7958 - acc: 0.7434 - val_loss: 1.6916 - val_acc: 0.5232\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7938 - acc: 0.7454\n",
      "Epoch 00064: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.7938 - acc: 0.7453 - val_loss: 1.6838 - val_acc: 0.5274\n",
      "Epoch 65/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7839 - acc: 0.7477\n",
      "Epoch 00065: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.7836 - acc: 0.7477 - val_loss: 1.6877 - val_acc: 0.5220\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7700 - acc: 0.7518\n",
      "Epoch 00066: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.7700 - acc: 0.7517 - val_loss: 1.6771 - val_acc: 0.5281\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7681 - acc: 0.7540\n",
      "Epoch 00067: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.7680 - acc: 0.7541 - val_loss: 1.7002 - val_acc: 0.5225\n",
      "Epoch 68/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7623 - acc: 0.7566\n",
      "Epoch 00068: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.7619 - acc: 0.7566 - val_loss: 1.6899 - val_acc: 0.5248\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7594 - acc: 0.7547\n",
      "Epoch 00069: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.7593 - acc: 0.7549 - val_loss: 1.6901 - val_acc: 0.5253\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7508 - acc: 0.7555\n",
      "Epoch 00070: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.7513 - acc: 0.7554 - val_loss: 1.6903 - val_acc: 0.5309\n",
      "Epoch 71/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7472 - acc: 0.7594\n",
      "Epoch 00071: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.7468 - acc: 0.7595 - val_loss: 1.6970 - val_acc: 0.5283\n",
      "Epoch 72/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7394 - acc: 0.7623\n",
      "Epoch 00072: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.7394 - acc: 0.7624 - val_loss: 1.6963 - val_acc: 0.5281\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7318 - acc: 0.7634\n",
      "Epoch 00073: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.7316 - acc: 0.7634 - val_loss: 1.6975 - val_acc: 0.5304\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7305 - acc: 0.7653\n",
      "Epoch 00074: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.7306 - acc: 0.7653 - val_loss: 1.7014 - val_acc: 0.5302\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7231 - acc: 0.7657\n",
      "Epoch 00075: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.7228 - acc: 0.7657 - val_loss: 1.6969 - val_acc: 0.5271\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7172 - acc: 0.7683\n",
      "Epoch 00076: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.7169 - acc: 0.7684 - val_loss: 1.6965 - val_acc: 0.5297\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7094 - acc: 0.7676\n",
      "Epoch 00077: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.7092 - acc: 0.7677 - val_loss: 1.6960 - val_acc: 0.5341\n",
      "Epoch 78/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7030 - acc: 0.7713\n",
      "Epoch 00078: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.7030 - acc: 0.7713 - val_loss: 1.6982 - val_acc: 0.5353\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7012 - acc: 0.7724\n",
      "Epoch 00079: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.7012 - acc: 0.7724 - val_loss: 1.7021 - val_acc: 0.5353\n",
      "Epoch 80/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6959 - acc: 0.7754\n",
      "Epoch 00080: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.6961 - acc: 0.7752 - val_loss: 1.7061 - val_acc: 0.5341\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6978 - acc: 0.7739\n",
      "Epoch 00081: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.6981 - acc: 0.7739 - val_loss: 1.7027 - val_acc: 0.5325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6895 - acc: 0.7772\n",
      "Epoch 00082: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 205us/sample - loss: 0.6896 - acc: 0.7770 - val_loss: 1.7146 - val_acc: 0.5297\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6832 - acc: 0.7764\n",
      "Epoch 00083: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.6835 - acc: 0.7763 - val_loss: 1.7174 - val_acc: 0.5346\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6782 - acc: 0.7812\n",
      "Epoch 00084: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 0.6785 - acc: 0.7812 - val_loss: 1.7227 - val_acc: 0.5411\n",
      "Epoch 85/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.6732 - acc: 0.7838\n",
      "Epoch 00085: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.6737 - acc: 0.7834 - val_loss: 1.7199 - val_acc: 0.5353\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6683 - acc: 0.7851\n",
      "Epoch 00086: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 0.6682 - acc: 0.7851 - val_loss: 1.7236 - val_acc: 0.5365\n",
      "Epoch 87/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6665 - acc: 0.7823\n",
      "Epoch 00087: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.6664 - acc: 0.7823 - val_loss: 1.7224 - val_acc: 0.5406\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6647 - acc: 0.7856\n",
      "Epoch 00088: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.6645 - acc: 0.7855 - val_loss: 1.7178 - val_acc: 0.5348\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6579 - acc: 0.7857\n",
      "Epoch 00089: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 0.6577 - acc: 0.7858 - val_loss: 1.7129 - val_acc: 0.5353\n",
      "Epoch 90/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6523 - acc: 0.7884\n",
      "Epoch 00090: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.6522 - acc: 0.7886 - val_loss: 1.7163 - val_acc: 0.5390\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6525 - acc: 0.7885\n",
      "Epoch 00091: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.6526 - acc: 0.7885 - val_loss: 1.7149 - val_acc: 0.5427\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6426 - acc: 0.7916\n",
      "Epoch 00092: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.6431 - acc: 0.7912 - val_loss: 1.7094 - val_acc: 0.5404\n",
      "Epoch 93/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6408 - acc: 0.7915\n",
      "Epoch 00093: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.6412 - acc: 0.7911 - val_loss: 1.7188 - val_acc: 0.5430\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6368 - acc: 0.7916\n",
      "Epoch 00094: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.6367 - acc: 0.7917 - val_loss: 1.7426 - val_acc: 0.5341\n",
      "Epoch 95/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.6336 - acc: 0.7939\n",
      "Epoch 00095: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.6337 - acc: 0.7938 - val_loss: 1.7207 - val_acc: 0.5392\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6268 - acc: 0.7965\n",
      "Epoch 00096: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.6272 - acc: 0.7964 - val_loss: 1.7187 - val_acc: 0.5420\n",
      "Epoch 97/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6223 - acc: 0.7980\n",
      "Epoch 00097: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.6225 - acc: 0.7980 - val_loss: 1.7391 - val_acc: 0.5411\n",
      "Epoch 98/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6235 - acc: 0.7985\n",
      "Epoch 00098: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.6236 - acc: 0.7985 - val_loss: 1.7416 - val_acc: 0.5399\n",
      "Epoch 99/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6186 - acc: 0.7995\n",
      "Epoch 00099: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6184 - acc: 0.7996 - val_loss: 1.7263 - val_acc: 0.5444\n",
      "Epoch 100/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6182 - acc: 0.7986\n",
      "Epoch 00100: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.6177 - acc: 0.7988 - val_loss: 1.7360 - val_acc: 0.5413\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6141 - acc: 0.7990\n",
      "Epoch 00101: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.6142 - acc: 0.7990 - val_loss: 1.7269 - val_acc: 0.5439\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6048 - acc: 0.8040\n",
      "Epoch 00102: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.6048 - acc: 0.8041 - val_loss: 1.7356 - val_acc: 0.5432\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6026 - acc: 0.8049\n",
      "Epoch 00103: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.6025 - acc: 0.8050 - val_loss: 1.7324 - val_acc: 0.5469\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5995 - acc: 0.8029\n",
      "Epoch 00104: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5993 - acc: 0.8030 - val_loss: 1.7368 - val_acc: 0.5434\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5964 - acc: 0.8039\n",
      "Epoch 00105: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5964 - acc: 0.8040 - val_loss: 1.7414 - val_acc: 0.5451\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.8052\n",
      "Epoch 00106: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5895 - acc: 0.8049 - val_loss: 1.7529 - val_acc: 0.5439\n",
      "Epoch 107/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5912 - acc: 0.8076\n",
      "Epoch 00107: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.5912 - acc: 0.8076 - val_loss: 1.7517 - val_acc: 0.5458\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5887 - acc: 0.8075\n",
      "Epoch 00108: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.5886 - acc: 0.8076 - val_loss: 1.7533 - val_acc: 0.5455\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5803 - acc: 0.8092\n",
      "Epoch 00109: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.5803 - acc: 0.8092 - val_loss: 1.7581 - val_acc: 0.5486\n",
      "Epoch 110/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5828 - acc: 0.8077\n",
      "Epoch 00110: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5831 - acc: 0.8076 - val_loss: 1.7454 - val_acc: 0.5488\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5689 - acc: 0.8127\n",
      "Epoch 00111: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5689 - acc: 0.8126 - val_loss: 1.7687 - val_acc: 0.5434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5746 - acc: 0.8113\n",
      "Epoch 00112: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.5744 - acc: 0.8114 - val_loss: 1.7531 - val_acc: 0.5490\n",
      "Epoch 113/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5667 - acc: 0.8145\n",
      "Epoch 00113: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.5659 - acc: 0.8148 - val_loss: 1.7565 - val_acc: 0.5488\n",
      "Epoch 114/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.8134\n",
      "Epoch 00114: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5648 - acc: 0.8136 - val_loss: 1.7606 - val_acc: 0.5504\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5624 - acc: 0.8160\n",
      "Epoch 00115: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.5624 - acc: 0.8160 - val_loss: 1.7689 - val_acc: 0.5455\n",
      "Epoch 116/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.8192\n",
      "Epoch 00116: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.5527 - acc: 0.8189 - val_loss: 1.7765 - val_acc: 0.5462\n",
      "Epoch 117/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.8160\n",
      "Epoch 00117: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.5584 - acc: 0.8160 - val_loss: 1.7637 - val_acc: 0.5486\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.8180\n",
      "Epoch 00118: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.5502 - acc: 0.8180 - val_loss: 1.7779 - val_acc: 0.5486\n",
      "Epoch 119/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.8195\n",
      "Epoch 00119: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.5516 - acc: 0.8193 - val_loss: 1.7882 - val_acc: 0.5460\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8231\n",
      "Epoch 00120: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.5450 - acc: 0.8230 - val_loss: 1.7829 - val_acc: 0.5495\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.8230\n",
      "Epoch 00121: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5405 - acc: 0.8230 - val_loss: 1.7900 - val_acc: 0.5432\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5356 - acc: 0.8256\n",
      "Epoch 00122: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.5356 - acc: 0.8256 - val_loss: 1.7951 - val_acc: 0.5523\n",
      "Epoch 123/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.8250\n",
      "Epoch 00123: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.5376 - acc: 0.8248 - val_loss: 1.7968 - val_acc: 0.5528\n",
      "Epoch 124/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.8266\n",
      "Epoch 00124: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.5305 - acc: 0.8266 - val_loss: 1.8006 - val_acc: 0.5493\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.8224\n",
      "Epoch 00125: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.5373 - acc: 0.8224 - val_loss: 1.7901 - val_acc: 0.5465\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.8270\n",
      "Epoch 00126: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.5297 - acc: 0.8271 - val_loss: 1.7875 - val_acc: 0.5537\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.8218\n",
      "Epoch 00127: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5388 - acc: 0.8218 - val_loss: 1.7794 - val_acc: 0.5521\n",
      "Epoch 128/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.8294\n",
      "Epoch 00128: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.5244 - acc: 0.8292 - val_loss: 1.8024 - val_acc: 0.5483\n",
      "Epoch 129/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5203 - acc: 0.8299\n",
      "Epoch 00129: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5206 - acc: 0.8298 - val_loss: 1.7887 - val_acc: 0.5544\n",
      "Epoch 130/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.8297\n",
      "Epoch 00130: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5144 - acc: 0.8299 - val_loss: 1.8113 - val_acc: 0.5488\n",
      "Epoch 131/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8331\n",
      "Epoch 00131: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5118 - acc: 0.8330 - val_loss: 1.8060 - val_acc: 0.5502\n",
      "Epoch 132/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.8346\n",
      "Epoch 00132: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5113 - acc: 0.8347 - val_loss: 1.7939 - val_acc: 0.5523\n",
      "Epoch 133/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5088 - acc: 0.8309\n",
      "Epoch 00133: val_loss did not improve from 1.65082\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.5092 - acc: 0.8306 - val_loss: 1.8006 - val_acc: 0.5542\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 159us/sample - loss: 1.7158 - acc: 0.4671\n",
      "Loss: 1.7157914078000427 Accuracy: 0.46708202\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.4930 - acc: 0.2115\n",
      "Epoch 00001: val_loss improved from inf to 2.16232, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/001-2.1623.hdf5\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 2.4921 - acc: 0.2119 - val_loss: 2.1623 - val_acc: 0.3471\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9921 - acc: 0.3825\n",
      "Epoch 00002: val_loss improved from 2.16232 to 1.79949, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/002-1.7995.hdf5\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 1.9921 - acc: 0.3825 - val_loss: 1.7995 - val_acc: 0.4545\n",
      "Epoch 3/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.7518 - acc: 0.4569\n",
      "Epoch 00003: val_loss improved from 1.79949 to 1.64642, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/003-1.6464.hdf5\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 1.7518 - acc: 0.4568 - val_loss: 1.6464 - val_acc: 0.4936\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6249 - acc: 0.4997\n",
      "Epoch 00004: val_loss improved from 1.64642 to 1.54406, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/004-1.5441.hdf5\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 1.6242 - acc: 0.4999 - val_loss: 1.5441 - val_acc: 0.5290\n",
      "Epoch 5/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.5407 - acc: 0.5236\n",
      "Epoch 00005: val_loss improved from 1.54406 to 1.50303, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/005-1.5030.hdf5\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 1.5400 - acc: 0.5237 - val_loss: 1.5030 - val_acc: 0.5339\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4761 - acc: 0.5458\n",
      "Epoch 00006: val_loss improved from 1.50303 to 1.45766, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/006-1.4577.hdf5\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 1.4761 - acc: 0.5457 - val_loss: 1.4577 - val_acc: 0.5535\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4305 - acc: 0.5611\n",
      "Epoch 00007: val_loss improved from 1.45766 to 1.41659, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/007-1.4166.hdf5\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 1.4303 - acc: 0.5611 - val_loss: 1.4166 - val_acc: 0.5621\n",
      "Epoch 8/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3855 - acc: 0.5743\n",
      "Epoch 00008: val_loss improved from 1.41659 to 1.38683, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/008-1.3868.hdf5\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 1.3857 - acc: 0.5744 - val_loss: 1.3868 - val_acc: 0.5777\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3466 - acc: 0.5874\n",
      "Epoch 00009: val_loss did not improve from 1.38683\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 1.3470 - acc: 0.5873 - val_loss: 1.3962 - val_acc: 0.5681\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3179 - acc: 0.5989\n",
      "Epoch 00010: val_loss improved from 1.38683 to 1.35734, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/010-1.3573.hdf5\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 1.3179 - acc: 0.5989 - val_loss: 1.3573 - val_acc: 0.5784\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2870 - acc: 0.6081\n",
      "Epoch 00011: val_loss improved from 1.35734 to 1.33428, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/011-1.3343.hdf5\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 1.2866 - acc: 0.6082 - val_loss: 1.3343 - val_acc: 0.5907\n",
      "Epoch 12/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2622 - acc: 0.6168\n",
      "Epoch 00012: val_loss improved from 1.33428 to 1.32946, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/012-1.3295.hdf5\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 1.2620 - acc: 0.6166 - val_loss: 1.3295 - val_acc: 0.5835\n",
      "Epoch 13/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2297 - acc: 0.6272\n",
      "Epoch 00013: val_loss improved from 1.32946 to 1.30217, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/013-1.3022.hdf5\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 1.2295 - acc: 0.6273 - val_loss: 1.3022 - val_acc: 0.5903\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2081 - acc: 0.6343\n",
      "Epoch 00014: val_loss improved from 1.30217 to 1.29512, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/014-1.2951.hdf5\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 1.2083 - acc: 0.6343 - val_loss: 1.2951 - val_acc: 0.5970\n",
      "Epoch 15/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1844 - acc: 0.6421\n",
      "Epoch 00015: val_loss improved from 1.29512 to 1.29245, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/015-1.2925.hdf5\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 1.1839 - acc: 0.6422 - val_loss: 1.2925 - val_acc: 0.6007\n",
      "Epoch 16/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1602 - acc: 0.6494\n",
      "Epoch 00016: val_loss did not improve from 1.29245\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 1.1604 - acc: 0.6493 - val_loss: 1.2939 - val_acc: 0.5940\n",
      "Epoch 17/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1410 - acc: 0.6528\n",
      "Epoch 00017: val_loss improved from 1.29245 to 1.29070, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/017-1.2907.hdf5\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 1.1416 - acc: 0.6525 - val_loss: 1.2907 - val_acc: 0.6000\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1206 - acc: 0.6594\n",
      "Epoch 00018: val_loss improved from 1.29070 to 1.28162, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/018-1.2816.hdf5\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 1.1209 - acc: 0.6596 - val_loss: 1.2816 - val_acc: 0.6059\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1008 - acc: 0.6675\n",
      "Epoch 00019: val_loss improved from 1.28162 to 1.27077, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/019-1.2708.hdf5\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 1.1010 - acc: 0.6671 - val_loss: 1.2708 - val_acc: 0.6089\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0858 - acc: 0.6716\n",
      "Epoch 00020: val_loss improved from 1.27077 to 1.25161, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/020-1.2516.hdf5\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 1.0851 - acc: 0.6717 - val_loss: 1.2516 - val_acc: 0.6140\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0631 - acc: 0.6790\n",
      "Epoch 00021: val_loss did not improve from 1.25161\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 1.0629 - acc: 0.6791 - val_loss: 1.2575 - val_acc: 0.6061\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0470 - acc: 0.6816\n",
      "Epoch 00022: val_loss did not improve from 1.25161\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 1.0470 - acc: 0.6817 - val_loss: 1.2694 - val_acc: 0.6024\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0325 - acc: 0.6837\n",
      "Epoch 00023: val_loss improved from 1.25161 to 1.23483, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/023-1.2348.hdf5\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 1.0319 - acc: 0.6839 - val_loss: 1.2348 - val_acc: 0.6175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0147 - acc: 0.6922\n",
      "Epoch 00024: val_loss improved from 1.23483 to 1.23414, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/024-1.2341.hdf5\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 1.0138 - acc: 0.6924 - val_loss: 1.2341 - val_acc: 0.6161\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9973 - acc: 0.6964\n",
      "Epoch 00025: val_loss improved from 1.23414 to 1.22667, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/025-1.2267.hdf5\n",
      "36805/36805 [==============================] - 9s 252us/sample - loss: 0.9973 - acc: 0.6964 - val_loss: 1.2267 - val_acc: 0.6173\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9834 - acc: 0.7024\n",
      "Epoch 00026: val_loss improved from 1.22667 to 1.22293, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/026-1.2229.hdf5\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.9835 - acc: 0.7022 - val_loss: 1.2229 - val_acc: 0.6231\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9648 - acc: 0.7075\n",
      "Epoch 00027: val_loss did not improve from 1.22293\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.9649 - acc: 0.7073 - val_loss: 1.2296 - val_acc: 0.6171\n",
      "Epoch 28/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9486 - acc: 0.7127\n",
      "Epoch 00028: val_loss did not improve from 1.22293\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.9489 - acc: 0.7126 - val_loss: 1.2431 - val_acc: 0.6101\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9356 - acc: 0.7139\n",
      "Epoch 00029: val_loss improved from 1.22293 to 1.22262, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/029-1.2226.hdf5\n",
      "36805/36805 [==============================] - 9s 254us/sample - loss: 0.9355 - acc: 0.7139 - val_loss: 1.2226 - val_acc: 0.6222\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9181 - acc: 0.7188\n",
      "Epoch 00030: val_loss improved from 1.22262 to 1.21586, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/030-1.2159.hdf5\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.9181 - acc: 0.7188 - val_loss: 1.2159 - val_acc: 0.6184\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9035 - acc: 0.7250\n",
      "Epoch 00031: val_loss improved from 1.21586 to 1.20272, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/031-1.2027.hdf5\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.9034 - acc: 0.7250 - val_loss: 1.2027 - val_acc: 0.6289\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8937 - acc: 0.7248\n",
      "Epoch 00032: val_loss did not improve from 1.20272\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.8938 - acc: 0.7247 - val_loss: 1.2106 - val_acc: 0.6226\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8854 - acc: 0.7283\n",
      "Epoch 00033: val_loss improved from 1.20272 to 1.20228, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/033-1.2023.hdf5\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.8853 - acc: 0.7282 - val_loss: 1.2023 - val_acc: 0.6324\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8687 - acc: 0.7316\n",
      "Epoch 00034: val_loss improved from 1.20228 to 1.20076, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/034-1.2008.hdf5\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.8686 - acc: 0.7316 - val_loss: 1.2008 - val_acc: 0.6322\n",
      "Epoch 35/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8560 - acc: 0.7371\n",
      "Epoch 00035: val_loss did not improve from 1.20076\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.8566 - acc: 0.7368 - val_loss: 1.2078 - val_acc: 0.6306\n",
      "Epoch 36/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8447 - acc: 0.7392\n",
      "Epoch 00036: val_loss improved from 1.20076 to 1.19405, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/036-1.1941.hdf5\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.8445 - acc: 0.7392 - val_loss: 1.1941 - val_acc: 0.6313\n",
      "Epoch 37/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8323 - acc: 0.7433\n",
      "Epoch 00037: val_loss improved from 1.19405 to 1.18658, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/037-1.1866.hdf5\n",
      "36805/36805 [==============================] - 9s 253us/sample - loss: 0.8327 - acc: 0.7433 - val_loss: 1.1866 - val_acc: 0.6359\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8209 - acc: 0.7483\n",
      "Epoch 00038: val_loss improved from 1.18658 to 1.18084, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/038-1.1808.hdf5\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.8211 - acc: 0.7484 - val_loss: 1.1808 - val_acc: 0.6352\n",
      "Epoch 39/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8089 - acc: 0.7489\n",
      "Epoch 00039: val_loss did not improve from 1.18084\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.8086 - acc: 0.7489 - val_loss: 1.1942 - val_acc: 0.6317\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7987 - acc: 0.7540\n",
      "Epoch 00040: val_loss did not improve from 1.18084\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.7987 - acc: 0.7539 - val_loss: 1.1919 - val_acc: 0.6322\n",
      "Epoch 41/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7865 - acc: 0.7569\n",
      "Epoch 00041: val_loss did not improve from 1.18084\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.7868 - acc: 0.7570 - val_loss: 1.1858 - val_acc: 0.6362\n",
      "Epoch 42/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7770 - acc: 0.7587\n",
      "Epoch 00042: val_loss improved from 1.18084 to 1.17525, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/042-1.1752.hdf5\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.7768 - acc: 0.7586 - val_loss: 1.1752 - val_acc: 0.6399\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7658 - acc: 0.7615\n",
      "Epoch 00043: val_loss did not improve from 1.17525\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.7658 - acc: 0.7614 - val_loss: 1.1814 - val_acc: 0.6375\n",
      "Epoch 44/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7626 - acc: 0.7639\n",
      "Epoch 00044: val_loss did not improve from 1.17525\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.7626 - acc: 0.7637 - val_loss: 1.1808 - val_acc: 0.6415\n",
      "Epoch 45/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7503 - acc: 0.7684\n",
      "Epoch 00045: val_loss did not improve from 1.17525\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.7497 - acc: 0.7685 - val_loss: 1.1840 - val_acc: 0.6401\n",
      "Epoch 46/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7367 - acc: 0.7712\n",
      "Epoch 00046: val_loss improved from 1.17525 to 1.16911, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/046-1.1691.hdf5\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.7365 - acc: 0.7713 - val_loss: 1.1691 - val_acc: 0.6448\n",
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7248 - acc: 0.7753\n",
      "Epoch 00047: val_loss did not improve from 1.16911\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.7244 - acc: 0.7754 - val_loss: 1.1757 - val_acc: 0.6399\n",
      "Epoch 48/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7207 - acc: 0.7744\n",
      "Epoch 00048: val_loss did not improve from 1.16911\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.7208 - acc: 0.7745 - val_loss: 1.1710 - val_acc: 0.6480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7104 - acc: 0.7814\n",
      "Epoch 00049: val_loss improved from 1.16911 to 1.16273, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/049-1.1627.hdf5\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.7106 - acc: 0.7811 - val_loss: 1.1627 - val_acc: 0.6445\n",
      "Epoch 50/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6968 - acc: 0.7829\n",
      "Epoch 00050: val_loss did not improve from 1.16273\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.6972 - acc: 0.7827 - val_loss: 1.1836 - val_acc: 0.6448\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.7839\n",
      "Epoch 00051: val_loss did not improve from 1.16273\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.6915 - acc: 0.7840 - val_loss: 1.1709 - val_acc: 0.6452\n",
      "Epoch 52/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6812 - acc: 0.7870\n",
      "Epoch 00052: val_loss did not improve from 1.16273\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.6805 - acc: 0.7873 - val_loss: 1.1672 - val_acc: 0.6469\n",
      "Epoch 53/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6748 - acc: 0.7872\n",
      "Epoch 00053: val_loss did not improve from 1.16273\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.6755 - acc: 0.7870 - val_loss: 1.1641 - val_acc: 0.6483\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6696 - acc: 0.7923\n",
      "Epoch 00054: val_loss did not improve from 1.16273\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.6695 - acc: 0.7923 - val_loss: 1.1672 - val_acc: 0.6518\n",
      "Epoch 55/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6568 - acc: 0.7928\n",
      "Epoch 00055: val_loss did not improve from 1.16273\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.6570 - acc: 0.7929 - val_loss: 1.1752 - val_acc: 0.6541\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6514 - acc: 0.7950\n",
      "Epoch 00056: val_loss did not improve from 1.16273\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.6509 - acc: 0.7952 - val_loss: 1.1659 - val_acc: 0.6506\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6395 - acc: 0.7999\n",
      "Epoch 00057: val_loss improved from 1.16273 to 1.16228, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/057-1.1623.hdf5\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.6391 - acc: 0.8000 - val_loss: 1.1623 - val_acc: 0.6534\n",
      "Epoch 58/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6328 - acc: 0.8014\n",
      "Epoch 00058: val_loss did not improve from 1.16228\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.6328 - acc: 0.8014 - val_loss: 1.1716 - val_acc: 0.6546\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6268 - acc: 0.8048\n",
      "Epoch 00059: val_loss did not improve from 1.16228\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.6265 - acc: 0.8049 - val_loss: 1.1795 - val_acc: 0.6515\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6216 - acc: 0.8030\n",
      "Epoch 00060: val_loss did not improve from 1.16228\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.6215 - acc: 0.8029 - val_loss: 1.1663 - val_acc: 0.6527\n",
      "Epoch 61/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6149 - acc: 0.8030\n",
      "Epoch 00061: val_loss did not improve from 1.16228\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.6150 - acc: 0.8030 - val_loss: 1.1811 - val_acc: 0.6515\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6008 - acc: 0.8096\n",
      "Epoch 00062: val_loss did not improve from 1.16228\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.6009 - acc: 0.8095 - val_loss: 1.1665 - val_acc: 0.6587\n",
      "Epoch 63/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5987 - acc: 0.8108\n",
      "Epoch 00063: val_loss did not improve from 1.16228\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.5985 - acc: 0.8110 - val_loss: 1.1668 - val_acc: 0.6573\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5935 - acc: 0.8123\n",
      "Epoch 00064: val_loss improved from 1.16228 to 1.16107, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/064-1.1611.hdf5\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.5934 - acc: 0.8123 - val_loss: 1.1611 - val_acc: 0.6583\n",
      "Epoch 65/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.8131\n",
      "Epoch 00065: val_loss did not improve from 1.16107\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.5890 - acc: 0.8130 - val_loss: 1.1775 - val_acc: 0.6534\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5777 - acc: 0.8183\n",
      "Epoch 00066: val_loss improved from 1.16107 to 1.15957, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/066-1.1596.hdf5\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.5781 - acc: 0.8184 - val_loss: 1.1596 - val_acc: 0.6608\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5745 - acc: 0.8168\n",
      "Epoch 00067: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.5745 - acc: 0.8168 - val_loss: 1.1735 - val_acc: 0.6592\n",
      "Epoch 68/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5690 - acc: 0.8201\n",
      "Epoch 00068: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.5690 - acc: 0.8199 - val_loss: 1.1600 - val_acc: 0.6597\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5602 - acc: 0.8246\n",
      "Epoch 00069: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.5604 - acc: 0.8245 - val_loss: 1.1766 - val_acc: 0.6587\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.8226\n",
      "Epoch 00070: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.5539 - acc: 0.8225 - val_loss: 1.1634 - val_acc: 0.6653\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.8289\n",
      "Epoch 00071: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.5456 - acc: 0.8290 - val_loss: 1.1740 - val_acc: 0.6611\n",
      "Epoch 72/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.8275\n",
      "Epoch 00072: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.5450 - acc: 0.8274 - val_loss: 1.1728 - val_acc: 0.6678\n",
      "Epoch 73/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.8284\n",
      "Epoch 00073: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.5426 - acc: 0.8283 - val_loss: 1.1698 - val_acc: 0.6629\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.8312\n",
      "Epoch 00074: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.5274 - acc: 0.8311 - val_loss: 1.1782 - val_acc: 0.6562\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.8292\n",
      "Epoch 00075: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.5304 - acc: 0.8292 - val_loss: 1.1882 - val_acc: 0.6601\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.8281\n",
      "Epoch 00076: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.5321 - acc: 0.8282 - val_loss: 1.1747 - val_acc: 0.6667\n",
      "Epoch 77/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.8353\n",
      "Epoch 00077: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.5196 - acc: 0.8355 - val_loss: 1.1715 - val_acc: 0.6678\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.8353\n",
      "Epoch 00078: val_loss did not improve from 1.15957\n",
      "36805/36805 [==============================] - 9s 250us/sample - loss: 0.5096 - acc: 0.8353 - val_loss: 1.1811 - val_acc: 0.6636\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8379\n",
      "Epoch 00079: val_loss improved from 1.15957 to 1.15818, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_3_conv_checkpoint/079-1.1582.hdf5\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.5040 - acc: 0.8380 - val_loss: 1.1582 - val_acc: 0.6683\n",
      "Epoch 80/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5032 - acc: 0.8363\n",
      "Epoch 00080: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.5032 - acc: 0.8364 - val_loss: 1.1745 - val_acc: 0.6660\n",
      "Epoch 81/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4964 - acc: 0.8417\n",
      "Epoch 00081: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4966 - acc: 0.8416 - val_loss: 1.1783 - val_acc: 0.6711\n",
      "Epoch 82/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4942 - acc: 0.8400\n",
      "Epoch 00082: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.4947 - acc: 0.8400 - val_loss: 1.1801 - val_acc: 0.6643\n",
      "Epoch 83/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4877 - acc: 0.8443\n",
      "Epoch 00083: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.4876 - acc: 0.8443 - val_loss: 1.1689 - val_acc: 0.6751\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8443\n",
      "Epoch 00084: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.4871 - acc: 0.8443 - val_loss: 1.1639 - val_acc: 0.6741\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.8469\n",
      "Epoch 00085: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.4780 - acc: 0.8467 - val_loss: 1.1737 - val_acc: 0.6706\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4759 - acc: 0.8465\n",
      "Epoch 00086: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.4756 - acc: 0.8467 - val_loss: 1.1714 - val_acc: 0.6739\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4797 - acc: 0.8467\n",
      "Epoch 00087: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.4796 - acc: 0.8467 - val_loss: 1.1718 - val_acc: 0.6683\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4695 - acc: 0.8475\n",
      "Epoch 00088: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.4695 - acc: 0.8474 - val_loss: 1.1716 - val_acc: 0.6753\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4610 - acc: 0.8515\n",
      "Epoch 00089: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4609 - acc: 0.8515 - val_loss: 1.1729 - val_acc: 0.6716\n",
      "Epoch 90/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4577 - acc: 0.8526\n",
      "Epoch 00090: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.4578 - acc: 0.8525 - val_loss: 1.1776 - val_acc: 0.6741\n",
      "Epoch 91/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4541 - acc: 0.8517\n",
      "Epoch 00091: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.4549 - acc: 0.8514 - val_loss: 1.1810 - val_acc: 0.6730\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8559\n",
      "Epoch 00092: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4474 - acc: 0.8558 - val_loss: 1.1784 - val_acc: 0.6765\n",
      "Epoch 93/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8538\n",
      "Epoch 00093: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.4478 - acc: 0.8537 - val_loss: 1.1746 - val_acc: 0.6832\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.8538\n",
      "Epoch 00094: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.4462 - acc: 0.8537 - val_loss: 1.1753 - val_acc: 0.6818\n",
      "Epoch 95/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.8572\n",
      "Epoch 00095: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.4409 - acc: 0.8572 - val_loss: 1.1854 - val_acc: 0.6785\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4374 - acc: 0.8596\n",
      "Epoch 00096: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4373 - acc: 0.8596 - val_loss: 1.1881 - val_acc: 0.6765\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8613\n",
      "Epoch 00097: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4284 - acc: 0.8613 - val_loss: 1.2012 - val_acc: 0.6792\n",
      "Epoch 98/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8607\n",
      "Epoch 00098: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4273 - acc: 0.8607 - val_loss: 1.1818 - val_acc: 0.6790\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.8617\n",
      "Epoch 00099: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.4265 - acc: 0.8616 - val_loss: 1.1760 - val_acc: 0.6778\n",
      "Epoch 100/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.8624\n",
      "Epoch 00100: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.4225 - acc: 0.8625 - val_loss: 1.1733 - val_acc: 0.6827\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8642\n",
      "Epoch 00101: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.4216 - acc: 0.8641 - val_loss: 1.1882 - val_acc: 0.6785\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8650\n",
      "Epoch 00102: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.4135 - acc: 0.8650 - val_loss: 1.2157 - val_acc: 0.6727\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8663\n",
      "Epoch 00103: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 248us/sample - loss: 0.4071 - acc: 0.8663 - val_loss: 1.1901 - val_acc: 0.6825\n",
      "Epoch 104/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8669\n",
      "Epoch 00104: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.4076 - acc: 0.8668 - val_loss: 1.1894 - val_acc: 0.6792\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8689\n",
      "Epoch 00105: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.4032 - acc: 0.8687 - val_loss: 1.2038 - val_acc: 0.6758\n",
      "Epoch 106/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8701\n",
      "Epoch 00106: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.4011 - acc: 0.8701 - val_loss: 1.1834 - val_acc: 0.6865\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8720\n",
      "Epoch 00107: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.3975 - acc: 0.8717 - val_loss: 1.2082 - val_acc: 0.6832\n",
      "Epoch 108/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.8740\n",
      "Epoch 00108: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3906 - acc: 0.8738 - val_loss: 1.1924 - val_acc: 0.6837\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8728\n",
      "Epoch 00109: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3917 - acc: 0.8728 - val_loss: 1.2040 - val_acc: 0.6795\n",
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8745\n",
      "Epoch 00110: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3884 - acc: 0.8744 - val_loss: 1.1908 - val_acc: 0.6792\n",
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3916 - acc: 0.8696\n",
      "Epoch 00111: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3916 - acc: 0.8696 - val_loss: 1.1917 - val_acc: 0.6860\n",
      "Epoch 112/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8751\n",
      "Epoch 00112: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3874 - acc: 0.8749 - val_loss: 1.1967 - val_acc: 0.6825\n",
      "Epoch 113/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8777\n",
      "Epoch 00113: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3758 - acc: 0.8777 - val_loss: 1.2110 - val_acc: 0.6867\n",
      "Epoch 114/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8782\n",
      "Epoch 00114: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3760 - acc: 0.8782 - val_loss: 1.1904 - val_acc: 0.6876\n",
      "Epoch 115/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8793\n",
      "Epoch 00115: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.3718 - acc: 0.8792 - val_loss: 1.2029 - val_acc: 0.6865\n",
      "Epoch 116/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8808\n",
      "Epoch 00116: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.3672 - acc: 0.8808 - val_loss: 1.2035 - val_acc: 0.6900\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8784\n",
      "Epoch 00117: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3731 - acc: 0.8783 - val_loss: 1.2068 - val_acc: 0.6879\n",
      "Epoch 118/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8795\n",
      "Epoch 00118: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.3668 - acc: 0.8796 - val_loss: 1.2015 - val_acc: 0.6839\n",
      "Epoch 119/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8821\n",
      "Epoch 00119: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3616 - acc: 0.8820 - val_loss: 1.1998 - val_acc: 0.6895\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3607 - acc: 0.8810\n",
      "Epoch 00120: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.3607 - acc: 0.8810 - val_loss: 1.2138 - val_acc: 0.6865\n",
      "Epoch 121/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3589 - acc: 0.8833\n",
      "Epoch 00121: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3586 - acc: 0.8834 - val_loss: 1.2147 - val_acc: 0.6886\n",
      "Epoch 122/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3595 - acc: 0.8820\n",
      "Epoch 00122: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.3597 - acc: 0.8819 - val_loss: 1.2085 - val_acc: 0.6888\n",
      "Epoch 123/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3543 - acc: 0.8839\n",
      "Epoch 00123: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3542 - acc: 0.8840 - val_loss: 1.2238 - val_acc: 0.6893\n",
      "Epoch 124/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8863\n",
      "Epoch 00124: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.3499 - acc: 0.8864 - val_loss: 1.2091 - val_acc: 0.6918\n",
      "Epoch 125/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8862\n",
      "Epoch 00125: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3507 - acc: 0.8859 - val_loss: 1.2167 - val_acc: 0.6883\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.8892\n",
      "Epoch 00126: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3399 - acc: 0.8892 - val_loss: 1.2097 - val_acc: 0.6928\n",
      "Epoch 127/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8868\n",
      "Epoch 00127: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3465 - acc: 0.8866 - val_loss: 1.2018 - val_acc: 0.6909\n",
      "Epoch 128/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.8884\n",
      "Epoch 00128: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.3385 - acc: 0.8886 - val_loss: 1.2229 - val_acc: 0.6930\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8888\n",
      "Epoch 00129: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3403 - acc: 0.8887 - val_loss: 1.2191 - val_acc: 0.6918\n",
      "Epoch 130/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.8888\n",
      "Epoch 00130: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.3369 - acc: 0.8887 - val_loss: 1.2284 - val_acc: 0.6935\n",
      "Epoch 131/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3359 - acc: 0.8911\n",
      "Epoch 00131: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3358 - acc: 0.8912 - val_loss: 1.2222 - val_acc: 0.6900\n",
      "Epoch 132/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8908\n",
      "Epoch 00132: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3326 - acc: 0.8908 - val_loss: 1.2244 - val_acc: 0.6916\n",
      "Epoch 133/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8925\n",
      "Epoch 00133: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3305 - acc: 0.8925 - val_loss: 1.2321 - val_acc: 0.6997\n",
      "Epoch 134/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.8931\n",
      "Epoch 00134: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3270 - acc: 0.8930 - val_loss: 1.2461 - val_acc: 0.6937\n",
      "Epoch 135/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.8918\n",
      "Epoch 00135: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3289 - acc: 0.8917 - val_loss: 1.2202 - val_acc: 0.6988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.8930\n",
      "Epoch 00136: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.3268 - acc: 0.8928 - val_loss: 1.2365 - val_acc: 0.6904\n",
      "Epoch 137/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8925\n",
      "Epoch 00137: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3251 - acc: 0.8925 - val_loss: 1.2296 - val_acc: 0.6969\n",
      "Epoch 138/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8948\n",
      "Epoch 00138: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3240 - acc: 0.8948 - val_loss: 1.2291 - val_acc: 0.6909\n",
      "Epoch 139/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8973\n",
      "Epoch 00139: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3175 - acc: 0.8973 - val_loss: 1.2322 - val_acc: 0.6953\n",
      "Epoch 140/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.8982\n",
      "Epoch 00140: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.3135 - acc: 0.8983 - val_loss: 1.2219 - val_acc: 0.6944\n",
      "Epoch 141/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8949\n",
      "Epoch 00141: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3174 - acc: 0.8950 - val_loss: 1.2411 - val_acc: 0.6932\n",
      "Epoch 142/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.8971\n",
      "Epoch 00142: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.3125 - acc: 0.8970 - val_loss: 1.2602 - val_acc: 0.6883\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.9001\n",
      "Epoch 00143: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.3085 - acc: 0.9001 - val_loss: 1.2338 - val_acc: 0.6958\n",
      "Epoch 144/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.8984\n",
      "Epoch 00144: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3089 - acc: 0.8983 - val_loss: 1.2250 - val_acc: 0.6904\n",
      "Epoch 145/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9001\n",
      "Epoch 00145: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.3081 - acc: 0.9000 - val_loss: 1.2346 - val_acc: 0.6972\n",
      "Epoch 146/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.8993\n",
      "Epoch 00146: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.3064 - acc: 0.8993 - val_loss: 1.2122 - val_acc: 0.7030\n",
      "Epoch 147/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9003\n",
      "Epoch 00147: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.3039 - acc: 0.9004 - val_loss: 1.2320 - val_acc: 0.7014\n",
      "Epoch 148/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9011\n",
      "Epoch 00148: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.3034 - acc: 0.9009 - val_loss: 1.2307 - val_acc: 0.6993\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.9009\n",
      "Epoch 00149: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 251us/sample - loss: 0.2994 - acc: 0.9009 - val_loss: 1.2376 - val_acc: 0.7011\n",
      "Epoch 150/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9003\n",
      "Epoch 00150: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.2988 - acc: 0.9003 - val_loss: 1.2245 - val_acc: 0.7014\n",
      "Epoch 151/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3018 - acc: 0.9008\n",
      "Epoch 00151: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3019 - acc: 0.9007 - val_loss: 1.2386 - val_acc: 0.6976\n",
      "Epoch 152/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9052\n",
      "Epoch 00152: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.2907 - acc: 0.9052 - val_loss: 1.2476 - val_acc: 0.7007\n",
      "Epoch 153/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9058\n",
      "Epoch 00153: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.2854 - acc: 0.9057 - val_loss: 1.2352 - val_acc: 0.7018\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9039\n",
      "Epoch 00154: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.2915 - acc: 0.9039 - val_loss: 1.2338 - val_acc: 0.7021\n",
      "Epoch 155/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.9064\n",
      "Epoch 00155: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.2879 - acc: 0.9064 - val_loss: 1.2481 - val_acc: 0.7046\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9065\n",
      "Epoch 00156: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.2848 - acc: 0.9065 - val_loss: 1.2384 - val_acc: 0.7018\n",
      "Epoch 157/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.9061\n",
      "Epoch 00157: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.2864 - acc: 0.9060 - val_loss: 1.2472 - val_acc: 0.6972\n",
      "Epoch 158/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9074\n",
      "Epoch 00158: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.2820 - acc: 0.9075 - val_loss: 1.2586 - val_acc: 0.6944\n",
      "Epoch 159/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9086\n",
      "Epoch 00159: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.2821 - acc: 0.9084 - val_loss: 1.2367 - val_acc: 0.7009\n",
      "Epoch 160/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9076\n",
      "Epoch 00160: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.2829 - acc: 0.9078 - val_loss: 1.2489 - val_acc: 0.6983\n",
      "Epoch 161/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9081\n",
      "Epoch 00161: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.2819 - acc: 0.9082 - val_loss: 1.2508 - val_acc: 0.6983\n",
      "Epoch 162/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9083\n",
      "Epoch 00162: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.2802 - acc: 0.9082 - val_loss: 1.2598 - val_acc: 0.7021\n",
      "Epoch 163/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9101\n",
      "Epoch 00163: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.2736 - acc: 0.9102 - val_loss: 1.2500 - val_acc: 0.7056\n",
      "Epoch 164/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9087\n",
      "Epoch 00164: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 246us/sample - loss: 0.2740 - acc: 0.9086 - val_loss: 1.2631 - val_acc: 0.7021\n",
      "Epoch 165/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9108\n",
      "Epoch 00165: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.2759 - acc: 0.9108 - val_loss: 1.2615 - val_acc: 0.7037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9110\n",
      "Epoch 00166: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.2745 - acc: 0.9110 - val_loss: 1.2684 - val_acc: 0.7000\n",
      "Epoch 167/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9075\n",
      "Epoch 00167: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.2725 - acc: 0.9075 - val_loss: 1.2554 - val_acc: 0.7032\n",
      "Epoch 168/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9106\n",
      "Epoch 00168: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.2688 - acc: 0.9107 - val_loss: 1.2635 - val_acc: 0.7018\n",
      "Epoch 169/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9137\n",
      "Epoch 00169: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.2657 - acc: 0.9137 - val_loss: 1.2647 - val_acc: 0.7035\n",
      "Epoch 170/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9116\n",
      "Epoch 00170: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 249us/sample - loss: 0.2685 - acc: 0.9117 - val_loss: 1.2796 - val_acc: 0.7035\n",
      "Epoch 171/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2664 - acc: 0.9132\n",
      "Epoch 00171: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.2667 - acc: 0.9131 - val_loss: 1.2524 - val_acc: 0.7058\n",
      "Epoch 172/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9119\n",
      "Epoch 00172: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.2697 - acc: 0.9120 - val_loss: 1.2768 - val_acc: 0.7058\n",
      "Epoch 173/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9141\n",
      "Epoch 00173: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.2609 - acc: 0.9141 - val_loss: 1.2810 - val_acc: 0.6993\n",
      "Epoch 174/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9126\n",
      "Epoch 00174: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.2603 - acc: 0.9126 - val_loss: 1.2582 - val_acc: 0.7065\n",
      "Epoch 175/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2627 - acc: 0.9137\n",
      "Epoch 00175: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.2628 - acc: 0.9137 - val_loss: 1.2555 - val_acc: 0.7056\n",
      "Epoch 176/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9150\n",
      "Epoch 00176: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 247us/sample - loss: 0.2593 - acc: 0.9149 - val_loss: 1.2590 - val_acc: 0.7088\n",
      "Epoch 177/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9154\n",
      "Epoch 00177: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.2545 - acc: 0.9153 - val_loss: 1.2793 - val_acc: 0.7030\n",
      "Epoch 178/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.9157\n",
      "Epoch 00178: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 0.2591 - acc: 0.9157 - val_loss: 1.2672 - val_acc: 0.7102\n",
      "Epoch 179/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9176\n",
      "Epoch 00179: val_loss did not improve from 1.15818\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.2533 - acc: 0.9176 - val_loss: 1.2580 - val_acc: 0.7051\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 189us/sample - loss: 1.2891 - acc: 0.6295\n",
      "Loss: 1.2891465662672017 Accuracy: 0.62949115\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.4013 - acc: 0.2305\n",
      "Epoch 00001: val_loss improved from inf to 1.90511, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/001-1.9051.hdf5\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 2.3995 - acc: 0.2312 - val_loss: 1.9051 - val_acc: 0.3911\n",
      "Epoch 2/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.7878 - acc: 0.4294\n",
      "Epoch 00002: val_loss improved from 1.90511 to 1.63356, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/002-1.6336.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 1.7881 - acc: 0.4291 - val_loss: 1.6336 - val_acc: 0.4945\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6213 - acc: 0.4841\n",
      "Epoch 00003: val_loss improved from 1.63356 to 1.52362, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/003-1.5236.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 1.6212 - acc: 0.4843 - val_loss: 1.5236 - val_acc: 0.5220\n",
      "Epoch 4/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.5227 - acc: 0.5176\n",
      "Epoch 00004: val_loss improved from 1.52362 to 1.45751, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/004-1.4575.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 1.5218 - acc: 0.5178 - val_loss: 1.4575 - val_acc: 0.5521\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4446 - acc: 0.5467\n",
      "Epoch 00005: val_loss improved from 1.45751 to 1.40302, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/005-1.4030.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 1.4447 - acc: 0.5467 - val_loss: 1.4030 - val_acc: 0.5644\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3830 - acc: 0.5669\n",
      "Epoch 00006: val_loss improved from 1.40302 to 1.32635, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/006-1.3263.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 1.3831 - acc: 0.5669 - val_loss: 1.3263 - val_acc: 0.5926\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3252 - acc: 0.5901\n",
      "Epoch 00007: val_loss improved from 1.32635 to 1.28320, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/007-1.2832.hdf5\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 1.3250 - acc: 0.5901 - val_loss: 1.2832 - val_acc: 0.6049\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.2795 - acc: 0.6037\n",
      "Epoch 00008: val_loss improved from 1.28320 to 1.24005, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/008-1.2400.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 1.2788 - acc: 0.6039 - val_loss: 1.2400 - val_acc: 0.6245\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2362 - acc: 0.6179\n",
      "Epoch 00009: val_loss improved from 1.24005 to 1.20128, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/009-1.2013.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 1.2361 - acc: 0.6178 - val_loss: 1.2013 - val_acc: 0.6448\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1969 - acc: 0.6306\n",
      "Epoch 00010: val_loss improved from 1.20128 to 1.17351, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/010-1.1735.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 1.1968 - acc: 0.6307 - val_loss: 1.1735 - val_acc: 0.6513\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1574 - acc: 0.6463\n",
      "Epoch 00011: val_loss improved from 1.17351 to 1.13983, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/011-1.1398.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 1.1574 - acc: 0.6463 - val_loss: 1.1398 - val_acc: 0.6553\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1233 - acc: 0.6564\n",
      "Epoch 00012: val_loss improved from 1.13983 to 1.11929, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/012-1.1193.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 1.1235 - acc: 0.6565 - val_loss: 1.1193 - val_acc: 0.6580\n",
      "Epoch 13/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0906 - acc: 0.6690\n",
      "Epoch 00013: val_loss improved from 1.11929 to 1.09662, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/013-1.0966.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 1.0904 - acc: 0.6687 - val_loss: 1.0966 - val_acc: 0.6734\n",
      "Epoch 14/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0645 - acc: 0.6761\n",
      "Epoch 00014: val_loss did not improve from 1.09662\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 1.0640 - acc: 0.6762 - val_loss: 1.1017 - val_acc: 0.6667\n",
      "Epoch 15/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.0328 - acc: 0.6855\n",
      "Epoch 00015: val_loss improved from 1.09662 to 1.05137, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/015-1.0514.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 1.0328 - acc: 0.6854 - val_loss: 1.0514 - val_acc: 0.6848\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0103 - acc: 0.6921\n",
      "Epoch 00016: val_loss did not improve from 1.05137\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 1.0100 - acc: 0.6922 - val_loss: 1.0534 - val_acc: 0.6876\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9835 - acc: 0.7011\n",
      "Epoch 00017: val_loss improved from 1.05137 to 1.03205, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/017-1.0321.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.9835 - acc: 0.7011 - val_loss: 1.0321 - val_acc: 0.6888\n",
      "Epoch 18/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9622 - acc: 0.7082\n",
      "Epoch 00018: val_loss improved from 1.03205 to 1.00342, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/018-1.0034.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.9615 - acc: 0.7086 - val_loss: 1.0034 - val_acc: 0.7009\n",
      "Epoch 19/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9366 - acc: 0.7155\n",
      "Epoch 00019: val_loss improved from 1.00342 to 1.00129, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/019-1.0013.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.9366 - acc: 0.7156 - val_loss: 1.0013 - val_acc: 0.7025\n",
      "Epoch 20/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9175 - acc: 0.7242\n",
      "Epoch 00020: val_loss improved from 1.00129 to 0.98943, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/020-0.9894.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.9168 - acc: 0.7242 - val_loss: 0.9894 - val_acc: 0.6937\n",
      "Epoch 21/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.7286\n",
      "Epoch 00021: val_loss improved from 0.98943 to 0.96700, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/021-0.9670.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.8967 - acc: 0.7286 - val_loss: 0.9670 - val_acc: 0.7100\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8740 - acc: 0.7354\n",
      "Epoch 00022: val_loss improved from 0.96700 to 0.95866, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/022-0.9587.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.8736 - acc: 0.7355 - val_loss: 0.9587 - val_acc: 0.7154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8554 - acc: 0.7410\n",
      "Epoch 00023: val_loss improved from 0.95866 to 0.93443, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/023-0.9344.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.8552 - acc: 0.7410 - val_loss: 0.9344 - val_acc: 0.7244\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8360 - acc: 0.7463\n",
      "Epoch 00024: val_loss improved from 0.93443 to 0.92387, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/024-0.9239.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.8360 - acc: 0.7463 - val_loss: 0.9239 - val_acc: 0.7298\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8211 - acc: 0.7509\n",
      "Epoch 00025: val_loss did not improve from 0.92387\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.8209 - acc: 0.7511 - val_loss: 0.9241 - val_acc: 0.7254\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8037 - acc: 0.7559\n",
      "Epoch 00026: val_loss improved from 0.92387 to 0.90846, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/026-0.9085.hdf5\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.8036 - acc: 0.7559 - val_loss: 0.9085 - val_acc: 0.7319\n",
      "Epoch 27/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7853 - acc: 0.7637\n",
      "Epoch 00027: val_loss improved from 0.90846 to 0.89479, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/027-0.8948.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.7854 - acc: 0.7635 - val_loss: 0.8948 - val_acc: 0.7319\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7700 - acc: 0.7640\n",
      "Epoch 00028: val_loss did not improve from 0.89479\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.7701 - acc: 0.7639 - val_loss: 0.9099 - val_acc: 0.7319\n",
      "Epoch 29/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7545 - acc: 0.7699\n",
      "Epoch 00029: val_loss did not improve from 0.89479\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.7539 - acc: 0.7701 - val_loss: 0.8977 - val_acc: 0.7389\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7409 - acc: 0.7741\n",
      "Epoch 00030: val_loss improved from 0.89479 to 0.87941, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/030-0.8794.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.7408 - acc: 0.7741 - val_loss: 0.8794 - val_acc: 0.7365\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7297 - acc: 0.7783\n",
      "Epoch 00031: val_loss did not improve from 0.87941\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.7297 - acc: 0.7783 - val_loss: 0.8875 - val_acc: 0.7400\n",
      "Epoch 32/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7142 - acc: 0.7811\n",
      "Epoch 00032: val_loss improved from 0.87941 to 0.86057, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/032-0.8606.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.7151 - acc: 0.7811 - val_loss: 0.8606 - val_acc: 0.7501\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6984 - acc: 0.7874\n",
      "Epoch 00033: val_loss improved from 0.86057 to 0.84190, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/033-0.8419.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.6984 - acc: 0.7874 - val_loss: 0.8419 - val_acc: 0.7526\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6862 - acc: 0.7946\n",
      "Epoch 00034: val_loss did not improve from 0.84190\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.6861 - acc: 0.7946 - val_loss: 0.8444 - val_acc: 0.7545\n",
      "Epoch 35/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6710 - acc: 0.7967\n",
      "Epoch 00035: val_loss improved from 0.84190 to 0.83932, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/035-0.8393.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.6708 - acc: 0.7967 - val_loss: 0.8393 - val_acc: 0.7594\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6617 - acc: 0.7979\n",
      "Epoch 00036: val_loss improved from 0.83932 to 0.83557, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/036-0.8356.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.6617 - acc: 0.7979 - val_loss: 0.8356 - val_acc: 0.7563\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6443 - acc: 0.8050\n",
      "Epoch 00037: val_loss did not improve from 0.83557\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.6442 - acc: 0.8050 - val_loss: 0.8384 - val_acc: 0.7543\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6370 - acc: 0.8073\n",
      "Epoch 00038: val_loss improved from 0.83557 to 0.82101, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/038-0.8210.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.6367 - acc: 0.8073 - val_loss: 0.8210 - val_acc: 0.7591\n",
      "Epoch 39/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6237 - acc: 0.8111\n",
      "Epoch 00039: val_loss improved from 0.82101 to 0.81836, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/039-0.8184.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.6238 - acc: 0.8111 - val_loss: 0.8184 - val_acc: 0.7645\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6159 - acc: 0.8125\n",
      "Epoch 00040: val_loss did not improve from 0.81836\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.6160 - acc: 0.8126 - val_loss: 0.8247 - val_acc: 0.7508\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6039 - acc: 0.8162\n",
      "Epoch 00041: val_loss improved from 0.81836 to 0.81336, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/041-0.8134.hdf5\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.6034 - acc: 0.8164 - val_loss: 0.8134 - val_acc: 0.7615\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5925 - acc: 0.8197\n",
      "Epoch 00042: val_loss did not improve from 0.81336\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.5922 - acc: 0.8196 - val_loss: 0.8151 - val_acc: 0.7577\n",
      "Epoch 43/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5797 - acc: 0.8220\n",
      "Epoch 00043: val_loss did not improve from 0.81336\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.5808 - acc: 0.8218 - val_loss: 0.8309 - val_acc: 0.7561\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5754 - acc: 0.8248\n",
      "Epoch 00044: val_loss did not improve from 0.81336\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.5751 - acc: 0.8249 - val_loss: 0.8171 - val_acc: 0.7617\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5617 - acc: 0.8296\n",
      "Epoch 00045: val_loss improved from 0.81336 to 0.81040, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/045-0.8104.hdf5\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.5617 - acc: 0.8296 - val_loss: 0.8104 - val_acc: 0.7687\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5552 - acc: 0.8289\n",
      "Epoch 00046: val_loss did not improve from 0.81040\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.5552 - acc: 0.8289 - val_loss: 0.8112 - val_acc: 0.7673\n",
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.8339\n",
      "Epoch 00047: val_loss did not improve from 0.81040\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.5429 - acc: 0.8339 - val_loss: 0.8274 - val_acc: 0.7626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.8334\n",
      "Epoch 00048: val_loss improved from 0.81040 to 0.80929, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/048-0.8093.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.5384 - acc: 0.8334 - val_loss: 0.8093 - val_acc: 0.7610\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8379\n",
      "Epoch 00049: val_loss improved from 0.80929 to 0.80254, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/049-0.8025.hdf5\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.5277 - acc: 0.8380 - val_loss: 0.8025 - val_acc: 0.7710\n",
      "Epoch 50/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.8406\n",
      "Epoch 00050: val_loss improved from 0.80254 to 0.79824, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/050-0.7982.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.5190 - acc: 0.8408 - val_loss: 0.7982 - val_acc: 0.7680\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.8446\n",
      "Epoch 00051: val_loss improved from 0.79824 to 0.79295, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/051-0.7929.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.5083 - acc: 0.8446 - val_loss: 0.7929 - val_acc: 0.7724\n",
      "Epoch 52/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.8406\n",
      "Epoch 00052: val_loss did not improve from 0.79295\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.5072 - acc: 0.8409 - val_loss: 0.7985 - val_acc: 0.7731\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.8485\n",
      "Epoch 00053: val_loss improved from 0.79295 to 0.78711, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/053-0.7871.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.4927 - acc: 0.8486 - val_loss: 0.7871 - val_acc: 0.7717\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4880 - acc: 0.8474\n",
      "Epoch 00054: val_loss did not improve from 0.78711\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.4879 - acc: 0.8474 - val_loss: 0.7944 - val_acc: 0.7720\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4820 - acc: 0.8491\n",
      "Epoch 00055: val_loss did not improve from 0.78711\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.4821 - acc: 0.8491 - val_loss: 0.7956 - val_acc: 0.7736\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4776 - acc: 0.8511\n",
      "Epoch 00056: val_loss did not improve from 0.78711\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.4777 - acc: 0.8511 - val_loss: 0.7929 - val_acc: 0.7741\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4661 - acc: 0.8538\n",
      "Epoch 00057: val_loss did not improve from 0.78711\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.4661 - acc: 0.8539 - val_loss: 0.7958 - val_acc: 0.7741\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.8577\n",
      "Epoch 00058: val_loss did not improve from 0.78711\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.4596 - acc: 0.8577 - val_loss: 0.7891 - val_acc: 0.7780\n",
      "Epoch 59/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4484 - acc: 0.8604\n",
      "Epoch 00059: val_loss did not improve from 0.78711\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.4486 - acc: 0.8605 - val_loss: 0.7982 - val_acc: 0.7747\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4477 - acc: 0.8606\n",
      "Epoch 00060: val_loss did not improve from 0.78711\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4473 - acc: 0.8607 - val_loss: 0.7964 - val_acc: 0.7803\n",
      "Epoch 61/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.8633\n",
      "Epoch 00061: val_loss did not improve from 0.78711\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4410 - acc: 0.8635 - val_loss: 0.8011 - val_acc: 0.7822\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.8645\n",
      "Epoch 00062: val_loss improved from 0.78711 to 0.78395, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/062-0.7839.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.4315 - acc: 0.8645 - val_loss: 0.7839 - val_acc: 0.7810\n",
      "Epoch 63/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.8654\n",
      "Epoch 00063: val_loss did not improve from 0.78395\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4280 - acc: 0.8653 - val_loss: 0.7873 - val_acc: 0.7792\n",
      "Epoch 64/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4227 - acc: 0.8692\n",
      "Epoch 00064: val_loss did not improve from 0.78395\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.4229 - acc: 0.8691 - val_loss: 0.7901 - val_acc: 0.7771\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4182 - acc: 0.8677\n",
      "Epoch 00065: val_loss did not improve from 0.78395\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.4183 - acc: 0.8677 - val_loss: 0.7918 - val_acc: 0.7761\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8700\n",
      "Epoch 00066: val_loss did not improve from 0.78395\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.4117 - acc: 0.8702 - val_loss: 0.7995 - val_acc: 0.7764\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8741\n",
      "Epoch 00067: val_loss improved from 0.78395 to 0.77799, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_4_conv_checkpoint/067-0.7780.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.4051 - acc: 0.8742 - val_loss: 0.7780 - val_acc: 0.7813\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8737\n",
      "Epoch 00068: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.4026 - acc: 0.8736 - val_loss: 0.8003 - val_acc: 0.7754\n",
      "Epoch 69/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3975 - acc: 0.8740\n",
      "Epoch 00069: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.3979 - acc: 0.8740 - val_loss: 0.7875 - val_acc: 0.7775\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8803\n",
      "Epoch 00070: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3891 - acc: 0.8803 - val_loss: 0.7919 - val_acc: 0.7838\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8790\n",
      "Epoch 00071: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3812 - acc: 0.8791 - val_loss: 0.7891 - val_acc: 0.7799\n",
      "Epoch 72/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8804\n",
      "Epoch 00072: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3789 - acc: 0.8803 - val_loss: 0.7966 - val_acc: 0.7799\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8835\n",
      "Epoch 00073: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3714 - acc: 0.8836 - val_loss: 0.7972 - val_acc: 0.7808\n",
      "Epoch 74/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8833\n",
      "Epoch 00074: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3710 - acc: 0.8832 - val_loss: 0.8003 - val_acc: 0.7813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3695 - acc: 0.8818\n",
      "Epoch 00075: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 9s 258us/sample - loss: 0.3698 - acc: 0.8817 - val_loss: 0.8017 - val_acc: 0.7829\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8849\n",
      "Epoch 00076: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3643 - acc: 0.8847 - val_loss: 0.8006 - val_acc: 0.7845\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8868\n",
      "Epoch 00077: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3564 - acc: 0.8868 - val_loss: 0.7957 - val_acc: 0.7838\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8886\n",
      "Epoch 00078: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3515 - acc: 0.8886 - val_loss: 0.8125 - val_acc: 0.7834\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8888\n",
      "Epoch 00079: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.3491 - acc: 0.8888 - val_loss: 0.8025 - val_acc: 0.7820\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8884\n",
      "Epoch 00080: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3491 - acc: 0.8884 - val_loss: 0.8154 - val_acc: 0.7803\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.8899\n",
      "Epoch 00081: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3444 - acc: 0.8899 - val_loss: 0.8057 - val_acc: 0.7801\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8900\n",
      "Epoch 00082: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3405 - acc: 0.8902 - val_loss: 0.7991 - val_acc: 0.7829\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3340 - acc: 0.8932\n",
      "Epoch 00083: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.3339 - acc: 0.8932 - val_loss: 0.8000 - val_acc: 0.7836\n",
      "Epoch 84/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8923\n",
      "Epoch 00084: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.3344 - acc: 0.8921 - val_loss: 0.8093 - val_acc: 0.7873\n",
      "Epoch 85/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8951\n",
      "Epoch 00085: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3273 - acc: 0.8951 - val_loss: 0.8061 - val_acc: 0.7845\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.8961\n",
      "Epoch 00086: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.3225 - acc: 0.8959 - val_loss: 0.8153 - val_acc: 0.7855\n",
      "Epoch 87/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8985\n",
      "Epoch 00087: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3172 - acc: 0.8987 - val_loss: 0.8044 - val_acc: 0.7862\n",
      "Epoch 88/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.8980\n",
      "Epoch 00088: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.3162 - acc: 0.8981 - val_loss: 0.7998 - val_acc: 0.7850\n",
      "Epoch 89/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.9001\n",
      "Epoch 00089: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.3128 - acc: 0.9001 - val_loss: 0.8150 - val_acc: 0.7836\n",
      "Epoch 90/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8995\n",
      "Epoch 00090: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.3121 - acc: 0.8995 - val_loss: 0.8016 - val_acc: 0.7855\n",
      "Epoch 91/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9013\n",
      "Epoch 00091: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.3085 - acc: 0.9013 - val_loss: 0.8041 - val_acc: 0.7831\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.9029\n",
      "Epoch 00092: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.3043 - acc: 0.9029 - val_loss: 0.8121 - val_acc: 0.7892\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.9030\n",
      "Epoch 00093: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.3055 - acc: 0.9030 - val_loss: 0.8055 - val_acc: 0.7873\n",
      "Epoch 94/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9028\n",
      "Epoch 00094: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.3019 - acc: 0.9029 - val_loss: 0.8313 - val_acc: 0.7834\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9049\n",
      "Epoch 00095: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2943 - acc: 0.9048 - val_loss: 0.8200 - val_acc: 0.7862\n",
      "Epoch 96/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9068\n",
      "Epoch 00096: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2910 - acc: 0.9070 - val_loss: 0.8169 - val_acc: 0.7845\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9085\n",
      "Epoch 00097: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.2889 - acc: 0.9083 - val_loss: 0.8092 - val_acc: 0.7904\n",
      "Epoch 98/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9068\n",
      "Epoch 00098: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2886 - acc: 0.9070 - val_loss: 0.8157 - val_acc: 0.7857\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9085\n",
      "Epoch 00099: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2823 - acc: 0.9084 - val_loss: 0.8208 - val_acc: 0.7845\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9092\n",
      "Epoch 00100: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2804 - acc: 0.9091 - val_loss: 0.8164 - val_acc: 0.7934\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9115\n",
      "Epoch 00101: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2770 - acc: 0.9115 - val_loss: 0.8237 - val_acc: 0.7890\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9100\n",
      "Epoch 00102: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2743 - acc: 0.9099 - val_loss: 0.8556 - val_acc: 0.7824\n",
      "Epoch 103/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9097\n",
      "Epoch 00103: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2737 - acc: 0.9098 - val_loss: 0.8194 - val_acc: 0.7899\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9123\n",
      "Epoch 00104: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2699 - acc: 0.9122 - val_loss: 0.8188 - val_acc: 0.7901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2664 - acc: 0.9138\n",
      "Epoch 00105: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2665 - acc: 0.9138 - val_loss: 0.8364 - val_acc: 0.7845\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9131\n",
      "Epoch 00106: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2655 - acc: 0.9132 - val_loss: 0.8255 - val_acc: 0.7908\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9155\n",
      "Epoch 00107: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.2605 - acc: 0.9156 - val_loss: 0.8274 - val_acc: 0.7897\n",
      "Epoch 108/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9155\n",
      "Epoch 00108: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2616 - acc: 0.9154 - val_loss: 0.8144 - val_acc: 0.7952\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9167\n",
      "Epoch 00109: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.2611 - acc: 0.9167 - val_loss: 0.8285 - val_acc: 0.7915\n",
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9182\n",
      "Epoch 00110: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2539 - acc: 0.9183 - val_loss: 0.8351 - val_acc: 0.7906\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9178\n",
      "Epoch 00111: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.2530 - acc: 0.9179 - val_loss: 0.8316 - val_acc: 0.7838\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.9180\n",
      "Epoch 00112: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2511 - acc: 0.9178 - val_loss: 0.8213 - val_acc: 0.7922\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9192\n",
      "Epoch 00113: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2500 - acc: 0.9190 - val_loss: 0.8345 - val_acc: 0.7869\n",
      "Epoch 114/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9186\n",
      "Epoch 00114: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2484 - acc: 0.9187 - val_loss: 0.8320 - val_acc: 0.7906\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.9190\n",
      "Epoch 00115: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2475 - acc: 0.9190 - val_loss: 0.8576 - val_acc: 0.7806\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9199\n",
      "Epoch 00116: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2481 - acc: 0.9198 - val_loss: 0.8427 - val_acc: 0.7936\n",
      "Epoch 117/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9214\n",
      "Epoch 00117: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2431 - acc: 0.9215 - val_loss: 0.8502 - val_acc: 0.7913\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9192\n",
      "Epoch 00118: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2437 - acc: 0.9193 - val_loss: 0.8299 - val_acc: 0.7925\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9238\n",
      "Epoch 00119: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2351 - acc: 0.9238 - val_loss: 0.8367 - val_acc: 0.7927\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9226\n",
      "Epoch 00120: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.2350 - acc: 0.9225 - val_loss: 0.8369 - val_acc: 0.7908\n",
      "Epoch 121/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9239\n",
      "Epoch 00121: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2344 - acc: 0.9237 - val_loss: 0.8517 - val_acc: 0.7934\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9247\n",
      "Epoch 00122: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2329 - acc: 0.9246 - val_loss: 0.8376 - val_acc: 0.7936\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9245\n",
      "Epoch 00123: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2289 - acc: 0.9246 - val_loss: 0.8468 - val_acc: 0.7973\n",
      "Epoch 124/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9243\n",
      "Epoch 00124: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2324 - acc: 0.9242 - val_loss: 0.8454 - val_acc: 0.7941\n",
      "Epoch 125/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9255\n",
      "Epoch 00125: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.2280 - acc: 0.9253 - val_loss: 0.8534 - val_acc: 0.7904\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9242\n",
      "Epoch 00126: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2284 - acc: 0.9243 - val_loss: 0.8582 - val_acc: 0.7859\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9278\n",
      "Epoch 00127: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2238 - acc: 0.9278 - val_loss: 0.8526 - val_acc: 0.7957\n",
      "Epoch 128/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9273\n",
      "Epoch 00128: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2221 - acc: 0.9272 - val_loss: 0.8399 - val_acc: 0.7929\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9288\n",
      "Epoch 00129: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2200 - acc: 0.9288 - val_loss: 0.8376 - val_acc: 0.7936\n",
      "Epoch 130/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9269\n",
      "Epoch 00130: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.2205 - acc: 0.9270 - val_loss: 0.8461 - val_acc: 0.7950\n",
      "Epoch 131/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9287\n",
      "Epoch 00131: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2190 - acc: 0.9285 - val_loss: 0.8385 - val_acc: 0.7934\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9299\n",
      "Epoch 00132: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2168 - acc: 0.9298 - val_loss: 0.8545 - val_acc: 0.7980\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9291\n",
      "Epoch 00133: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.2169 - acc: 0.9291 - val_loss: 0.8448 - val_acc: 0.7918\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9315\n",
      "Epoch 00134: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2089 - acc: 0.9315 - val_loss: 0.8710 - val_acc: 0.7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9308\n",
      "Epoch 00135: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2130 - acc: 0.9309 - val_loss: 0.8537 - val_acc: 0.7890\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9306\n",
      "Epoch 00136: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2121 - acc: 0.9306 - val_loss: 0.8557 - val_acc: 0.7929\n",
      "Epoch 137/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9313\n",
      "Epoch 00137: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.2097 - acc: 0.9313 - val_loss: 0.8401 - val_acc: 0.8008\n",
      "Epoch 138/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9304\n",
      "Epoch 00138: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2115 - acc: 0.9303 - val_loss: 0.8510 - val_acc: 0.7985\n",
      "Epoch 139/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9331\n",
      "Epoch 00139: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.2065 - acc: 0.9332 - val_loss: 0.8527 - val_acc: 0.7934\n",
      "Epoch 140/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9312\n",
      "Epoch 00140: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2093 - acc: 0.9313 - val_loss: 0.8400 - val_acc: 0.7987\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9332\n",
      "Epoch 00141: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2061 - acc: 0.9332 - val_loss: 0.8494 - val_acc: 0.8001\n",
      "Epoch 142/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9326\n",
      "Epoch 00142: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.2034 - acc: 0.9324 - val_loss: 0.8590 - val_acc: 0.7952\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9349\n",
      "Epoch 00143: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1994 - acc: 0.9348 - val_loss: 0.8668 - val_acc: 0.7980\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9318\n",
      "Epoch 00144: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.2071 - acc: 0.9317 - val_loss: 0.8455 - val_acc: 0.8018\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9347\n",
      "Epoch 00145: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.2013 - acc: 0.9347 - val_loss: 0.8471 - val_acc: 0.7971\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9353\n",
      "Epoch 00146: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1990 - acc: 0.9352 - val_loss: 0.8449 - val_acc: 0.8025\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9351\n",
      "Epoch 00147: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1977 - acc: 0.9351 - val_loss: 0.8603 - val_acc: 0.7955\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9358\n",
      "Epoch 00148: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1980 - acc: 0.9358 - val_loss: 0.8419 - val_acc: 0.8004\n",
      "Epoch 149/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9370\n",
      "Epoch 00149: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.1934 - acc: 0.9369 - val_loss: 0.8558 - val_acc: 0.7969\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9385\n",
      "Epoch 00150: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.1921 - acc: 0.9385 - val_loss: 0.8668 - val_acc: 0.7987\n",
      "Epoch 151/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9355\n",
      "Epoch 00151: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.1969 - acc: 0.9353 - val_loss: 0.8751 - val_acc: 0.8034\n",
      "Epoch 152/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9374\n",
      "Epoch 00152: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 259us/sample - loss: 0.1923 - acc: 0.9375 - val_loss: 0.8542 - val_acc: 0.7999\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9375\n",
      "Epoch 00153: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 261us/sample - loss: 0.1916 - acc: 0.9375 - val_loss: 0.8524 - val_acc: 0.7997\n",
      "Epoch 154/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9388\n",
      "Epoch 00154: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.1860 - acc: 0.9387 - val_loss: 0.8641 - val_acc: 0.8046\n",
      "Epoch 155/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9391\n",
      "Epoch 00155: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1873 - acc: 0.9392 - val_loss: 0.8607 - val_acc: 0.7983\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9378\n",
      "Epoch 00156: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.1907 - acc: 0.9377 - val_loss: 0.8377 - val_acc: 0.7997\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9380\n",
      "Epoch 00157: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1891 - acc: 0.9379 - val_loss: 0.8620 - val_acc: 0.8057\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9404\n",
      "Epoch 00158: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1824 - acc: 0.9404 - val_loss: 0.8667 - val_acc: 0.8022\n",
      "Epoch 159/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9399\n",
      "Epoch 00159: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.1843 - acc: 0.9400 - val_loss: 0.8720 - val_acc: 0.7999\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9388\n",
      "Epoch 00160: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.1829 - acc: 0.9388 - val_loss: 0.8659 - val_acc: 0.8039\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9406\n",
      "Epoch 00161: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1813 - acc: 0.9406 - val_loss: 0.8686 - val_acc: 0.7999\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9420\n",
      "Epoch 00162: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 260us/sample - loss: 0.1754 - acc: 0.9420 - val_loss: 0.8711 - val_acc: 0.8055\n",
      "Epoch 163/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9393\n",
      "Epoch 00163: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1827 - acc: 0.9393 - val_loss: 0.8733 - val_acc: 0.7973\n",
      "Epoch 164/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9408\n",
      "Epoch 00164: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.1801 - acc: 0.9409 - val_loss: 0.8671 - val_acc: 0.8046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9416\n",
      "Epoch 00165: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.1764 - acc: 0.9414 - val_loss: 0.8669 - val_acc: 0.7994\n",
      "Epoch 166/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9420\n",
      "Epoch 00166: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.1761 - acc: 0.9420 - val_loss: 0.8783 - val_acc: 0.8032\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9401\n",
      "Epoch 00167: val_loss did not improve from 0.77799\n",
      "36805/36805 [==============================] - 10s 262us/sample - loss: 0.1801 - acc: 0.9401 - val_loss: 0.8570 - val_acc: 0.8034\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 208us/sample - loss: 0.8589 - acc: 0.7556\n",
      "Loss: 0.8589462688778791 Accuracy: 0.75555557\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4454 - acc: 0.2027\n",
      "Epoch 00001: val_loss improved from inf to 1.93947, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/001-1.9395.hdf5\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 2.4447 - acc: 0.2030 - val_loss: 1.9395 - val_acc: 0.3753\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8154 - acc: 0.4173\n",
      "Epoch 00002: val_loss improved from 1.93947 to 1.65142, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/002-1.6514.hdf5\n",
      "36805/36805 [==============================] - 11s 285us/sample - loss: 1.8155 - acc: 0.4173 - val_loss: 1.6514 - val_acc: 0.4934\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6267 - acc: 0.4862\n",
      "Epoch 00003: val_loss improved from 1.65142 to 1.48203, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/003-1.4820.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 1.6270 - acc: 0.4862 - val_loss: 1.4820 - val_acc: 0.5404\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5003 - acc: 0.5290\n",
      "Epoch 00004: val_loss improved from 1.48203 to 1.37194, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/004-1.3719.hdf5\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 1.5003 - acc: 0.5290 - val_loss: 1.3719 - val_acc: 0.5837\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4018 - acc: 0.5628\n",
      "Epoch 00005: val_loss improved from 1.37194 to 1.30546, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/005-1.3055.hdf5\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 1.4016 - acc: 0.5627 - val_loss: 1.3055 - val_acc: 0.6042\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3245 - acc: 0.5886\n",
      "Epoch 00006: val_loss improved from 1.30546 to 1.22261, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/006-1.2226.hdf5\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 1.3243 - acc: 0.5886 - val_loss: 1.2226 - val_acc: 0.6380\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2640 - acc: 0.6088\n",
      "Epoch 00007: val_loss improved from 1.22261 to 1.18968, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/007-1.1897.hdf5\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 1.2646 - acc: 0.6088 - val_loss: 1.1897 - val_acc: 0.6499\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2112 - acc: 0.6297\n",
      "Epoch 00008: val_loss improved from 1.18968 to 1.14494, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/008-1.1449.hdf5\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 1.2113 - acc: 0.6296 - val_loss: 1.1449 - val_acc: 0.6578\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1633 - acc: 0.6447\n",
      "Epoch 00009: val_loss improved from 1.14494 to 1.08134, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/009-1.0813.hdf5\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 1.1640 - acc: 0.6446 - val_loss: 1.0813 - val_acc: 0.6872\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1251 - acc: 0.6553\n",
      "Epoch 00010: val_loss improved from 1.08134 to 1.06415, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/010-1.0642.hdf5\n",
      "36805/36805 [==============================] - 10s 274us/sample - loss: 1.1244 - acc: 0.6555 - val_loss: 1.0642 - val_acc: 0.6790\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0868 - acc: 0.6689\n",
      "Epoch 00011: val_loss improved from 1.06415 to 1.01856, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/011-1.0186.hdf5\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 1.0872 - acc: 0.6689 - val_loss: 1.0186 - val_acc: 0.7023\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0438 - acc: 0.6819\n",
      "Epoch 00012: val_loss improved from 1.01856 to 0.97570, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/012-0.9757.hdf5\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 1.0441 - acc: 0.6816 - val_loss: 0.9757 - val_acc: 0.7156\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0170 - acc: 0.6931\n",
      "Epoch 00013: val_loss improved from 0.97570 to 0.96609, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/013-0.9661.hdf5\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 1.0172 - acc: 0.6931 - val_loss: 0.9661 - val_acc: 0.7172\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9835 - acc: 0.7028\n",
      "Epoch 00014: val_loss improved from 0.96609 to 0.92020, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/014-0.9202.hdf5\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.9834 - acc: 0.7028 - val_loss: 0.9202 - val_acc: 0.7254\n",
      "Epoch 15/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9501 - acc: 0.7125\n",
      "Epoch 00015: val_loss improved from 0.92020 to 0.90692, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/015-0.9069.hdf5\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.9506 - acc: 0.7123 - val_loss: 0.9069 - val_acc: 0.7347\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9294 - acc: 0.7165\n",
      "Epoch 00016: val_loss improved from 0.90692 to 0.87010, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/016-0.8701.hdf5\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.9282 - acc: 0.7169 - val_loss: 0.8701 - val_acc: 0.7454\n",
      "Epoch 17/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8960 - acc: 0.7281\n",
      "Epoch 00017: val_loss did not improve from 0.87010\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.8965 - acc: 0.7280 - val_loss: 0.9054 - val_acc: 0.7333\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8756 - acc: 0.7340\n",
      "Epoch 00018: val_loss improved from 0.87010 to 0.82985, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/018-0.8298.hdf5\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.8756 - acc: 0.7341 - val_loss: 0.8298 - val_acc: 0.7591\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8446 - acc: 0.7470\n",
      "Epoch 00019: val_loss did not improve from 0.82985\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.8445 - acc: 0.7470 - val_loss: 0.8334 - val_acc: 0.7591\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8267 - acc: 0.7495\n",
      "Epoch 00020: val_loss improved from 0.82985 to 0.79422, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/020-0.7942.hdf5\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.8267 - acc: 0.7495 - val_loss: 0.7942 - val_acc: 0.7638\n",
      "Epoch 21/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8010 - acc: 0.7588\n",
      "Epoch 00021: val_loss improved from 0.79422 to 0.77722, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/021-0.7772.hdf5\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.8010 - acc: 0.7587 - val_loss: 0.7772 - val_acc: 0.7750\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7815 - acc: 0.7650\n",
      "Epoch 00022: val_loss improved from 0.77722 to 0.76580, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/022-0.7658.hdf5\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.7815 - acc: 0.7650 - val_loss: 0.7658 - val_acc: 0.7754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7606 - acc: 0.7710\n",
      "Epoch 00023: val_loss did not improve from 0.76580\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.7607 - acc: 0.7710 - val_loss: 0.7857 - val_acc: 0.7692\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.7768\n",
      "Epoch 00024: val_loss improved from 0.76580 to 0.73132, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/024-0.7313.hdf5\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.7362 - acc: 0.7767 - val_loss: 0.7313 - val_acc: 0.7918\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7197 - acc: 0.7836\n",
      "Epoch 00025: val_loss improved from 0.73132 to 0.71210, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/025-0.7121.hdf5\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.7195 - acc: 0.7836 - val_loss: 0.7121 - val_acc: 0.7948\n",
      "Epoch 26/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7046 - acc: 0.7882\n",
      "Epoch 00026: val_loss improved from 0.71210 to 0.70681, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/026-0.7068.hdf5\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.7051 - acc: 0.7881 - val_loss: 0.7068 - val_acc: 0.7966\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6835 - acc: 0.7952\n",
      "Epoch 00027: val_loss improved from 0.70681 to 0.68577, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/027-0.6858.hdf5\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.6835 - acc: 0.7952 - val_loss: 0.6858 - val_acc: 0.8046\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6657 - acc: 0.8011\n",
      "Epoch 00028: val_loss did not improve from 0.68577\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.6658 - acc: 0.8011 - val_loss: 0.6882 - val_acc: 0.8039\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6536 - acc: 0.8014\n",
      "Epoch 00029: val_loss improved from 0.68577 to 0.65396, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/029-0.6540.hdf5\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.6531 - acc: 0.8015 - val_loss: 0.6540 - val_acc: 0.8120\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6392 - acc: 0.8079\n",
      "Epoch 00030: val_loss did not improve from 0.65396\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.6392 - acc: 0.8079 - val_loss: 0.7109 - val_acc: 0.7892\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6217 - acc: 0.8117\n",
      "Epoch 00031: val_loss improved from 0.65396 to 0.63575, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/031-0.6358.hdf5\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.6214 - acc: 0.8118 - val_loss: 0.6358 - val_acc: 0.8188\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6094 - acc: 0.8150\n",
      "Epoch 00032: val_loss improved from 0.63575 to 0.61624, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/032-0.6162.hdf5\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.6091 - acc: 0.8151 - val_loss: 0.6162 - val_acc: 0.8206\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5952 - acc: 0.8206\n",
      "Epoch 00033: val_loss did not improve from 0.61624\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.5952 - acc: 0.8206 - val_loss: 0.6327 - val_acc: 0.8211\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5755 - acc: 0.8236\n",
      "Epoch 00034: val_loss did not improve from 0.61624\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.5755 - acc: 0.8237 - val_loss: 0.6512 - val_acc: 0.8078\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5681 - acc: 0.8285\n",
      "Epoch 00035: val_loss improved from 0.61624 to 0.60239, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/035-0.6024.hdf5\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.5687 - acc: 0.8286 - val_loss: 0.6024 - val_acc: 0.8297\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.8344\n",
      "Epoch 00036: val_loss improved from 0.60239 to 0.58068, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/036-0.5807.hdf5\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.5542 - acc: 0.8344 - val_loss: 0.5807 - val_acc: 0.8318\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.8360\n",
      "Epoch 00037: val_loss did not improve from 0.58068\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.5399 - acc: 0.8359 - val_loss: 0.5902 - val_acc: 0.8332\n",
      "Epoch 38/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.8383\n",
      "Epoch 00038: val_loss did not improve from 0.58068\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.5377 - acc: 0.8383 - val_loss: 0.6179 - val_acc: 0.8234\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5199 - acc: 0.8432\n",
      "Epoch 00039: val_loss improved from 0.58068 to 0.56889, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/039-0.5689.hdf5\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.5199 - acc: 0.8431 - val_loss: 0.5689 - val_acc: 0.8411\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5071 - acc: 0.8472\n",
      "Epoch 00040: val_loss improved from 0.56889 to 0.56548, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/040-0.5655.hdf5\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.5072 - acc: 0.8472 - val_loss: 0.5655 - val_acc: 0.8421\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.8503\n",
      "Epoch 00041: val_loss improved from 0.56548 to 0.55560, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/041-0.5556.hdf5\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.4974 - acc: 0.8503 - val_loss: 0.5556 - val_acc: 0.8463\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8516\n",
      "Epoch 00042: val_loss improved from 0.55560 to 0.54610, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/042-0.5461.hdf5\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.4878 - acc: 0.8517 - val_loss: 0.5461 - val_acc: 0.8458\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8525\n",
      "Epoch 00043: val_loss did not improve from 0.54610\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.4821 - acc: 0.8526 - val_loss: 0.5505 - val_acc: 0.8470\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4718 - acc: 0.8561\n",
      "Epoch 00044: val_loss improved from 0.54610 to 0.54605, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/044-0.5461.hdf5\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.4716 - acc: 0.8562 - val_loss: 0.5461 - val_acc: 0.8463\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.8612\n",
      "Epoch 00045: val_loss did not improve from 0.54605\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.4597 - acc: 0.8609 - val_loss: 0.5633 - val_acc: 0.8430\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.8609\n",
      "Epoch 00046: val_loss improved from 0.54605 to 0.53818, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/046-0.5382.hdf5\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.4569 - acc: 0.8609 - val_loss: 0.5382 - val_acc: 0.8493\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8635\n",
      "Epoch 00047: val_loss did not improve from 0.53818\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.4497 - acc: 0.8635 - val_loss: 0.5467 - val_acc: 0.8488\n",
      "Epoch 48/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4433 - acc: 0.8634\n",
      "Epoch 00048: val_loss did not improve from 0.53818\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.4432 - acc: 0.8636 - val_loss: 0.5486 - val_acc: 0.8423\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4308 - acc: 0.8671\n",
      "Epoch 00049: val_loss improved from 0.53818 to 0.51145, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/049-0.5114.hdf5\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.4309 - acc: 0.8671 - val_loss: 0.5114 - val_acc: 0.8551\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4261 - acc: 0.8702\n",
      "Epoch 00050: val_loss improved from 0.51145 to 0.50667, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/050-0.5067.hdf5\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.4259 - acc: 0.8703 - val_loss: 0.5067 - val_acc: 0.8577\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4202 - acc: 0.8713\n",
      "Epoch 00051: val_loss did not improve from 0.50667\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.4201 - acc: 0.8714 - val_loss: 0.5071 - val_acc: 0.8598\n",
      "Epoch 52/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8731\n",
      "Epoch 00052: val_loss did not improve from 0.50667\n",
      "36805/36805 [==============================] - 11s 285us/sample - loss: 0.4136 - acc: 0.8732 - val_loss: 0.5137 - val_acc: 0.8558\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8751\n",
      "Epoch 00053: val_loss did not improve from 0.50667\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.4044 - acc: 0.8751 - val_loss: 0.5151 - val_acc: 0.8539\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8779\n",
      "Epoch 00054: val_loss improved from 0.50667 to 0.50395, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/054-0.5039.hdf5\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.4001 - acc: 0.8780 - val_loss: 0.5039 - val_acc: 0.8588\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8786\n",
      "Epoch 00055: val_loss did not improve from 0.50395\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.3917 - acc: 0.8786 - val_loss: 0.5297 - val_acc: 0.8474\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.8801\n",
      "Epoch 00056: val_loss did not improve from 0.50395\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.3885 - acc: 0.8802 - val_loss: 0.5092 - val_acc: 0.8609\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8816\n",
      "Epoch 00057: val_loss improved from 0.50395 to 0.49755, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/057-0.4976.hdf5\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.3828 - acc: 0.8815 - val_loss: 0.4976 - val_acc: 0.8602\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3685 - acc: 0.8847\n",
      "Epoch 00058: val_loss improved from 0.49755 to 0.49599, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/058-0.4960.hdf5\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.3686 - acc: 0.8847 - val_loss: 0.4960 - val_acc: 0.8654\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8855\n",
      "Epoch 00059: val_loss did not improve from 0.49599\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.3733 - acc: 0.8853 - val_loss: 0.5372 - val_acc: 0.8507\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8886\n",
      "Epoch 00060: val_loss improved from 0.49599 to 0.49018, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/060-0.4902.hdf5\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.3613 - acc: 0.8886 - val_loss: 0.4902 - val_acc: 0.8677\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8910\n",
      "Epoch 00061: val_loss improved from 0.49018 to 0.48814, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/061-0.4881.hdf5\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.3542 - acc: 0.8911 - val_loss: 0.4881 - val_acc: 0.8614\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8914\n",
      "Epoch 00062: val_loss improved from 0.48814 to 0.48646, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/062-0.4865.hdf5\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.3553 - acc: 0.8915 - val_loss: 0.4865 - val_acc: 0.8675\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3423 - acc: 0.8923\n",
      "Epoch 00063: val_loss did not improve from 0.48646\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.3423 - acc: 0.8924 - val_loss: 0.4913 - val_acc: 0.8649\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3398 - acc: 0.8948\n",
      "Epoch 00064: val_loss did not improve from 0.48646\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.3395 - acc: 0.8949 - val_loss: 0.4896 - val_acc: 0.8619\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.8983\n",
      "Epoch 00065: val_loss improved from 0.48646 to 0.48596, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/065-0.4860.hdf5\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.3329 - acc: 0.8983 - val_loss: 0.4860 - val_acc: 0.8721\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.8960\n",
      "Epoch 00066: val_loss did not improve from 0.48596\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.3315 - acc: 0.8962 - val_loss: 0.4884 - val_acc: 0.8670\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8995\n",
      "Epoch 00067: val_loss did not improve from 0.48596\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.3254 - acc: 0.8995 - val_loss: 0.4953 - val_acc: 0.8644\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.8995\n",
      "Epoch 00068: val_loss improved from 0.48596 to 0.48171, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/068-0.4817.hdf5\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.3202 - acc: 0.8995 - val_loss: 0.4817 - val_acc: 0.8693\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.9004\n",
      "Epoch 00069: val_loss did not improve from 0.48171\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.3190 - acc: 0.9005 - val_loss: 0.4969 - val_acc: 0.8682\n",
      "Epoch 70/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.8999\n",
      "Epoch 00070: val_loss improved from 0.48171 to 0.47449, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/070-0.4745.hdf5\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.3171 - acc: 0.8997 - val_loss: 0.4745 - val_acc: 0.8696\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.9031\n",
      "Epoch 00071: val_loss did not improve from 0.47449\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.3129 - acc: 0.9031 - val_loss: 0.5206 - val_acc: 0.8619\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3123 - acc: 0.9024\n",
      "Epoch 00072: val_loss did not improve from 0.47449\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.3120 - acc: 0.9025 - val_loss: 0.4825 - val_acc: 0.8730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.9054\n",
      "Epoch 00073: val_loss did not improve from 0.47449\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.3016 - acc: 0.9054 - val_loss: 0.4980 - val_acc: 0.8658\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9086\n",
      "Epoch 00074: val_loss improved from 0.47449 to 0.47301, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/074-0.4730.hdf5\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.2924 - acc: 0.9086 - val_loss: 0.4730 - val_acc: 0.8703\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9086\n",
      "Epoch 00075: val_loss improved from 0.47301 to 0.46863, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/075-0.4686.hdf5\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.2912 - acc: 0.9085 - val_loss: 0.4686 - val_acc: 0.8768\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9099\n",
      "Epoch 00076: val_loss did not improve from 0.46863\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.2901 - acc: 0.9099 - val_loss: 0.4823 - val_acc: 0.8703\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.9102\n",
      "Epoch 00077: val_loss did not improve from 0.46863\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.2861 - acc: 0.9102 - val_loss: 0.4786 - val_acc: 0.8758\n",
      "Epoch 78/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9124\n",
      "Epoch 00078: val_loss did not improve from 0.46863\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.2794 - acc: 0.9125 - val_loss: 0.4723 - val_acc: 0.8747\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9143\n",
      "Epoch 00079: val_loss did not improve from 0.46863\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.2782 - acc: 0.9143 - val_loss: 0.4811 - val_acc: 0.8758\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9138\n",
      "Epoch 00080: val_loss did not improve from 0.46863\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.2748 - acc: 0.9137 - val_loss: 0.4849 - val_acc: 0.8698\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9129\n",
      "Epoch 00081: val_loss improved from 0.46863 to 0.46851, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/081-0.4685.hdf5\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.2754 - acc: 0.9128 - val_loss: 0.4685 - val_acc: 0.8786\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9154\n",
      "Epoch 00082: val_loss did not improve from 0.46851\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.2680 - acc: 0.9154 - val_loss: 0.4865 - val_acc: 0.8717\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9153\n",
      "Epoch 00083: val_loss did not improve from 0.46851\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.2630 - acc: 0.9153 - val_loss: 0.4775 - val_acc: 0.8777\n",
      "Epoch 84/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9188\n",
      "Epoch 00084: val_loss did not improve from 0.46851\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.2556 - acc: 0.9188 - val_loss: 0.4780 - val_acc: 0.8770\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9155\n",
      "Epoch 00085: val_loss improved from 0.46851 to 0.45422, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_5_conv_checkpoint/085-0.4542.hdf5\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.2616 - acc: 0.9154 - val_loss: 0.4542 - val_acc: 0.8821\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9194\n",
      "Epoch 00086: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.2553 - acc: 0.9194 - val_loss: 0.4730 - val_acc: 0.8779\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9182\n",
      "Epoch 00087: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.2515 - acc: 0.9184 - val_loss: 0.5139 - val_acc: 0.8703\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9208\n",
      "Epoch 00088: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.2497 - acc: 0.9208 - val_loss: 0.4952 - val_acc: 0.8742\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9227\n",
      "Epoch 00089: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.2446 - acc: 0.9228 - val_loss: 0.4660 - val_acc: 0.8803\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9224\n",
      "Epoch 00090: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.2435 - acc: 0.9224 - val_loss: 0.4888 - val_acc: 0.8772\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9221\n",
      "Epoch 00091: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.2410 - acc: 0.9221 - val_loss: 0.4823 - val_acc: 0.8784\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9253\n",
      "Epoch 00092: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.2352 - acc: 0.9254 - val_loss: 0.4755 - val_acc: 0.8821\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9265\n",
      "Epoch 00093: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.2330 - acc: 0.9265 - val_loss: 0.4709 - val_acc: 0.8810\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9265\n",
      "Epoch 00094: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.2351 - acc: 0.9265 - val_loss: 0.4714 - val_acc: 0.8805\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9246\n",
      "Epoch 00095: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.2321 - acc: 0.9245 - val_loss: 0.4732 - val_acc: 0.8817\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9252\n",
      "Epoch 00096: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.2281 - acc: 0.9252 - val_loss: 0.4744 - val_acc: 0.8765\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9264\n",
      "Epoch 00097: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.2246 - acc: 0.9264 - val_loss: 0.4931 - val_acc: 0.8784\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9273\n",
      "Epoch 00098: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.2242 - acc: 0.9273 - val_loss: 0.4820 - val_acc: 0.8763\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9284\n",
      "Epoch 00099: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.2235 - acc: 0.9285 - val_loss: 0.4763 - val_acc: 0.8791\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9281\n",
      "Epoch 00100: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.2221 - acc: 0.9281 - val_loss: 0.4817 - val_acc: 0.8796\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9285\n",
      "Epoch 00101: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.2198 - acc: 0.9287 - val_loss: 0.4911 - val_acc: 0.8789\n",
      "Epoch 102/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9300\n",
      "Epoch 00102: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.2154 - acc: 0.9297 - val_loss: 0.4743 - val_acc: 0.8828\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9302\n",
      "Epoch 00103: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.2119 - acc: 0.9302 - val_loss: 0.4690 - val_acc: 0.8856\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9321\n",
      "Epoch 00104: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.2081 - acc: 0.9320 - val_loss: 0.4661 - val_acc: 0.8854\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9313\n",
      "Epoch 00105: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.2127 - acc: 0.9313 - val_loss: 0.4659 - val_acc: 0.8845\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9335\n",
      "Epoch 00106: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.2072 - acc: 0.9335 - val_loss: 0.4897 - val_acc: 0.8833\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9323\n",
      "Epoch 00107: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.2072 - acc: 0.9322 - val_loss: 0.4690 - val_acc: 0.8849\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9365\n",
      "Epoch 00108: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.2011 - acc: 0.9365 - val_loss: 0.4772 - val_acc: 0.8884\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9364\n",
      "Epoch 00109: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.1972 - acc: 0.9364 - val_loss: 0.4707 - val_acc: 0.8852\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9359\n",
      "Epoch 00110: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.1972 - acc: 0.9359 - val_loss: 0.4630 - val_acc: 0.8887\n",
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9377\n",
      "Epoch 00111: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.1968 - acc: 0.9377 - val_loss: 0.4729 - val_acc: 0.8877\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9377\n",
      "Epoch 00112: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1937 - acc: 0.9376 - val_loss: 0.4744 - val_acc: 0.8861\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9374\n",
      "Epoch 00113: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.1966 - acc: 0.9374 - val_loss: 0.4785 - val_acc: 0.8852\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9380\n",
      "Epoch 00114: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1874 - acc: 0.9381 - val_loss: 0.4867 - val_acc: 0.8800\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9392\n",
      "Epoch 00115: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1914 - acc: 0.9392 - val_loss: 0.4746 - val_acc: 0.8854\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9372\n",
      "Epoch 00116: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1910 - acc: 0.9373 - val_loss: 0.4728 - val_acc: 0.8861\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9383\n",
      "Epoch 00117: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1893 - acc: 0.9382 - val_loss: 0.4806 - val_acc: 0.8861\n",
      "Epoch 118/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9395\n",
      "Epoch 00118: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1882 - acc: 0.9394 - val_loss: 0.4702 - val_acc: 0.8870\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9399\n",
      "Epoch 00119: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1823 - acc: 0.9400 - val_loss: 0.4787 - val_acc: 0.8847\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9413\n",
      "Epoch 00120: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1777 - acc: 0.9413 - val_loss: 0.4872 - val_acc: 0.8845\n",
      "Epoch 121/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9409\n",
      "Epoch 00121: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.1800 - acc: 0.9409 - val_loss: 0.5116 - val_acc: 0.8824\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9417\n",
      "Epoch 00122: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.1790 - acc: 0.9417 - val_loss: 0.4775 - val_acc: 0.8873\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9408\n",
      "Epoch 00123: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1800 - acc: 0.9408 - val_loss: 0.4892 - val_acc: 0.8877\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9417\n",
      "Epoch 00124: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.1766 - acc: 0.9418 - val_loss: 0.4861 - val_acc: 0.8873\n",
      "Epoch 125/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9427\n",
      "Epoch 00125: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.1749 - acc: 0.9425 - val_loss: 0.4687 - val_acc: 0.8898\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9459\n",
      "Epoch 00126: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.1653 - acc: 0.9459 - val_loss: 0.4834 - val_acc: 0.8866\n",
      "Epoch 127/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9446\n",
      "Epoch 00127: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1692 - acc: 0.9446 - val_loss: 0.5152 - val_acc: 0.8833\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9459\n",
      "Epoch 00128: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 276us/sample - loss: 0.1669 - acc: 0.9459 - val_loss: 0.4954 - val_acc: 0.8861\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9452\n",
      "Epoch 00129: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1684 - acc: 0.9451 - val_loss: 0.4712 - val_acc: 0.8896\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9458\n",
      "Epoch 00130: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.1648 - acc: 0.9458 - val_loss: 0.5093 - val_acc: 0.8838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9455\n",
      "Epoch 00131: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.1647 - acc: 0.9456 - val_loss: 0.4946 - val_acc: 0.8873\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9451\n",
      "Epoch 00132: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1651 - acc: 0.9451 - val_loss: 0.4960 - val_acc: 0.8849\n",
      "Epoch 133/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9485\n",
      "Epoch 00133: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1612 - acc: 0.9485 - val_loss: 0.4953 - val_acc: 0.8891\n",
      "Epoch 134/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9474\n",
      "Epoch 00134: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.1606 - acc: 0.9475 - val_loss: 0.4815 - val_acc: 0.8884\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9491\n",
      "Epoch 00135: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.1575 - acc: 0.9491 - val_loss: 0.4877 - val_acc: 0.8861\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9455\n",
      "Epoch 00136: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1651 - acc: 0.9455 - val_loss: 0.4996 - val_acc: 0.8887\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9497\n",
      "Epoch 00137: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 277us/sample - loss: 0.1521 - acc: 0.9497 - val_loss: 0.4968 - val_acc: 0.8882\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9495\n",
      "Epoch 00138: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1539 - acc: 0.9494 - val_loss: 0.4890 - val_acc: 0.8847\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9489\n",
      "Epoch 00139: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1559 - acc: 0.9488 - val_loss: 0.4768 - val_acc: 0.8894\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9506\n",
      "Epoch 00140: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.1515 - acc: 0.9506 - val_loss: 0.4929 - val_acc: 0.8863\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9484\n",
      "Epoch 00141: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.1577 - acc: 0.9484 - val_loss: 0.5102 - val_acc: 0.8852\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9509\n",
      "Epoch 00142: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.1500 - acc: 0.9509 - val_loss: 0.4896 - val_acc: 0.8915\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9501\n",
      "Epoch 00143: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1500 - acc: 0.9501 - val_loss: 0.5005 - val_acc: 0.8835\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9513\n",
      "Epoch 00144: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1481 - acc: 0.9513 - val_loss: 0.4793 - val_acc: 0.8863\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9520\n",
      "Epoch 00145: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.1458 - acc: 0.9520 - val_loss: 0.4910 - val_acc: 0.8877\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9525\n",
      "Epoch 00146: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.1451 - acc: 0.9525 - val_loss: 0.4917 - val_acc: 0.8887\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9531\n",
      "Epoch 00147: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1436 - acc: 0.9530 - val_loss: 0.5150 - val_acc: 0.8884\n",
      "Epoch 148/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9515\n",
      "Epoch 00148: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1473 - acc: 0.9516 - val_loss: 0.4872 - val_acc: 0.8889\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9533\n",
      "Epoch 00149: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1423 - acc: 0.9533 - val_loss: 0.4978 - val_acc: 0.8889\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9538\n",
      "Epoch 00150: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.1418 - acc: 0.9538 - val_loss: 0.5005 - val_acc: 0.8868\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9542\n",
      "Epoch 00151: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1386 - acc: 0.9542 - val_loss: 0.5074 - val_acc: 0.8915\n",
      "Epoch 152/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9556\n",
      "Epoch 00152: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.1374 - acc: 0.9554 - val_loss: 0.4906 - val_acc: 0.8910\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9536\n",
      "Epoch 00153: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1395 - acc: 0.9536 - val_loss: 0.5034 - val_acc: 0.8894\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9563\n",
      "Epoch 00154: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.1353 - acc: 0.9563 - val_loss: 0.5238 - val_acc: 0.8866\n",
      "Epoch 155/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9545\n",
      "Epoch 00155: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1372 - acc: 0.9545 - val_loss: 0.5004 - val_acc: 0.8894\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9549\n",
      "Epoch 00156: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1372 - acc: 0.9550 - val_loss: 0.5084 - val_acc: 0.8859\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9546\n",
      "Epoch 00157: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.1346 - acc: 0.9547 - val_loss: 0.5035 - val_acc: 0.8884\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9568\n",
      "Epoch 00158: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1301 - acc: 0.9568 - val_loss: 0.5357 - val_acc: 0.8856\n",
      "Epoch 159/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9563\n",
      "Epoch 00159: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.1342 - acc: 0.9563 - val_loss: 0.5089 - val_acc: 0.8921\n",
      "Epoch 160/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9569\n",
      "Epoch 00160: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1314 - acc: 0.9568 - val_loss: 0.4969 - val_acc: 0.8919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9561\n",
      "Epoch 00161: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1343 - acc: 0.9561 - val_loss: 0.5229 - val_acc: 0.8847\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9567\n",
      "Epoch 00162: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1309 - acc: 0.9566 - val_loss: 0.4999 - val_acc: 0.8898\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9561\n",
      "Epoch 00163: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.1316 - acc: 0.9561 - val_loss: 0.5158 - val_acc: 0.8905\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9575\n",
      "Epoch 00164: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1320 - acc: 0.9576 - val_loss: 0.4999 - val_acc: 0.8945\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9576\n",
      "Epoch 00165: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.1277 - acc: 0.9576 - val_loss: 0.4907 - val_acc: 0.8924\n",
      "Epoch 166/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9588\n",
      "Epoch 00166: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.1281 - acc: 0.9588 - val_loss: 0.5299 - val_acc: 0.8861\n",
      "Epoch 167/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9583\n",
      "Epoch 00167: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.1283 - acc: 0.9584 - val_loss: 0.5026 - val_acc: 0.8905\n",
      "Epoch 168/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9575\n",
      "Epoch 00168: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.1304 - acc: 0.9575 - val_loss: 0.4950 - val_acc: 0.8919\n",
      "Epoch 169/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9591\n",
      "Epoch 00169: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.1241 - acc: 0.9591 - val_loss: 0.5112 - val_acc: 0.8908\n",
      "Epoch 170/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9590\n",
      "Epoch 00170: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.1253 - acc: 0.9591 - val_loss: 0.5255 - val_acc: 0.8877\n",
      "Epoch 171/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9589\n",
      "Epoch 00171: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.1253 - acc: 0.9588 - val_loss: 0.5120 - val_acc: 0.8921\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9592\n",
      "Epoch 00172: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 283us/sample - loss: 0.1250 - acc: 0.9592 - val_loss: 0.5069 - val_acc: 0.8873\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9613\n",
      "Epoch 00173: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 280us/sample - loss: 0.1198 - acc: 0.9613 - val_loss: 0.5049 - val_acc: 0.8924\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9623\n",
      "Epoch 00174: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.1164 - acc: 0.9623 - val_loss: 0.5001 - val_acc: 0.8942\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9617\n",
      "Epoch 00175: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1183 - acc: 0.9617 - val_loss: 0.4969 - val_acc: 0.8919\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9611\n",
      "Epoch 00176: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.1189 - acc: 0.9611 - val_loss: 0.5137 - val_acc: 0.8928\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9616\n",
      "Epoch 00177: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.1184 - acc: 0.9617 - val_loss: 0.5160 - val_acc: 0.8917\n",
      "Epoch 178/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9611\n",
      "Epoch 00178: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.1195 - acc: 0.9610 - val_loss: 0.4922 - val_acc: 0.8931\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9601\n",
      "Epoch 00179: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 11s 285us/sample - loss: 0.1203 - acc: 0.9601 - val_loss: 0.5083 - val_acc: 0.8924\n",
      "Epoch 180/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9620\n",
      "Epoch 00180: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 279us/sample - loss: 0.1157 - acc: 0.9622 - val_loss: 0.5228 - val_acc: 0.8891\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9602\n",
      "Epoch 00181: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 0.1181 - acc: 0.9602 - val_loss: 0.5054 - val_acc: 0.8896\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9624\n",
      "Epoch 00182: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.1181 - acc: 0.9624 - val_loss: 0.5036 - val_acc: 0.8919\n",
      "Epoch 183/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9622\n",
      "Epoch 00183: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1131 - acc: 0.9622 - val_loss: 0.5059 - val_acc: 0.8945\n",
      "Epoch 184/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9611\n",
      "Epoch 00184: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 282us/sample - loss: 0.1179 - acc: 0.9611 - val_loss: 0.5132 - val_acc: 0.8891\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9604\n",
      "Epoch 00185: val_loss did not improve from 0.45422\n",
      "36805/36805 [==============================] - 10s 281us/sample - loss: 0.1163 - acc: 0.9604 - val_loss: 0.6014 - val_acc: 0.8845\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 237us/sample - loss: 0.5174 - acc: 0.8492\n",
      "Loss: 0.5174186083015252 Accuracy: 0.84922117\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3144 - acc: 0.2407\n",
      "Epoch 00001: val_loss improved from inf to 1.75361, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/001-1.7536.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 2.3133 - acc: 0.2412 - val_loss: 1.7536 - val_acc: 0.4354\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6448 - acc: 0.4697\n",
      "Epoch 00002: val_loss improved from 1.75361 to 1.40416, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/002-1.4042.hdf5\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 1.6448 - acc: 0.4696 - val_loss: 1.4042 - val_acc: 0.5660\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3638 - acc: 0.5640\n",
      "Epoch 00003: val_loss improved from 1.40416 to 1.15975, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/003-1.1597.hdf5\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 1.3637 - acc: 0.5640 - val_loss: 1.1597 - val_acc: 0.6499\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1918 - acc: 0.6256\n",
      "Epoch 00004: val_loss improved from 1.15975 to 1.05889, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/004-1.0589.hdf5\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 1.1914 - acc: 0.6258 - val_loss: 1.0589 - val_acc: 0.6806\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0574 - acc: 0.6731\n",
      "Epoch 00005: val_loss improved from 1.05889 to 0.95088, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/005-0.9509.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 1.0570 - acc: 0.6732 - val_loss: 0.9509 - val_acc: 0.7240\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9533 - acc: 0.7054\n",
      "Epoch 00006: val_loss improved from 0.95088 to 0.79818, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/006-0.7982.hdf5\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.9532 - acc: 0.7054 - val_loss: 0.7982 - val_acc: 0.7654\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8694 - acc: 0.7350\n",
      "Epoch 00007: val_loss improved from 0.79818 to 0.75897, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/007-0.7590.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.8693 - acc: 0.7351 - val_loss: 0.7590 - val_acc: 0.7806\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8010 - acc: 0.7551\n",
      "Epoch 00008: val_loss improved from 0.75897 to 0.71725, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/008-0.7173.hdf5\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.8009 - acc: 0.7551 - val_loss: 0.7173 - val_acc: 0.7964\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7450 - acc: 0.7743\n",
      "Epoch 00009: val_loss improved from 0.71725 to 0.66154, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/009-0.6615.hdf5\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.7451 - acc: 0.7743 - val_loss: 0.6615 - val_acc: 0.8104\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6982 - acc: 0.7865\n",
      "Epoch 00010: val_loss improved from 0.66154 to 0.60935, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/010-0.6093.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.6982 - acc: 0.7864 - val_loss: 0.6093 - val_acc: 0.8272\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6515 - acc: 0.8012\n",
      "Epoch 00011: val_loss improved from 0.60935 to 0.56834, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/011-0.5683.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.6514 - acc: 0.8012 - val_loss: 0.5683 - val_acc: 0.8376\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6160 - acc: 0.8126\n",
      "Epoch 00012: val_loss improved from 0.56834 to 0.53782, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/012-0.5378.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.6165 - acc: 0.8125 - val_loss: 0.5378 - val_acc: 0.8477\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5837 - acc: 0.8208\n",
      "Epoch 00013: val_loss improved from 0.53782 to 0.50209, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/013-0.5021.hdf5\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.5834 - acc: 0.8209 - val_loss: 0.5021 - val_acc: 0.8563\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.8315\n",
      "Epoch 00014: val_loss improved from 0.50209 to 0.48571, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/014-0.4857.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.5522 - acc: 0.8316 - val_loss: 0.4857 - val_acc: 0.8635\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.8411\n",
      "Epoch 00015: val_loss improved from 0.48571 to 0.45354, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/015-0.4535.hdf5\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.5284 - acc: 0.8410 - val_loss: 0.4535 - val_acc: 0.8691\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5005 - acc: 0.8461\n",
      "Epoch 00016: val_loss improved from 0.45354 to 0.45015, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/016-0.4501.hdf5\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.5003 - acc: 0.8461 - val_loss: 0.4501 - val_acc: 0.8700\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4837 - acc: 0.8529\n",
      "Epoch 00017: val_loss improved from 0.45015 to 0.42229, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/017-0.4223.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.4837 - acc: 0.8529 - val_loss: 0.4223 - val_acc: 0.8770\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8580\n",
      "Epoch 00018: val_loss improved from 0.42229 to 0.41410, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/018-0.4141.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.4620 - acc: 0.8580 - val_loss: 0.4141 - val_acc: 0.8842\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8633\n",
      "Epoch 00019: val_loss improved from 0.41410 to 0.38954, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/019-0.3895.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.4407 - acc: 0.8633 - val_loss: 0.3895 - val_acc: 0.8896\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.8655\n",
      "Epoch 00020: val_loss did not improve from 0.38954\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.4292 - acc: 0.8654 - val_loss: 0.3974 - val_acc: 0.8838\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8710\n",
      "Epoch 00021: val_loss improved from 0.38954 to 0.36408, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/021-0.3641.hdf5\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.4144 - acc: 0.8711 - val_loss: 0.3641 - val_acc: 0.8968\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8752\n",
      "Epoch 00022: val_loss did not improve from 0.36408\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.3987 - acc: 0.8751 - val_loss: 0.3673 - val_acc: 0.8975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8799\n",
      "Epoch 00023: val_loss did not improve from 0.36408\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.3869 - acc: 0.8800 - val_loss: 0.3734 - val_acc: 0.8896\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3700 - acc: 0.8849\n",
      "Epoch 00024: val_loss improved from 0.36408 to 0.34741, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/024-0.3474.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.3703 - acc: 0.8847 - val_loss: 0.3474 - val_acc: 0.8998\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8882\n",
      "Epoch 00025: val_loss improved from 0.34741 to 0.33925, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/025-0.3392.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.3614 - acc: 0.8882 - val_loss: 0.3392 - val_acc: 0.9047\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8898\n",
      "Epoch 00026: val_loss improved from 0.33925 to 0.33388, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/026-0.3339.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.3504 - acc: 0.8898 - val_loss: 0.3339 - val_acc: 0.9054\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3412 - acc: 0.8950\n",
      "Epoch 00027: val_loss improved from 0.33388 to 0.32647, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/027-0.3265.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.3410 - acc: 0.8951 - val_loss: 0.3265 - val_acc: 0.9038\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.8963\n",
      "Epoch 00028: val_loss improved from 0.32647 to 0.31664, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/028-0.3166.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.3316 - acc: 0.8964 - val_loss: 0.3166 - val_acc: 0.9115\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.8981\n",
      "Epoch 00029: val_loss did not improve from 0.31664\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.3265 - acc: 0.8981 - val_loss: 0.3191 - val_acc: 0.9080\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.8996\n",
      "Epoch 00030: val_loss improved from 0.31664 to 0.31614, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/030-0.3161.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.3152 - acc: 0.8995 - val_loss: 0.3161 - val_acc: 0.9050\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9041\n",
      "Epoch 00031: val_loss improved from 0.31614 to 0.29948, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/031-0.2995.hdf5\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.3061 - acc: 0.9041 - val_loss: 0.2995 - val_acc: 0.9166\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.9047\n",
      "Epoch 00032: val_loss improved from 0.29948 to 0.29668, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/032-0.2967.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.3039 - acc: 0.9046 - val_loss: 0.2967 - val_acc: 0.9131\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9075\n",
      "Epoch 00033: val_loss did not improve from 0.29668\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.2916 - acc: 0.9075 - val_loss: 0.2983 - val_acc: 0.9099\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9092\n",
      "Epoch 00034: val_loss did not improve from 0.29668\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.2849 - acc: 0.9092 - val_loss: 0.3175 - val_acc: 0.9045\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9106\n",
      "Epoch 00035: val_loss did not improve from 0.29668\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.2812 - acc: 0.9106 - val_loss: 0.3032 - val_acc: 0.9133\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9113\n",
      "Epoch 00036: val_loss improved from 0.29668 to 0.29519, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/036-0.2952.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.2740 - acc: 0.9113 - val_loss: 0.2952 - val_acc: 0.9161\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9125\n",
      "Epoch 00037: val_loss improved from 0.29519 to 0.29356, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/037-0.2936.hdf5\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.2690 - acc: 0.9125 - val_loss: 0.2936 - val_acc: 0.9110\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.9165\n",
      "Epoch 00038: val_loss improved from 0.29356 to 0.28622, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/038-0.2862.hdf5\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.2606 - acc: 0.9165 - val_loss: 0.2862 - val_acc: 0.9182\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9181\n",
      "Epoch 00039: val_loss did not improve from 0.28622\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.2567 - acc: 0.9181 - val_loss: 0.2877 - val_acc: 0.9182\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9183\n",
      "Epoch 00040: val_loss improved from 0.28622 to 0.27197, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/040-0.2720.hdf5\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.2546 - acc: 0.9182 - val_loss: 0.2720 - val_acc: 0.9220\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9207\n",
      "Epoch 00041: val_loss did not improve from 0.27197\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.2466 - acc: 0.9207 - val_loss: 0.2803 - val_acc: 0.9208\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9212\n",
      "Epoch 00042: val_loss did not improve from 0.27197\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.2437 - acc: 0.9213 - val_loss: 0.2845 - val_acc: 0.9185\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9230\n",
      "Epoch 00043: val_loss did not improve from 0.27197\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.2395 - acc: 0.9230 - val_loss: 0.2745 - val_acc: 0.9196\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9242\n",
      "Epoch 00044: val_loss improved from 0.27197 to 0.27029, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/044-0.2703.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.2292 - acc: 0.9241 - val_loss: 0.2703 - val_acc: 0.9238\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9257\n",
      "Epoch 00045: val_loss did not improve from 0.27029\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.2277 - acc: 0.9257 - val_loss: 0.2774 - val_acc: 0.9224\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9290\n",
      "Epoch 00046: val_loss did not improve from 0.27029\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.2223 - acc: 0.9290 - val_loss: 0.2846 - val_acc: 0.9210\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9288\n",
      "Epoch 00047: val_loss did not improve from 0.27029\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.2202 - acc: 0.9288 - val_loss: 0.2759 - val_acc: 0.9210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9300\n",
      "Epoch 00048: val_loss did not improve from 0.27029\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.2143 - acc: 0.9299 - val_loss: 0.2995 - val_acc: 0.9113\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9320\n",
      "Epoch 00049: val_loss did not improve from 0.27029\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.2114 - acc: 0.9320 - val_loss: 0.2717 - val_acc: 0.9229\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9294\n",
      "Epoch 00050: val_loss did not improve from 0.27029\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.2146 - acc: 0.9294 - val_loss: 0.2730 - val_acc: 0.9280\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9334\n",
      "Epoch 00051: val_loss improved from 0.27029 to 0.26447, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/051-0.2645.hdf5\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.2049 - acc: 0.9334 - val_loss: 0.2645 - val_acc: 0.9255\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9348\n",
      "Epoch 00052: val_loss did not improve from 0.26447\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.2003 - acc: 0.9347 - val_loss: 0.2733 - val_acc: 0.9241\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9344\n",
      "Epoch 00053: val_loss improved from 0.26447 to 0.26038, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/053-0.2604.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1995 - acc: 0.9344 - val_loss: 0.2604 - val_acc: 0.9280\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9370\n",
      "Epoch 00054: val_loss improved from 0.26038 to 0.25649, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/054-0.2565.hdf5\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.1956 - acc: 0.9370 - val_loss: 0.2565 - val_acc: 0.9285\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9382\n",
      "Epoch 00055: val_loss did not improve from 0.25649\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1898 - acc: 0.9382 - val_loss: 0.2685 - val_acc: 0.9264\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9375\n",
      "Epoch 00056: val_loss did not improve from 0.25649\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1872 - acc: 0.9374 - val_loss: 0.2590 - val_acc: 0.9283\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9401\n",
      "Epoch 00057: val_loss improved from 0.25649 to 0.25617, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/057-0.2562.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1816 - acc: 0.9401 - val_loss: 0.2562 - val_acc: 0.9301\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9402\n",
      "Epoch 00058: val_loss did not improve from 0.25617\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1792 - acc: 0.9403 - val_loss: 0.2575 - val_acc: 0.9255\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9397\n",
      "Epoch 00059: val_loss did not improve from 0.25617\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1792 - acc: 0.9397 - val_loss: 0.2629 - val_acc: 0.9304\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9404\n",
      "Epoch 00060: val_loss improved from 0.25617 to 0.25402, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/060-0.2540.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1805 - acc: 0.9405 - val_loss: 0.2540 - val_acc: 0.9292\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9423\n",
      "Epoch 00061: val_loss did not improve from 0.25402\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1702 - acc: 0.9424 - val_loss: 0.2695 - val_acc: 0.9278\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9428\n",
      "Epoch 00062: val_loss improved from 0.25402 to 0.25066, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/062-0.2507.hdf5\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1726 - acc: 0.9427 - val_loss: 0.2507 - val_acc: 0.9294\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9428\n",
      "Epoch 00063: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1701 - acc: 0.9428 - val_loss: 0.2515 - val_acc: 0.9334\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9448\n",
      "Epoch 00064: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1631 - acc: 0.9448 - val_loss: 0.2664 - val_acc: 0.9231\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9442\n",
      "Epoch 00065: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1650 - acc: 0.9441 - val_loss: 0.2530 - val_acc: 0.9306\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9482\n",
      "Epoch 00066: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1575 - acc: 0.9483 - val_loss: 0.2558 - val_acc: 0.9304\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9473\n",
      "Epoch 00067: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1597 - acc: 0.9473 - val_loss: 0.2623 - val_acc: 0.9320\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9454\n",
      "Epoch 00068: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1576 - acc: 0.9455 - val_loss: 0.2619 - val_acc: 0.9301\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9483\n",
      "Epoch 00069: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1540 - acc: 0.9483 - val_loss: 0.2559 - val_acc: 0.9278\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9492\n",
      "Epoch 00070: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.1506 - acc: 0.9492 - val_loss: 0.2565 - val_acc: 0.9276\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9493\n",
      "Epoch 00071: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1526 - acc: 0.9493 - val_loss: 0.2965 - val_acc: 0.9222\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9485\n",
      "Epoch 00072: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1501 - acc: 0.9485 - val_loss: 0.2568 - val_acc: 0.9320\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9486\n",
      "Epoch 00073: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1480 - acc: 0.9487 - val_loss: 0.2507 - val_acc: 0.9338\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9537\n",
      "Epoch 00074: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1424 - acc: 0.9537 - val_loss: 0.2720 - val_acc: 0.9292\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9535\n",
      "Epoch 00075: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.1392 - acc: 0.9535 - val_loss: 0.2632 - val_acc: 0.9355\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9522\n",
      "Epoch 00076: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1379 - acc: 0.9522 - val_loss: 0.2713 - val_acc: 0.9271\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9534\n",
      "Epoch 00077: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1386 - acc: 0.9534 - val_loss: 0.2539 - val_acc: 0.9352\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9529\n",
      "Epoch 00078: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1375 - acc: 0.9530 - val_loss: 0.2594 - val_acc: 0.9304\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9542\n",
      "Epoch 00079: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1346 - acc: 0.9543 - val_loss: 0.2542 - val_acc: 0.9331\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9542\n",
      "Epoch 00080: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1338 - acc: 0.9540 - val_loss: 0.2645 - val_acc: 0.9334\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9548\n",
      "Epoch 00081: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1328 - acc: 0.9548 - val_loss: 0.2567 - val_acc: 0.9304\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9587\n",
      "Epoch 00082: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1231 - acc: 0.9586 - val_loss: 0.2675 - val_acc: 0.9301\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9564\n",
      "Epoch 00083: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.1292 - acc: 0.9564 - val_loss: 0.2609 - val_acc: 0.9311\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9597\n",
      "Epoch 00084: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1219 - acc: 0.9597 - val_loss: 0.2533 - val_acc: 0.9320\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9570\n",
      "Epoch 00085: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.1251 - acc: 0.9570 - val_loss: 0.2517 - val_acc: 0.9373\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9582\n",
      "Epoch 00086: val_loss did not improve from 0.25066\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1228 - acc: 0.9582 - val_loss: 0.2573 - val_acc: 0.9369\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9590\n",
      "Epoch 00087: val_loss improved from 0.25066 to 0.25051, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/087-0.2505.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1192 - acc: 0.9591 - val_loss: 0.2505 - val_acc: 0.9364\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9604\n",
      "Epoch 00088: val_loss did not improve from 0.25051\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1184 - acc: 0.9604 - val_loss: 0.2602 - val_acc: 0.9357\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9598\n",
      "Epoch 00089: val_loss improved from 0.25051 to 0.24848, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/089-0.2485.hdf5\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.1168 - acc: 0.9598 - val_loss: 0.2485 - val_acc: 0.9369\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9614\n",
      "Epoch 00090: val_loss did not improve from 0.24848\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1156 - acc: 0.9613 - val_loss: 0.2652 - val_acc: 0.9378\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9607\n",
      "Epoch 00091: val_loss did not improve from 0.24848\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1163 - acc: 0.9606 - val_loss: 0.2617 - val_acc: 0.9329\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9596\n",
      "Epoch 00092: val_loss did not improve from 0.24848\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1162 - acc: 0.9596 - val_loss: 0.2551 - val_acc: 0.9343\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9629\n",
      "Epoch 00093: val_loss did not improve from 0.24848\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.1081 - acc: 0.9629 - val_loss: 0.2608 - val_acc: 0.9348\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9624\n",
      "Epoch 00094: val_loss did not improve from 0.24848\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1104 - acc: 0.9623 - val_loss: 0.2649 - val_acc: 0.9320\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9615\n",
      "Epoch 00095: val_loss did not improve from 0.24848\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.1125 - acc: 0.9614 - val_loss: 0.2553 - val_acc: 0.9331\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9634\n",
      "Epoch 00096: val_loss did not improve from 0.24848\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1062 - acc: 0.9635 - val_loss: 0.2498 - val_acc: 0.9352\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9648\n",
      "Epoch 00097: val_loss improved from 0.24848 to 0.24208, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/097-0.2421.hdf5\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.1064 - acc: 0.9648 - val_loss: 0.2421 - val_acc: 0.9350\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9650\n",
      "Epoch 00098: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.1014 - acc: 0.9650 - val_loss: 0.2601 - val_acc: 0.9357\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9652\n",
      "Epoch 00099: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1024 - acc: 0.9652 - val_loss: 0.2518 - val_acc: 0.9380\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9635\n",
      "Epoch 00100: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1045 - acc: 0.9635 - val_loss: 0.2572 - val_acc: 0.9317\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9653\n",
      "Epoch 00101: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.1012 - acc: 0.9653 - val_loss: 0.2512 - val_acc: 0.9364\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9661\n",
      "Epoch 00102: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0985 - acc: 0.9661 - val_loss: 0.2684 - val_acc: 0.9336\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9662\n",
      "Epoch 00103: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0993 - acc: 0.9661 - val_loss: 0.2675 - val_acc: 0.9341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9652\n",
      "Epoch 00104: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1008 - acc: 0.9653 - val_loss: 0.2530 - val_acc: 0.9343\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9664\n",
      "Epoch 00105: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.0974 - acc: 0.9665 - val_loss: 0.2544 - val_acc: 0.9385\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9679\n",
      "Epoch 00106: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0938 - acc: 0.9679 - val_loss: 0.2645 - val_acc: 0.9387\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9679\n",
      "Epoch 00107: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0941 - acc: 0.9679 - val_loss: 0.2578 - val_acc: 0.9376\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9674\n",
      "Epoch 00108: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0950 - acc: 0.9673 - val_loss: 0.2516 - val_acc: 0.9366\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9685\n",
      "Epoch 00109: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0927 - acc: 0.9684 - val_loss: 0.2443 - val_acc: 0.9408\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9692\n",
      "Epoch 00110: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0908 - acc: 0.9692 - val_loss: 0.2605 - val_acc: 0.9362\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9683\n",
      "Epoch 00111: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0915 - acc: 0.9683 - val_loss: 0.2648 - val_acc: 0.9355\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9702\n",
      "Epoch 00112: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0869 - acc: 0.9702 - val_loss: 0.2678 - val_acc: 0.9369\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9706\n",
      "Epoch 00113: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0879 - acc: 0.9706 - val_loss: 0.2473 - val_acc: 0.9380\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9702\n",
      "Epoch 00114: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0875 - acc: 0.9702 - val_loss: 0.2508 - val_acc: 0.9383\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9697\n",
      "Epoch 00115: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0872 - acc: 0.9697 - val_loss: 0.2564 - val_acc: 0.9357\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9714\n",
      "Epoch 00116: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0844 - acc: 0.9714 - val_loss: 0.2518 - val_acc: 0.9415\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9708\n",
      "Epoch 00117: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0855 - acc: 0.9709 - val_loss: 0.2559 - val_acc: 0.9378\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9724\n",
      "Epoch 00118: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.0828 - acc: 0.9724 - val_loss: 0.2508 - val_acc: 0.9385\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9724\n",
      "Epoch 00119: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0806 - acc: 0.9725 - val_loss: 0.2680 - val_acc: 0.9385\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9719\n",
      "Epoch 00120: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0824 - acc: 0.9719 - val_loss: 0.2669 - val_acc: 0.9343\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9714\n",
      "Epoch 00121: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0816 - acc: 0.9714 - val_loss: 0.2703 - val_acc: 0.9385\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9727\n",
      "Epoch 00122: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0799 - acc: 0.9727 - val_loss: 0.2710 - val_acc: 0.9373\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9719\n",
      "Epoch 00123: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0818 - acc: 0.9719 - val_loss: 0.2608 - val_acc: 0.9390\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9717\n",
      "Epoch 00124: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.0807 - acc: 0.9717 - val_loss: 0.2639 - val_acc: 0.9411\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9736\n",
      "Epoch 00125: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0778 - acc: 0.9736 - val_loss: 0.2520 - val_acc: 0.9376\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9736\n",
      "Epoch 00126: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0778 - acc: 0.9736 - val_loss: 0.2538 - val_acc: 0.9408\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9732\n",
      "Epoch 00127: val_loss did not improve from 0.24208\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.0785 - acc: 0.9731 - val_loss: 0.2587 - val_acc: 0.9394\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9749\n",
      "Epoch 00128: val_loss improved from 0.24208 to 0.23800, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_6_conv_checkpoint/128-0.2380.hdf5\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0742 - acc: 0.9749 - val_loss: 0.2380 - val_acc: 0.9432\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9748\n",
      "Epoch 00129: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0754 - acc: 0.9748 - val_loss: 0.2636 - val_acc: 0.9364\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9751\n",
      "Epoch 00130: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0729 - acc: 0.9751 - val_loss: 0.2549 - val_acc: 0.9390\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9751\n",
      "Epoch 00131: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.0744 - acc: 0.9751 - val_loss: 0.2579 - val_acc: 0.9385\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9748\n",
      "Epoch 00132: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0724 - acc: 0.9748 - val_loss: 0.2673 - val_acc: 0.9387\n",
      "Epoch 133/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9737\n",
      "Epoch 00133: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0760 - acc: 0.9737 - val_loss: 0.2686 - val_acc: 0.9373\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9766\n",
      "Epoch 00134: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0702 - acc: 0.9766 - val_loss: 0.2585 - val_acc: 0.9364\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9749\n",
      "Epoch 00135: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0719 - acc: 0.9749 - val_loss: 0.2464 - val_acc: 0.9415\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9759\n",
      "Epoch 00136: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0704 - acc: 0.9759 - val_loss: 0.2642 - val_acc: 0.9404\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9760\n",
      "Epoch 00137: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0682 - acc: 0.9760 - val_loss: 0.2665 - val_acc: 0.9399\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9759\n",
      "Epoch 00138: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.0704 - acc: 0.9760 - val_loss: 0.2668 - val_acc: 0.9352\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9758\n",
      "Epoch 00139: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.0697 - acc: 0.9758 - val_loss: 0.2528 - val_acc: 0.9418\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9784\n",
      "Epoch 00140: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0663 - acc: 0.9784 - val_loss: 0.2614 - val_acc: 0.9394\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9761\n",
      "Epoch 00141: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0675 - acc: 0.9761 - val_loss: 0.2899 - val_acc: 0.9390\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9769\n",
      "Epoch 00142: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0692 - acc: 0.9769 - val_loss: 0.2562 - val_acc: 0.9390\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9781\n",
      "Epoch 00143: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0674 - acc: 0.9781 - val_loss: 0.2593 - val_acc: 0.9427\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9769\n",
      "Epoch 00144: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0670 - acc: 0.9769 - val_loss: 0.2672 - val_acc: 0.9376\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9762\n",
      "Epoch 00145: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0712 - acc: 0.9762 - val_loss: 0.2640 - val_acc: 0.9453\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9799\n",
      "Epoch 00146: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.0602 - acc: 0.9798 - val_loss: 0.2650 - val_acc: 0.9429\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9782\n",
      "Epoch 00147: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0635 - acc: 0.9781 - val_loss: 0.2733 - val_acc: 0.9364\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9786\n",
      "Epoch 00148: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0634 - acc: 0.9786 - val_loss: 0.2879 - val_acc: 0.9362\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9783\n",
      "Epoch 00149: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0617 - acc: 0.9783 - val_loss: 0.2855 - val_acc: 0.9401\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9788\n",
      "Epoch 00150: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0619 - acc: 0.9789 - val_loss: 0.2774 - val_acc: 0.9385\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9793\n",
      "Epoch 00151: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0623 - acc: 0.9793 - val_loss: 0.2661 - val_acc: 0.9415\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9778\n",
      "Epoch 00152: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0649 - acc: 0.9777 - val_loss: 0.2629 - val_acc: 0.9434\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9797\n",
      "Epoch 00153: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0599 - acc: 0.9797 - val_loss: 0.2741 - val_acc: 0.9434\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9796\n",
      "Epoch 00154: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0581 - acc: 0.9796 - val_loss: 0.2755 - val_acc: 0.9422\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9814\n",
      "Epoch 00155: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0558 - acc: 0.9814 - val_loss: 0.2614 - val_acc: 0.9420\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9808\n",
      "Epoch 00156: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0581 - acc: 0.9808 - val_loss: 0.2730 - val_acc: 0.9385\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9801\n",
      "Epoch 00157: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0595 - acc: 0.9801 - val_loss: 0.2702 - val_acc: 0.9373\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9812\n",
      "Epoch 00158: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0563 - acc: 0.9812 - val_loss: 0.2737 - val_acc: 0.9415\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9798\n",
      "Epoch 00159: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0592 - acc: 0.9798 - val_loss: 0.2658 - val_acc: 0.9448\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9819\n",
      "Epoch 00160: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0537 - acc: 0.9819 - val_loss: 0.2795 - val_acc: 0.9436\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9802\n",
      "Epoch 00161: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0578 - acc: 0.9802 - val_loss: 0.2756 - val_acc: 0.9462\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9813\n",
      "Epoch 00162: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.0551 - acc: 0.9813 - val_loss: 0.2805 - val_acc: 0.9397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9802\n",
      "Epoch 00163: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0575 - acc: 0.9802 - val_loss: 0.2674 - val_acc: 0.9408\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9813\n",
      "Epoch 00164: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.0541 - acc: 0.9813 - val_loss: 0.2645 - val_acc: 0.9415\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9813\n",
      "Epoch 00165: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0561 - acc: 0.9813 - val_loss: 0.2643 - val_acc: 0.9434\n",
      "Epoch 166/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9826\n",
      "Epoch 00166: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0521 - acc: 0.9826 - val_loss: 0.2762 - val_acc: 0.9413\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9812\n",
      "Epoch 00167: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0548 - acc: 0.9813 - val_loss: 0.2649 - val_acc: 0.9420\n",
      "Epoch 168/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9809\n",
      "Epoch 00168: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0563 - acc: 0.9809 - val_loss: 0.2709 - val_acc: 0.9441\n",
      "Epoch 169/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9821\n",
      "Epoch 00169: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0525 - acc: 0.9821 - val_loss: 0.2720 - val_acc: 0.9441\n",
      "Epoch 170/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9824\n",
      "Epoch 00170: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0515 - acc: 0.9824 - val_loss: 0.2672 - val_acc: 0.9415\n",
      "Epoch 171/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9809\n",
      "Epoch 00171: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0562 - acc: 0.9809 - val_loss: 0.2549 - val_acc: 0.9450\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9817\n",
      "Epoch 00172: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0529 - acc: 0.9817 - val_loss: 0.2959 - val_acc: 0.9394\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9830\n",
      "Epoch 00173: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0508 - acc: 0.9830 - val_loss: 0.2648 - val_acc: 0.9439\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9827\n",
      "Epoch 00174: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0498 - acc: 0.9827 - val_loss: 0.2678 - val_acc: 0.9420\n",
      "Epoch 175/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9830\n",
      "Epoch 00175: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0497 - acc: 0.9830 - val_loss: 0.2698 - val_acc: 0.9422\n",
      "Epoch 176/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9845\n",
      "Epoch 00176: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0483 - acc: 0.9845 - val_loss: 0.2684 - val_acc: 0.9439\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9830\n",
      "Epoch 00177: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0520 - acc: 0.9830 - val_loss: 0.2668 - val_acc: 0.9448\n",
      "Epoch 178/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9830\n",
      "Epoch 00178: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0502 - acc: 0.9830 - val_loss: 0.2679 - val_acc: 0.9415\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9837\n",
      "Epoch 00179: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0491 - acc: 0.9837 - val_loss: 0.2645 - val_acc: 0.9425\n",
      "Epoch 180/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9835\n",
      "Epoch 00180: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.0495 - acc: 0.9834 - val_loss: 0.2721 - val_acc: 0.9457\n",
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9827\n",
      "Epoch 00181: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0516 - acc: 0.9826 - val_loss: 0.2678 - val_acc: 0.9413\n",
      "Epoch 182/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9839\n",
      "Epoch 00182: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0472 - acc: 0.9839 - val_loss: 0.2638 - val_acc: 0.9441\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9855\n",
      "Epoch 00183: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0462 - acc: 0.9855 - val_loss: 0.2968 - val_acc: 0.9399\n",
      "Epoch 184/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9854\n",
      "Epoch 00184: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.0446 - acc: 0.9854 - val_loss: 0.2796 - val_acc: 0.9415\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9839\n",
      "Epoch 00185: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0490 - acc: 0.9839 - val_loss: 0.2735 - val_acc: 0.9429\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9822\n",
      "Epoch 00186: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0533 - acc: 0.9822 - val_loss: 0.2685 - val_acc: 0.9439\n",
      "Epoch 187/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9856\n",
      "Epoch 00187: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0432 - acc: 0.9855 - val_loss: 0.2786 - val_acc: 0.9450\n",
      "Epoch 188/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9833\n",
      "Epoch 00188: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0512 - acc: 0.9833 - val_loss: 0.2676 - val_acc: 0.9425\n",
      "Epoch 189/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9853\n",
      "Epoch 00189: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0442 - acc: 0.9853 - val_loss: 0.2705 - val_acc: 0.9467\n",
      "Epoch 190/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9840\n",
      "Epoch 00190: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.0471 - acc: 0.9840 - val_loss: 0.2690 - val_acc: 0.9439\n",
      "Epoch 191/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9857\n",
      "Epoch 00191: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0435 - acc: 0.9857 - val_loss: 0.2756 - val_acc: 0.9441\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9846\n",
      "Epoch 00192: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0453 - acc: 0.9846 - val_loss: 0.2716 - val_acc: 0.9448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9853\n",
      "Epoch 00193: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0449 - acc: 0.9853 - val_loss: 0.2861 - val_acc: 0.9427\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9863\n",
      "Epoch 00194: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0419 - acc: 0.9863 - val_loss: 0.2770 - val_acc: 0.9425\n",
      "Epoch 195/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9849\n",
      "Epoch 00195: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0433 - acc: 0.9849 - val_loss: 0.2802 - val_acc: 0.9420\n",
      "Epoch 196/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9859\n",
      "Epoch 00196: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0416 - acc: 0.9859 - val_loss: 0.2640 - val_acc: 0.9439\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9864\n",
      "Epoch 00197: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0405 - acc: 0.9864 - val_loss: 0.2646 - val_acc: 0.9455\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9860\n",
      "Epoch 00198: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0409 - acc: 0.9860 - val_loss: 0.2736 - val_acc: 0.9443\n",
      "Epoch 199/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9850\n",
      "Epoch 00199: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0448 - acc: 0.9850 - val_loss: 0.2619 - val_acc: 0.9455\n",
      "Epoch 200/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9870\n",
      "Epoch 00200: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0392 - acc: 0.9870 - val_loss: 0.2660 - val_acc: 0.9420\n",
      "Epoch 201/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9849\n",
      "Epoch 00201: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0448 - acc: 0.9849 - val_loss: 0.2601 - val_acc: 0.9446\n",
      "Epoch 202/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9851\n",
      "Epoch 00202: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0439 - acc: 0.9851 - val_loss: 0.2601 - val_acc: 0.9450\n",
      "Epoch 203/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9866\n",
      "Epoch 00203: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0401 - acc: 0.9866 - val_loss: 0.2758 - val_acc: 0.9450\n",
      "Epoch 204/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9862\n",
      "Epoch 00204: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.0424 - acc: 0.9862 - val_loss: 0.2634 - val_acc: 0.9455\n",
      "Epoch 205/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9858\n",
      "Epoch 00205: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0416 - acc: 0.9858 - val_loss: 0.2741 - val_acc: 0.9422\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9854\n",
      "Epoch 00206: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0432 - acc: 0.9854 - val_loss: 0.2656 - val_acc: 0.9450\n",
      "Epoch 207/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9864\n",
      "Epoch 00207: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0408 - acc: 0.9864 - val_loss: 0.2609 - val_acc: 0.9446\n",
      "Epoch 208/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9867\n",
      "Epoch 00208: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0390 - acc: 0.9867 - val_loss: 0.2756 - val_acc: 0.9450\n",
      "Epoch 209/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9866\n",
      "Epoch 00209: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0385 - acc: 0.9866 - val_loss: 0.2821 - val_acc: 0.9427\n",
      "Epoch 210/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9856\n",
      "Epoch 00210: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0416 - acc: 0.9856 - val_loss: 0.2925 - val_acc: 0.9411\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9869\n",
      "Epoch 00211: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0407 - acc: 0.9869 - val_loss: 0.2970 - val_acc: 0.9406\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9862\n",
      "Epoch 00212: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0404 - acc: 0.9862 - val_loss: 0.2550 - val_acc: 0.9474\n",
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9880\n",
      "Epoch 00213: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0370 - acc: 0.9880 - val_loss: 0.2713 - val_acc: 0.9453\n",
      "Epoch 214/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9876\n",
      "Epoch 00214: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0376 - acc: 0.9877 - val_loss: 0.2797 - val_acc: 0.9453\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9868\n",
      "Epoch 00215: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0404 - acc: 0.9868 - val_loss: 0.2681 - val_acc: 0.9495\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 00216: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0387 - acc: 0.9870 - val_loss: 0.2784 - val_acc: 0.9441\n",
      "Epoch 217/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9873\n",
      "Epoch 00217: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0384 - acc: 0.9873 - val_loss: 0.2717 - val_acc: 0.9453\n",
      "Epoch 218/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9887\n",
      "Epoch 00218: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0356 - acc: 0.9887 - val_loss: 0.2642 - val_acc: 0.9462\n",
      "Epoch 219/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9868\n",
      "Epoch 00219: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0378 - acc: 0.9868 - val_loss: 0.2677 - val_acc: 0.9476\n",
      "Epoch 220/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9880\n",
      "Epoch 00220: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0354 - acc: 0.9880 - val_loss: 0.2877 - val_acc: 0.9432\n",
      "Epoch 221/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9871\n",
      "Epoch 00221: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0400 - acc: 0.9871 - val_loss: 0.2808 - val_acc: 0.9460\n",
      "Epoch 222/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9875\n",
      "Epoch 00222: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0367 - acc: 0.9874 - val_loss: 0.2764 - val_acc: 0.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9873\n",
      "Epoch 00223: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0381 - acc: 0.9874 - val_loss: 0.3121 - val_acc: 0.9441\n",
      "Epoch 224/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9890\n",
      "Epoch 00224: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0340 - acc: 0.9890 - val_loss: 0.2766 - val_acc: 0.9469\n",
      "Epoch 225/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9880\n",
      "Epoch 00225: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0361 - acc: 0.9881 - val_loss: 0.2827 - val_acc: 0.9474\n",
      "Epoch 226/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9876\n",
      "Epoch 00226: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0355 - acc: 0.9876 - val_loss: 0.2860 - val_acc: 0.9483\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9881\n",
      "Epoch 00227: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0360 - acc: 0.9881 - val_loss: 0.2735 - val_acc: 0.9453\n",
      "Epoch 228/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9881\n",
      "Epoch 00228: val_loss did not improve from 0.23800\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0358 - acc: 0.9882 - val_loss: 0.2917 - val_acc: 0.9446\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 250us/sample - loss: 0.3043 - acc: 0.9180\n",
      "Loss: 0.3043327937430682 Accuracy: 0.9179647\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0759 - acc: 0.3265\n",
      "Epoch 00001: val_loss improved from inf to 1.38257, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/001-1.3826.hdf5\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 2.0746 - acc: 0.3269 - val_loss: 1.3826 - val_acc: 0.5656\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2870 - acc: 0.5867\n",
      "Epoch 00002: val_loss improved from 1.38257 to 0.97218, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/002-0.9722.hdf5\n",
      "36805/36805 [==============================] - 12s 329us/sample - loss: 1.2872 - acc: 0.5866 - val_loss: 0.9722 - val_acc: 0.7004\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0149 - acc: 0.6755\n",
      "Epoch 00003: val_loss improved from 0.97218 to 0.79614, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/003-0.7961.hdf5\n",
      "36805/36805 [==============================] - 12s 334us/sample - loss: 1.0151 - acc: 0.6755 - val_loss: 0.7961 - val_acc: 0.7452\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8686 - acc: 0.7180\n",
      "Epoch 00004: val_loss improved from 0.79614 to 0.67590, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/004-0.6759.hdf5\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.8680 - acc: 0.7183 - val_loss: 0.6759 - val_acc: 0.7929\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7596 - acc: 0.7550\n",
      "Epoch 00005: val_loss improved from 0.67590 to 0.60806, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/005-0.6081.hdf5\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.7594 - acc: 0.7551 - val_loss: 0.6081 - val_acc: 0.8148\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6830 - acc: 0.7791\n",
      "Epoch 00006: val_loss improved from 0.60806 to 0.54942, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/006-0.5494.hdf5\n",
      "36805/36805 [==============================] - 12s 332us/sample - loss: 0.6832 - acc: 0.7789 - val_loss: 0.5494 - val_acc: 0.8206\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6097 - acc: 0.8059\n",
      "Epoch 00007: val_loss improved from 0.54942 to 0.47324, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/007-0.4732.hdf5\n",
      "36805/36805 [==============================] - 12s 330us/sample - loss: 0.6097 - acc: 0.8059 - val_loss: 0.4732 - val_acc: 0.8574\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.8192\n",
      "Epoch 00008: val_loss improved from 0.47324 to 0.44574, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/008-0.4457.hdf5\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.5609 - acc: 0.8192 - val_loss: 0.4457 - val_acc: 0.8621\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8349\n",
      "Epoch 00009: val_loss improved from 0.44574 to 0.41894, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/009-0.4189.hdf5\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.5117 - acc: 0.8349 - val_loss: 0.4189 - val_acc: 0.8728\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4737 - acc: 0.8475\n",
      "Epoch 00010: val_loss improved from 0.41894 to 0.36750, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/010-0.3675.hdf5\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.4738 - acc: 0.8474 - val_loss: 0.3675 - val_acc: 0.8866\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8568\n",
      "Epoch 00011: val_loss did not improve from 0.36750\n",
      "36805/36805 [==============================] - 12s 332us/sample - loss: 0.4425 - acc: 0.8569 - val_loss: 0.4100 - val_acc: 0.8677\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8668\n",
      "Epoch 00012: val_loss improved from 0.36750 to 0.32866, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/012-0.3287.hdf5\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.4140 - acc: 0.8668 - val_loss: 0.3287 - val_acc: 0.9015\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8727\n",
      "Epoch 00013: val_loss improved from 0.32866 to 0.30714, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/013-0.3071.hdf5\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.3870 - acc: 0.8728 - val_loss: 0.3071 - val_acc: 0.9113\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8803\n",
      "Epoch 00014: val_loss did not improve from 0.30714\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.3683 - acc: 0.8803 - val_loss: 0.3127 - val_acc: 0.9026\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8877\n",
      "Epoch 00015: val_loss improved from 0.30714 to 0.28397, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/015-0.2840.hdf5\n",
      "36805/36805 [==============================] - 12s 329us/sample - loss: 0.3436 - acc: 0.8877 - val_loss: 0.2840 - val_acc: 0.9108\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8927\n",
      "Epoch 00016: val_loss improved from 0.28397 to 0.26828, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/016-0.2683.hdf5\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.3284 - acc: 0.8927 - val_loss: 0.2683 - val_acc: 0.9182\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.8978\n",
      "Epoch 00017: val_loss did not improve from 0.26828\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.3129 - acc: 0.8978 - val_loss: 0.2749 - val_acc: 0.9157\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9019\n",
      "Epoch 00018: val_loss improved from 0.26828 to 0.25217, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/018-0.2522.hdf5\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.3012 - acc: 0.9019 - val_loss: 0.2522 - val_acc: 0.9217\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9074\n",
      "Epoch 00019: val_loss improved from 0.25217 to 0.25064, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/019-0.2506.hdf5\n",
      "36805/36805 [==============================] - 12s 329us/sample - loss: 0.2838 - acc: 0.9075 - val_loss: 0.2506 - val_acc: 0.9227\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9113\n",
      "Epoch 00020: val_loss did not improve from 0.25064\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.2729 - acc: 0.9113 - val_loss: 0.2540 - val_acc: 0.9187\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9121\n",
      "Epoch 00021: val_loss improved from 0.25064 to 0.24479, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/021-0.2448.hdf5\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.2652 - acc: 0.9122 - val_loss: 0.2448 - val_acc: 0.9241\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9135\n",
      "Epoch 00022: val_loss improved from 0.24479 to 0.23438, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/022-0.2344.hdf5\n",
      "36805/36805 [==============================] - 12s 333us/sample - loss: 0.2561 - acc: 0.9135 - val_loss: 0.2344 - val_acc: 0.9290\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9200\n",
      "Epoch 00023: val_loss did not improve from 0.23438\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.2395 - acc: 0.9200 - val_loss: 0.2509 - val_acc: 0.9220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9217\n",
      "Epoch 00024: val_loss improved from 0.23438 to 0.23147, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/024-0.2315.hdf5\n",
      "36805/36805 [==============================] - 12s 331us/sample - loss: 0.2335 - acc: 0.9217 - val_loss: 0.2315 - val_acc: 0.9306\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9248\n",
      "Epoch 00025: val_loss did not improve from 0.23147\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.2264 - acc: 0.9248 - val_loss: 0.2354 - val_acc: 0.9304\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9264\n",
      "Epoch 00026: val_loss did not improve from 0.23147\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.2188 - acc: 0.9264 - val_loss: 0.2368 - val_acc: 0.9266\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9315\n",
      "Epoch 00027: val_loss improved from 0.23147 to 0.22796, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/027-0.2280.hdf5\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.2089 - acc: 0.9314 - val_loss: 0.2280 - val_acc: 0.9304\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9320\n",
      "Epoch 00028: val_loss improved from 0.22796 to 0.21061, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/028-0.2106.hdf5\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.2014 - acc: 0.9320 - val_loss: 0.2106 - val_acc: 0.9383\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9349\n",
      "Epoch 00029: val_loss did not improve from 0.21061\n",
      "36805/36805 [==============================] - 12s 330us/sample - loss: 0.1931 - acc: 0.9350 - val_loss: 0.2164 - val_acc: 0.9327\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9371\n",
      "Epoch 00030: val_loss did not improve from 0.21061\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.1876 - acc: 0.9371 - val_loss: 0.2150 - val_acc: 0.9366\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9389\n",
      "Epoch 00031: val_loss did not improve from 0.21061\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.1823 - acc: 0.9389 - val_loss: 0.2206 - val_acc: 0.9329\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9409\n",
      "Epoch 00032: val_loss did not improve from 0.21061\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.1772 - acc: 0.9409 - val_loss: 0.2133 - val_acc: 0.9336\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9447\n",
      "Epoch 00033: val_loss improved from 0.21061 to 0.19881, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/033-0.1988.hdf5\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.1663 - acc: 0.9447 - val_loss: 0.1988 - val_acc: 0.9397\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9428\n",
      "Epoch 00034: val_loss did not improve from 0.19881\n",
      "36805/36805 [==============================] - 12s 330us/sample - loss: 0.1679 - acc: 0.9428 - val_loss: 0.2402 - val_acc: 0.9262\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9448\n",
      "Epoch 00035: val_loss did not improve from 0.19881\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.1625 - acc: 0.9449 - val_loss: 0.2075 - val_acc: 0.9376\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9475\n",
      "Epoch 00036: val_loss improved from 0.19881 to 0.19858, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/036-0.1986.hdf5\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.1546 - acc: 0.9475 - val_loss: 0.1986 - val_acc: 0.9411\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9500\n",
      "Epoch 00037: val_loss did not improve from 0.19858\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.1475 - acc: 0.9501 - val_loss: 0.2320 - val_acc: 0.9324\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9502\n",
      "Epoch 00038: val_loss did not improve from 0.19858\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.1489 - acc: 0.9502 - val_loss: 0.2052 - val_acc: 0.9413\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9521\n",
      "Epoch 00039: val_loss improved from 0.19858 to 0.19487, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_7_conv_checkpoint/039-0.1949.hdf5\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.1389 - acc: 0.9521 - val_loss: 0.1949 - val_acc: 0.9455\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9545\n",
      "Epoch 00040: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 330us/sample - loss: 0.1336 - acc: 0.9545 - val_loss: 0.2115 - val_acc: 0.9387\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9548\n",
      "Epoch 00041: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.1309 - acc: 0.9548 - val_loss: 0.2025 - val_acc: 0.9411\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9548\n",
      "Epoch 00042: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.1301 - acc: 0.9548 - val_loss: 0.2036 - val_acc: 0.9436\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9572\n",
      "Epoch 00043: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.1259 - acc: 0.9572 - val_loss: 0.2079 - val_acc: 0.9387\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9564\n",
      "Epoch 00044: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.1254 - acc: 0.9564 - val_loss: 0.1996 - val_acc: 0.9446\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9594\n",
      "Epoch 00045: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.1179 - acc: 0.9594 - val_loss: 0.2082 - val_acc: 0.9420\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9607\n",
      "Epoch 00046: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.1151 - acc: 0.9607 - val_loss: 0.1990 - val_acc: 0.9478\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9614\n",
      "Epoch 00047: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.1130 - acc: 0.9615 - val_loss: 0.2006 - val_acc: 0.9420\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9610\n",
      "Epoch 00048: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.1083 - acc: 0.9610 - val_loss: 0.1988 - val_acc: 0.9441\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9636\n",
      "Epoch 00049: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.1052 - acc: 0.9637 - val_loss: 0.2108 - val_acc: 0.9404\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9646\n",
      "Epoch 00050: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.1013 - acc: 0.9646 - val_loss: 0.2168 - val_acc: 0.9413\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9643\n",
      "Epoch 00051: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0993 - acc: 0.9644 - val_loss: 0.2041 - val_acc: 0.9406\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9675\n",
      "Epoch 00052: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0932 - acc: 0.9675 - val_loss: 0.2067 - val_acc: 0.9406\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9683\n",
      "Epoch 00053: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 329us/sample - loss: 0.0905 - acc: 0.9683 - val_loss: 0.2203 - val_acc: 0.9422\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9677\n",
      "Epoch 00054: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.0930 - acc: 0.9677 - val_loss: 0.2178 - val_acc: 0.9439\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9686\n",
      "Epoch 00055: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0904 - acc: 0.9686 - val_loss: 0.2095 - val_acc: 0.9392\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9701\n",
      "Epoch 00056: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.0870 - acc: 0.9701 - val_loss: 0.2175 - val_acc: 0.9453\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9704\n",
      "Epoch 00057: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0884 - acc: 0.9705 - val_loss: 0.2165 - val_acc: 0.9443\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9704\n",
      "Epoch 00058: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0843 - acc: 0.9704 - val_loss: 0.2203 - val_acc: 0.9422\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9721\n",
      "Epoch 00059: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0807 - acc: 0.9721 - val_loss: 0.2145 - val_acc: 0.9397\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9725\n",
      "Epoch 00060: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.0811 - acc: 0.9725 - val_loss: 0.2033 - val_acc: 0.9446\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9734\n",
      "Epoch 00061: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0770 - acc: 0.9735 - val_loss: 0.2112 - val_acc: 0.9448\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9740\n",
      "Epoch 00062: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0749 - acc: 0.9740 - val_loss: 0.2202 - val_acc: 0.9408\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9749\n",
      "Epoch 00063: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0721 - acc: 0.9749 - val_loss: 0.2530 - val_acc: 0.9383\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9726\n",
      "Epoch 00064: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0782 - acc: 0.9726 - val_loss: 0.2292 - val_acc: 0.9418\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9736\n",
      "Epoch 00065: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0757 - acc: 0.9736 - val_loss: 0.2033 - val_acc: 0.9453\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9778\n",
      "Epoch 00066: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0662 - acc: 0.9777 - val_loss: 0.2159 - val_acc: 0.9462\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9760\n",
      "Epoch 00067: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0680 - acc: 0.9761 - val_loss: 0.2121 - val_acc: 0.9462\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9782\n",
      "Epoch 00068: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0643 - acc: 0.9782 - val_loss: 0.2261 - val_acc: 0.9436\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9782\n",
      "Epoch 00069: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0637 - acc: 0.9782 - val_loss: 0.2088 - val_acc: 0.9483\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9790\n",
      "Epoch 00070: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0622 - acc: 0.9789 - val_loss: 0.2253 - val_acc: 0.9450\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9796\n",
      "Epoch 00071: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.0603 - acc: 0.9796 - val_loss: 0.2262 - val_acc: 0.9429\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9787\n",
      "Epoch 00072: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0616 - acc: 0.9787 - val_loss: 0.2148 - val_acc: 0.9492\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9799\n",
      "Epoch 00073: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.0569 - acc: 0.9799 - val_loss: 0.2173 - val_acc: 0.9448\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9790\n",
      "Epoch 00074: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 329us/sample - loss: 0.0605 - acc: 0.9790 - val_loss: 0.2272 - val_acc: 0.9460\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9805\n",
      "Epoch 00075: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0574 - acc: 0.9804 - val_loss: 0.2203 - val_acc: 0.9478\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9804\n",
      "Epoch 00076: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0569 - acc: 0.9804 - val_loss: 0.2351 - val_acc: 0.9425\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9823\n",
      "Epoch 00077: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.0499 - acc: 0.9823 - val_loss: 0.2454 - val_acc: 0.9464\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9822\n",
      "Epoch 00078: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0524 - acc: 0.9822 - val_loss: 0.2292 - val_acc: 0.9462\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9818\n",
      "Epoch 00079: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0519 - acc: 0.9817 - val_loss: 0.2298 - val_acc: 0.9471\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9817\n",
      "Epoch 00080: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0528 - acc: 0.9816 - val_loss: 0.2514 - val_acc: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9811\n",
      "Epoch 00081: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0545 - acc: 0.9811 - val_loss: 0.2135 - val_acc: 0.9504\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9827\n",
      "Epoch 00082: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0491 - acc: 0.9827 - val_loss: 0.2235 - val_acc: 0.9483\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9834\n",
      "Epoch 00083: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0492 - acc: 0.9834 - val_loss: 0.2414 - val_acc: 0.9478\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9838\n",
      "Epoch 00084: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0509 - acc: 0.9838 - val_loss: 0.2195 - val_acc: 0.9462\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9833\n",
      "Epoch 00085: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0477 - acc: 0.9833 - val_loss: 0.2314 - val_acc: 0.9488\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9838\n",
      "Epoch 00086: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0472 - acc: 0.9838 - val_loss: 0.2251 - val_acc: 0.9478\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9853\n",
      "Epoch 00087: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0432 - acc: 0.9853 - val_loss: 0.2174 - val_acc: 0.9513\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9861\n",
      "Epoch 00088: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0403 - acc: 0.9861 - val_loss: 0.2287 - val_acc: 0.9518\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9846\n",
      "Epoch 00089: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0449 - acc: 0.9846 - val_loss: 0.2498 - val_acc: 0.9469\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9845\n",
      "Epoch 00090: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0446 - acc: 0.9845 - val_loss: 0.2128 - val_acc: 0.9504\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9855\n",
      "Epoch 00091: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.0422 - acc: 0.9855 - val_loss: 0.2417 - val_acc: 0.9502\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9853\n",
      "Epoch 00092: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 330us/sample - loss: 0.0447 - acc: 0.9853 - val_loss: 0.2298 - val_acc: 0.9490\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9863\n",
      "Epoch 00093: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0406 - acc: 0.9862 - val_loss: 0.2331 - val_acc: 0.9483\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9864\n",
      "Epoch 00094: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0409 - acc: 0.9864 - val_loss: 0.2180 - val_acc: 0.9502\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9860\n",
      "Epoch 00095: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0433 - acc: 0.9860 - val_loss: 0.2284 - val_acc: 0.9474\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9858\n",
      "Epoch 00096: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0416 - acc: 0.9858 - val_loss: 0.2366 - val_acc: 0.9481\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9870\n",
      "Epoch 00097: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.0388 - acc: 0.9871 - val_loss: 0.2495 - val_acc: 0.9462\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9846\n",
      "Epoch 00098: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0439 - acc: 0.9846 - val_loss: 0.2165 - val_acc: 0.9504\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9877\n",
      "Epoch 00099: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 330us/sample - loss: 0.0384 - acc: 0.9877 - val_loss: 0.2464 - val_acc: 0.9495\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9873\n",
      "Epoch 00100: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 332us/sample - loss: 0.0363 - acc: 0.9873 - val_loss: 0.2416 - val_acc: 0.9490\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9867\n",
      "Epoch 00101: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0390 - acc: 0.9867 - val_loss: 0.2318 - val_acc: 0.9513\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9881\n",
      "Epoch 00102: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0357 - acc: 0.9881 - val_loss: 0.2252 - val_acc: 0.9525\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9892\n",
      "Epoch 00103: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0330 - acc: 0.9892 - val_loss: 0.2323 - val_acc: 0.9511\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9873\n",
      "Epoch 00104: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0384 - acc: 0.9873 - val_loss: 0.2782 - val_acc: 0.9453\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9875\n",
      "Epoch 00105: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0384 - acc: 0.9876 - val_loss: 0.2346 - val_acc: 0.9520\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9869\n",
      "Epoch 00106: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0368 - acc: 0.9868 - val_loss: 0.2438 - val_acc: 0.9488\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9884\n",
      "Epoch 00107: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.0355 - acc: 0.9884 - val_loss: 0.2122 - val_acc: 0.9529\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9883\n",
      "Epoch 00108: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.0340 - acc: 0.9883 - val_loss: 0.2376 - val_acc: 0.9522\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9902\n",
      "Epoch 00109: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0300 - acc: 0.9902 - val_loss: 0.2473 - val_acc: 0.9502\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9884\n",
      "Epoch 00110: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0350 - acc: 0.9884 - val_loss: 0.2300 - val_acc: 0.9499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9892\n",
      "Epoch 00111: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0313 - acc: 0.9892 - val_loss: 0.2515 - val_acc: 0.9469\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9894\n",
      "Epoch 00112: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.0316 - acc: 0.9894 - val_loss: 0.2656 - val_acc: 0.9448\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9882\n",
      "Epoch 00113: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0356 - acc: 0.9882 - val_loss: 0.2209 - val_acc: 0.9509\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9900\n",
      "Epoch 00114: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0297 - acc: 0.9899 - val_loss: 0.2215 - val_acc: 0.9515\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9898\n",
      "Epoch 00115: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.0318 - acc: 0.9898 - val_loss: 0.2765 - val_acc: 0.9471\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9884\n",
      "Epoch 00116: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 329us/sample - loss: 0.0357 - acc: 0.9884 - val_loss: 0.2380 - val_acc: 0.9522\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9906\n",
      "Epoch 00117: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0280 - acc: 0.9906 - val_loss: 0.2251 - val_acc: 0.9532\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9896\n",
      "Epoch 00118: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0319 - acc: 0.9896 - val_loss: 0.2545 - val_acc: 0.9471\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9904\n",
      "Epoch 00119: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0276 - acc: 0.9904 - val_loss: 0.2515 - val_acc: 0.9504\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9910\n",
      "Epoch 00120: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.0281 - acc: 0.9910 - val_loss: 0.2416 - val_acc: 0.9520\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9889\n",
      "Epoch 00121: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0330 - acc: 0.9889 - val_loss: 0.2327 - val_acc: 0.9525\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9902\n",
      "Epoch 00122: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 329us/sample - loss: 0.0277 - acc: 0.9902 - val_loss: 0.2455 - val_acc: 0.9499\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9911\n",
      "Epoch 00123: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0281 - acc: 0.9910 - val_loss: 0.2386 - val_acc: 0.9527\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9909\n",
      "Epoch 00124: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0266 - acc: 0.9910 - val_loss: 0.2629 - val_acc: 0.9492\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9909\n",
      "Epoch 00125: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.0270 - acc: 0.9909 - val_loss: 0.2263 - val_acc: 0.9518\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9919\n",
      "Epoch 00126: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0262 - acc: 0.9918 - val_loss: 0.2394 - val_acc: 0.9532\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9903\n",
      "Epoch 00127: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 329us/sample - loss: 0.0312 - acc: 0.9904 - val_loss: 0.2670 - val_acc: 0.9443\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9907\n",
      "Epoch 00128: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0279 - acc: 0.9907 - val_loss: 0.2412 - val_acc: 0.9543\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9921\n",
      "Epoch 00129: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0248 - acc: 0.9921 - val_loss: 0.2367 - val_acc: 0.9529\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9904\n",
      "Epoch 00130: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.0282 - acc: 0.9904 - val_loss: 0.2583 - val_acc: 0.9495\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9900\n",
      "Epoch 00131: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0308 - acc: 0.9900 - val_loss: 0.2316 - val_acc: 0.9534\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9916\n",
      "Epoch 00132: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.0259 - acc: 0.9916 - val_loss: 0.2408 - val_acc: 0.9522\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9917\n",
      "Epoch 00133: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0241 - acc: 0.9917 - val_loss: 0.2379 - val_acc: 0.9506\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9917\n",
      "Epoch 00134: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0232 - acc: 0.9917 - val_loss: 0.2554 - val_acc: 0.9504\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9919\n",
      "Epoch 00135: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.0245 - acc: 0.9919 - val_loss: 0.2564 - val_acc: 0.9522\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9920\n",
      "Epoch 00136: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.0245 - acc: 0.9920 - val_loss: 0.2513 - val_acc: 0.9511\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9925\n",
      "Epoch 00137: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0235 - acc: 0.9924 - val_loss: 0.2598 - val_acc: 0.9525\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9915\n",
      "Epoch 00138: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.0251 - acc: 0.9915 - val_loss: 0.2642 - val_acc: 0.9483\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9910\n",
      "Epoch 00139: val_loss did not improve from 0.19487\n",
      "36805/36805 [==============================] - 12s 325us/sample - loss: 0.0265 - acc: 0.9910 - val_loss: 0.2399 - val_acc: 0.9534\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 264us/sample - loss: 0.2636 - acc: 0.9196\n",
      "Loss: 0.26358092670500094 Accuracy: 0.9196262\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.8025 - acc: 0.4125\n",
      "Epoch 00001: val_loss improved from inf to 1.00706, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/001-1.0071.hdf5\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 1.7998 - acc: 0.4134 - val_loss: 1.0071 - val_acc: 0.6765\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9711 - acc: 0.6847\n",
      "Epoch 00002: val_loss improved from 1.00706 to 0.70624, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/002-0.7062.hdf5\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.9715 - acc: 0.6847 - val_loss: 0.7062 - val_acc: 0.7673\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7395 - acc: 0.7622\n",
      "Epoch 00003: val_loss improved from 0.70624 to 0.65518, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/003-0.6552.hdf5\n",
      "36805/36805 [==============================] - 14s 390us/sample - loss: 0.7396 - acc: 0.7621 - val_loss: 0.6552 - val_acc: 0.7852\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5979 - acc: 0.8060\n",
      "Epoch 00004: val_loss improved from 0.65518 to 0.48433, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/004-0.4843.hdf5\n",
      "36805/36805 [==============================] - 14s 389us/sample - loss: 0.5977 - acc: 0.8061 - val_loss: 0.4843 - val_acc: 0.8425\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5056 - acc: 0.8358\n",
      "Epoch 00005: val_loss improved from 0.48433 to 0.40792, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/005-0.4079.hdf5\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.5056 - acc: 0.8358 - val_loss: 0.4079 - val_acc: 0.8689\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8556\n",
      "Epoch 00006: val_loss improved from 0.40792 to 0.34058, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/006-0.3406.hdf5\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.4421 - acc: 0.8555 - val_loss: 0.3406 - val_acc: 0.8928\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8734\n",
      "Epoch 00007: val_loss improved from 0.34058 to 0.33101, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/007-0.3310.hdf5\n",
      "36805/36805 [==============================] - 14s 388us/sample - loss: 0.3885 - acc: 0.8734 - val_loss: 0.3310 - val_acc: 0.8896\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8851\n",
      "Epoch 00008: val_loss improved from 0.33101 to 0.31666, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/008-0.3167.hdf5\n",
      "36805/36805 [==============================] - 14s 390us/sample - loss: 0.3517 - acc: 0.8852 - val_loss: 0.3167 - val_acc: 0.8991\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.8955\n",
      "Epoch 00009: val_loss improved from 0.31666 to 0.27375, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/009-0.2738.hdf5\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.3196 - acc: 0.8954 - val_loss: 0.2738 - val_acc: 0.9108\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9027\n",
      "Epoch 00010: val_loss did not improve from 0.27375\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.2937 - acc: 0.9028 - val_loss: 0.2749 - val_acc: 0.9133\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9116\n",
      "Epoch 00011: val_loss improved from 0.27375 to 0.26052, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/011-0.2605.hdf5\n",
      "36805/36805 [==============================] - 14s 388us/sample - loss: 0.2691 - acc: 0.9115 - val_loss: 0.2605 - val_acc: 0.9208\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9185\n",
      "Epoch 00012: val_loss improved from 0.26052 to 0.24357, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/012-0.2436.hdf5\n",
      "36805/36805 [==============================] - 14s 388us/sample - loss: 0.2450 - acc: 0.9186 - val_loss: 0.2436 - val_acc: 0.9206\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9242\n",
      "Epoch 00013: val_loss improved from 0.24357 to 0.23053, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/013-0.2305.hdf5\n",
      "36805/36805 [==============================] - 14s 388us/sample - loss: 0.2304 - acc: 0.9243 - val_loss: 0.2305 - val_acc: 0.9250\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9295\n",
      "Epoch 00014: val_loss improved from 0.23053 to 0.22152, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/014-0.2215.hdf5\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.2104 - acc: 0.9294 - val_loss: 0.2215 - val_acc: 0.9283\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9347\n",
      "Epoch 00015: val_loss did not improve from 0.22152\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.1965 - acc: 0.9346 - val_loss: 0.2236 - val_acc: 0.9292\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9382\n",
      "Epoch 00016: val_loss improved from 0.22152 to 0.21268, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/016-0.2127.hdf5\n",
      "36805/36805 [==============================] - 14s 388us/sample - loss: 0.1864 - acc: 0.9382 - val_loss: 0.2127 - val_acc: 0.9348\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9442\n",
      "Epoch 00017: val_loss improved from 0.21268 to 0.21053, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/017-0.2105.hdf5\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.1646 - acc: 0.9442 - val_loss: 0.2105 - val_acc: 0.9320\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9465\n",
      "Epoch 00018: val_loss did not improve from 0.21053\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.1588 - acc: 0.9465 - val_loss: 0.2174 - val_acc: 0.9327\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9489\n",
      "Epoch 00019: val_loss improved from 0.21053 to 0.21012, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/019-0.2101.hdf5\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.1497 - acc: 0.9489 - val_loss: 0.2101 - val_acc: 0.9369\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9535\n",
      "Epoch 00020: val_loss did not improve from 0.21012\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.1390 - acc: 0.9535 - val_loss: 0.2211 - val_acc: 0.9331\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9570\n",
      "Epoch 00021: val_loss did not improve from 0.21012\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.1259 - acc: 0.9570 - val_loss: 0.2159 - val_acc: 0.9329\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9594\n",
      "Epoch 00022: val_loss improved from 0.21012 to 0.19908, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/022-0.1991.hdf5\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.1182 - acc: 0.9595 - val_loss: 0.1991 - val_acc: 0.9385\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9615\n",
      "Epoch 00023: val_loss improved from 0.19908 to 0.19702, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_DO_8_conv_checkpoint/023-0.1970.hdf5\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.1129 - acc: 0.9615 - val_loss: 0.1970 - val_acc: 0.9415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9649\n",
      "Epoch 00024: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.1023 - acc: 0.9650 - val_loss: 0.2064 - val_acc: 0.9371\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9663\n",
      "Epoch 00025: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0999 - acc: 0.9662 - val_loss: 0.2224 - val_acc: 0.9348\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9699\n",
      "Epoch 00026: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0879 - acc: 0.9699 - val_loss: 0.2210 - val_acc: 0.9369\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9701\n",
      "Epoch 00027: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0863 - acc: 0.9701 - val_loss: 0.2153 - val_acc: 0.9404\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9737\n",
      "Epoch 00028: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0762 - acc: 0.9737 - val_loss: 0.2657 - val_acc: 0.9313\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9734\n",
      "Epoch 00029: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0792 - acc: 0.9734 - val_loss: 0.2381 - val_acc: 0.9408\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9757\n",
      "Epoch 00030: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0719 - acc: 0.9757 - val_loss: 0.2328 - val_acc: 0.9364\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9788\n",
      "Epoch 00031: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0634 - acc: 0.9787 - val_loss: 0.2204 - val_acc: 0.9420\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9790\n",
      "Epoch 00032: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0621 - acc: 0.9790 - val_loss: 0.2524 - val_acc: 0.9383\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9798\n",
      "Epoch 00033: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0607 - acc: 0.9797 - val_loss: 0.2623 - val_acc: 0.9357\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9801\n",
      "Epoch 00034: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0585 - acc: 0.9801 - val_loss: 0.2639 - val_acc: 0.9366\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9816\n",
      "Epoch 00035: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0558 - acc: 0.9816 - val_loss: 0.2294 - val_acc: 0.9413\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9845\n",
      "Epoch 00036: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0486 - acc: 0.9844 - val_loss: 0.2570 - val_acc: 0.9380\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9836\n",
      "Epoch 00037: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0507 - acc: 0.9836 - val_loss: 0.2196 - val_acc: 0.9446\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9838\n",
      "Epoch 00038: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0481 - acc: 0.9838 - val_loss: 0.2480 - val_acc: 0.9413\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9851\n",
      "Epoch 00039: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0441 - acc: 0.9851 - val_loss: 0.2610 - val_acc: 0.9420\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9849\n",
      "Epoch 00040: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0467 - acc: 0.9849 - val_loss: 0.2811 - val_acc: 0.9376\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9861\n",
      "Epoch 00041: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0434 - acc: 0.9861 - val_loss: 0.2533 - val_acc: 0.9432\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9873\n",
      "Epoch 00042: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0398 - acc: 0.9873 - val_loss: 0.2704 - val_acc: 0.9399\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9873\n",
      "Epoch 00043: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0387 - acc: 0.9872 - val_loss: 0.2498 - val_acc: 0.9408\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9885\n",
      "Epoch 00044: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0363 - acc: 0.9885 - val_loss: 0.2664 - val_acc: 0.9441\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9886\n",
      "Epoch 00045: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0349 - acc: 0.9886 - val_loss: 0.2667 - val_acc: 0.9446\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9887\n",
      "Epoch 00046: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0353 - acc: 0.9887 - val_loss: 0.3001 - val_acc: 0.9378\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9883\n",
      "Epoch 00047: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0362 - acc: 0.9883 - val_loss: 0.2739 - val_acc: 0.9422\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9894\n",
      "Epoch 00048: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0321 - acc: 0.9894 - val_loss: 0.2616 - val_acc: 0.9478\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9911\n",
      "Epoch 00049: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0285 - acc: 0.9911 - val_loss: 0.2751 - val_acc: 0.9450\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9891\n",
      "Epoch 00050: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0342 - acc: 0.9891 - val_loss: 0.3033 - val_acc: 0.9406\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9903\n",
      "Epoch 00051: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0296 - acc: 0.9903 - val_loss: 0.2951 - val_acc: 0.9436\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9908\n",
      "Epoch 00052: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0288 - acc: 0.9908 - val_loss: 0.2912 - val_acc: 0.9420\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9896\n",
      "Epoch 00053: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0326 - acc: 0.9896 - val_loss: 0.3154 - val_acc: 0.9397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9917\n",
      "Epoch 00054: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0261 - acc: 0.9917 - val_loss: 0.2737 - val_acc: 0.9467\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9923\n",
      "Epoch 00055: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0246 - acc: 0.9923 - val_loss: 0.2932 - val_acc: 0.9434\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9920\n",
      "Epoch 00056: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0243 - acc: 0.9920 - val_loss: 0.2766 - val_acc: 0.9455\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9920\n",
      "Epoch 00057: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0255 - acc: 0.9920 - val_loss: 0.2912 - val_acc: 0.9408\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9928\n",
      "Epoch 00058: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0232 - acc: 0.9928 - val_loss: 0.2936 - val_acc: 0.9418\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9913\n",
      "Epoch 00059: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0262 - acc: 0.9913 - val_loss: 0.3353 - val_acc: 0.9383\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9923\n",
      "Epoch 00060: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0253 - acc: 0.9923 - val_loss: 0.2844 - val_acc: 0.9488\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9937\n",
      "Epoch 00061: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0205 - acc: 0.9937 - val_loss: 0.3123 - val_acc: 0.9448\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9941\n",
      "Epoch 00062: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0198 - acc: 0.9941 - val_loss: 0.3011 - val_acc: 0.9478\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9930\n",
      "Epoch 00063: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0223 - acc: 0.9930 - val_loss: 0.2976 - val_acc: 0.9464\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9942\n",
      "Epoch 00064: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0182 - acc: 0.9942 - val_loss: 0.3141 - val_acc: 0.9411\n",
      "Epoch 65/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9936\n",
      "Epoch 00065: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0211 - acc: 0.9936 - val_loss: 0.3182 - val_acc: 0.9415\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9933\n",
      "Epoch 00066: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0208 - acc: 0.9933 - val_loss: 0.3087 - val_acc: 0.9436\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9942\n",
      "Epoch 00067: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0183 - acc: 0.9942 - val_loss: 0.2931 - val_acc: 0.9478\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9931\n",
      "Epoch 00068: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0207 - acc: 0.9931 - val_loss: 0.3037 - val_acc: 0.9432\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9935\n",
      "Epoch 00069: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0219 - acc: 0.9935 - val_loss: 0.3388 - val_acc: 0.9404\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9941\n",
      "Epoch 00070: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0196 - acc: 0.9941 - val_loss: 0.3150 - val_acc: 0.9455\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9942\n",
      "Epoch 00071: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0196 - acc: 0.9941 - val_loss: 0.3301 - val_acc: 0.9457\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9953\n",
      "Epoch 00072: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0159 - acc: 0.9953 - val_loss: 0.2956 - val_acc: 0.9462\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9942\n",
      "Epoch 00073: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0190 - acc: 0.9942 - val_loss: 0.3071 - val_acc: 0.9469\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9945\n",
      "Epoch 00074: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0189 - acc: 0.9945 - val_loss: 0.2889 - val_acc: 0.9495\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9955\n",
      "Epoch 00075: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0149 - acc: 0.9955 - val_loss: 0.3438 - val_acc: 0.9383\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9942\n",
      "Epoch 00076: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0191 - acc: 0.9942 - val_loss: 0.3161 - val_acc: 0.9436\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9947\n",
      "Epoch 00077: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0161 - acc: 0.9947 - val_loss: 0.3108 - val_acc: 0.9464\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9955\n",
      "Epoch 00078: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0145 - acc: 0.9955 - val_loss: 0.2867 - val_acc: 0.9495\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9948\n",
      "Epoch 00079: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0168 - acc: 0.9948 - val_loss: 0.2913 - val_acc: 0.9492\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9953\n",
      "Epoch 00080: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0160 - acc: 0.9953 - val_loss: 0.3525 - val_acc: 0.9427\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9960\n",
      "Epoch 00081: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0120 - acc: 0.9960 - val_loss: 0.2959 - val_acc: 0.9481\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9945\n",
      "Epoch 00082: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0179 - acc: 0.9945 - val_loss: 0.2921 - val_acc: 0.9513\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9959\n",
      "Epoch 00083: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0140 - acc: 0.9959 - val_loss: 0.2908 - val_acc: 0.9506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9956- ETA: 1s - loss: 0.014\n",
      "Epoch 00084: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0144 - acc: 0.9955 - val_loss: 0.3022 - val_acc: 0.9499\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9952\n",
      "Epoch 00085: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0145 - acc: 0.9952 - val_loss: 0.3298 - val_acc: 0.9464\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9942\n",
      "Epoch 00086: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0179 - acc: 0.9942 - val_loss: 0.3529 - val_acc: 0.9418\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9957\n",
      "Epoch 00087: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0128 - acc: 0.9957 - val_loss: 0.3059 - val_acc: 0.9485\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9952\n",
      "Epoch 00088: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0142 - acc: 0.9952 - val_loss: 0.3388 - val_acc: 0.9457\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9967\n",
      "Epoch 00089: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0114 - acc: 0.9967 - val_loss: 0.3195 - val_acc: 0.9474\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9949\n",
      "Epoch 00090: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0164 - acc: 0.9949 - val_loss: 0.3287 - val_acc: 0.9464\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9968\n",
      "Epoch 00091: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0107 - acc: 0.9968 - val_loss: 0.3173 - val_acc: 0.9478\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9957\n",
      "Epoch 00092: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0126 - acc: 0.9957 - val_loss: 0.3038 - val_acc: 0.9525\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9963\n",
      "Epoch 00093: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0114 - acc: 0.9963 - val_loss: 0.3248 - val_acc: 0.9483\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9956\n",
      "Epoch 00094: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0131 - acc: 0.9956 - val_loss: 0.3137 - val_acc: 0.9478\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9966\n",
      "Epoch 00095: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0123 - acc: 0.9966 - val_loss: 0.3206 - val_acc: 0.9478\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9971\n",
      "Epoch 00096: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0091 - acc: 0.9971 - val_loss: 0.3164 - val_acc: 0.9525\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9957\n",
      "Epoch 00097: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0137 - acc: 0.9957 - val_loss: 0.3332 - val_acc: 0.9504\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9960\n",
      "Epoch 00098: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0133 - acc: 0.9960 - val_loss: 0.3530 - val_acc: 0.9478\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9968\n",
      "Epoch 00099: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0100 - acc: 0.9968 - val_loss: 0.3677 - val_acc: 0.9399\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9954\n",
      "Epoch 00100: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0153 - acc: 0.9954 - val_loss: 0.3470 - val_acc: 0.9457\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9968\n",
      "Epoch 00101: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0103 - acc: 0.9968 - val_loss: 0.3025 - val_acc: 0.9490\n",
      "Epoch 102/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9961\n",
      "Epoch 00102: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0124 - acc: 0.9961 - val_loss: 0.3133 - val_acc: 0.9488\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9967\n",
      "Epoch 00103: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0111 - acc: 0.9967 - val_loss: 0.3936 - val_acc: 0.9376\n",
      "Epoch 104/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9961\n",
      "Epoch 00104: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0126 - acc: 0.9961 - val_loss: 0.3177 - val_acc: 0.9495\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9965\n",
      "Epoch 00105: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0121 - acc: 0.9965 - val_loss: 0.3248 - val_acc: 0.9450\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9970\n",
      "Epoch 00106: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0102 - acc: 0.9970 - val_loss: 0.3263 - val_acc: 0.9467\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9972\n",
      "Epoch 00107: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0096 - acc: 0.9973 - val_loss: 0.3935 - val_acc: 0.9434\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9964\n",
      "Epoch 00108: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0131 - acc: 0.9964 - val_loss: 0.3500 - val_acc: 0.9460\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9969\n",
      "Epoch 00109: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0096 - acc: 0.9969 - val_loss: 0.3086 - val_acc: 0.9490\n",
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9965\n",
      "Epoch 00110: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0111 - acc: 0.9965 - val_loss: 0.3191 - val_acc: 0.9509\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9979\n",
      "Epoch 00111: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0073 - acc: 0.9979 - val_loss: 0.3276 - val_acc: 0.9504\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9967\n",
      "Epoch 00112: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0104 - acc: 0.9966 - val_loss: 0.3524 - val_acc: 0.9476\n",
      "Epoch 113/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9952\n",
      "Epoch 00113: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0148 - acc: 0.9952 - val_loss: 0.3149 - val_acc: 0.9488\n",
      "Epoch 114/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9980\n",
      "Epoch 00114: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0063 - acc: 0.9980 - val_loss: 0.3394 - val_acc: 0.9488\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9974\n",
      "Epoch 00115: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0082 - acc: 0.9974 - val_loss: 0.3724 - val_acc: 0.9446\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9970\n",
      "Epoch 00116: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0097 - acc: 0.9970 - val_loss: 0.3366 - val_acc: 0.9483\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9968\n",
      "Epoch 00117: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0112 - acc: 0.9968 - val_loss: 0.3631 - val_acc: 0.9460\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9965\n",
      "Epoch 00118: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0129 - acc: 0.9965 - val_loss: 0.3318 - val_acc: 0.9464\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9978\n",
      "Epoch 00119: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0074 - acc: 0.9978 - val_loss: 0.3245 - val_acc: 0.9464\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9970\n",
      "Epoch 00120: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.3300 - val_acc: 0.9490\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9976\n",
      "Epoch 00121: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0079 - acc: 0.9976 - val_loss: 0.3384 - val_acc: 0.9464\n",
      "Epoch 122/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9968\n",
      "Epoch 00122: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0120 - acc: 0.9968 - val_loss: 0.3277 - val_acc: 0.9483\n",
      "Epoch 123/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9968\n",
      "Epoch 00123: val_loss did not improve from 0.19702\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0101 - acc: 0.9968 - val_loss: 0.3245 - val_acc: 0.9495\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 313us/sample - loss: 0.2944 - acc: 0.9132\n",
      "Loss: 0.2944057830892743 Accuracy: 0.913188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    base = '1D_CNN_only_conv_conv_5_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_only_conv_conv_5_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_only_conv_conv_5_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                682512    \n",
      "=================================================================\n",
      "Total params: 682,560\n",
      "Trainable params: 682,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 233us/sample - loss: 2.2452 - acc: 0.2818\n",
      "Loss: 2.2452293619807513 Accuracy: 0.28182763\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_37 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                454672    \n",
      "=================================================================\n",
      "Total params: 455,376\n",
      "Trainable params: 455,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 254us/sample - loss: 1.7158 - acc: 0.4671\n",
      "Loss: 1.7157914078000427 Accuracy: 0.46708202\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_39 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                302608    \n",
      "=================================================================\n",
      "Total params: 305,904\n",
      "Trainable params: 305,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 281us/sample - loss: 1.2891 - acc: 0.6295\n",
      "Loss: 1.2891465662672017 Accuracy: 0.62949115\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                200720    \n",
      "=================================================================\n",
      "Total params: 214,320\n",
      "Trainable params: 214,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 303us/sample - loss: 0.8589 - acc: 0.7556\n",
      "Loss: 0.8589462688778791 Accuracy: 0.75555557\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                131088    \n",
      "=================================================================\n",
      "Total params: 185,776\n",
      "Trainable params: 185,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 303us/sample - loss: 0.5174 - acc: 0.8492\n",
      "Loss: 0.5174186083015252 Accuracy: 0.84922117\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                81936     \n",
      "=================================================================\n",
      "Total params: 300,720\n",
      "Trainable params: 300,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 314us/sample - loss: 0.3043 - acc: 0.9180\n",
      "Loss: 0.3043327937430682 Accuracy: 0.9179647\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_57 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                49168     \n",
      "=================================================================\n",
      "Total params: 923,824\n",
      "Trainable params: 923,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 318us/sample - loss: 0.2636 - acc: 0.9196\n",
      "Loss: 0.26358092670500094 Accuracy: 0.9196262\n",
      "\n",
      "1D_CNN_only_conv_conv_5_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_64 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 2, 1024)           2622464   \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 3,513,520\n",
      "Trainable params: 3,513,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 366us/sample - loss: 0.2944 - acc: 0.9132\n",
      "Loss: 0.2944057830892743 Accuracy: 0.913188\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_only_conv_conv_5_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
