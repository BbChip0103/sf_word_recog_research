{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2064        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 44,304\n",
      "Trainable params: 43,920\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 64)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2064        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 65,104\n",
      "Trainable params: 64,592\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 64)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 192)          0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 192)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           3088        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 107,728\n",
      "Trainable params: 106,960\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           4112        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 191,312\n",
      "Trainable params: 190,288\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 128)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256)          0           global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           4112        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 273,872\n",
      "Trainable params: 272,592\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 256)          0           global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           4112        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 356,432\n",
      "Trainable params: 354,896\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5546 - acc: 0.1880\n",
      "Epoch 00001: val_loss improved from inf to 2.39308, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/001-2.3931.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 2.5546 - acc: 0.1880 - val_loss: 2.3931 - val_acc: 0.3156\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2497 - acc: 0.2824\n",
      "Epoch 00002: val_loss improved from 2.39308 to 2.06287, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/002-2.0629.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.2497 - acc: 0.2824 - val_loss: 2.0629 - val_acc: 0.4498\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1332 - acc: 0.3171\n",
      "Epoch 00003: val_loss improved from 2.06287 to 1.95458, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/003-1.9546.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.1332 - acc: 0.3171 - val_loss: 1.9546 - val_acc: 0.4745\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0414 - acc: 0.3487\n",
      "Epoch 00004: val_loss improved from 1.95458 to 1.86179, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/004-1.8618.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.0414 - acc: 0.3487 - val_loss: 1.8618 - val_acc: 0.4973\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9653 - acc: 0.3721\n",
      "Epoch 00005: val_loss improved from 1.86179 to 1.78502, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/005-1.7850.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.9653 - acc: 0.3721 - val_loss: 1.7850 - val_acc: 0.5204\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9021 - acc: 0.3946\n",
      "Epoch 00006: val_loss improved from 1.78502 to 1.72518, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/006-1.7252.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.9021 - acc: 0.3946 - val_loss: 1.7252 - val_acc: 0.5372\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8431 - acc: 0.4104\n",
      "Epoch 00007: val_loss improved from 1.72518 to 1.67866, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/007-1.6787.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.8431 - acc: 0.4104 - val_loss: 1.6787 - val_acc: 0.5027\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7876 - acc: 0.4324\n",
      "Epoch 00008: val_loss improved from 1.67866 to 1.62769, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/008-1.6277.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.7875 - acc: 0.4324 - val_loss: 1.6277 - val_acc: 0.5348\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7457 - acc: 0.4486\n",
      "Epoch 00009: val_loss improved from 1.62769 to 1.55108, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/009-1.5511.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.7458 - acc: 0.4486 - val_loss: 1.5511 - val_acc: 0.5830\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7025 - acc: 0.4604\n",
      "Epoch 00010: val_loss improved from 1.55108 to 1.51711, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/010-1.5171.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.7026 - acc: 0.4604 - val_loss: 1.5171 - val_acc: 0.5998\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6725 - acc: 0.4712\n",
      "Epoch 00011: val_loss improved from 1.51711 to 1.48208, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/011-1.4821.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.6725 - acc: 0.4712 - val_loss: 1.4821 - val_acc: 0.5938\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6381 - acc: 0.4821\n",
      "Epoch 00012: val_loss improved from 1.48208 to 1.47034, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/012-1.4703.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.6382 - acc: 0.4821 - val_loss: 1.4703 - val_acc: 0.5819\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6094 - acc: 0.4943\n",
      "Epoch 00013: val_loss did not improve from 1.47034\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.6096 - acc: 0.4943 - val_loss: 1.5119 - val_acc: 0.5302\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5803 - acc: 0.5033\n",
      "Epoch 00014: val_loss improved from 1.47034 to 1.38500, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/014-1.3850.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.5804 - acc: 0.5032 - val_loss: 1.3850 - val_acc: 0.6301\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5645 - acc: 0.5096\n",
      "Epoch 00015: val_loss did not improve from 1.38500\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.5645 - acc: 0.5096 - val_loss: 1.4474 - val_acc: 0.5623\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5420 - acc: 0.5148\n",
      "Epoch 00016: val_loss improved from 1.38500 to 1.36248, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/016-1.3625.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.5420 - acc: 0.5148 - val_loss: 1.3625 - val_acc: 0.6010\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5179 - acc: 0.5191\n",
      "Epoch 00017: val_loss did not improve from 1.36248\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.5179 - acc: 0.5191 - val_loss: 1.6019 - val_acc: 0.4517\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5050 - acc: 0.5284\n",
      "Epoch 00018: val_loss improved from 1.36248 to 1.29943, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/018-1.2994.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.5050 - acc: 0.5284 - val_loss: 1.2994 - val_acc: 0.6364\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4887 - acc: 0.5351\n",
      "Epoch 00019: val_loss improved from 1.29943 to 1.27507, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/019-1.2751.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4888 - acc: 0.5351 - val_loss: 1.2751 - val_acc: 0.6476\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4721 - acc: 0.5397\n",
      "Epoch 00020: val_loss did not improve from 1.27507\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4721 - acc: 0.5397 - val_loss: 1.3331 - val_acc: 0.6028\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4582 - acc: 0.5436\n",
      "Epoch 00021: val_loss did not improve from 1.27507\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4582 - acc: 0.5435 - val_loss: 1.6282 - val_acc: 0.4377\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4404 - acc: 0.5502\n",
      "Epoch 00022: val_loss did not improve from 1.27507\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4403 - acc: 0.5503 - val_loss: 1.4616 - val_acc: 0.5155\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4244 - acc: 0.5555\n",
      "Epoch 00023: val_loss did not improve from 1.27507\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4243 - acc: 0.5555 - val_loss: 1.5585 - val_acc: 0.4715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4116 - acc: 0.5585\n",
      "Epoch 00024: val_loss improved from 1.27507 to 1.23886, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/024-1.2389.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4115 - acc: 0.5585 - val_loss: 1.2389 - val_acc: 0.6452\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4069 - acc: 0.5604\n",
      "Epoch 00025: val_loss did not improve from 1.23886\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4068 - acc: 0.5604 - val_loss: 1.2880 - val_acc: 0.6140\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3937 - acc: 0.5668\n",
      "Epoch 00026: val_loss improved from 1.23886 to 1.23629, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/026-1.2363.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3937 - acc: 0.5668 - val_loss: 1.2363 - val_acc: 0.6534\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3861 - acc: 0.5699\n",
      "Epoch 00027: val_loss did not improve from 1.23629\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3862 - acc: 0.5699 - val_loss: 1.2946 - val_acc: 0.6063\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3750 - acc: 0.5733\n",
      "Epoch 00028: val_loss improved from 1.23629 to 1.22257, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/028-1.2226.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3750 - acc: 0.5733 - val_loss: 1.2226 - val_acc: 0.6317\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3640 - acc: 0.5776\n",
      "Epoch 00029: val_loss improved from 1.22257 to 1.15058, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/029-1.1506.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3639 - acc: 0.5776 - val_loss: 1.1506 - val_acc: 0.6650\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3577 - acc: 0.5804\n",
      "Epoch 00030: val_loss did not improve from 1.15058\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3576 - acc: 0.5805 - val_loss: 1.7937 - val_acc: 0.3967\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3485 - acc: 0.5781\n",
      "Epoch 00031: val_loss improved from 1.15058 to 1.13295, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/031-1.1330.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3485 - acc: 0.5781 - val_loss: 1.1330 - val_acc: 0.6692\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3324 - acc: 0.5900\n",
      "Epoch 00032: val_loss did not improve from 1.13295\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3326 - acc: 0.5900 - val_loss: 1.2218 - val_acc: 0.6275\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3275 - acc: 0.5870\n",
      "Epoch 00033: val_loss did not improve from 1.13295\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3275 - acc: 0.5870 - val_loss: 1.2782 - val_acc: 0.6021\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3219 - acc: 0.5897\n",
      "Epoch 00034: val_loss did not improve from 1.13295\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3218 - acc: 0.5897 - val_loss: 1.9203 - val_acc: 0.3948\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3179 - acc: 0.5917\n",
      "Epoch 00035: val_loss did not improve from 1.13295\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3178 - acc: 0.5917 - val_loss: 1.1406 - val_acc: 0.6702\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3076 - acc: 0.5958\n",
      "Epoch 00036: val_loss did not improve from 1.13295\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3078 - acc: 0.5957 - val_loss: 1.7789 - val_acc: 0.3795\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3082 - acc: 0.5974\n",
      "Epoch 00037: val_loss improved from 1.13295 to 1.10256, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/037-1.1026.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3082 - acc: 0.5974 - val_loss: 1.1026 - val_acc: 0.6855\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3001 - acc: 0.5999\n",
      "Epoch 00038: val_loss improved from 1.10256 to 1.08738, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/038-1.0874.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3001 - acc: 0.5999 - val_loss: 1.0874 - val_acc: 0.6983\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2908 - acc: 0.6002\n",
      "Epoch 00039: val_loss improved from 1.08738 to 1.08317, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/039-1.0832.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2907 - acc: 0.6003 - val_loss: 1.0832 - val_acc: 0.6900\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2812 - acc: 0.6056\n",
      "Epoch 00040: val_loss did not improve from 1.08317\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2812 - acc: 0.6057 - val_loss: 1.1841 - val_acc: 0.6250\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2732 - acc: 0.6059\n",
      "Epoch 00041: val_loss did not improve from 1.08317\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2732 - acc: 0.6059 - val_loss: 1.3766 - val_acc: 0.5390\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2674 - acc: 0.6057\n",
      "Epoch 00042: val_loss did not improve from 1.08317\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2674 - acc: 0.6057 - val_loss: 2.9010 - val_acc: 0.3119\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2647 - acc: 0.6115\n",
      "Epoch 00043: val_loss did not improve from 1.08317\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2647 - acc: 0.6114 - val_loss: 1.0904 - val_acc: 0.6893\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2623 - acc: 0.6100\n",
      "Epoch 00044: val_loss improved from 1.08317 to 1.06704, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/044-1.0670.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2625 - acc: 0.6099 - val_loss: 1.0670 - val_acc: 0.6867\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2558 - acc: 0.6121\n",
      "Epoch 00045: val_loss did not improve from 1.06704\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2558 - acc: 0.6120 - val_loss: 1.0683 - val_acc: 0.6914\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2465 - acc: 0.6168\n",
      "Epoch 00046: val_loss did not improve from 1.06704\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2466 - acc: 0.6168 - val_loss: 1.3926 - val_acc: 0.5169\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2400 - acc: 0.6142\n",
      "Epoch 00047: val_loss did not improve from 1.06704\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2400 - acc: 0.6141 - val_loss: 1.1337 - val_acc: 0.6634\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2341 - acc: 0.6203\n",
      "Epoch 00048: val_loss did not improve from 1.06704\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2342 - acc: 0.6202 - val_loss: 1.1397 - val_acc: 0.6350\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2392 - acc: 0.6179\n",
      "Epoch 00049: val_loss did not improve from 1.06704\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2393 - acc: 0.6179 - val_loss: 1.1336 - val_acc: 0.6487\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2369 - acc: 0.6220\n",
      "Epoch 00050: val_loss did not improve from 1.06704\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2371 - acc: 0.6219 - val_loss: 1.0702 - val_acc: 0.6937\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2267 - acc: 0.6219\n",
      "Epoch 00051: val_loss improved from 1.06704 to 1.04694, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/051-1.0469.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2267 - acc: 0.6220 - val_loss: 1.0469 - val_acc: 0.6841\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2246 - acc: 0.6259\n",
      "Epoch 00052: val_loss improved from 1.04694 to 1.03938, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/052-1.0394.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2247 - acc: 0.6258 - val_loss: 1.0394 - val_acc: 0.6962\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2144 - acc: 0.6250\n",
      "Epoch 00053: val_loss improved from 1.03938 to 1.01176, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/053-1.0118.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2144 - acc: 0.6250 - val_loss: 1.0118 - val_acc: 0.7109\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2176 - acc: 0.6272\n",
      "Epoch 00054: val_loss improved from 1.01176 to 1.00389, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/054-1.0039.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2179 - acc: 0.6272 - val_loss: 1.0039 - val_acc: 0.7102\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2120 - acc: 0.6286\n",
      "Epoch 00055: val_loss did not improve from 1.00389\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2120 - acc: 0.6286 - val_loss: 1.0303 - val_acc: 0.6997\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2075 - acc: 0.6293\n",
      "Epoch 00056: val_loss did not improve from 1.00389\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2076 - acc: 0.6293 - val_loss: 2.0203 - val_acc: 0.4232\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2074 - acc: 0.6271\n",
      "Epoch 00057: val_loss improved from 1.00389 to 0.99531, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/057-0.9953.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2074 - acc: 0.6271 - val_loss: 0.9953 - val_acc: 0.7167\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2013 - acc: 0.6318\n",
      "Epoch 00058: val_loss did not improve from 0.99531\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2014 - acc: 0.6318 - val_loss: 1.0075 - val_acc: 0.7149\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1944 - acc: 0.6322\n",
      "Epoch 00059: val_loss did not improve from 0.99531\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1943 - acc: 0.6323 - val_loss: 1.1001 - val_acc: 0.6371\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1954 - acc: 0.6347\n",
      "Epoch 00060: val_loss improved from 0.99531 to 0.98303, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/060-0.9830.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1954 - acc: 0.6347 - val_loss: 0.9830 - val_acc: 0.7191\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1867 - acc: 0.6373\n",
      "Epoch 00061: val_loss did not improve from 0.98303\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1866 - acc: 0.6373 - val_loss: 1.0755 - val_acc: 0.6618\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1821 - acc: 0.6367\n",
      "Epoch 00062: val_loss did not improve from 0.98303\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1821 - acc: 0.6367 - val_loss: 1.1412 - val_acc: 0.6364\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1809 - acc: 0.6369\n",
      "Epoch 00063: val_loss did not improve from 0.98303\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1810 - acc: 0.6368 - val_loss: 1.0230 - val_acc: 0.6774\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1793 - acc: 0.6378\n",
      "Epoch 00064: val_loss did not improve from 0.98303\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1792 - acc: 0.6378 - val_loss: 1.0058 - val_acc: 0.6939\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1712 - acc: 0.6400\n",
      "Epoch 00065: val_loss did not improve from 0.98303\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1713 - acc: 0.6399 - val_loss: 1.0423 - val_acc: 0.6813\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1687 - acc: 0.6436\n",
      "Epoch 00066: val_loss did not improve from 0.98303\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1686 - acc: 0.6436 - val_loss: 1.2507 - val_acc: 0.5698\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1672 - acc: 0.6410\n",
      "Epoch 00067: val_loss improved from 0.98303 to 0.97902, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/067-0.9790.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1673 - acc: 0.6410 - val_loss: 0.9790 - val_acc: 0.7188\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1647 - acc: 0.6405\n",
      "Epoch 00068: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1648 - acc: 0.6405 - val_loss: 1.1511 - val_acc: 0.6252\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1545 - acc: 0.6480\n",
      "Epoch 00069: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1545 - acc: 0.6480 - val_loss: 1.1770 - val_acc: 0.6168\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1561 - acc: 0.6439\n",
      "Epoch 00070: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1563 - acc: 0.6439 - val_loss: 1.0569 - val_acc: 0.6702\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1543 - acc: 0.6450\n",
      "Epoch 00071: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1543 - acc: 0.6449 - val_loss: 1.0233 - val_acc: 0.6921\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1513 - acc: 0.6464\n",
      "Epoch 00072: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1514 - acc: 0.6464 - val_loss: 1.1258 - val_acc: 0.6497\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1523 - acc: 0.6480\n",
      "Epoch 00073: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1523 - acc: 0.6480 - val_loss: 1.6879 - val_acc: 0.4966\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1531 - acc: 0.6454\n",
      "Epoch 00074: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1533 - acc: 0.6454 - val_loss: 1.0278 - val_acc: 0.6771\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1421 - acc: 0.6512\n",
      "Epoch 00075: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1421 - acc: 0.6512 - val_loss: 1.8366 - val_acc: 0.4302\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1445 - acc: 0.6493\n",
      "Epoch 00076: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1445 - acc: 0.6493 - val_loss: 1.0993 - val_acc: 0.6378\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1395 - acc: 0.6530\n",
      "Epoch 00077: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1395 - acc: 0.6530 - val_loss: 0.9853 - val_acc: 0.7086\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1352 - acc: 0.6519\n",
      "Epoch 00078: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1351 - acc: 0.6519 - val_loss: 1.2921 - val_acc: 0.5851\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1316 - acc: 0.6538\n",
      "Epoch 00079: val_loss did not improve from 0.97902\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1317 - acc: 0.6537 - val_loss: 1.3320 - val_acc: 0.5740\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1310 - acc: 0.6544\n",
      "Epoch 00080: val_loss improved from 0.97902 to 0.95205, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/080-0.9520.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1309 - acc: 0.6544 - val_loss: 0.9520 - val_acc: 0.7314\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1318 - acc: 0.6558\n",
      "Epoch 00081: val_loss improved from 0.95205 to 0.94357, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/081-0.9436.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1317 - acc: 0.6558 - val_loss: 0.9436 - val_acc: 0.7242\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1158 - acc: 0.6573\n",
      "Epoch 00082: val_loss improved from 0.94357 to 0.92753, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/082-0.9275.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1159 - acc: 0.6572 - val_loss: 0.9275 - val_acc: 0.7324\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1314 - acc: 0.6556\n",
      "Epoch 00083: val_loss did not improve from 0.92753\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1315 - acc: 0.6556 - val_loss: 1.2480 - val_acc: 0.5765\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1208 - acc: 0.6579\n",
      "Epoch 00084: val_loss did not improve from 0.92753\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1207 - acc: 0.6580 - val_loss: 0.9406 - val_acc: 0.7298\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1180 - acc: 0.6595\n",
      "Epoch 00085: val_loss did not improve from 0.92753\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1179 - acc: 0.6596 - val_loss: 1.0518 - val_acc: 0.6636\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1097 - acc: 0.6598\n",
      "Epoch 00086: val_loss improved from 0.92753 to 0.91785, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/086-0.9178.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1097 - acc: 0.6598 - val_loss: 0.9178 - val_acc: 0.7412\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1148 - acc: 0.6598\n",
      "Epoch 00087: val_loss did not improve from 0.91785\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1148 - acc: 0.6598 - val_loss: 1.0304 - val_acc: 0.6862\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1073 - acc: 0.6614\n",
      "Epoch 00088: val_loss improved from 0.91785 to 0.89976, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/088-0.8998.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1073 - acc: 0.6614 - val_loss: 0.8998 - val_acc: 0.7382\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1028 - acc: 0.6615\n",
      "Epoch 00089: val_loss did not improve from 0.89976\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1028 - acc: 0.6615 - val_loss: 1.0608 - val_acc: 0.6634\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1076 - acc: 0.6632\n",
      "Epoch 00090: val_loss did not improve from 0.89976\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1076 - acc: 0.6632 - val_loss: 1.2123 - val_acc: 0.5980\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1031 - acc: 0.6652\n",
      "Epoch 00091: val_loss did not improve from 0.89976\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1031 - acc: 0.6651 - val_loss: 1.1777 - val_acc: 0.5966\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0981 - acc: 0.6637\n",
      "Epoch 00092: val_loss did not improve from 0.89976\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0980 - acc: 0.6638 - val_loss: 1.1452 - val_acc: 0.6317\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0967 - acc: 0.6644\n",
      "Epoch 00093: val_loss improved from 0.89976 to 0.89820, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/093-0.8982.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0966 - acc: 0.6644 - val_loss: 0.8982 - val_acc: 0.7475\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0986 - acc: 0.6677\n",
      "Epoch 00094: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0986 - acc: 0.6677 - val_loss: 0.9731 - val_acc: 0.6907\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0923 - acc: 0.6687\n",
      "Epoch 00095: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0924 - acc: 0.6687 - val_loss: 0.9554 - val_acc: 0.7172\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0916 - acc: 0.6702\n",
      "Epoch 00096: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0918 - acc: 0.6702 - val_loss: 1.0091 - val_acc: 0.6923\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0912 - acc: 0.6672\n",
      "Epoch 00097: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0913 - acc: 0.6672 - val_loss: 1.0607 - val_acc: 0.6492\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0901 - acc: 0.6645\n",
      "Epoch 00098: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0901 - acc: 0.6645 - val_loss: 0.9456 - val_acc: 0.7098\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0922 - acc: 0.6690\n",
      "Epoch 00099: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0923 - acc: 0.6690 - val_loss: 0.9869 - val_acc: 0.6969\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0880 - acc: 0.6699\n",
      "Epoch 00100: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0880 - acc: 0.6699 - val_loss: 0.9242 - val_acc: 0.7191\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0856 - acc: 0.6698\n",
      "Epoch 00101: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0857 - acc: 0.6698 - val_loss: 0.9144 - val_acc: 0.7170\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0872 - acc: 0.6697\n",
      "Epoch 00102: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0872 - acc: 0.6697 - val_loss: 1.0401 - val_acc: 0.6706\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0842 - acc: 0.6705\n",
      "Epoch 00103: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0842 - acc: 0.6705 - val_loss: 1.0175 - val_acc: 0.6797\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0781 - acc: 0.6725\n",
      "Epoch 00104: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0781 - acc: 0.6725 - val_loss: 1.8128 - val_acc: 0.4503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0779 - acc: 0.6702\n",
      "Epoch 00105: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0778 - acc: 0.6702 - val_loss: 0.9812 - val_acc: 0.6993\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0744 - acc: 0.6726\n",
      "Epoch 00106: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0744 - acc: 0.6726 - val_loss: 0.9551 - val_acc: 0.7151\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0706 - acc: 0.6736\n",
      "Epoch 00107: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0706 - acc: 0.6737 - val_loss: 0.9722 - val_acc: 0.6809\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0700 - acc: 0.6757\n",
      "Epoch 00108: val_loss did not improve from 0.89820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0700 - acc: 0.6757 - val_loss: 1.1539 - val_acc: 0.6129\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0726 - acc: 0.6750\n",
      "Epoch 00109: val_loss improved from 0.89820 to 0.85830, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/109-0.8583.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0726 - acc: 0.6750 - val_loss: 0.8583 - val_acc: 0.7536\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0639 - acc: 0.6771\n",
      "Epoch 00110: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0638 - acc: 0.6771 - val_loss: 1.2361 - val_acc: 0.5952\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0655 - acc: 0.6783\n",
      "Epoch 00111: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0654 - acc: 0.6783 - val_loss: 0.9794 - val_acc: 0.6918\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0628 - acc: 0.6763\n",
      "Epoch 00112: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0630 - acc: 0.6762 - val_loss: 0.8998 - val_acc: 0.7307\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0595 - acc: 0.6793\n",
      "Epoch 00113: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0597 - acc: 0.6793 - val_loss: 1.0714 - val_acc: 0.6387\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0582 - acc: 0.6776\n",
      "Epoch 00114: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0582 - acc: 0.6775 - val_loss: 1.1409 - val_acc: 0.6110\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0586 - acc: 0.6794\n",
      "Epoch 00115: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0588 - acc: 0.6794 - val_loss: 1.5580 - val_acc: 0.5111\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0614 - acc: 0.6761\n",
      "Epoch 00116: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0614 - acc: 0.6761 - val_loss: 1.8541 - val_acc: 0.4391\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0578 - acc: 0.6786\n",
      "Epoch 00117: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0578 - acc: 0.6785 - val_loss: 1.0721 - val_acc: 0.6275\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0524 - acc: 0.6821\n",
      "Epoch 00118: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0523 - acc: 0.6821 - val_loss: 1.8456 - val_acc: 0.4423\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0554 - acc: 0.6795\n",
      "Epoch 00119: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0554 - acc: 0.6794 - val_loss: 1.3005 - val_acc: 0.5865\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0445 - acc: 0.6833\n",
      "Epoch 00120: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0445 - acc: 0.6833 - val_loss: 3.3747 - val_acc: 0.2853\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0528 - acc: 0.6806\n",
      "Epoch 00121: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0528 - acc: 0.6806 - val_loss: 1.0045 - val_acc: 0.6713\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0467 - acc: 0.6821\n",
      "Epoch 00122: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0467 - acc: 0.6821 - val_loss: 2.4247 - val_acc: 0.4055\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0451 - acc: 0.6842\n",
      "Epoch 00123: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0451 - acc: 0.6842 - val_loss: 1.2981 - val_acc: 0.5949\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0423 - acc: 0.6822\n",
      "Epoch 00124: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0422 - acc: 0.6822 - val_loss: 0.9205 - val_acc: 0.7263\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0389 - acc: 0.6862\n",
      "Epoch 00125: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0389 - acc: 0.6862 - val_loss: 1.0236 - val_acc: 0.6669\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0403 - acc: 0.6838\n",
      "Epoch 00126: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0403 - acc: 0.6837 - val_loss: 1.6196 - val_acc: 0.5087\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0350 - acc: 0.6863\n",
      "Epoch 00127: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0352 - acc: 0.6863 - val_loss: 0.8741 - val_acc: 0.7575\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0390 - acc: 0.6854\n",
      "Epoch 00128: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0390 - acc: 0.6853 - val_loss: 1.0323 - val_acc: 0.6783\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0409 - acc: 0.6833\n",
      "Epoch 00129: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0409 - acc: 0.6833 - val_loss: 1.1252 - val_acc: 0.6171\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0302 - acc: 0.6868\n",
      "Epoch 00130: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0302 - acc: 0.6868 - val_loss: 0.9509 - val_acc: 0.6995\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0332 - acc: 0.6851\n",
      "Epoch 00131: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0335 - acc: 0.6851 - val_loss: 0.8687 - val_acc: 0.7491\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0343 - acc: 0.6889\n",
      "Epoch 00132: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0344 - acc: 0.6889 - val_loss: 1.2088 - val_acc: 0.5833\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0266 - acc: 0.6877\n",
      "Epoch 00133: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0265 - acc: 0.6877 - val_loss: 0.8720 - val_acc: 0.7303\n",
      "Epoch 134/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0344 - acc: 0.6839\n",
      "Epoch 00134: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0345 - acc: 0.6838 - val_loss: 1.4640 - val_acc: 0.5188\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0267 - acc: 0.6901\n",
      "Epoch 00135: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0266 - acc: 0.6902 - val_loss: 0.9244 - val_acc: 0.7193\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0245 - acc: 0.6899\n",
      "Epoch 00136: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0245 - acc: 0.6899 - val_loss: 0.9739 - val_acc: 0.6946\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0299 - acc: 0.6905\n",
      "Epoch 00137: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0300 - acc: 0.6904 - val_loss: 1.1243 - val_acc: 0.6352\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0271 - acc: 0.6905\n",
      "Epoch 00138: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0271 - acc: 0.6904 - val_loss: 0.9005 - val_acc: 0.7258\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0204 - acc: 0.6911\n",
      "Epoch 00139: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0204 - acc: 0.6910 - val_loss: 0.9507 - val_acc: 0.7100\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0179 - acc: 0.6919\n",
      "Epoch 00140: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0180 - acc: 0.6919 - val_loss: 1.0674 - val_acc: 0.6429\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0205 - acc: 0.6917\n",
      "Epoch 00141: val_loss did not improve from 0.85830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0204 - acc: 0.6917 - val_loss: 2.9608 - val_acc: 0.3198\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0128 - acc: 0.6933\n",
      "Epoch 00142: val_loss improved from 0.85830 to 0.82147, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/142-0.8215.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0128 - acc: 0.6933 - val_loss: 0.8215 - val_acc: 0.7703\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0201 - acc: 0.6914\n",
      "Epoch 00143: val_loss did not improve from 0.82147\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0201 - acc: 0.6915 - val_loss: 0.8506 - val_acc: 0.7491\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0143 - acc: 0.6941\n",
      "Epoch 00144: val_loss did not improve from 0.82147\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0144 - acc: 0.6941 - val_loss: 4.6062 - val_acc: 0.2548\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0132 - acc: 0.6918\n",
      "Epoch 00145: val_loss did not improve from 0.82147\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0131 - acc: 0.6918 - val_loss: 0.8262 - val_acc: 0.7643\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0122 - acc: 0.6948\n",
      "Epoch 00146: val_loss did not improve from 0.82147\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0122 - acc: 0.6948 - val_loss: 2.2531 - val_acc: 0.3885\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0090 - acc: 0.6953\n",
      "Epoch 00147: val_loss improved from 0.82147 to 0.82119, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/147-0.8212.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0091 - acc: 0.6952 - val_loss: 0.8212 - val_acc: 0.7533\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0138 - acc: 0.6927\n",
      "Epoch 00148: val_loss improved from 0.82119 to 0.81739, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/148-0.8174.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0137 - acc: 0.6927 - val_loss: 0.8174 - val_acc: 0.7713\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0123 - acc: 0.6950\n",
      "Epoch 00149: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0123 - acc: 0.6950 - val_loss: 3.5319 - val_acc: 0.3324\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0080 - acc: 0.6954\n",
      "Epoch 00150: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0079 - acc: 0.6953 - val_loss: 0.8721 - val_acc: 0.7566\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0037 - acc: 0.6977\n",
      "Epoch 00151: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0036 - acc: 0.6978 - val_loss: 0.8490 - val_acc: 0.7666\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0017 - acc: 0.6956\n",
      "Epoch 00152: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0017 - acc: 0.6956 - val_loss: 1.9180 - val_acc: 0.4356\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0038 - acc: 0.6936\n",
      "Epoch 00153: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0038 - acc: 0.6936 - val_loss: 4.3808 - val_acc: 0.3305\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0040 - acc: 0.6962\n",
      "Epoch 00154: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0040 - acc: 0.6962 - val_loss: 0.8534 - val_acc: 0.7473\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9985 - acc: 0.6995\n",
      "Epoch 00155: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9985 - acc: 0.6995 - val_loss: 0.9953 - val_acc: 0.6746\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0002 - acc: 0.6961\n",
      "Epoch 00156: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0002 - acc: 0.6961 - val_loss: 2.7660 - val_acc: 0.3648\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9973 - acc: 0.6977\n",
      "Epoch 00157: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9973 - acc: 0.6977 - val_loss: 0.8675 - val_acc: 0.7365\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0054 - acc: 0.6974\n",
      "Epoch 00158: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0054 - acc: 0.6974 - val_loss: 1.7273 - val_acc: 0.4983\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9946 - acc: 0.6984\n",
      "Epoch 00159: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9946 - acc: 0.6984 - val_loss: 1.1338 - val_acc: 0.6327\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9931 - acc: 0.7021\n",
      "Epoch 00160: val_loss did not improve from 0.81739\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9931 - acc: 0.7021 - val_loss: 0.8948 - val_acc: 0.7303\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9913 - acc: 0.7005\n",
      "Epoch 00161: val_loss improved from 0.81739 to 0.80538, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/161-0.8054.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9913 - acc: 0.7005 - val_loss: 0.8054 - val_acc: 0.7773\n",
      "Epoch 162/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9894 - acc: 0.7017\n",
      "Epoch 00162: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9895 - acc: 0.7017 - val_loss: 0.8905 - val_acc: 0.7349\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9910 - acc: 0.7013\n",
      "Epoch 00163: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9914 - acc: 0.7012 - val_loss: 0.9426 - val_acc: 0.6960\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9919 - acc: 0.6997\n",
      "Epoch 00164: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9919 - acc: 0.6997 - val_loss: 1.0573 - val_acc: 0.6597\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9934 - acc: 0.6996\n",
      "Epoch 00165: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9934 - acc: 0.6996 - val_loss: 0.9462 - val_acc: 0.7198\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9856 - acc: 0.6994\n",
      "Epoch 00166: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9857 - acc: 0.6994 - val_loss: 0.9498 - val_acc: 0.6997\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9816 - acc: 0.7042\n",
      "Epoch 00167: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9816 - acc: 0.7043 - val_loss: 2.3235 - val_acc: 0.4337\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9817 - acc: 0.7014\n",
      "Epoch 00168: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9819 - acc: 0.7013 - val_loss: 0.8726 - val_acc: 0.7468\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9911 - acc: 0.7010\n",
      "Epoch 00169: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9910 - acc: 0.7010 - val_loss: 1.5203 - val_acc: 0.5192\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9838 - acc: 0.7062\n",
      "Epoch 00170: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9837 - acc: 0.7063 - val_loss: 0.8216 - val_acc: 0.7584\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9795 - acc: 0.7049\n",
      "Epoch 00171: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9798 - acc: 0.7048 - val_loss: 1.0844 - val_acc: 0.6394\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9782 - acc: 0.7040\n",
      "Epoch 00172: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9782 - acc: 0.7041 - val_loss: 0.8336 - val_acc: 0.7596\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9853 - acc: 0.7032\n",
      "Epoch 00173: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9856 - acc: 0.7031 - val_loss: 0.9596 - val_acc: 0.7065\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9808 - acc: 0.7034\n",
      "Epoch 00174: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9810 - acc: 0.7034 - val_loss: 0.8868 - val_acc: 0.7123\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9830 - acc: 0.7032\n",
      "Epoch 00175: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9830 - acc: 0.7032 - val_loss: 0.8076 - val_acc: 0.7689\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9756 - acc: 0.7060\n",
      "Epoch 00176: val_loss did not improve from 0.80538\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9756 - acc: 0.7060 - val_loss: 1.2738 - val_acc: 0.5928\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9732 - acc: 0.7084\n",
      "Epoch 00177: val_loss improved from 0.80538 to 0.77527, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/177-0.7753.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9733 - acc: 0.7084 - val_loss: 0.7753 - val_acc: 0.7803\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9736 - acc: 0.7066\n",
      "Epoch 00178: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9736 - acc: 0.7066 - val_loss: 2.3591 - val_acc: 0.4778\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9730 - acc: 0.7057\n",
      "Epoch 00179: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9730 - acc: 0.7056 - val_loss: 1.3307 - val_acc: 0.5584\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9698 - acc: 0.7070\n",
      "Epoch 00180: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9698 - acc: 0.7070 - val_loss: 1.6303 - val_acc: 0.4943\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9677 - acc: 0.7081\n",
      "Epoch 00181: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9676 - acc: 0.7082 - val_loss: 1.1644 - val_acc: 0.6229\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9758 - acc: 0.7104\n",
      "Epoch 00182: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9759 - acc: 0.7103 - val_loss: 0.8504 - val_acc: 0.7407\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9736 - acc: 0.7088\n",
      "Epoch 00183: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9738 - acc: 0.7087 - val_loss: 0.9610 - val_acc: 0.6935\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9768 - acc: 0.7041\n",
      "Epoch 00184: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9769 - acc: 0.7040 - val_loss: 2.1314 - val_acc: 0.4239\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9694 - acc: 0.7069\n",
      "Epoch 00185: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9694 - acc: 0.7069 - val_loss: 2.2404 - val_acc: 0.4717\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9697 - acc: 0.7098\n",
      "Epoch 00186: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9697 - acc: 0.7098 - val_loss: 0.7809 - val_acc: 0.7815\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9687 - acc: 0.7100\n",
      "Epoch 00187: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9687 - acc: 0.7101 - val_loss: 1.0001 - val_acc: 0.6904\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9677 - acc: 0.7080\n",
      "Epoch 00188: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9677 - acc: 0.7080 - val_loss: 0.7806 - val_acc: 0.7859\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9564 - acc: 0.7139\n",
      "Epoch 00189: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9564 - acc: 0.7139 - val_loss: 0.9137 - val_acc: 0.7195\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9613 - acc: 0.7078\n",
      "Epoch 00190: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9614 - acc: 0.7078 - val_loss: 0.8087 - val_acc: 0.7724\n",
      "Epoch 191/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9550 - acc: 0.7110\n",
      "Epoch 00191: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9550 - acc: 0.7110 - val_loss: 1.0310 - val_acc: 0.6515\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9640 - acc: 0.7118\n",
      "Epoch 00192: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9640 - acc: 0.7118 - val_loss: 1.0816 - val_acc: 0.6382\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9639 - acc: 0.7118\n",
      "Epoch 00193: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9639 - acc: 0.7118 - val_loss: 0.8551 - val_acc: 0.7512\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9549 - acc: 0.7133\n",
      "Epoch 00194: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9549 - acc: 0.7133 - val_loss: 0.7921 - val_acc: 0.7729\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9639 - acc: 0.7077\n",
      "Epoch 00195: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9640 - acc: 0.7077 - val_loss: 1.0617 - val_acc: 0.6550\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9558 - acc: 0.7124\n",
      "Epoch 00196: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9559 - acc: 0.7124 - val_loss: 0.7935 - val_acc: 0.7650\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9591 - acc: 0.7110\n",
      "Epoch 00197: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9591 - acc: 0.7111 - val_loss: 1.0538 - val_acc: 0.6587\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9605 - acc: 0.7148\n",
      "Epoch 00198: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9604 - acc: 0.7148 - val_loss: 5.5917 - val_acc: 0.2942\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9554 - acc: 0.7148\n",
      "Epoch 00199: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9553 - acc: 0.7148 - val_loss: 0.9510 - val_acc: 0.6874\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9551 - acc: 0.7163\n",
      "Epoch 00200: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9551 - acc: 0.7163 - val_loss: 0.8202 - val_acc: 0.7610\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9483 - acc: 0.7166\n",
      "Epoch 00201: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9482 - acc: 0.7166 - val_loss: 1.7575 - val_acc: 0.5176\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9511 - acc: 0.7150\n",
      "Epoch 00202: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9511 - acc: 0.7150 - val_loss: 1.1789 - val_acc: 0.6131\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9463 - acc: 0.7161\n",
      "Epoch 00203: val_loss did not improve from 0.77527\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9463 - acc: 0.7161 - val_loss: 0.8313 - val_acc: 0.7501\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9498 - acc: 0.7170\n",
      "Epoch 00204: val_loss improved from 0.77527 to 0.74413, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/204-0.7441.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9498 - acc: 0.7170 - val_loss: 0.7441 - val_acc: 0.7892\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9456 - acc: 0.7172\n",
      "Epoch 00205: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9457 - acc: 0.7172 - val_loss: 1.1573 - val_acc: 0.6324\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9523 - acc: 0.7116\n",
      "Epoch 00206: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9526 - acc: 0.7116 - val_loss: 2.5528 - val_acc: 0.3953\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9463 - acc: 0.7168\n",
      "Epoch 00207: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9464 - acc: 0.7168 - val_loss: 2.2211 - val_acc: 0.4384\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9457 - acc: 0.7154\n",
      "Epoch 00208: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9457 - acc: 0.7154 - val_loss: 0.8520 - val_acc: 0.7473\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9464 - acc: 0.7138\n",
      "Epoch 00209: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9463 - acc: 0.7138 - val_loss: 1.5835 - val_acc: 0.5374\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9442 - acc: 0.7146\n",
      "Epoch 00210: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9441 - acc: 0.7147 - val_loss: 3.3264 - val_acc: 0.3718\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9490 - acc: 0.7153\n",
      "Epoch 00211: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9489 - acc: 0.7153 - val_loss: 0.8070 - val_acc: 0.7622\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9380 - acc: 0.7190\n",
      "Epoch 00212: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9379 - acc: 0.7190 - val_loss: 1.1995 - val_acc: 0.6287\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9401 - acc: 0.7168\n",
      "Epoch 00213: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9402 - acc: 0.7168 - val_loss: 1.4362 - val_acc: 0.5623\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9375 - acc: 0.7193\n",
      "Epoch 00214: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9375 - acc: 0.7193 - val_loss: 0.7814 - val_acc: 0.7731\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9374 - acc: 0.7182\n",
      "Epoch 00215: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9374 - acc: 0.7182 - val_loss: 1.8105 - val_acc: 0.4971\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9421 - acc: 0.7174\n",
      "Epoch 00216: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9421 - acc: 0.7174 - val_loss: 4.4693 - val_acc: 0.3333\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9330 - acc: 0.7189\n",
      "Epoch 00217: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9331 - acc: 0.7189 - val_loss: 0.8882 - val_acc: 0.7345\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9341 - acc: 0.7194\n",
      "Epoch 00218: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9341 - acc: 0.7194 - val_loss: 0.8745 - val_acc: 0.7305\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9364 - acc: 0.7181\n",
      "Epoch 00219: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9363 - acc: 0.7181 - val_loss: 3.4788 - val_acc: 0.3440\n",
      "Epoch 220/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9330 - acc: 0.7216\n",
      "Epoch 00220: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9330 - acc: 0.7216 - val_loss: 2.0416 - val_acc: 0.4370\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9315 - acc: 0.7222\n",
      "Epoch 00221: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9315 - acc: 0.7222 - val_loss: 1.6117 - val_acc: 0.5225\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9380 - acc: 0.7185\n",
      "Epoch 00222: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9381 - acc: 0.7185 - val_loss: 0.9344 - val_acc: 0.7058\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9389 - acc: 0.7175\n",
      "Epoch 00223: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9388 - acc: 0.7175 - val_loss: 0.8979 - val_acc: 0.7135\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9293 - acc: 0.7217\n",
      "Epoch 00224: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9292 - acc: 0.7217 - val_loss: 1.0900 - val_acc: 0.6723\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9278 - acc: 0.7229\n",
      "Epoch 00225: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9277 - acc: 0.7230 - val_loss: 0.7951 - val_acc: 0.7747\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9290 - acc: 0.7204\n",
      "Epoch 00226: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9290 - acc: 0.7204 - val_loss: 3.0416 - val_acc: 0.3515\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9260 - acc: 0.7228\n",
      "Epoch 00227: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9260 - acc: 0.7227 - val_loss: 0.8973 - val_acc: 0.7193\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9315 - acc: 0.7231\n",
      "Epoch 00228: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9317 - acc: 0.7230 - val_loss: 0.8160 - val_acc: 0.7552\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9295 - acc: 0.7224\n",
      "Epoch 00229: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9295 - acc: 0.7224 - val_loss: 0.9616 - val_acc: 0.6907\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9241 - acc: 0.7223\n",
      "Epoch 00230: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9242 - acc: 0.7222 - val_loss: 1.0478 - val_acc: 0.6648\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9284 - acc: 0.7218\n",
      "Epoch 00231: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9284 - acc: 0.7218 - val_loss: 0.7970 - val_acc: 0.7741\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9286 - acc: 0.7207\n",
      "Epoch 00232: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9285 - acc: 0.7207 - val_loss: 1.7481 - val_acc: 0.5122\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9223 - acc: 0.7224\n",
      "Epoch 00233: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9223 - acc: 0.7224 - val_loss: 0.7871 - val_acc: 0.7689\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9224 - acc: 0.7229\n",
      "Epoch 00234: val_loss did not improve from 0.74413\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9225 - acc: 0.7229 - val_loss: 1.1188 - val_acc: 0.6369\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9253 - acc: 0.7238\n",
      "Epoch 00235: val_loss improved from 0.74413 to 0.73128, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/235-0.7313.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9254 - acc: 0.7238 - val_loss: 0.7313 - val_acc: 0.7966\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9250 - acc: 0.7237\n",
      "Epoch 00236: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9252 - acc: 0.7237 - val_loss: 2.7207 - val_acc: 0.3993\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9227 - acc: 0.7225\n",
      "Epoch 00237: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9226 - acc: 0.7225 - val_loss: 2.3666 - val_acc: 0.4675\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9143 - acc: 0.7257\n",
      "Epoch 00238: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9143 - acc: 0.7257 - val_loss: 2.3494 - val_acc: 0.4768\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9134 - acc: 0.7250\n",
      "Epoch 00239: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9134 - acc: 0.7250 - val_loss: 0.9899 - val_acc: 0.6893\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9290 - acc: 0.7220\n",
      "Epoch 00240: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9290 - acc: 0.7220 - val_loss: 5.4749 - val_acc: 0.3576\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9228 - acc: 0.7219\n",
      "Epoch 00241: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9230 - acc: 0.7219 - val_loss: 0.8547 - val_acc: 0.7263\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9268 - acc: 0.7235\n",
      "Epoch 00242: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9270 - acc: 0.7235 - val_loss: 3.5174 - val_acc: 0.3720\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9177 - acc: 0.7232\n",
      "Epoch 00243: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9176 - acc: 0.7232 - val_loss: 7.8714 - val_acc: 0.2439\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9195 - acc: 0.7258\n",
      "Epoch 00244: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9195 - acc: 0.7258 - val_loss: 1.6936 - val_acc: 0.5153\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9168 - acc: 0.7230\n",
      "Epoch 00245: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9168 - acc: 0.7231 - val_loss: 1.5682 - val_acc: 0.5430\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9185 - acc: 0.7266\n",
      "Epoch 00246: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9185 - acc: 0.7266 - val_loss: 0.8524 - val_acc: 0.7372\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9176 - acc: 0.7269\n",
      "Epoch 00247: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9178 - acc: 0.7269 - val_loss: 0.9452 - val_acc: 0.7014\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9234 - acc: 0.7231\n",
      "Epoch 00248: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9235 - acc: 0.7231 - val_loss: 0.8550 - val_acc: 0.7424\n",
      "Epoch 249/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9093 - acc: 0.7281\n",
      "Epoch 00249: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9093 - acc: 0.7281 - val_loss: 1.9769 - val_acc: 0.4614\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9122 - acc: 0.7259\n",
      "Epoch 00250: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9121 - acc: 0.7259 - val_loss: 0.8816 - val_acc: 0.7251\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9100 - acc: 0.7282\n",
      "Epoch 00251: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9100 - acc: 0.7282 - val_loss: 0.7473 - val_acc: 0.7864\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9115 - acc: 0.7253\n",
      "Epoch 00252: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9115 - acc: 0.7253 - val_loss: 1.0404 - val_acc: 0.6555\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9139 - acc: 0.7277\n",
      "Epoch 00253: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9139 - acc: 0.7277 - val_loss: 3.5631 - val_acc: 0.3159\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9114 - acc: 0.7276\n",
      "Epoch 00254: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9114 - acc: 0.7276 - val_loss: 1.7176 - val_acc: 0.5502\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9149 - acc: 0.7271\n",
      "Epoch 00255: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9149 - acc: 0.7271 - val_loss: 0.7530 - val_acc: 0.7822\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9091 - acc: 0.7283\n",
      "Epoch 00256: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9090 - acc: 0.7284 - val_loss: 0.7980 - val_acc: 0.7638\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9066 - acc: 0.7281\n",
      "Epoch 00257: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9066 - acc: 0.7281 - val_loss: 0.7586 - val_acc: 0.7845\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9055 - acc: 0.7294\n",
      "Epoch 00258: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9054 - acc: 0.7294 - val_loss: 0.8472 - val_acc: 0.7384\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9036 - acc: 0.7285\n",
      "Epoch 00259: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9037 - acc: 0.7285 - val_loss: 0.7921 - val_acc: 0.7671\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9072 - acc: 0.7291\n",
      "Epoch 00260: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9071 - acc: 0.7292 - val_loss: 1.1454 - val_acc: 0.6762\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9059 - acc: 0.7279\n",
      "Epoch 00261: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9058 - acc: 0.7280 - val_loss: 5.2285 - val_acc: 0.2478\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9069 - acc: 0.7270\n",
      "Epoch 00262: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9069 - acc: 0.7269 - val_loss: 0.8226 - val_acc: 0.7526\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9009 - acc: 0.7304\n",
      "Epoch 00263: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9008 - acc: 0.7304 - val_loss: 0.7826 - val_acc: 0.7682\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9059 - acc: 0.7314\n",
      "Epoch 00264: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9059 - acc: 0.7314 - val_loss: 3.3297 - val_acc: 0.3443\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9033 - acc: 0.7309\n",
      "Epoch 00265: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9034 - acc: 0.7309 - val_loss: 0.7559 - val_acc: 0.7838\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9049 - acc: 0.7287\n",
      "Epoch 00266: val_loss did not improve from 0.73128\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9050 - acc: 0.7287 - val_loss: 0.7707 - val_acc: 0.7904\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8968 - acc: 0.7333\n",
      "Epoch 00267: val_loss improved from 0.73128 to 0.71386, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/267-0.7139.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8968 - acc: 0.7333 - val_loss: 0.7139 - val_acc: 0.7994\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8942 - acc: 0.7318\n",
      "Epoch 00268: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8943 - acc: 0.7317 - val_loss: 1.0688 - val_acc: 0.6669\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9016 - acc: 0.7286\n",
      "Epoch 00269: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9016 - acc: 0.7286 - val_loss: 0.9178 - val_acc: 0.7165\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9019 - acc: 0.7299\n",
      "Epoch 00270: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9022 - acc: 0.7298 - val_loss: 0.7792 - val_acc: 0.7720\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8993 - acc: 0.7286\n",
      "Epoch 00271: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8993 - acc: 0.7286 - val_loss: 3.4682 - val_acc: 0.2760\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8994 - acc: 0.7305\n",
      "Epoch 00272: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8996 - acc: 0.7304 - val_loss: 0.8580 - val_acc: 0.7324\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9033 - acc: 0.7284\n",
      "Epoch 00273: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9032 - acc: 0.7284 - val_loss: 0.7620 - val_acc: 0.7806\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8939 - acc: 0.7328\n",
      "Epoch 00274: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8939 - acc: 0.7328 - val_loss: 1.1197 - val_acc: 0.6168\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8934 - acc: 0.7326\n",
      "Epoch 00275: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8933 - acc: 0.7326 - val_loss: 0.7513 - val_acc: 0.7843\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9007 - acc: 0.7296\n",
      "Epoch 00276: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9013 - acc: 0.7296 - val_loss: 3.7412 - val_acc: 0.3338\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8929 - acc: 0.7343\n",
      "Epoch 00277: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8931 - acc: 0.7342 - val_loss: 0.7379 - val_acc: 0.7906\n",
      "Epoch 278/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8961 - acc: 0.7310\n",
      "Epoch 00278: val_loss did not improve from 0.71386\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8961 - acc: 0.7310 - val_loss: 7.6137 - val_acc: 0.2364\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8961 - acc: 0.7327\n",
      "Epoch 00279: val_loss improved from 0.71386 to 0.71179, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/279-0.7118.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8960 - acc: 0.7327 - val_loss: 0.7118 - val_acc: 0.8046\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8914 - acc: 0.7341\n",
      "Epoch 00280: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8913 - acc: 0.7342 - val_loss: 1.0650 - val_acc: 0.6657\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8928 - acc: 0.7340\n",
      "Epoch 00281: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8928 - acc: 0.7340 - val_loss: 1.0830 - val_acc: 0.6327\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9016 - acc: 0.7337\n",
      "Epoch 00282: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9015 - acc: 0.7338 - val_loss: 0.9599 - val_acc: 0.7126\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8883 - acc: 0.7348\n",
      "Epoch 00283: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8884 - acc: 0.7348 - val_loss: 2.1625 - val_acc: 0.4055\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8910 - acc: 0.7337\n",
      "Epoch 00284: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8909 - acc: 0.7337 - val_loss: 1.0273 - val_acc: 0.6774\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8919 - acc: 0.7338\n",
      "Epoch 00285: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8920 - acc: 0.7337 - val_loss: 5.9898 - val_acc: 0.2178\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8913 - acc: 0.7349\n",
      "Epoch 00286: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8915 - acc: 0.7349 - val_loss: 0.8558 - val_acc: 0.7473\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8879 - acc: 0.7340\n",
      "Epoch 00287: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8880 - acc: 0.7340 - val_loss: 4.0245 - val_acc: 0.3382\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8893 - acc: 0.7326\n",
      "Epoch 00288: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8893 - acc: 0.7326 - val_loss: 0.8104 - val_acc: 0.7629\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8839 - acc: 0.7355\n",
      "Epoch 00289: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8838 - acc: 0.7355 - val_loss: 0.7691 - val_acc: 0.7794\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8866 - acc: 0.7347\n",
      "Epoch 00290: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8865 - acc: 0.7347 - val_loss: 0.7465 - val_acc: 0.7848\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8934 - acc: 0.7345\n",
      "Epoch 00291: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8937 - acc: 0.7345 - val_loss: 1.4602 - val_acc: 0.5917\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8865 - acc: 0.7362\n",
      "Epoch 00292: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8865 - acc: 0.7362 - val_loss: 1.4360 - val_acc: 0.5905\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8861 - acc: 0.7366\n",
      "Epoch 00293: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8860 - acc: 0.7366 - val_loss: 0.7394 - val_acc: 0.7992\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8830 - acc: 0.7338\n",
      "Epoch 00294: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8831 - acc: 0.7338 - val_loss: 0.8231 - val_acc: 0.7526\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8880 - acc: 0.7365\n",
      "Epoch 00295: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8881 - acc: 0.7365 - val_loss: 0.9335 - val_acc: 0.7049\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8804 - acc: 0.7371\n",
      "Epoch 00296: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8803 - acc: 0.7372 - val_loss: 0.8860 - val_acc: 0.7053\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8902 - acc: 0.7327\n",
      "Epoch 00297: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8902 - acc: 0.7327 - val_loss: 1.1284 - val_acc: 0.6567\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8840 - acc: 0.7356\n",
      "Epoch 00298: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8839 - acc: 0.7356 - val_loss: 0.9166 - val_acc: 0.7014\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8780 - acc: 0.7376\n",
      "Epoch 00299: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8779 - acc: 0.7376 - val_loss: 1.9815 - val_acc: 0.5013\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8851 - acc: 0.7350\n",
      "Epoch 00300: val_loss did not improve from 0.71179\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8850 - acc: 0.7350 - val_loss: 1.3629 - val_acc: 0.5863\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8863 - acc: 0.7360\n",
      "Epoch 00301: val_loss improved from 0.71179 to 0.70467, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_3_conv_checkpoint/301-0.7047.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8866 - acc: 0.7359 - val_loss: 0.7047 - val_acc: 0.8118\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8885 - acc: 0.7326\n",
      "Epoch 00302: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8885 - acc: 0.7326 - val_loss: 0.9764 - val_acc: 0.6988\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8778 - acc: 0.7368\n",
      "Epoch 00303: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8779 - acc: 0.7368 - val_loss: 0.7863 - val_acc: 0.7601\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8771 - acc: 0.7383\n",
      "Epoch 00304: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8770 - acc: 0.7383 - val_loss: 1.4688 - val_acc: 0.5390\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8822 - acc: 0.7385\n",
      "Epoch 00305: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8821 - acc: 0.7385 - val_loss: 0.8306 - val_acc: 0.7361\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8774 - acc: 0.7383\n",
      "Epoch 00306: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8774 - acc: 0.7382 - val_loss: 0.7685 - val_acc: 0.7671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8721 - acc: 0.7398\n",
      "Epoch 00307: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8721 - acc: 0.7398 - val_loss: 1.2786 - val_acc: 0.5924\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8765 - acc: 0.7362\n",
      "Epoch 00308: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8765 - acc: 0.7362 - val_loss: 1.3013 - val_acc: 0.5931\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8736 - acc: 0.7385\n",
      "Epoch 00309: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8736 - acc: 0.7385 - val_loss: 0.7832 - val_acc: 0.7605\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8786 - acc: 0.7376\n",
      "Epoch 00310: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8786 - acc: 0.7376 - val_loss: 0.7377 - val_acc: 0.7899\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8714 - acc: 0.7387\n",
      "Epoch 00311: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8714 - acc: 0.7388 - val_loss: 0.7832 - val_acc: 0.7668\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8653 - acc: 0.7404\n",
      "Epoch 00312: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8655 - acc: 0.7404 - val_loss: 0.8779 - val_acc: 0.7384\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8786 - acc: 0.7367\n",
      "Epoch 00313: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8786 - acc: 0.7367 - val_loss: 1.2179 - val_acc: 0.6471\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8720 - acc: 0.7364\n",
      "Epoch 00314: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8721 - acc: 0.7364 - val_loss: 0.7387 - val_acc: 0.7873\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8773 - acc: 0.7376\n",
      "Epoch 00315: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8772 - acc: 0.7376 - val_loss: 0.8206 - val_acc: 0.7494\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8735 - acc: 0.7418\n",
      "Epoch 00316: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8736 - acc: 0.7418 - val_loss: 2.6957 - val_acc: 0.4745\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8739 - acc: 0.7364\n",
      "Epoch 00317: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8738 - acc: 0.7364 - val_loss: 1.4543 - val_acc: 0.5768\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8717 - acc: 0.7424\n",
      "Epoch 00318: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8716 - acc: 0.7424 - val_loss: 0.7665 - val_acc: 0.7775\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8725 - acc: 0.7405\n",
      "Epoch 00319: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8724 - acc: 0.7406 - val_loss: 8.6216 - val_acc: 0.2411\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8698 - acc: 0.7412\n",
      "Epoch 00320: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8698 - acc: 0.7412 - val_loss: 1.2174 - val_acc: 0.6296\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8719 - acc: 0.7397\n",
      "Epoch 00321: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8720 - acc: 0.7397 - val_loss: 5.1280 - val_acc: 0.2287\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8660 - acc: 0.7421\n",
      "Epoch 00322: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8660 - acc: 0.7420 - val_loss: 1.1025 - val_acc: 0.6250\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8708 - acc: 0.7402\n",
      "Epoch 00323: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8709 - acc: 0.7401 - val_loss: 1.4987 - val_acc: 0.5579\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8712 - acc: 0.7402\n",
      "Epoch 00324: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8712 - acc: 0.7402 - val_loss: 1.8047 - val_acc: 0.5213\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8709 - acc: 0.7378\n",
      "Epoch 00325: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8710 - acc: 0.7378 - val_loss: 0.7384 - val_acc: 0.7906\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8679 - acc: 0.7393\n",
      "Epoch 00326: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8680 - acc: 0.7392 - val_loss: 0.7539 - val_acc: 0.7754\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8714 - acc: 0.7390\n",
      "Epoch 00327: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8715 - acc: 0.7389 - val_loss: 0.9671 - val_acc: 0.6895\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8704 - acc: 0.7392\n",
      "Epoch 00328: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8704 - acc: 0.7392 - val_loss: 0.7383 - val_acc: 0.7838\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8682 - acc: 0.7399\n",
      "Epoch 00329: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8682 - acc: 0.7400 - val_loss: 3.3369 - val_acc: 0.3194\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8666 - acc: 0.7412\n",
      "Epoch 00330: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8666 - acc: 0.7411 - val_loss: 0.8033 - val_acc: 0.7594\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8679 - acc: 0.7419\n",
      "Epoch 00331: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8679 - acc: 0.7419 - val_loss: 2.7001 - val_acc: 0.4771\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8625 - acc: 0.7416\n",
      "Epoch 00332: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8625 - acc: 0.7416 - val_loss: 0.9374 - val_acc: 0.6960\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8642 - acc: 0.7429\n",
      "Epoch 00333: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8642 - acc: 0.7429 - val_loss: 6.7100 - val_acc: 0.2837\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8637 - acc: 0.7447\n",
      "Epoch 00334: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8637 - acc: 0.7447 - val_loss: 0.7442 - val_acc: 0.7782\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8704 - acc: 0.7414\n",
      "Epoch 00335: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8703 - acc: 0.7414 - val_loss: 1.9679 - val_acc: 0.4554\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8643 - acc: 0.7411\n",
      "Epoch 00336: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8647 - acc: 0.7410 - val_loss: 0.7650 - val_acc: 0.7827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8678 - acc: 0.7407\n",
      "Epoch 00337: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8680 - acc: 0.7406 - val_loss: 0.7661 - val_acc: 0.7922\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8653 - acc: 0.7436\n",
      "Epoch 00338: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8652 - acc: 0.7436 - val_loss: 3.5177 - val_acc: 0.3692\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8610 - acc: 0.7418\n",
      "Epoch 00339: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8610 - acc: 0.7418 - val_loss: 1.6695 - val_acc: 0.5271\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8638 - acc: 0.7431\n",
      "Epoch 00340: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8638 - acc: 0.7431 - val_loss: 0.7689 - val_acc: 0.7715\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8633 - acc: 0.7439\n",
      "Epoch 00341: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8632 - acc: 0.7439 - val_loss: 0.8837 - val_acc: 0.7303\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8604 - acc: 0.7434\n",
      "Epoch 00342: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8604 - acc: 0.7433 - val_loss: 0.7261 - val_acc: 0.7950\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8630 - acc: 0.7414\n",
      "Epoch 00343: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8630 - acc: 0.7414 - val_loss: 7.1110 - val_acc: 0.2110\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8541 - acc: 0.7468\n",
      "Epoch 00344: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8541 - acc: 0.7468 - val_loss: 0.7488 - val_acc: 0.7773\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8573 - acc: 0.7437\n",
      "Epoch 00345: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8575 - acc: 0.7437 - val_loss: 0.7441 - val_acc: 0.7913\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8687 - acc: 0.7397\n",
      "Epoch 00346: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8688 - acc: 0.7396 - val_loss: 3.8759 - val_acc: 0.3927\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8589 - acc: 0.7461\n",
      "Epoch 00347: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8590 - acc: 0.7461 - val_loss: 6.1428 - val_acc: 0.2369\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8571 - acc: 0.7433\n",
      "Epoch 00348: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8571 - acc: 0.7433 - val_loss: 1.0553 - val_acc: 0.6655\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8601 - acc: 0.7416\n",
      "Epoch 00349: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8601 - acc: 0.7416 - val_loss: 1.7834 - val_acc: 0.5339\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8582 - acc: 0.7448\n",
      "Epoch 00350: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8582 - acc: 0.7448 - val_loss: 1.2247 - val_acc: 0.6226\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8584 - acc: 0.7433\n",
      "Epoch 00351: val_loss did not improve from 0.70467\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8585 - acc: 0.7432 - val_loss: 6.5071 - val_acc: 0.3478\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 558us/sample - loss: 0.7707 - acc: 0.7778\n",
      "Loss: 0.770710183526868 Accuracy: 0.7777778\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4354 - acc: 0.2261\n",
      "Epoch 00001: val_loss improved from inf to 2.24286, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/001-2.2429.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 2.4353 - acc: 0.2262 - val_loss: 2.2429 - val_acc: 0.3743\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0500 - acc: 0.3451\n",
      "Epoch 00002: val_loss improved from 2.24286 to 1.78875, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/002-1.7887.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 2.0500 - acc: 0.3451 - val_loss: 1.7887 - val_acc: 0.5302\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8758 - acc: 0.4068\n",
      "Epoch 00003: val_loss improved from 1.78875 to 1.60709, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/003-1.6071.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.8758 - acc: 0.4068 - val_loss: 1.6071 - val_acc: 0.5616\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7392 - acc: 0.4534\n",
      "Epoch 00004: val_loss improved from 1.60709 to 1.50491, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/004-1.5049.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.7394 - acc: 0.4533 - val_loss: 1.5049 - val_acc: 0.6087\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6439 - acc: 0.4852\n",
      "Epoch 00005: val_loss improved from 1.50491 to 1.37861, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/005-1.3786.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.6440 - acc: 0.4851 - val_loss: 1.3786 - val_acc: 0.6417\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5609 - acc: 0.5145\n",
      "Epoch 00006: val_loss improved from 1.37861 to 1.33332, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/006-1.3333.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.5609 - acc: 0.5145 - val_loss: 1.3333 - val_acc: 0.6459\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4937 - acc: 0.5369\n",
      "Epoch 00007: val_loss did not improve from 1.33332\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.4936 - acc: 0.5369 - val_loss: 1.5626 - val_acc: 0.4740\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4365 - acc: 0.5566\n",
      "Epoch 00008: val_loss improved from 1.33332 to 1.23087, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/008-1.2309.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.4366 - acc: 0.5566 - val_loss: 1.2309 - val_acc: 0.6711\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3879 - acc: 0.5714\n",
      "Epoch 00009: val_loss did not improve from 1.23087\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.3879 - acc: 0.5714 - val_loss: 1.3332 - val_acc: 0.5819\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3431 - acc: 0.5902\n",
      "Epoch 00010: val_loss improved from 1.23087 to 1.20516, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/010-1.2052.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.3432 - acc: 0.5902 - val_loss: 1.2052 - val_acc: 0.6511\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3105 - acc: 0.6000\n",
      "Epoch 00011: val_loss improved from 1.20516 to 1.15095, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/011-1.1509.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.3106 - acc: 0.6000 - val_loss: 1.1509 - val_acc: 0.6585\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2764 - acc: 0.6127\n",
      "Epoch 00012: val_loss improved from 1.15095 to 1.08531, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/012-1.0853.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.2764 - acc: 0.6127 - val_loss: 1.0853 - val_acc: 0.6909\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2424 - acc: 0.6259\n",
      "Epoch 00013: val_loss did not improve from 1.08531\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.2425 - acc: 0.6259 - val_loss: 1.0982 - val_acc: 0.6704\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2177 - acc: 0.6272\n",
      "Epoch 00014: val_loss did not improve from 1.08531\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.2177 - acc: 0.6272 - val_loss: 1.3751 - val_acc: 0.5469\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1910 - acc: 0.6392\n",
      "Epoch 00015: val_loss improved from 1.08531 to 0.98862, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/015-0.9886.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1910 - acc: 0.6393 - val_loss: 0.9886 - val_acc: 0.7317\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1723 - acc: 0.6431\n",
      "Epoch 00016: val_loss improved from 0.98862 to 0.93491, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/016-0.9349.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1724 - acc: 0.6431 - val_loss: 0.9349 - val_acc: 0.7510\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1563 - acc: 0.6501\n",
      "Epoch 00017: val_loss did not improve from 0.93491\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1565 - acc: 0.6500 - val_loss: 0.9936 - val_acc: 0.7293\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1325 - acc: 0.6549\n",
      "Epoch 00018: val_loss did not improve from 0.93491\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1326 - acc: 0.6549 - val_loss: 1.2042 - val_acc: 0.6143\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1105 - acc: 0.6645\n",
      "Epoch 00019: val_loss did not improve from 0.93491\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1104 - acc: 0.6646 - val_loss: 0.9407 - val_acc: 0.7510\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1051 - acc: 0.6663\n",
      "Epoch 00020: val_loss improved from 0.93491 to 0.90929, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/020-0.9093.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1051 - acc: 0.6662 - val_loss: 0.9093 - val_acc: 0.7587\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0824 - acc: 0.6721\n",
      "Epoch 00021: val_loss improved from 0.90929 to 0.88106, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/021-0.8811.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0824 - acc: 0.6721 - val_loss: 0.8811 - val_acc: 0.7638\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0683 - acc: 0.6771\n",
      "Epoch 00022: val_loss did not improve from 0.88106\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0683 - acc: 0.6771 - val_loss: 0.9291 - val_acc: 0.7522\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0596 - acc: 0.6827\n",
      "Epoch 00023: val_loss did not improve from 0.88106\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0597 - acc: 0.6827 - val_loss: 1.2113 - val_acc: 0.5986\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0507 - acc: 0.6812\n",
      "Epoch 00024: val_loss did not improve from 0.88106\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0507 - acc: 0.6812 - val_loss: 0.9633 - val_acc: 0.6976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0426 - acc: 0.6862\n",
      "Epoch 00025: val_loss did not improve from 0.88106\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0428 - acc: 0.6862 - val_loss: 1.2670 - val_acc: 0.5856\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0193 - acc: 0.6976\n",
      "Epoch 00026: val_loss improved from 0.88106 to 0.85400, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/026-0.8540.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0192 - acc: 0.6976 - val_loss: 0.8540 - val_acc: 0.7666\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0184 - acc: 0.6920\n",
      "Epoch 00027: val_loss improved from 0.85400 to 0.82911, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/027-0.8291.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0186 - acc: 0.6920 - val_loss: 0.8291 - val_acc: 0.7747\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0056 - acc: 0.6993\n",
      "Epoch 00028: val_loss did not improve from 0.82911\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0056 - acc: 0.6993 - val_loss: 0.8296 - val_acc: 0.7664\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9942 - acc: 0.7016\n",
      "Epoch 00029: val_loss improved from 0.82911 to 0.77868, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/029-0.7787.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9942 - acc: 0.7016 - val_loss: 0.7787 - val_acc: 0.7936\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9925 - acc: 0.7021\n",
      "Epoch 00030: val_loss did not improve from 0.77868\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9926 - acc: 0.7021 - val_loss: 0.8180 - val_acc: 0.7731\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9828 - acc: 0.7014\n",
      "Epoch 00031: val_loss did not improve from 0.77868\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9828 - acc: 0.7015 - val_loss: 0.7924 - val_acc: 0.7836\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9767 - acc: 0.7071\n",
      "Epoch 00032: val_loss improved from 0.77868 to 0.77775, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/032-0.7778.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9768 - acc: 0.7071 - val_loss: 0.7778 - val_acc: 0.7759\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9627 - acc: 0.7121\n",
      "Epoch 00033: val_loss did not improve from 0.77775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9627 - acc: 0.7121 - val_loss: 0.8338 - val_acc: 0.7694\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9582 - acc: 0.7111\n",
      "Epoch 00034: val_loss did not improve from 0.77775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9582 - acc: 0.7112 - val_loss: 0.8378 - val_acc: 0.7636\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9480 - acc: 0.7146\n",
      "Epoch 00035: val_loss did not improve from 0.77775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9480 - acc: 0.7146 - val_loss: 0.8038 - val_acc: 0.7741\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9456 - acc: 0.7172\n",
      "Epoch 00036: val_loss improved from 0.77775 to 0.77431, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/036-0.7743.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9457 - acc: 0.7172 - val_loss: 0.7743 - val_acc: 0.7831\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9402 - acc: 0.7181\n",
      "Epoch 00037: val_loss did not improve from 0.77431\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9402 - acc: 0.7181 - val_loss: 0.7989 - val_acc: 0.7745\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9300 - acc: 0.7194\n",
      "Epoch 00038: val_loss improved from 0.77431 to 0.75541, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/038-0.7554.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9299 - acc: 0.7195 - val_loss: 0.7554 - val_acc: 0.7890\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9258 - acc: 0.7220\n",
      "Epoch 00039: val_loss did not improve from 0.75541\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9258 - acc: 0.7220 - val_loss: 0.7684 - val_acc: 0.7890\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9218 - acc: 0.7256\n",
      "Epoch 00040: val_loss did not improve from 0.75541\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9222 - acc: 0.7255 - val_loss: 0.9069 - val_acc: 0.7270\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9195 - acc: 0.7266\n",
      "Epoch 00041: val_loss did not improve from 0.75541\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9195 - acc: 0.7266 - val_loss: 0.9328 - val_acc: 0.6993\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9059 - acc: 0.7301\n",
      "Epoch 00042: val_loss improved from 0.75541 to 0.70235, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/042-0.7024.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9059 - acc: 0.7300 - val_loss: 0.7024 - val_acc: 0.8148\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9104 - acc: 0.7254\n",
      "Epoch 00043: val_loss did not improve from 0.70235\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9105 - acc: 0.7254 - val_loss: 0.7459 - val_acc: 0.7945\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9004 - acc: 0.7321\n",
      "Epoch 00044: val_loss did not improve from 0.70235\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9005 - acc: 0.7320 - val_loss: 0.8572 - val_acc: 0.7440\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8996 - acc: 0.7316\n",
      "Epoch 00045: val_loss did not improve from 0.70235\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8995 - acc: 0.7316 - val_loss: 0.9121 - val_acc: 0.7200\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8924 - acc: 0.7350\n",
      "Epoch 00046: val_loss did not improve from 0.70235\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8924 - acc: 0.7350 - val_loss: 2.6047 - val_acc: 0.4617\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8799 - acc: 0.7372\n",
      "Epoch 00047: val_loss did not improve from 0.70235\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8800 - acc: 0.7372 - val_loss: 0.7687 - val_acc: 0.7736\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8820 - acc: 0.7357\n",
      "Epoch 00048: val_loss did not improve from 0.70235\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8819 - acc: 0.7357 - val_loss: 0.7989 - val_acc: 0.7615\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8762 - acc: 0.7401\n",
      "Epoch 00049: val_loss did not improve from 0.70235\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8762 - acc: 0.7401 - val_loss: 0.7398 - val_acc: 0.7943\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8718 - acc: 0.7404\n",
      "Epoch 00050: val_loss did not improve from 0.70235\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8718 - acc: 0.7403 - val_loss: 1.7965 - val_acc: 0.5313\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8673 - acc: 0.7430\n",
      "Epoch 00051: val_loss did not improve from 0.70235\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8673 - acc: 0.7430 - val_loss: 0.8192 - val_acc: 0.7379\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8722 - acc: 0.7405\n",
      "Epoch 00052: val_loss did not improve from 0.70235\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8724 - acc: 0.7404 - val_loss: 0.7136 - val_acc: 0.8064\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8536 - acc: 0.7442\n",
      "Epoch 00053: val_loss improved from 0.70235 to 0.67615, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/053-0.6762.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8538 - acc: 0.7441 - val_loss: 0.6762 - val_acc: 0.8223\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8484 - acc: 0.7471\n",
      "Epoch 00054: val_loss did not improve from 0.67615\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8485 - acc: 0.7470 - val_loss: 1.0637 - val_acc: 0.6532\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8582 - acc: 0.7477\n",
      "Epoch 00055: val_loss improved from 0.67615 to 0.66022, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/055-0.6602.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8582 - acc: 0.7477 - val_loss: 0.6602 - val_acc: 0.8234\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8417 - acc: 0.7481\n",
      "Epoch 00056: val_loss did not improve from 0.66022\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8417 - acc: 0.7481 - val_loss: 0.9404 - val_acc: 0.7000\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8442 - acc: 0.7496\n",
      "Epoch 00057: val_loss did not improve from 0.66022\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8442 - acc: 0.7495 - val_loss: 2.5189 - val_acc: 0.3792\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8378 - acc: 0.7490\n",
      "Epoch 00058: val_loss improved from 0.66022 to 0.65845, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/058-0.6584.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8377 - acc: 0.7490 - val_loss: 0.6584 - val_acc: 0.8197\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8353 - acc: 0.7505\n",
      "Epoch 00059: val_loss did not improve from 0.65845\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8353 - acc: 0.7505 - val_loss: 0.7438 - val_acc: 0.7887\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8345 - acc: 0.7520\n",
      "Epoch 00060: val_loss did not improve from 0.65845\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8346 - acc: 0.7519 - val_loss: 0.7742 - val_acc: 0.7668\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8327 - acc: 0.7531\n",
      "Epoch 00061: val_loss did not improve from 0.65845\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8329 - acc: 0.7530 - val_loss: 2.1944 - val_acc: 0.4111\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8260 - acc: 0.7566\n",
      "Epoch 00062: val_loss improved from 0.65845 to 0.65347, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/062-0.6535.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8260 - acc: 0.7565 - val_loss: 0.6535 - val_acc: 0.8237\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8205 - acc: 0.7567\n",
      "Epoch 00063: val_loss improved from 0.65347 to 0.64289, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/063-0.6429.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8205 - acc: 0.7567 - val_loss: 0.6429 - val_acc: 0.8279\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8263 - acc: 0.7538\n",
      "Epoch 00064: val_loss did not improve from 0.64289\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8264 - acc: 0.7538 - val_loss: 0.6505 - val_acc: 0.8244\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8136 - acc: 0.7588\n",
      "Epoch 00065: val_loss did not improve from 0.64289\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8139 - acc: 0.7588 - val_loss: 0.7372 - val_acc: 0.7929\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8197 - acc: 0.7549\n",
      "Epoch 00066: val_loss improved from 0.64289 to 0.63076, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/066-0.6308.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8197 - acc: 0.7550 - val_loss: 0.6308 - val_acc: 0.8346\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8113 - acc: 0.7570\n",
      "Epoch 00067: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8112 - acc: 0.7570 - val_loss: 3.8637 - val_acc: 0.3864\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8074 - acc: 0.7601\n",
      "Epoch 00068: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8074 - acc: 0.7601 - val_loss: 0.6470 - val_acc: 0.8279\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8035 - acc: 0.7616\n",
      "Epoch 00069: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8035 - acc: 0.7616 - val_loss: 0.6430 - val_acc: 0.8290\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8055 - acc: 0.7585\n",
      "Epoch 00070: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8056 - acc: 0.7585 - val_loss: 0.7061 - val_acc: 0.7985\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7970 - acc: 0.7635\n",
      "Epoch 00071: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7971 - acc: 0.7635 - val_loss: 0.7289 - val_acc: 0.7787\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7939 - acc: 0.7640\n",
      "Epoch 00072: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7940 - acc: 0.7639 - val_loss: 3.9558 - val_acc: 0.3189\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7949 - acc: 0.7628\n",
      "Epoch 00073: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7950 - acc: 0.7627 - val_loss: 0.6639 - val_acc: 0.8204\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7981 - acc: 0.7603\n",
      "Epoch 00074: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7982 - acc: 0.7603 - val_loss: 0.6887 - val_acc: 0.8039\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7895 - acc: 0.7661\n",
      "Epoch 00075: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7897 - acc: 0.7660 - val_loss: 0.6391 - val_acc: 0.8297\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7910 - acc: 0.7646\n",
      "Epoch 00076: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7910 - acc: 0.7647 - val_loss: 0.9648 - val_acc: 0.6909\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7810 - acc: 0.7676\n",
      "Epoch 00077: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7810 - acc: 0.7676 - val_loss: 0.9780 - val_acc: 0.6911\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7848 - acc: 0.7690\n",
      "Epoch 00078: val_loss did not improve from 0.63076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7848 - acc: 0.7690 - val_loss: 0.6901 - val_acc: 0.8060\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7788 - acc: 0.7704\n",
      "Epoch 00079: val_loss improved from 0.63076 to 0.60913, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/079-0.6091.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7788 - acc: 0.7704 - val_loss: 0.6091 - val_acc: 0.8358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7775 - acc: 0.7670\n",
      "Epoch 00080: val_loss did not improve from 0.60913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7774 - acc: 0.7670 - val_loss: 2.3462 - val_acc: 0.4950\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7749 - acc: 0.7689\n",
      "Epoch 00081: val_loss did not improve from 0.60913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7748 - acc: 0.7689 - val_loss: 1.0565 - val_acc: 0.6758\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7696 - acc: 0.7717\n",
      "Epoch 00082: val_loss did not improve from 0.60913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7696 - acc: 0.7716 - val_loss: 2.6482 - val_acc: 0.4635\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7679 - acc: 0.7710\n",
      "Epoch 00083: val_loss did not improve from 0.60913\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7679 - acc: 0.7710 - val_loss: 1.4163 - val_acc: 0.5989\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7661 - acc: 0.7715\n",
      "Epoch 00084: val_loss did not improve from 0.60913\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7661 - acc: 0.7715 - val_loss: 0.6372 - val_acc: 0.8190\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7662 - acc: 0.7728\n",
      "Epoch 00085: val_loss did not improve from 0.60913\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7663 - acc: 0.7728 - val_loss: 1.2834 - val_acc: 0.5986\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7611 - acc: 0.7714\n",
      "Epoch 00086: val_loss improved from 0.60913 to 0.60602, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/086-0.6060.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7611 - acc: 0.7714 - val_loss: 0.6060 - val_acc: 0.8372\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7630 - acc: 0.7742\n",
      "Epoch 00087: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7629 - acc: 0.7742 - val_loss: 0.6737 - val_acc: 0.8099\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7508 - acc: 0.7780\n",
      "Epoch 00088: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7508 - acc: 0.7781 - val_loss: 0.7151 - val_acc: 0.7829\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7581 - acc: 0.7744\n",
      "Epoch 00089: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7581 - acc: 0.7744 - val_loss: 0.6610 - val_acc: 0.8097\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7523 - acc: 0.7752\n",
      "Epoch 00090: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7523 - acc: 0.7751 - val_loss: 0.7289 - val_acc: 0.7782\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7541 - acc: 0.7782\n",
      "Epoch 00091: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7541 - acc: 0.7782 - val_loss: 1.0293 - val_acc: 0.6753\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7499 - acc: 0.7751\n",
      "Epoch 00092: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7499 - acc: 0.7751 - val_loss: 0.9750 - val_acc: 0.6902\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7483 - acc: 0.7769\n",
      "Epoch 00093: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7484 - acc: 0.7768 - val_loss: 0.6160 - val_acc: 0.8267\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7469 - acc: 0.7782\n",
      "Epoch 00094: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7469 - acc: 0.7782 - val_loss: 0.8093 - val_acc: 0.7501\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7380 - acc: 0.7802\n",
      "Epoch 00095: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7379 - acc: 0.7802 - val_loss: 0.6320 - val_acc: 0.8241\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7417 - acc: 0.7799\n",
      "Epoch 00096: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7417 - acc: 0.7799 - val_loss: 1.5819 - val_acc: 0.5523\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7411 - acc: 0.7809\n",
      "Epoch 00097: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7411 - acc: 0.7808 - val_loss: 0.6146 - val_acc: 0.8351\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7360 - acc: 0.7808\n",
      "Epoch 00098: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7361 - acc: 0.7808 - val_loss: 0.7617 - val_acc: 0.7624\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7368 - acc: 0.7820\n",
      "Epoch 00099: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7367 - acc: 0.7820 - val_loss: 0.9079 - val_acc: 0.7165\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7415 - acc: 0.7801\n",
      "Epoch 00100: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7415 - acc: 0.7801 - val_loss: 0.7036 - val_acc: 0.7915\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7351 - acc: 0.7836\n",
      "Epoch 00101: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7351 - acc: 0.7836 - val_loss: 0.6304 - val_acc: 0.8225\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7268 - acc: 0.7835\n",
      "Epoch 00102: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7269 - acc: 0.7834 - val_loss: 0.6989 - val_acc: 0.7717\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7287 - acc: 0.7830\n",
      "Epoch 00103: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7287 - acc: 0.7829 - val_loss: 0.6470 - val_acc: 0.8176\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7299 - acc: 0.7835\n",
      "Epoch 00104: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7300 - acc: 0.7835 - val_loss: 0.6293 - val_acc: 0.8197\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7269 - acc: 0.7863\n",
      "Epoch 00105: val_loss improved from 0.60602 to 0.57693, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/105-0.5769.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7269 - acc: 0.7863 - val_loss: 0.5769 - val_acc: 0.8465\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7200 - acc: 0.7849\n",
      "Epoch 00106: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7202 - acc: 0.7849 - val_loss: 0.7361 - val_acc: 0.7750\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7286 - acc: 0.7820\n",
      "Epoch 00107: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7286 - acc: 0.7820 - val_loss: 1.4621 - val_acc: 0.5600\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7250 - acc: 0.7815\n",
      "Epoch 00108: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7252 - acc: 0.7814 - val_loss: 0.6807 - val_acc: 0.7971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7198 - acc: 0.7868\n",
      "Epoch 00109: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7198 - acc: 0.7869 - val_loss: 0.6786 - val_acc: 0.8008\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7146 - acc: 0.7890\n",
      "Epoch 00110: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7147 - acc: 0.7890 - val_loss: 0.5855 - val_acc: 0.8449\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7245 - acc: 0.7844\n",
      "Epoch 00111: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7245 - acc: 0.7844 - val_loss: 0.6041 - val_acc: 0.8330\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7187 - acc: 0.7863\n",
      "Epoch 00112: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7187 - acc: 0.7863 - val_loss: 5.2241 - val_acc: 0.3482\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7161 - acc: 0.7876\n",
      "Epoch 00113: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7161 - acc: 0.7876 - val_loss: 0.6207 - val_acc: 0.8211\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7134 - acc: 0.7858\n",
      "Epoch 00114: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7136 - acc: 0.7858 - val_loss: 0.6140 - val_acc: 0.8334\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7106 - acc: 0.7887\n",
      "Epoch 00115: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7106 - acc: 0.7888 - val_loss: 0.6297 - val_acc: 0.8195\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7078 - acc: 0.7898\n",
      "Epoch 00116: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7077 - acc: 0.7898 - val_loss: 1.4976 - val_acc: 0.5821\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.7904\n",
      "Epoch 00117: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7060 - acc: 0.7904 - val_loss: 0.7664 - val_acc: 0.7771\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7051 - acc: 0.7921\n",
      "Epoch 00118: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7050 - acc: 0.7921 - val_loss: 0.6184 - val_acc: 0.8325\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7035 - acc: 0.7894\n",
      "Epoch 00119: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7037 - acc: 0.7893 - val_loss: 0.6726 - val_acc: 0.8062\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7014 - acc: 0.7908\n",
      "Epoch 00120: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7013 - acc: 0.7908 - val_loss: 0.7447 - val_acc: 0.7605\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6972 - acc: 0.7921\n",
      "Epoch 00121: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6973 - acc: 0.7921 - val_loss: 1.3701 - val_acc: 0.6282\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7024 - acc: 0.7906\n",
      "Epoch 00122: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7025 - acc: 0.7906 - val_loss: 1.8497 - val_acc: 0.5423\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6993 - acc: 0.7922\n",
      "Epoch 00123: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6996 - acc: 0.7921 - val_loss: 0.7802 - val_acc: 0.7251\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6952 - acc: 0.7943\n",
      "Epoch 00124: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6953 - acc: 0.7943 - val_loss: 0.6218 - val_acc: 0.8302\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6904 - acc: 0.7946\n",
      "Epoch 00125: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6905 - acc: 0.7946 - val_loss: 0.6532 - val_acc: 0.8232\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6948 - acc: 0.7924\n",
      "Epoch 00126: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6948 - acc: 0.7923 - val_loss: 0.6885 - val_acc: 0.7976\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6942 - acc: 0.7926\n",
      "Epoch 00127: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6942 - acc: 0.7926 - val_loss: 0.8851 - val_acc: 0.7151\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6896 - acc: 0.7970\n",
      "Epoch 00128: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6897 - acc: 0.7970 - val_loss: 0.7264 - val_acc: 0.7892\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6858 - acc: 0.7963\n",
      "Epoch 00129: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6859 - acc: 0.7963 - val_loss: 0.6786 - val_acc: 0.7831\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6838 - acc: 0.7965\n",
      "Epoch 00130: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6838 - acc: 0.7966 - val_loss: 0.5922 - val_acc: 0.8332\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.7974\n",
      "Epoch 00131: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6803 - acc: 0.7974 - val_loss: 2.1873 - val_acc: 0.5549\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6775 - acc: 0.7984\n",
      "Epoch 00132: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6775 - acc: 0.7984 - val_loss: 0.6019 - val_acc: 0.8269\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.8015\n",
      "Epoch 00133: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6734 - acc: 0.8014 - val_loss: 0.6503 - val_acc: 0.7892\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.7979\n",
      "Epoch 00134: val_loss did not improve from 0.57693\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6810 - acc: 0.7979 - val_loss: 0.5874 - val_acc: 0.8358\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6858 - acc: 0.7952\n",
      "Epoch 00135: val_loss improved from 0.57693 to 0.56615, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/135-0.5661.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6859 - acc: 0.7951 - val_loss: 0.5661 - val_acc: 0.8477\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6797 - acc: 0.7974\n",
      "Epoch 00136: val_loss did not improve from 0.56615\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6797 - acc: 0.7974 - val_loss: 0.6926 - val_acc: 0.7766\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6788 - acc: 0.8001\n",
      "Epoch 00137: val_loss did not improve from 0.56615\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6789 - acc: 0.8001 - val_loss: 0.9084 - val_acc: 0.7137\n",
      "Epoch 138/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6790 - acc: 0.7975\n",
      "Epoch 00138: val_loss did not improve from 0.56615\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6789 - acc: 0.7975 - val_loss: 0.6433 - val_acc: 0.8043\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6777 - acc: 0.8001\n",
      "Epoch 00139: val_loss improved from 0.56615 to 0.55072, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/139-0.5507.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6777 - acc: 0.8001 - val_loss: 0.5507 - val_acc: 0.8551\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6782 - acc: 0.7998\n",
      "Epoch 00140: val_loss did not improve from 0.55072\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6784 - acc: 0.7997 - val_loss: 0.7510 - val_acc: 0.7668\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6742 - acc: 0.8005\n",
      "Epoch 00141: val_loss did not improve from 0.55072\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6742 - acc: 0.8005 - val_loss: 0.7619 - val_acc: 0.7708\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6677 - acc: 0.8041\n",
      "Epoch 00142: val_loss did not improve from 0.55072\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6677 - acc: 0.8041 - val_loss: 0.8579 - val_acc: 0.7170\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6760 - acc: 0.7999\n",
      "Epoch 00143: val_loss did not improve from 0.55072\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6763 - acc: 0.7998 - val_loss: 0.5812 - val_acc: 0.8397\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6730 - acc: 0.7992\n",
      "Epoch 00144: val_loss improved from 0.55072 to 0.52699, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/144-0.5270.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6730 - acc: 0.7992 - val_loss: 0.5270 - val_acc: 0.8616\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.8043\n",
      "Epoch 00145: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6662 - acc: 0.8043 - val_loss: 0.6046 - val_acc: 0.8302\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6701 - acc: 0.8029\n",
      "Epoch 00146: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6703 - acc: 0.8029 - val_loss: 0.6046 - val_acc: 0.8164\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6602 - acc: 0.8024\n",
      "Epoch 00147: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6603 - acc: 0.8023 - val_loss: 0.5733 - val_acc: 0.8418\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6649 - acc: 0.8000\n",
      "Epoch 00148: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6650 - acc: 0.8000 - val_loss: 0.6240 - val_acc: 0.8232\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6640 - acc: 0.8009\n",
      "Epoch 00149: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6642 - acc: 0.8009 - val_loss: 2.4528 - val_acc: 0.4580\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6641 - acc: 0.8021\n",
      "Epoch 00150: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6640 - acc: 0.8021 - val_loss: 0.6261 - val_acc: 0.8239\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6608 - acc: 0.8046\n",
      "Epoch 00151: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6608 - acc: 0.8046 - val_loss: 0.6596 - val_acc: 0.8006\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6635 - acc: 0.8024\n",
      "Epoch 00152: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6636 - acc: 0.8023 - val_loss: 0.6249 - val_acc: 0.8218\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6598 - acc: 0.8051\n",
      "Epoch 00153: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6598 - acc: 0.8051 - val_loss: 0.8243 - val_acc: 0.7398\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6607 - acc: 0.8042\n",
      "Epoch 00154: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6608 - acc: 0.8042 - val_loss: 1.4070 - val_acc: 0.6289\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6570 - acc: 0.8048\n",
      "Epoch 00155: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6570 - acc: 0.8048 - val_loss: 0.5654 - val_acc: 0.8446\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6537 - acc: 0.8038\n",
      "Epoch 00156: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6536 - acc: 0.8039 - val_loss: 0.5501 - val_acc: 0.8470\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6556 - acc: 0.8062\n",
      "Epoch 00157: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6556 - acc: 0.8062 - val_loss: 0.5764 - val_acc: 0.8388\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6531 - acc: 0.8061\n",
      "Epoch 00158: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6535 - acc: 0.8060 - val_loss: 0.6303 - val_acc: 0.8209\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6651 - acc: 0.8036\n",
      "Epoch 00159: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6651 - acc: 0.8036 - val_loss: 0.5802 - val_acc: 0.8393\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6476 - acc: 0.8086\n",
      "Epoch 00160: val_loss did not improve from 0.52699\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6477 - acc: 0.8086 - val_loss: 0.8674 - val_acc: 0.7475\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6492 - acc: 0.8054\n",
      "Epoch 00161: val_loss improved from 0.52699 to 0.52119, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/161-0.5212.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6492 - acc: 0.8055 - val_loss: 0.5212 - val_acc: 0.8621\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6466 - acc: 0.8093\n",
      "Epoch 00162: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6466 - acc: 0.8093 - val_loss: 1.5448 - val_acc: 0.5851\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6429 - acc: 0.8073\n",
      "Epoch 00163: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6429 - acc: 0.8073 - val_loss: 0.6294 - val_acc: 0.8262\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6467 - acc: 0.8080\n",
      "Epoch 00164: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6467 - acc: 0.8080 - val_loss: 2.0356 - val_acc: 0.5215\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6468 - acc: 0.8103\n",
      "Epoch 00165: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6468 - acc: 0.8103 - val_loss: 4.3174 - val_acc: 0.3797\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6440 - acc: 0.8101\n",
      "Epoch 00166: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6439 - acc: 0.8101 - val_loss: 1.1028 - val_acc: 0.6683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6449 - acc: 0.8092\n",
      "Epoch 00167: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6450 - acc: 0.8092 - val_loss: 1.6038 - val_acc: 0.6007\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6454 - acc: 0.8084\n",
      "Epoch 00168: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6455 - acc: 0.8084 - val_loss: 7.9138 - val_acc: 0.2662\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6460 - acc: 0.8096\n",
      "Epoch 00169: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6461 - acc: 0.8096 - val_loss: 0.6791 - val_acc: 0.8036\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6384 - acc: 0.8099\n",
      "Epoch 00170: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6384 - acc: 0.8099 - val_loss: 0.6250 - val_acc: 0.8178\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6406 - acc: 0.8112\n",
      "Epoch 00171: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6406 - acc: 0.8112 - val_loss: 3.3131 - val_acc: 0.4831\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6458 - acc: 0.8077\n",
      "Epoch 00172: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6458 - acc: 0.8077 - val_loss: 0.6438 - val_acc: 0.8085\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6319 - acc: 0.8105\n",
      "Epoch 00173: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6323 - acc: 0.8105 - val_loss: 0.5291 - val_acc: 0.8535\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6369 - acc: 0.8116\n",
      "Epoch 00174: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6369 - acc: 0.8115 - val_loss: 0.8609 - val_acc: 0.7011\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6327 - acc: 0.8129\n",
      "Epoch 00175: val_loss did not improve from 0.52119\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6328 - acc: 0.8129 - val_loss: 0.9367 - val_acc: 0.7235\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6339 - acc: 0.8119\n",
      "Epoch 00176: val_loss improved from 0.52119 to 0.51914, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/176-0.5191.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6339 - acc: 0.8119 - val_loss: 0.5191 - val_acc: 0.8593\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6325 - acc: 0.8120\n",
      "Epoch 00177: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6325 - acc: 0.8120 - val_loss: 0.6006 - val_acc: 0.8409\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6329 - acc: 0.8119\n",
      "Epoch 00178: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6329 - acc: 0.8119 - val_loss: 0.5736 - val_acc: 0.8353\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6327 - acc: 0.8125\n",
      "Epoch 00179: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6327 - acc: 0.8124 - val_loss: 3.5164 - val_acc: 0.3923\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6230 - acc: 0.8130\n",
      "Epoch 00180: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6229 - acc: 0.8130 - val_loss: 0.5596 - val_acc: 0.8556\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6315 - acc: 0.8126\n",
      "Epoch 00181: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6315 - acc: 0.8126 - val_loss: 1.5535 - val_acc: 0.5865\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6254 - acc: 0.8139\n",
      "Epoch 00182: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6255 - acc: 0.8139 - val_loss: 0.7599 - val_acc: 0.7701\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6322 - acc: 0.8120\n",
      "Epoch 00183: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6323 - acc: 0.8120 - val_loss: 0.7860 - val_acc: 0.7552\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6291 - acc: 0.8136\n",
      "Epoch 00184: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6290 - acc: 0.8136 - val_loss: 0.5979 - val_acc: 0.8192\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6318 - acc: 0.8133\n",
      "Epoch 00185: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6318 - acc: 0.8133 - val_loss: 1.9975 - val_acc: 0.5604\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6260 - acc: 0.8136\n",
      "Epoch 00186: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6260 - acc: 0.8136 - val_loss: 0.6086 - val_acc: 0.8267\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6198 - acc: 0.8160\n",
      "Epoch 00187: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6200 - acc: 0.8160 - val_loss: 0.5560 - val_acc: 0.8491\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6303 - acc: 0.8129\n",
      "Epoch 00188: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6303 - acc: 0.8129 - val_loss: 2.4913 - val_acc: 0.4591\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6192 - acc: 0.8147\n",
      "Epoch 00189: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6193 - acc: 0.8147 - val_loss: 0.5955 - val_acc: 0.8339\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6196 - acc: 0.8163\n",
      "Epoch 00190: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6197 - acc: 0.8162 - val_loss: 0.5492 - val_acc: 0.8421\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6192 - acc: 0.8170\n",
      "Epoch 00191: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6192 - acc: 0.8170 - val_loss: 0.8111 - val_acc: 0.7633\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6197 - acc: 0.8159\n",
      "Epoch 00192: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6197 - acc: 0.8159 - val_loss: 0.8712 - val_acc: 0.7482\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6192 - acc: 0.8145\n",
      "Epoch 00193: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6191 - acc: 0.8145 - val_loss: 0.5823 - val_acc: 0.8295\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6186 - acc: 0.8167\n",
      "Epoch 00194: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6186 - acc: 0.8167 - val_loss: 0.5313 - val_acc: 0.8535\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6121 - acc: 0.8163\n",
      "Epoch 00195: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6121 - acc: 0.8162 - val_loss: 0.5985 - val_acc: 0.8213\n",
      "Epoch 196/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6194 - acc: 0.8145\n",
      "Epoch 00196: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6193 - acc: 0.8145 - val_loss: 0.5992 - val_acc: 0.8241\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6141 - acc: 0.8174\n",
      "Epoch 00197: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6141 - acc: 0.8174 - val_loss: 0.8747 - val_acc: 0.7354\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6127 - acc: 0.8164\n",
      "Epoch 00198: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6129 - acc: 0.8164 - val_loss: 0.6782 - val_acc: 0.8099\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6165 - acc: 0.8153\n",
      "Epoch 00199: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6165 - acc: 0.8153 - val_loss: 1.9428 - val_acc: 0.5222\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6087 - acc: 0.8174\n",
      "Epoch 00200: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6088 - acc: 0.8173 - val_loss: 1.1642 - val_acc: 0.6625\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6136 - acc: 0.8165\n",
      "Epoch 00201: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6137 - acc: 0.8165 - val_loss: 2.0665 - val_acc: 0.5008\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6091 - acc: 0.8180\n",
      "Epoch 00202: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6091 - acc: 0.8180 - val_loss: 0.5652 - val_acc: 0.8400\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6106 - acc: 0.8183\n",
      "Epoch 00203: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6105 - acc: 0.8183 - val_loss: 0.6172 - val_acc: 0.8216\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6111 - acc: 0.8174\n",
      "Epoch 00204: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6110 - acc: 0.8174 - val_loss: 0.7920 - val_acc: 0.7501\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6113 - acc: 0.8178\n",
      "Epoch 00205: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6115 - acc: 0.8177 - val_loss: 0.6052 - val_acc: 0.8262\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6079 - acc: 0.8196\n",
      "Epoch 00206: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6078 - acc: 0.8196 - val_loss: 0.5630 - val_acc: 0.8418\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6045 - acc: 0.8182\n",
      "Epoch 00207: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6045 - acc: 0.8181 - val_loss: 1.1500 - val_acc: 0.6816\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6093 - acc: 0.8188\n",
      "Epoch 00208: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6093 - acc: 0.8188 - val_loss: 0.5543 - val_acc: 0.8528\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6051 - acc: 0.8182\n",
      "Epoch 00209: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6051 - acc: 0.8183 - val_loss: 0.9609 - val_acc: 0.7098\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6066 - acc: 0.8214\n",
      "Epoch 00210: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6066 - acc: 0.8214 - val_loss: 0.6108 - val_acc: 0.8269\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6106 - acc: 0.8183\n",
      "Epoch 00211: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6105 - acc: 0.8184 - val_loss: 0.6020 - val_acc: 0.8244\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6039 - acc: 0.8195\n",
      "Epoch 00212: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6040 - acc: 0.8194 - val_loss: 0.5311 - val_acc: 0.8579\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6002 - acc: 0.8211\n",
      "Epoch 00213: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6002 - acc: 0.8211 - val_loss: 2.2537 - val_acc: 0.4440\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6041 - acc: 0.8196\n",
      "Epoch 00214: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6043 - acc: 0.8196 - val_loss: 0.7727 - val_acc: 0.7685\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6047 - acc: 0.8194\n",
      "Epoch 00215: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6047 - acc: 0.8194 - val_loss: 0.5455 - val_acc: 0.8449\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.8206\n",
      "Epoch 00216: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5941 - acc: 0.8206 - val_loss: 1.5290 - val_acc: 0.6157\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.8201\n",
      "Epoch 00217: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6019 - acc: 0.8201 - val_loss: 0.5726 - val_acc: 0.8339\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.8226\n",
      "Epoch 00218: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5961 - acc: 0.8226 - val_loss: 0.9669 - val_acc: 0.6976\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6040 - acc: 0.8179\n",
      "Epoch 00219: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6041 - acc: 0.8179 - val_loss: 0.8847 - val_acc: 0.7251\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5932 - acc: 0.8240\n",
      "Epoch 00220: val_loss improved from 0.51914 to 0.51666, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/220-0.5167.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5932 - acc: 0.8240 - val_loss: 0.5167 - val_acc: 0.8619\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5975 - acc: 0.8208\n",
      "Epoch 00221: val_loss did not improve from 0.51666\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5975 - acc: 0.8207 - val_loss: 0.5522 - val_acc: 0.8425\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5933 - acc: 0.8229\n",
      "Epoch 00222: val_loss did not improve from 0.51666\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5933 - acc: 0.8229 - val_loss: 0.5410 - val_acc: 0.8442\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5930 - acc: 0.8232\n",
      "Epoch 00223: val_loss did not improve from 0.51666\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5930 - acc: 0.8232 - val_loss: 0.5743 - val_acc: 0.8390\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5925 - acc: 0.8233\n",
      "Epoch 00224: val_loss did not improve from 0.51666\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5926 - acc: 0.8232 - val_loss: 1.9919 - val_acc: 0.5290\n",
      "Epoch 225/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5928 - acc: 0.8235\n",
      "Epoch 00225: val_loss did not improve from 0.51666\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5928 - acc: 0.8235 - val_loss: 0.5959 - val_acc: 0.8297\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5934 - acc: 0.8232\n",
      "Epoch 00226: val_loss did not improve from 0.51666\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5936 - acc: 0.8232 - val_loss: 0.8569 - val_acc: 0.7419\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5903 - acc: 0.8240\n",
      "Epoch 00227: val_loss did not improve from 0.51666\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5904 - acc: 0.8240 - val_loss: 0.5307 - val_acc: 0.8528\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5901 - acc: 0.8260\n",
      "Epoch 00228: val_loss improved from 0.51666 to 0.49691, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/228-0.4969.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5901 - acc: 0.8259 - val_loss: 0.4969 - val_acc: 0.8698\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5839 - acc: 0.8258\n",
      "Epoch 00229: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5840 - acc: 0.8258 - val_loss: 0.5447 - val_acc: 0.8446\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5899 - acc: 0.8238\n",
      "Epoch 00230: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5899 - acc: 0.8239 - val_loss: 0.5187 - val_acc: 0.8626\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5880 - acc: 0.8246\n",
      "Epoch 00231: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5879 - acc: 0.8246 - val_loss: 1.8763 - val_acc: 0.5532\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5896 - acc: 0.8256\n",
      "Epoch 00232: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5896 - acc: 0.8256 - val_loss: 1.6117 - val_acc: 0.5870\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.8243\n",
      "Epoch 00233: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5903 - acc: 0.8243 - val_loss: 0.5410 - val_acc: 0.8507\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5938 - acc: 0.8240\n",
      "Epoch 00234: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5943 - acc: 0.8239 - val_loss: 0.5678 - val_acc: 0.8307\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5822 - acc: 0.8242\n",
      "Epoch 00235: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5822 - acc: 0.8242 - val_loss: 0.8581 - val_acc: 0.7352\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5836 - acc: 0.8251\n",
      "Epoch 00236: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5836 - acc: 0.8251 - val_loss: 0.6215 - val_acc: 0.8241\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5848 - acc: 0.8255\n",
      "Epoch 00237: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5848 - acc: 0.8255 - val_loss: 1.1873 - val_acc: 0.6632\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.8267\n",
      "Epoch 00238: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5815 - acc: 0.8268 - val_loss: 0.5156 - val_acc: 0.8612\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.8268\n",
      "Epoch 00239: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5789 - acc: 0.8268 - val_loss: 0.6066 - val_acc: 0.8197\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5780 - acc: 0.8286\n",
      "Epoch 00240: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5781 - acc: 0.8285 - val_loss: 0.7472 - val_acc: 0.7533\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5805 - acc: 0.8263\n",
      "Epoch 00241: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5806 - acc: 0.8263 - val_loss: 0.5211 - val_acc: 0.8602\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5850 - acc: 0.8250\n",
      "Epoch 00242: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5849 - acc: 0.8251 - val_loss: 0.6285 - val_acc: 0.8183\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5812 - acc: 0.8263\n",
      "Epoch 00243: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5813 - acc: 0.8263 - val_loss: 1.5543 - val_acc: 0.6124\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5782 - acc: 0.8282\n",
      "Epoch 00244: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5782 - acc: 0.8282 - val_loss: 0.5203 - val_acc: 0.8553\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5746 - acc: 0.8250\n",
      "Epoch 00245: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5746 - acc: 0.8249 - val_loss: 0.5016 - val_acc: 0.8640\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5744 - acc: 0.8289\n",
      "Epoch 00246: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5746 - acc: 0.8289 - val_loss: 5.9990 - val_acc: 0.3007\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5726 - acc: 0.8273\n",
      "Epoch 00247: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5725 - acc: 0.8273 - val_loss: 0.7830 - val_acc: 0.7549\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5747 - acc: 0.8280\n",
      "Epoch 00248: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5747 - acc: 0.8280 - val_loss: 0.6489 - val_acc: 0.7913\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5769 - acc: 0.8289\n",
      "Epoch 00249: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5770 - acc: 0.8289 - val_loss: 3.0406 - val_acc: 0.4512\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5778 - acc: 0.8280\n",
      "Epoch 00250: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5778 - acc: 0.8280 - val_loss: 1.6130 - val_acc: 0.5754\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5747 - acc: 0.8277\n",
      "Epoch 00251: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5748 - acc: 0.8277 - val_loss: 0.5881 - val_acc: 0.8304\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.8283\n",
      "Epoch 00252: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5738 - acc: 0.8283 - val_loss: 1.8804 - val_acc: 0.5537\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.8292\n",
      "Epoch 00253: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5733 - acc: 0.8292 - val_loss: 0.5646 - val_acc: 0.8470\n",
      "Epoch 254/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.8299\n",
      "Epoch 00254: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5679 - acc: 0.8298 - val_loss: 4.8874 - val_acc: 0.3345\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5714 - acc: 0.8294\n",
      "Epoch 00255: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5714 - acc: 0.8294 - val_loss: 1.7504 - val_acc: 0.5618\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.8318\n",
      "Epoch 00256: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5675 - acc: 0.8318 - val_loss: 0.6363 - val_acc: 0.8118\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5745 - acc: 0.8286\n",
      "Epoch 00257: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5745 - acc: 0.8286 - val_loss: 0.5143 - val_acc: 0.8672\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5703 - acc: 0.8294\n",
      "Epoch 00258: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5702 - acc: 0.8294 - val_loss: 0.5359 - val_acc: 0.8474\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5645 - acc: 0.8311\n",
      "Epoch 00259: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5645 - acc: 0.8311 - val_loss: 2.0520 - val_acc: 0.5269\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5717 - acc: 0.8302\n",
      "Epoch 00260: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5717 - acc: 0.8302 - val_loss: 1.3670 - val_acc: 0.6110\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5723 - acc: 0.8272\n",
      "Epoch 00261: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5723 - acc: 0.8272 - val_loss: 2.1407 - val_acc: 0.5283\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.8305\n",
      "Epoch 00262: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5697 - acc: 0.8305 - val_loss: 6.8265 - val_acc: 0.2858\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5627 - acc: 0.8336\n",
      "Epoch 00263: val_loss did not improve from 0.49691\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5627 - acc: 0.8336 - val_loss: 1.2145 - val_acc: 0.6592\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5602 - acc: 0.8319\n",
      "Epoch 00264: val_loss improved from 0.49691 to 0.48568, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/264-0.4857.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5602 - acc: 0.8319 - val_loss: 0.4857 - val_acc: 0.8682\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.8336\n",
      "Epoch 00265: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5619 - acc: 0.8336 - val_loss: 0.5740 - val_acc: 0.8332\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5668 - acc: 0.8319\n",
      "Epoch 00266: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5667 - acc: 0.8319 - val_loss: 0.6173 - val_acc: 0.8053\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5685 - acc: 0.8326\n",
      "Epoch 00267: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5685 - acc: 0.8325 - val_loss: 0.7540 - val_acc: 0.7794\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.8321\n",
      "Epoch 00268: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5578 - acc: 0.8320 - val_loss: 0.5077 - val_acc: 0.8635\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5644 - acc: 0.8310\n",
      "Epoch 00269: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5644 - acc: 0.8311 - val_loss: 0.5899 - val_acc: 0.8358\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.8314\n",
      "Epoch 00270: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5598 - acc: 0.8314 - val_loss: 0.8878 - val_acc: 0.7384\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5621 - acc: 0.8322\n",
      "Epoch 00271: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5621 - acc: 0.8322 - val_loss: 1.2323 - val_acc: 0.6303\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5621 - acc: 0.8311\n",
      "Epoch 00272: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5620 - acc: 0.8312 - val_loss: 0.7981 - val_acc: 0.7566\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5616 - acc: 0.8310\n",
      "Epoch 00273: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5615 - acc: 0.8310 - val_loss: 0.7066 - val_acc: 0.7857\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5581 - acc: 0.8308\n",
      "Epoch 00274: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5581 - acc: 0.8308 - val_loss: 0.5680 - val_acc: 0.8486\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5630 - acc: 0.8326\n",
      "Epoch 00275: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5629 - acc: 0.8326 - val_loss: 0.7455 - val_acc: 0.7813\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.8348\n",
      "Epoch 00276: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5578 - acc: 0.8348 - val_loss: 0.9797 - val_acc: 0.6816\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.8335\n",
      "Epoch 00277: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5563 - acc: 0.8334 - val_loss: 5.7738 - val_acc: 0.2560\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.8352\n",
      "Epoch 00278: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5533 - acc: 0.8352 - val_loss: 0.5546 - val_acc: 0.8470\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.8333\n",
      "Epoch 00279: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5556 - acc: 0.8333 - val_loss: 0.6878 - val_acc: 0.7720\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.8348\n",
      "Epoch 00280: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5552 - acc: 0.8348 - val_loss: 0.5745 - val_acc: 0.8404\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5542 - acc: 0.8346\n",
      "Epoch 00281: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5544 - acc: 0.8346 - val_loss: 0.8851 - val_acc: 0.7300\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.8339\n",
      "Epoch 00282: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5546 - acc: 0.8339 - val_loss: 1.0767 - val_acc: 0.6907\n",
      "Epoch 283/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.8328\n",
      "Epoch 00283: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5483 - acc: 0.8328 - val_loss: 1.0005 - val_acc: 0.7056\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5454 - acc: 0.8383\n",
      "Epoch 00284: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5455 - acc: 0.8383 - val_loss: 0.4990 - val_acc: 0.8605\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.8358\n",
      "Epoch 00285: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5490 - acc: 0.8358 - val_loss: 0.6328 - val_acc: 0.8018\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.8326\n",
      "Epoch 00286: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5502 - acc: 0.8326 - val_loss: 0.5381 - val_acc: 0.8484\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.8344\n",
      "Epoch 00287: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5513 - acc: 0.8343 - val_loss: 0.5798 - val_acc: 0.8237\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.8351\n",
      "Epoch 00288: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5496 - acc: 0.8351 - val_loss: 0.8192 - val_acc: 0.7603\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5515 - acc: 0.8348\n",
      "Epoch 00289: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5516 - acc: 0.8348 - val_loss: 2.2841 - val_acc: 0.5122\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.8344\n",
      "Epoch 00290: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5526 - acc: 0.8345 - val_loss: 1.6076 - val_acc: 0.5966\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5446 - acc: 0.8377\n",
      "Epoch 00291: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5446 - acc: 0.8377 - val_loss: 0.6292 - val_acc: 0.8195\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.8375\n",
      "Epoch 00292: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5425 - acc: 0.8375 - val_loss: 0.5191 - val_acc: 0.8446\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.8366\n",
      "Epoch 00293: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5442 - acc: 0.8365 - val_loss: 0.5397 - val_acc: 0.8465\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.8367\n",
      "Epoch 00294: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5475 - acc: 0.8367 - val_loss: 0.5528 - val_acc: 0.8526\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5471 - acc: 0.8360\n",
      "Epoch 00295: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5471 - acc: 0.8359 - val_loss: 0.8539 - val_acc: 0.7340\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5464 - acc: 0.8365\n",
      "Epoch 00296: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5465 - acc: 0.8364 - val_loss: 0.5738 - val_acc: 0.8435\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.8360\n",
      "Epoch 00297: val_loss did not improve from 0.48568\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5452 - acc: 0.8360 - val_loss: 0.5390 - val_acc: 0.8479\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.8358\n",
      "Epoch 00298: val_loss improved from 0.48568 to 0.48004, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_4_conv_checkpoint/298-0.4800.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5437 - acc: 0.8357 - val_loss: 0.4800 - val_acc: 0.8726\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.8353\n",
      "Epoch 00299: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5459 - acc: 0.8353 - val_loss: 0.4994 - val_acc: 0.8663\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.8383\n",
      "Epoch 00300: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5450 - acc: 0.8382 - val_loss: 1.4882 - val_acc: 0.6408\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.8377\n",
      "Epoch 00301: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5444 - acc: 0.8377 - val_loss: 1.9902 - val_acc: 0.5670\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5461 - acc: 0.8369\n",
      "Epoch 00302: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5461 - acc: 0.8369 - val_loss: 1.0384 - val_acc: 0.7063\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.8369\n",
      "Epoch 00303: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5429 - acc: 0.8368 - val_loss: 0.5851 - val_acc: 0.8171\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.8355\n",
      "Epoch 00304: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5521 - acc: 0.8355 - val_loss: 0.4948 - val_acc: 0.8647\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.8405\n",
      "Epoch 00305: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5372 - acc: 0.8405 - val_loss: 0.6085 - val_acc: 0.8209\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.8385\n",
      "Epoch 00306: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5410 - acc: 0.8384 - val_loss: 0.7821 - val_acc: 0.7717\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.8394\n",
      "Epoch 00307: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5380 - acc: 0.8394 - val_loss: 1.8854 - val_acc: 0.5816\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5345 - acc: 0.8398\n",
      "Epoch 00308: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5344 - acc: 0.8398 - val_loss: 1.1797 - val_acc: 0.6867\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5362 - acc: 0.8372\n",
      "Epoch 00309: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5362 - acc: 0.8372 - val_loss: 0.4915 - val_acc: 0.8654\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8398\n",
      "Epoch 00310: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5324 - acc: 0.8398 - val_loss: 0.5636 - val_acc: 0.8351\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.8386\n",
      "Epoch 00311: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5345 - acc: 0.8386 - val_loss: 0.6051 - val_acc: 0.8272\n",
      "Epoch 312/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8403\n",
      "Epoch 00312: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5318 - acc: 0.8403 - val_loss: 0.5908 - val_acc: 0.8360\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.8410\n",
      "Epoch 00313: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5335 - acc: 0.8410 - val_loss: 1.4560 - val_acc: 0.6138\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.8382\n",
      "Epoch 00314: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5371 - acc: 0.8382 - val_loss: 0.6442 - val_acc: 0.8029\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.8408\n",
      "Epoch 00315: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5291 - acc: 0.8408 - val_loss: 0.6736 - val_acc: 0.7864\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.8411\n",
      "Epoch 00316: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5306 - acc: 0.8411 - val_loss: 0.5473 - val_acc: 0.8546\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.8376\n",
      "Epoch 00317: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5373 - acc: 0.8376 - val_loss: 0.9009 - val_acc: 0.7284\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.8391\n",
      "Epoch 00318: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5325 - acc: 0.8391 - val_loss: 0.5246 - val_acc: 0.8544\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.8421\n",
      "Epoch 00319: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5333 - acc: 0.8421 - val_loss: 0.7659 - val_acc: 0.7682\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.8397\n",
      "Epoch 00320: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5359 - acc: 0.8397 - val_loss: 0.6325 - val_acc: 0.8209\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.8421\n",
      "Epoch 00321: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5278 - acc: 0.8420 - val_loss: 0.5168 - val_acc: 0.8605\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.8404\n",
      "Epoch 00322: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5311 - acc: 0.8404 - val_loss: 0.5636 - val_acc: 0.8428\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.8420\n",
      "Epoch 00323: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5296 - acc: 0.8419 - val_loss: 0.8225 - val_acc: 0.7645\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8378\n",
      "Epoch 00324: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5375 - acc: 0.8378 - val_loss: 0.6124 - val_acc: 0.8192\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.8409\n",
      "Epoch 00325: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5314 - acc: 0.8409 - val_loss: 0.5675 - val_acc: 0.8246\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5330 - acc: 0.8378\n",
      "Epoch 00326: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5332 - acc: 0.8377 - val_loss: 1.3529 - val_acc: 0.6355\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5319 - acc: 0.8403\n",
      "Epoch 00327: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5319 - acc: 0.8403 - val_loss: 1.7249 - val_acc: 0.6035\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.8417\n",
      "Epoch 00328: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5274 - acc: 0.8417 - val_loss: 2.1034 - val_acc: 0.5695\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.8420\n",
      "Epoch 00329: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5252 - acc: 0.8420 - val_loss: 0.7258 - val_acc: 0.7734\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.8434\n",
      "Epoch 00330: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5233 - acc: 0.8433 - val_loss: 0.5536 - val_acc: 0.8453\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.8438\n",
      "Epoch 00331: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5258 - acc: 0.8439 - val_loss: 0.6280 - val_acc: 0.8206\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.8426\n",
      "Epoch 00332: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5213 - acc: 0.8426 - val_loss: 1.3060 - val_acc: 0.6334\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.8440\n",
      "Epoch 00333: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5253 - acc: 0.8440 - val_loss: 1.3498 - val_acc: 0.6434\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.8418\n",
      "Epoch 00334: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5231 - acc: 0.8418 - val_loss: 1.4241 - val_acc: 0.6639\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.8411\n",
      "Epoch 00335: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5241 - acc: 0.8411 - val_loss: 0.9182 - val_acc: 0.7277\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.8434\n",
      "Epoch 00336: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5246 - acc: 0.8433 - val_loss: 0.5748 - val_acc: 0.8265\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.8437\n",
      "Epoch 00337: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5229 - acc: 0.8437 - val_loss: 0.6212 - val_acc: 0.8206\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.8443\n",
      "Epoch 00338: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5195 - acc: 0.8443 - val_loss: 0.6831 - val_acc: 0.7994\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.8440\n",
      "Epoch 00339: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5240 - acc: 0.8440 - val_loss: 0.9423 - val_acc: 0.7051\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.8454\n",
      "Epoch 00340: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5196 - acc: 0.8453 - val_loss: 1.9209 - val_acc: 0.6096\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.8439\n",
      "Epoch 00341: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5242 - acc: 0.8438 - val_loss: 0.6106 - val_acc: 0.8211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.8429\n",
      "Epoch 00342: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5183 - acc: 0.8428 - val_loss: 0.5563 - val_acc: 0.8402\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.8413\n",
      "Epoch 00343: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5199 - acc: 0.8413 - val_loss: 1.5155 - val_acc: 0.6431\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5190 - acc: 0.8444\n",
      "Epoch 00344: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5190 - acc: 0.8444 - val_loss: 3.9507 - val_acc: 0.4321\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.8443\n",
      "Epoch 00345: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5219 - acc: 0.8443 - val_loss: 1.0224 - val_acc: 0.7021\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5161 - acc: 0.8462\n",
      "Epoch 00346: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5164 - acc: 0.8462 - val_loss: 0.6456 - val_acc: 0.7983\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8435\n",
      "Epoch 00347: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5188 - acc: 0.8435 - val_loss: 0.4926 - val_acc: 0.8663\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.8438\n",
      "Epoch 00348: val_loss did not improve from 0.48004\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5157 - acc: 0.8438 - val_loss: 6.8153 - val_acc: 0.2881\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 589us/sample - loss: 0.5613 - acc: 0.8401\n",
      "Loss: 0.5612862356726626 Accuracy: 0.84008306\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1931 - acc: 0.3042\n",
      "Epoch 00001: val_loss improved from inf to 1.92370, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/001-1.9237.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 2.1932 - acc: 0.3042 - val_loss: 1.9237 - val_acc: 0.4677\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7062 - acc: 0.4648\n",
      "Epoch 00002: val_loss improved from 1.92370 to 1.37213, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/002-1.3721.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.7063 - acc: 0.4648 - val_loss: 1.3721 - val_acc: 0.6371\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4939 - acc: 0.5445\n",
      "Epoch 00003: val_loss improved from 1.37213 to 1.21429, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/003-1.2143.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.4940 - acc: 0.5444 - val_loss: 1.2143 - val_acc: 0.6685\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3501 - acc: 0.5936\n",
      "Epoch 00004: val_loss improved from 1.21429 to 1.09434, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/004-1.0943.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.3502 - acc: 0.5936 - val_loss: 1.0943 - val_acc: 0.7207\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2502 - acc: 0.6260\n",
      "Epoch 00005: val_loss improved from 1.09434 to 1.05215, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/005-1.0521.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.2504 - acc: 0.6259 - val_loss: 1.0521 - val_acc: 0.7270\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1713 - acc: 0.6528\n",
      "Epoch 00006: val_loss improved from 1.05215 to 0.99468, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/006-0.9947.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1713 - acc: 0.6528 - val_loss: 0.9947 - val_acc: 0.7261\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1044 - acc: 0.6745\n",
      "Epoch 00007: val_loss improved from 0.99468 to 0.87860, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/007-0.8786.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1044 - acc: 0.6745 - val_loss: 0.8786 - val_acc: 0.7792\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0505 - acc: 0.6883\n",
      "Epoch 00008: val_loss improved from 0.87860 to 0.87769, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/008-0.8777.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0506 - acc: 0.6882 - val_loss: 0.8777 - val_acc: 0.7619\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0093 - acc: 0.7048\n",
      "Epoch 00009: val_loss improved from 0.87769 to 0.79561, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/009-0.7956.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.0093 - acc: 0.7048 - val_loss: 0.7956 - val_acc: 0.7941\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9672 - acc: 0.7153\n",
      "Epoch 00010: val_loss did not improve from 0.79561\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9672 - acc: 0.7153 - val_loss: 0.8784 - val_acc: 0.7454\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9326 - acc: 0.7255\n",
      "Epoch 00011: val_loss did not improve from 0.79561\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.9326 - acc: 0.7255 - val_loss: 0.9541 - val_acc: 0.7030\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9028 - acc: 0.7364\n",
      "Epoch 00012: val_loss improved from 0.79561 to 0.74676, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/012-0.7468.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.9028 - acc: 0.7363 - val_loss: 0.7468 - val_acc: 0.8090\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8733 - acc: 0.7452\n",
      "Epoch 00013: val_loss did not improve from 0.74676\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8733 - acc: 0.7451 - val_loss: 0.7511 - val_acc: 0.8060\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8505 - acc: 0.7518\n",
      "Epoch 00014: val_loss did not improve from 0.74676\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8505 - acc: 0.7518 - val_loss: 1.2588 - val_acc: 0.5926\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8416 - acc: 0.7535\n",
      "Epoch 00015: val_loss improved from 0.74676 to 0.68860, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/015-0.6886.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8415 - acc: 0.7535 - val_loss: 0.6886 - val_acc: 0.8297\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8129 - acc: 0.7617\n",
      "Epoch 00016: val_loss did not improve from 0.68860\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8129 - acc: 0.7617 - val_loss: 1.0909 - val_acc: 0.6555\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7976 - acc: 0.7657\n",
      "Epoch 00017: val_loss improved from 0.68860 to 0.67923, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/017-0.6792.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7975 - acc: 0.7658 - val_loss: 0.6792 - val_acc: 0.8111\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7817 - acc: 0.7736\n",
      "Epoch 00018: val_loss did not improve from 0.67923\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7816 - acc: 0.7736 - val_loss: 0.9742 - val_acc: 0.6855\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7724 - acc: 0.7755\n",
      "Epoch 00019: val_loss did not improve from 0.67923\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7724 - acc: 0.7754 - val_loss: 0.6844 - val_acc: 0.8034\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7546 - acc: 0.7804\n",
      "Epoch 00020: val_loss did not improve from 0.67923\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7545 - acc: 0.7804 - val_loss: 0.7033 - val_acc: 0.7969\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7415 - acc: 0.7843\n",
      "Epoch 00021: val_loss did not improve from 0.67923\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7419 - acc: 0.7843 - val_loss: 0.7350 - val_acc: 0.7880\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7259 - acc: 0.7874\n",
      "Epoch 00022: val_loss improved from 0.67923 to 0.59061, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/022-0.5906.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7262 - acc: 0.7873 - val_loss: 0.5906 - val_acc: 0.8428\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7196 - acc: 0.7891\n",
      "Epoch 00023: val_loss did not improve from 0.59061\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7196 - acc: 0.7891 - val_loss: 0.8314 - val_acc: 0.7403\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7068 - acc: 0.7963\n",
      "Epoch 00024: val_loss did not improve from 0.59061\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7068 - acc: 0.7963 - val_loss: 0.6141 - val_acc: 0.8283\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6968 - acc: 0.7955\n",
      "Epoch 00025: val_loss improved from 0.59061 to 0.57081, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/025-0.5708.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6969 - acc: 0.7954 - val_loss: 0.5708 - val_acc: 0.8463\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6837 - acc: 0.8003\n",
      "Epoch 00026: val_loss did not improve from 0.57081\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6837 - acc: 0.8003 - val_loss: 0.6258 - val_acc: 0.8244\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6736 - acc: 0.8052\n",
      "Epoch 00027: val_loss improved from 0.57081 to 0.55768, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/027-0.5577.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6738 - acc: 0.8051 - val_loss: 0.5577 - val_acc: 0.8516\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6688 - acc: 0.8056\n",
      "Epoch 00028: val_loss did not improve from 0.55768\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6688 - acc: 0.8056 - val_loss: 2.8899 - val_acc: 0.4193\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6512 - acc: 0.8109\n",
      "Epoch 00029: val_loss did not improve from 0.55768\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6511 - acc: 0.8109 - val_loss: 0.6703 - val_acc: 0.8146\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6479 - acc: 0.8138\n",
      "Epoch 00030: val_loss did not improve from 0.55768\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6479 - acc: 0.8138 - val_loss: 0.7889 - val_acc: 0.7594\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6398 - acc: 0.8147\n",
      "Epoch 00031: val_loss improved from 0.55768 to 0.52553, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/031-0.5255.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6399 - acc: 0.8146 - val_loss: 0.5255 - val_acc: 0.8549\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6282 - acc: 0.8179\n",
      "Epoch 00032: val_loss did not improve from 0.52553\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6283 - acc: 0.8179 - val_loss: 0.7831 - val_acc: 0.7598\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6298 - acc: 0.8173\n",
      "Epoch 00033: val_loss did not improve from 0.52553\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6299 - acc: 0.8173 - val_loss: 0.5401 - val_acc: 0.8593\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6210 - acc: 0.8202\n",
      "Epoch 00034: val_loss improved from 0.52553 to 0.52166, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/034-0.5217.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6209 - acc: 0.8203 - val_loss: 0.5217 - val_acc: 0.8663\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6057 - acc: 0.8237\n",
      "Epoch 00035: val_loss did not improve from 0.52166\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6057 - acc: 0.8237 - val_loss: 0.6719 - val_acc: 0.7983\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.8273\n",
      "Epoch 00036: val_loss improved from 0.52166 to 0.49827, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/036-0.4983.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6028 - acc: 0.8273 - val_loss: 0.4983 - val_acc: 0.8754\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5943 - acc: 0.8295\n",
      "Epoch 00037: val_loss did not improve from 0.49827\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5943 - acc: 0.8295 - val_loss: 0.5324 - val_acc: 0.8647\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5836 - acc: 0.8310\n",
      "Epoch 00038: val_loss did not improve from 0.49827\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5836 - acc: 0.8310 - val_loss: 0.6192 - val_acc: 0.8237\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.8329\n",
      "Epoch 00039: val_loss did not improve from 0.49827\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5815 - acc: 0.8329 - val_loss: 0.5647 - val_acc: 0.8386\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5756 - acc: 0.8309\n",
      "Epoch 00040: val_loss did not improve from 0.49827\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5757 - acc: 0.8309 - val_loss: 0.5099 - val_acc: 0.8707\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.8360\n",
      "Epoch 00041: val_loss improved from 0.49827 to 0.47152, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/041-0.4715.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5702 - acc: 0.8359 - val_loss: 0.4715 - val_acc: 0.8793\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.8356\n",
      "Epoch 00042: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5671 - acc: 0.8356 - val_loss: 0.5127 - val_acc: 0.8556\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.8403\n",
      "Epoch 00043: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5547 - acc: 0.8403 - val_loss: 0.6764 - val_acc: 0.7948\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.8437\n",
      "Epoch 00044: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5463 - acc: 0.8436 - val_loss: 0.5035 - val_acc: 0.8614\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.8425\n",
      "Epoch 00045: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5452 - acc: 0.8424 - val_loss: 0.5975 - val_acc: 0.8283\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5393 - acc: 0.8429\n",
      "Epoch 00046: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5393 - acc: 0.8429 - val_loss: 0.4793 - val_acc: 0.8623\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.8453\n",
      "Epoch 00047: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5353 - acc: 0.8452 - val_loss: 0.4984 - val_acc: 0.8675\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.8473\n",
      "Epoch 00048: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5274 - acc: 0.8472 - val_loss: 0.5403 - val_acc: 0.8481\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5236 - acc: 0.8492\n",
      "Epoch 00049: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5236 - acc: 0.8492 - val_loss: 0.4921 - val_acc: 0.8654\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.8503\n",
      "Epoch 00050: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5216 - acc: 0.8503 - val_loss: 0.5125 - val_acc: 0.8579\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.8505\n",
      "Epoch 00051: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5118 - acc: 0.8504 - val_loss: 0.4921 - val_acc: 0.8679\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.8510\n",
      "Epoch 00052: val_loss did not improve from 0.47152\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5139 - acc: 0.8509 - val_loss: 0.6989 - val_acc: 0.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.8540\n",
      "Epoch 00053: val_loss improved from 0.47152 to 0.45965, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/053-0.4597.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5027 - acc: 0.8539 - val_loss: 0.4597 - val_acc: 0.8705\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5002 - acc: 0.8555\n",
      "Epoch 00054: val_loss did not improve from 0.45965\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5002 - acc: 0.8555 - val_loss: 0.4774 - val_acc: 0.8744\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4969 - acc: 0.8563\n",
      "Epoch 00055: val_loss improved from 0.45965 to 0.42151, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/055-0.4215.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4969 - acc: 0.8563 - val_loss: 0.4215 - val_acc: 0.8861\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4888 - acc: 0.8573\n",
      "Epoch 00056: val_loss did not improve from 0.42151\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4888 - acc: 0.8573 - val_loss: 0.5143 - val_acc: 0.8630\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4792 - acc: 0.8613\n",
      "Epoch 00057: val_loss did not improve from 0.42151\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4797 - acc: 0.8613 - val_loss: 0.4580 - val_acc: 0.8775\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4867 - acc: 0.8590\n",
      "Epoch 00058: val_loss did not improve from 0.42151\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4867 - acc: 0.8590 - val_loss: 0.4313 - val_acc: 0.8887\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4795 - acc: 0.8635\n",
      "Epoch 00059: val_loss did not improve from 0.42151\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4798 - acc: 0.8634 - val_loss: 0.4238 - val_acc: 0.8884\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4729 - acc: 0.8632\n",
      "Epoch 00060: val_loss did not improve from 0.42151\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4729 - acc: 0.8633 - val_loss: 0.4442 - val_acc: 0.8873\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.8642\n",
      "Epoch 00061: val_loss did not improve from 0.42151\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4696 - acc: 0.8641 - val_loss: 0.4580 - val_acc: 0.8791\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4616 - acc: 0.8656\n",
      "Epoch 00062: val_loss did not improve from 0.42151\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4618 - acc: 0.8655 - val_loss: 0.4305 - val_acc: 0.8868\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4656 - acc: 0.8639\n",
      "Epoch 00063: val_loss did not improve from 0.42151\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4656 - acc: 0.8638 - val_loss: 0.4318 - val_acc: 0.8877\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.8686\n",
      "Epoch 00064: val_loss improved from 0.42151 to 0.38830, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/064-0.3883.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4547 - acc: 0.8686 - val_loss: 0.3883 - val_acc: 0.9010\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4529 - acc: 0.8692\n",
      "Epoch 00065: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4530 - acc: 0.8692 - val_loss: 0.4613 - val_acc: 0.8798\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.8690\n",
      "Epoch 00066: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4522 - acc: 0.8690 - val_loss: 0.6124 - val_acc: 0.8169\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.8702\n",
      "Epoch 00067: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4439 - acc: 0.8702 - val_loss: 0.4852 - val_acc: 0.8647\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4433 - acc: 0.8718\n",
      "Epoch 00068: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4435 - acc: 0.8718 - val_loss: 0.4145 - val_acc: 0.8896\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.8694\n",
      "Epoch 00069: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4435 - acc: 0.8694 - val_loss: 0.3902 - val_acc: 0.8991\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4360 - acc: 0.8730\n",
      "Epoch 00070: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4361 - acc: 0.8730 - val_loss: 0.5101 - val_acc: 0.8607\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8735\n",
      "Epoch 00071: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4362 - acc: 0.8734 - val_loss: 0.3942 - val_acc: 0.8998\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4301 - acc: 0.8755\n",
      "Epoch 00072: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4303 - acc: 0.8755 - val_loss: 0.4555 - val_acc: 0.8758\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4232 - acc: 0.8764\n",
      "Epoch 00073: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4233 - acc: 0.8764 - val_loss: 0.4197 - val_acc: 0.8959\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4206 - acc: 0.8775\n",
      "Epoch 00074: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4207 - acc: 0.8775 - val_loss: 0.4017 - val_acc: 0.8931\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.8780\n",
      "Epoch 00075: val_loss did not improve from 0.38830\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4205 - acc: 0.8780 - val_loss: 0.9346 - val_acc: 0.7284\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8780\n",
      "Epoch 00076: val_loss improved from 0.38830 to 0.36219, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/076-0.3622.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4154 - acc: 0.8779 - val_loss: 0.3622 - val_acc: 0.9019\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4193 - acc: 0.8776\n",
      "Epoch 00077: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4193 - acc: 0.8776 - val_loss: 0.4066 - val_acc: 0.8840\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8787\n",
      "Epoch 00078: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4126 - acc: 0.8787 - val_loss: 0.4069 - val_acc: 0.8901\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8827\n",
      "Epoch 00079: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4044 - acc: 0.8827 - val_loss: 0.9689 - val_acc: 0.7298\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8801\n",
      "Epoch 00080: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4069 - acc: 0.8801 - val_loss: 0.3787 - val_acc: 0.8998\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4016 - acc: 0.8835\n",
      "Epoch 00081: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4015 - acc: 0.8835 - val_loss: 0.3701 - val_acc: 0.8973\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8837\n",
      "Epoch 00082: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3973 - acc: 0.8837 - val_loss: 0.3746 - val_acc: 0.9043\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8820\n",
      "Epoch 00083: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4010 - acc: 0.8819 - val_loss: 0.5062 - val_acc: 0.8551\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8856\n",
      "Epoch 00084: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3903 - acc: 0.8855 - val_loss: 0.3687 - val_acc: 0.9024\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8863\n",
      "Epoch 00085: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3909 - acc: 0.8863 - val_loss: 0.4899 - val_acc: 0.8633\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.8852\n",
      "Epoch 00086: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3897 - acc: 0.8852 - val_loss: 0.4190 - val_acc: 0.8826\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3811 - acc: 0.8876\n",
      "Epoch 00087: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3812 - acc: 0.8876 - val_loss: 0.5402 - val_acc: 0.8365\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8891\n",
      "Epoch 00088: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3766 - acc: 0.8891 - val_loss: 0.4498 - val_acc: 0.8761\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8896\n",
      "Epoch 00089: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3769 - acc: 0.8896 - val_loss: 0.3826 - val_acc: 0.9066\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8879\n",
      "Epoch 00090: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3816 - acc: 0.8878 - val_loss: 0.3711 - val_acc: 0.9050\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8904\n",
      "Epoch 00091: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3720 - acc: 0.8904 - val_loss: 0.3678 - val_acc: 0.9038\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.8925\n",
      "Epoch 00092: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3695 - acc: 0.8925 - val_loss: 0.4276 - val_acc: 0.8789\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8895\n",
      "Epoch 00093: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3684 - acc: 0.8895 - val_loss: 0.3721 - val_acc: 0.9036\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8928\n",
      "Epoch 00094: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3620 - acc: 0.8928 - val_loss: 0.3865 - val_acc: 0.9010\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3640 - acc: 0.8916\n",
      "Epoch 00095: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3640 - acc: 0.8916 - val_loss: 0.3631 - val_acc: 0.9043\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8960\n",
      "Epoch 00096: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3584 - acc: 0.8960 - val_loss: 0.4148 - val_acc: 0.8863\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8940\n",
      "Epoch 00097: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3608 - acc: 0.8940 - val_loss: 0.4396 - val_acc: 0.8777\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8949\n",
      "Epoch 00098: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3575 - acc: 0.8949 - val_loss: 0.3858 - val_acc: 0.8991\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.8940\n",
      "Epoch 00099: val_loss did not improve from 0.36219\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3563 - acc: 0.8940 - val_loss: 0.4072 - val_acc: 0.8921\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8968\n",
      "Epoch 00100: val_loss improved from 0.36219 to 0.34737, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/100-0.3474.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3507 - acc: 0.8968 - val_loss: 0.3474 - val_acc: 0.9054\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.8971\n",
      "Epoch 00101: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3444 - acc: 0.8971 - val_loss: 0.3895 - val_acc: 0.8935\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3421 - acc: 0.8992\n",
      "Epoch 00102: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3420 - acc: 0.8992 - val_loss: 0.4370 - val_acc: 0.8800\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3460 - acc: 0.8990\n",
      "Epoch 00103: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3460 - acc: 0.8990 - val_loss: 0.3954 - val_acc: 0.8954\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.9002\n",
      "Epoch 00104: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3436 - acc: 0.9002 - val_loss: 0.4182 - val_acc: 0.8919\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.8984\n",
      "Epoch 00105: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3426 - acc: 0.8984 - val_loss: 0.4065 - val_acc: 0.8898\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.9019\n",
      "Epoch 00106: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3290 - acc: 0.9019 - val_loss: 0.3610 - val_acc: 0.9092\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.9001\n",
      "Epoch 00107: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3377 - acc: 0.9000 - val_loss: 0.4011 - val_acc: 0.9008\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.9019\n",
      "Epoch 00108: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3343 - acc: 0.9019 - val_loss: 0.3657 - val_acc: 0.9082\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.9023\n",
      "Epoch 00109: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3308 - acc: 0.9024 - val_loss: 0.5089 - val_acc: 0.8707\n",
      "Epoch 110/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.9046\n",
      "Epoch 00110: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3246 - acc: 0.9046 - val_loss: 0.4241 - val_acc: 0.8933\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.9009\n",
      "Epoch 00111: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3312 - acc: 0.9008 - val_loss: 0.8324 - val_acc: 0.7645\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.9040\n",
      "Epoch 00112: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3249 - acc: 0.9040 - val_loss: 0.3712 - val_acc: 0.9050\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.9040\n",
      "Epoch 00113: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3261 - acc: 0.9040 - val_loss: 0.3912 - val_acc: 0.8961\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.9056\n",
      "Epoch 00114: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3184 - acc: 0.9056 - val_loss: 0.3970 - val_acc: 0.8933\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.9066\n",
      "Epoch 00115: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3165 - acc: 0.9066 - val_loss: 0.4084 - val_acc: 0.8966\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.9061\n",
      "Epoch 00116: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3206 - acc: 0.9060 - val_loss: 0.3817 - val_acc: 0.9026\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.9064\n",
      "Epoch 00117: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3162 - acc: 0.9065 - val_loss: 0.4730 - val_acc: 0.8588\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.9066\n",
      "Epoch 00118: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3131 - acc: 0.9066 - val_loss: 0.3715 - val_acc: 0.9033\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.9064\n",
      "Epoch 00119: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3145 - acc: 0.9064 - val_loss: 0.3612 - val_acc: 0.9106\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9083\n",
      "Epoch 00120: val_loss did not improve from 0.34737\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3116 - acc: 0.9082 - val_loss: 0.4440 - val_acc: 0.8805\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.9063\n",
      "Epoch 00121: val_loss improved from 0.34737 to 0.32840, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/121-0.3284.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3175 - acc: 0.9063 - val_loss: 0.3284 - val_acc: 0.9136\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9091\n",
      "Epoch 00122: val_loss did not improve from 0.32840\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3074 - acc: 0.9091 - val_loss: 0.3497 - val_acc: 0.9089\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9074\n",
      "Epoch 00123: val_loss did not improve from 0.32840\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3072 - acc: 0.9073 - val_loss: 0.4558 - val_acc: 0.8887\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.9103\n",
      "Epoch 00124: val_loss did not improve from 0.32840\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3060 - acc: 0.9103 - val_loss: 0.5615 - val_acc: 0.8446\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.9077\n",
      "Epoch 00125: val_loss did not improve from 0.32840\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3097 - acc: 0.9077 - val_loss: 0.7180 - val_acc: 0.7936\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.9109\n",
      "Epoch 00126: val_loss improved from 0.32840 to 0.32719, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/126-0.3272.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2994 - acc: 0.9109 - val_loss: 0.3272 - val_acc: 0.9203\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9109\n",
      "Epoch 00127: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2957 - acc: 0.9109 - val_loss: 0.4465 - val_acc: 0.8919\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9116\n",
      "Epoch 00128: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2967 - acc: 0.9116 - val_loss: 0.3547 - val_acc: 0.8994\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.9081\n",
      "Epoch 00129: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3055 - acc: 0.9081 - val_loss: 0.4432 - val_acc: 0.8807\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9138\n",
      "Epoch 00130: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2911 - acc: 0.9138 - val_loss: 0.4543 - val_acc: 0.8751\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9132\n",
      "Epoch 00131: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2950 - acc: 0.9132 - val_loss: 0.3780 - val_acc: 0.9003\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9125\n",
      "Epoch 00132: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2908 - acc: 0.9125 - val_loss: 0.3700 - val_acc: 0.9080\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9103\n",
      "Epoch 00133: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2981 - acc: 0.9103 - val_loss: 0.3631 - val_acc: 0.9075\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9148\n",
      "Epoch 00134: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2859 - acc: 0.9148 - val_loss: 0.3320 - val_acc: 0.9117\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9150\n",
      "Epoch 00135: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2830 - acc: 0.9150 - val_loss: 0.3542 - val_acc: 0.9108\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9156\n",
      "Epoch 00136: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2832 - acc: 0.9156 - val_loss: 0.6887 - val_acc: 0.8116\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9170\n",
      "Epoch 00137: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2807 - acc: 0.9170 - val_loss: 0.3921 - val_acc: 0.8917\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9152\n",
      "Epoch 00138: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2844 - acc: 0.9152 - val_loss: 0.4719 - val_acc: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.9157\n",
      "Epoch 00139: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2801 - acc: 0.9157 - val_loss: 0.3957 - val_acc: 0.8961\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9135\n",
      "Epoch 00140: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2837 - acc: 0.9135 - val_loss: 0.3392 - val_acc: 0.9096\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.9186\n",
      "Epoch 00141: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2769 - acc: 0.9186 - val_loss: 0.4479 - val_acc: 0.8777\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9186\n",
      "Epoch 00142: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2714 - acc: 0.9186 - val_loss: 0.4167 - val_acc: 0.8924\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9182\n",
      "Epoch 00143: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2739 - acc: 0.9182 - val_loss: 0.3848 - val_acc: 0.8991\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9169\n",
      "Epoch 00144: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2721 - acc: 0.9169 - val_loss: 0.4493 - val_acc: 0.8747\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9196\n",
      "Epoch 00145: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2706 - acc: 0.9196 - val_loss: 0.3759 - val_acc: 0.9003\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9175\n",
      "Epoch 00146: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2768 - acc: 0.9175 - val_loss: 0.4186 - val_acc: 0.8896\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.9187\n",
      "Epoch 00147: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2693 - acc: 0.9187 - val_loss: 0.3407 - val_acc: 0.9124\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9185\n",
      "Epoch 00148: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2673 - acc: 0.9185 - val_loss: 0.3989 - val_acc: 0.8954\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9188\n",
      "Epoch 00149: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2713 - acc: 0.9188 - val_loss: 0.3390 - val_acc: 0.9171\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9197\n",
      "Epoch 00150: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2650 - acc: 0.9197 - val_loss: 0.3523 - val_acc: 0.9126\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9227\n",
      "Epoch 00151: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2600 - acc: 0.9227 - val_loss: 0.4035 - val_acc: 0.8968\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2624 - acc: 0.9217\n",
      "Epoch 00152: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2623 - acc: 0.9217 - val_loss: 0.5066 - val_acc: 0.8672\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9243\n",
      "Epoch 00153: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2552 - acc: 0.9243 - val_loss: 0.3751 - val_acc: 0.9068\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2556 - acc: 0.9213\n",
      "Epoch 00154: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2557 - acc: 0.9213 - val_loss: 0.3724 - val_acc: 0.8994\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.9227\n",
      "Epoch 00155: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2591 - acc: 0.9226 - val_loss: 0.3719 - val_acc: 0.9082\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9227\n",
      "Epoch 00156: val_loss did not improve from 0.32719\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2559 - acc: 0.9227 - val_loss: 0.3429 - val_acc: 0.9068\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9253\n",
      "Epoch 00157: val_loss improved from 0.32719 to 0.32688, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/157-0.3269.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2529 - acc: 0.9253 - val_loss: 0.3269 - val_acc: 0.9203\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9248\n",
      "Epoch 00158: val_loss did not improve from 0.32688\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2511 - acc: 0.9248 - val_loss: 0.3678 - val_acc: 0.9061\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9244\n",
      "Epoch 00159: val_loss did not improve from 0.32688\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2516 - acc: 0.9244 - val_loss: 0.5616 - val_acc: 0.8432\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9225\n",
      "Epoch 00160: val_loss did not improve from 0.32688\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2539 - acc: 0.9225 - val_loss: 0.3440 - val_acc: 0.9166\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9232\n",
      "Epoch 00161: val_loss did not improve from 0.32688\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2554 - acc: 0.9232 - val_loss: 0.3696 - val_acc: 0.8989\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9270\n",
      "Epoch 00162: val_loss did not improve from 0.32688\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2452 - acc: 0.9270 - val_loss: 0.3581 - val_acc: 0.9082\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9246\n",
      "Epoch 00163: val_loss improved from 0.32688 to 0.32340, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/163-0.3234.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2528 - acc: 0.9246 - val_loss: 0.3234 - val_acc: 0.9185\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9244\n",
      "Epoch 00164: val_loss did not improve from 0.32340\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2458 - acc: 0.9244 - val_loss: 0.3321 - val_acc: 0.9171\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9274\n",
      "Epoch 00165: val_loss did not improve from 0.32340\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2426 - acc: 0.9274 - val_loss: 0.5246 - val_acc: 0.8614\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9255\n",
      "Epoch 00166: val_loss did not improve from 0.32340\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2449 - acc: 0.9255 - val_loss: 0.3383 - val_acc: 0.9124\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9271\n",
      "Epoch 00167: val_loss improved from 0.32340 to 0.32090, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/167-0.3209.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2396 - acc: 0.9271 - val_loss: 0.3209 - val_acc: 0.9159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9254\n",
      "Epoch 00168: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2445 - acc: 0.9254 - val_loss: 0.4004 - val_acc: 0.8901\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9256\n",
      "Epoch 00169: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2410 - acc: 0.9256 - val_loss: 0.4863 - val_acc: 0.8684\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9291\n",
      "Epoch 00170: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2392 - acc: 0.9291 - val_loss: 0.3471 - val_acc: 0.9157\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9283\n",
      "Epoch 00171: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2416 - acc: 0.9284 - val_loss: 0.6241 - val_acc: 0.8328\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9279\n",
      "Epoch 00172: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2394 - acc: 0.9279 - val_loss: 0.3377 - val_acc: 0.9185\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9283\n",
      "Epoch 00173: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2367 - acc: 0.9283 - val_loss: 0.3813 - val_acc: 0.9047\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9280\n",
      "Epoch 00174: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2363 - acc: 0.9280 - val_loss: 0.3460 - val_acc: 0.9147\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9295\n",
      "Epoch 00175: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2345 - acc: 0.9295 - val_loss: 0.3596 - val_acc: 0.9064\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9312\n",
      "Epoch 00176: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2314 - acc: 0.9312 - val_loss: 0.4371 - val_acc: 0.8793\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9295\n",
      "Epoch 00177: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2322 - acc: 0.9294 - val_loss: 0.3375 - val_acc: 0.9152\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9281\n",
      "Epoch 00178: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2340 - acc: 0.9281 - val_loss: 0.3685 - val_acc: 0.9038\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9319\n",
      "Epoch 00179: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2294 - acc: 0.9319 - val_loss: 0.4462 - val_acc: 0.8791\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9293\n",
      "Epoch 00180: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2265 - acc: 0.9293 - val_loss: 0.3983 - val_acc: 0.9005\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9295\n",
      "Epoch 00181: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2296 - acc: 0.9294 - val_loss: 0.3499 - val_acc: 0.9147\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9288\n",
      "Epoch 00182: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2350 - acc: 0.9287 - val_loss: 0.3307 - val_acc: 0.9152\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9336\n",
      "Epoch 00183: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2205 - acc: 0.9337 - val_loss: 0.3218 - val_acc: 0.9150\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9317\n",
      "Epoch 00184: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2282 - acc: 0.9317 - val_loss: 0.3651 - val_acc: 0.9078\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9330\n",
      "Epoch 00185: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2214 - acc: 0.9331 - val_loss: 0.3489 - val_acc: 0.9096\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9348\n",
      "Epoch 00186: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2192 - acc: 0.9347 - val_loss: 1.2809 - val_acc: 0.7191\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9323\n",
      "Epoch 00187: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2240 - acc: 0.9322 - val_loss: 0.3710 - val_acc: 0.9096\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9324\n",
      "Epoch 00188: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2245 - acc: 0.9324 - val_loss: 0.3929 - val_acc: 0.9061\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9336\n",
      "Epoch 00189: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2167 - acc: 0.9336 - val_loss: 0.4002 - val_acc: 0.9019\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9345\n",
      "Epoch 00190: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2163 - acc: 0.9344 - val_loss: 0.3606 - val_acc: 0.9108\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9345\n",
      "Epoch 00191: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2207 - acc: 0.9344 - val_loss: 0.3836 - val_acc: 0.9010\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9320\n",
      "Epoch 00192: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2202 - acc: 0.9320 - val_loss: 0.3826 - val_acc: 0.9099\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9356\n",
      "Epoch 00193: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2150 - acc: 0.9356 - val_loss: 0.3442 - val_acc: 0.9164\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9349\n",
      "Epoch 00194: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2137 - acc: 0.9349 - val_loss: 0.3737 - val_acc: 0.8984\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9359\n",
      "Epoch 00195: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2155 - acc: 0.9359 - val_loss: 0.3283 - val_acc: 0.9203\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9372\n",
      "Epoch 00196: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2080 - acc: 0.9372 - val_loss: 0.4982 - val_acc: 0.8765\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9352\n",
      "Epoch 00197: val_loss did not improve from 0.32090\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2144 - acc: 0.9352 - val_loss: 0.3954 - val_acc: 0.8987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9326\n",
      "Epoch 00198: val_loss improved from 0.32090 to 0.31598, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/198-0.3160.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2188 - acc: 0.9326 - val_loss: 0.3160 - val_acc: 0.9194\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9357\n",
      "Epoch 00199: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2112 - acc: 0.9356 - val_loss: 0.7072 - val_acc: 0.8230\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9339\n",
      "Epoch 00200: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2168 - acc: 0.9339 - val_loss: 0.3266 - val_acc: 0.9157\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9377\n",
      "Epoch 00201: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2027 - acc: 0.9376 - val_loss: 0.3428 - val_acc: 0.9087\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9361\n",
      "Epoch 00202: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2103 - acc: 0.9361 - val_loss: 0.3359 - val_acc: 0.9126\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9371\n",
      "Epoch 00203: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2078 - acc: 0.9371 - val_loss: 0.3966 - val_acc: 0.9029\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9357\n",
      "Epoch 00204: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2113 - acc: 0.9357 - val_loss: 0.3432 - val_acc: 0.9129\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9386\n",
      "Epoch 00205: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2034 - acc: 0.9386 - val_loss: 0.4012 - val_acc: 0.8998\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9389\n",
      "Epoch 00206: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2031 - acc: 0.9389 - val_loss: 0.3963 - val_acc: 0.8966\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9391\n",
      "Epoch 00207: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2012 - acc: 0.9391 - val_loss: 0.3840 - val_acc: 0.9131\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9388\n",
      "Epoch 00208: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2009 - acc: 0.9388 - val_loss: 0.3253 - val_acc: 0.9196\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9381\n",
      "Epoch 00209: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2035 - acc: 0.9381 - val_loss: 0.3500 - val_acc: 0.9129\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9384\n",
      "Epoch 00210: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2022 - acc: 0.9384 - val_loss: 0.4466 - val_acc: 0.8817\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9370\n",
      "Epoch 00211: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2108 - acc: 0.9370 - val_loss: 0.3536 - val_acc: 0.9157\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9397\n",
      "Epoch 00212: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1956 - acc: 0.9397 - val_loss: 0.3396 - val_acc: 0.9159\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9402\n",
      "Epoch 00213: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1955 - acc: 0.9402 - val_loss: 0.3361 - val_acc: 0.9192\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9378\n",
      "Epoch 00214: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2037 - acc: 0.9378 - val_loss: 0.3975 - val_acc: 0.8970\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9413\n",
      "Epoch 00215: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1917 - acc: 0.9413 - val_loss: 0.3215 - val_acc: 0.9234\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9393\n",
      "Epoch 00216: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1982 - acc: 0.9393 - val_loss: 0.4807 - val_acc: 0.8758\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9403\n",
      "Epoch 00217: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1961 - acc: 0.9403 - val_loss: 0.4848 - val_acc: 0.8751\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9423\n",
      "Epoch 00218: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1939 - acc: 0.9423 - val_loss: 0.6766 - val_acc: 0.8176\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9419\n",
      "Epoch 00219: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1930 - acc: 0.9419 - val_loss: 0.3831 - val_acc: 0.9061\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9397\n",
      "Epoch 00220: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1962 - acc: 0.9397 - val_loss: 0.3649 - val_acc: 0.9113\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9422\n",
      "Epoch 00221: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1907 - acc: 0.9422 - val_loss: 0.3415 - val_acc: 0.9175\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9406\n",
      "Epoch 00222: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1934 - acc: 0.9406 - val_loss: 0.3495 - val_acc: 0.9164\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9436\n",
      "Epoch 00223: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1883 - acc: 0.9436 - val_loss: 0.4187 - val_acc: 0.8959\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9433\n",
      "Epoch 00224: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1863 - acc: 0.9433 - val_loss: 0.3718 - val_acc: 0.9117\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9415\n",
      "Epoch 00225: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1889 - acc: 0.9415 - val_loss: 0.3606 - val_acc: 0.9078\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9415\n",
      "Epoch 00226: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1911 - acc: 0.9414 - val_loss: 0.3776 - val_acc: 0.9064\n",
      "Epoch 227/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9419\n",
      "Epoch 00227: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1902 - acc: 0.9418 - val_loss: 0.3559 - val_acc: 0.9110\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9430\n",
      "Epoch 00228: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1915 - acc: 0.9430 - val_loss: 0.4318 - val_acc: 0.8935\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9446\n",
      "Epoch 00229: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1839 - acc: 0.9445 - val_loss: 0.4019 - val_acc: 0.9094\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9434\n",
      "Epoch 00230: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1871 - acc: 0.9434 - val_loss: 0.7456 - val_acc: 0.8181\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9424\n",
      "Epoch 00231: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1855 - acc: 0.9424 - val_loss: 0.4472 - val_acc: 0.8803\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9436\n",
      "Epoch 00232: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1837 - acc: 0.9436 - val_loss: 0.3739 - val_acc: 0.9103\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9430\n",
      "Epoch 00233: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1890 - acc: 0.9430 - val_loss: 0.3313 - val_acc: 0.9171\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9441\n",
      "Epoch 00234: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1825 - acc: 0.9441 - val_loss: 0.3851 - val_acc: 0.9040\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9444\n",
      "Epoch 00235: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1810 - acc: 0.9444 - val_loss: 0.4021 - val_acc: 0.9003\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9427\n",
      "Epoch 00236: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1854 - acc: 0.9426 - val_loss: 0.3740 - val_acc: 0.9075\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9446\n",
      "Epoch 00237: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1815 - acc: 0.9447 - val_loss: 0.3346 - val_acc: 0.9136\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9453\n",
      "Epoch 00238: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1789 - acc: 0.9453 - val_loss: 0.4009 - val_acc: 0.9066\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9445\n",
      "Epoch 00239: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1794 - acc: 0.9445 - val_loss: 0.3404 - val_acc: 0.9178\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9477\n",
      "Epoch 00240: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1733 - acc: 0.9477 - val_loss: 0.3561 - val_acc: 0.9106\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9467\n",
      "Epoch 00241: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1750 - acc: 0.9467 - val_loss: 0.3581 - val_acc: 0.9161\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9450\n",
      "Epoch 00242: val_loss did not improve from 0.31598\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1800 - acc: 0.9450 - val_loss: 0.3500 - val_acc: 0.9152\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9470\n",
      "Epoch 00243: val_loss improved from 0.31598 to 0.30838, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_5_conv_checkpoint/243-0.3084.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1748 - acc: 0.9470 - val_loss: 0.3084 - val_acc: 0.9213\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9456\n",
      "Epoch 00244: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1764 - acc: 0.9456 - val_loss: 0.3451 - val_acc: 0.9157\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9473\n",
      "Epoch 00245: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1725 - acc: 0.9473 - val_loss: 0.3359 - val_acc: 0.9203\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9460\n",
      "Epoch 00246: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1769 - acc: 0.9459 - val_loss: 0.3395 - val_acc: 0.9173\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9473\n",
      "Epoch 00247: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1728 - acc: 0.9472 - val_loss: 0.3748 - val_acc: 0.9126\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9476\n",
      "Epoch 00248: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1717 - acc: 0.9475 - val_loss: 0.3964 - val_acc: 0.9003\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9468\n",
      "Epoch 00249: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1738 - acc: 0.9469 - val_loss: 0.7109 - val_acc: 0.8341\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9488\n",
      "Epoch 00250: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1694 - acc: 0.9488 - val_loss: 0.3452 - val_acc: 0.9166\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9476\n",
      "Epoch 00251: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1712 - acc: 0.9476 - val_loss: 0.4531 - val_acc: 0.8903\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9480\n",
      "Epoch 00252: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1705 - acc: 0.9479 - val_loss: 0.3578 - val_acc: 0.9150\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9446\n",
      "Epoch 00253: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1796 - acc: 0.9447 - val_loss: 0.4837 - val_acc: 0.8735\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9493\n",
      "Epoch 00254: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1644 - acc: 0.9492 - val_loss: 0.4039 - val_acc: 0.9101\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9459\n",
      "Epoch 00255: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1761 - acc: 0.9459 - val_loss: 0.3307 - val_acc: 0.9164\n",
      "Epoch 256/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9487\n",
      "Epoch 00256: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1672 - acc: 0.9487 - val_loss: 0.3968 - val_acc: 0.9017\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9483\n",
      "Epoch 00257: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1697 - acc: 0.9483 - val_loss: 0.3953 - val_acc: 0.9061\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9485\n",
      "Epoch 00258: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1686 - acc: 0.9485 - val_loss: 0.3643 - val_acc: 0.9189\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9482\n",
      "Epoch 00259: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1682 - acc: 0.9482 - val_loss: 0.3961 - val_acc: 0.9040\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9502\n",
      "Epoch 00260: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1640 - acc: 0.9503 - val_loss: 0.4397 - val_acc: 0.8977\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9494\n",
      "Epoch 00261: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1639 - acc: 0.9494 - val_loss: 0.4720 - val_acc: 0.8847\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9504\n",
      "Epoch 00262: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1606 - acc: 0.9504 - val_loss: 0.3920 - val_acc: 0.8987\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9490\n",
      "Epoch 00263: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1638 - acc: 0.9490 - val_loss: 0.3406 - val_acc: 0.9173\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9508\n",
      "Epoch 00264: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1624 - acc: 0.9508 - val_loss: 0.3480 - val_acc: 0.9164\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9508\n",
      "Epoch 00265: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1625 - acc: 0.9508 - val_loss: 0.3451 - val_acc: 0.9164\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9517\n",
      "Epoch 00266: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1576 - acc: 0.9517 - val_loss: 0.3508 - val_acc: 0.9180\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9533\n",
      "Epoch 00267: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1578 - acc: 0.9532 - val_loss: 0.3279 - val_acc: 0.9175\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9510\n",
      "Epoch 00268: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1609 - acc: 0.9510 - val_loss: 0.3542 - val_acc: 0.9171\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9505\n",
      "Epoch 00269: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1632 - acc: 0.9505 - val_loss: 0.4032 - val_acc: 0.9068\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9518\n",
      "Epoch 00270: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1570 - acc: 0.9518 - val_loss: 0.4867 - val_acc: 0.8733\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9511\n",
      "Epoch 00271: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1612 - acc: 0.9511 - val_loss: 0.3378 - val_acc: 0.9159\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9530\n",
      "Epoch 00272: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1520 - acc: 0.9529 - val_loss: 0.4694 - val_acc: 0.8931\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9504\n",
      "Epoch 00273: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1659 - acc: 0.9504 - val_loss: 0.4804 - val_acc: 0.8854\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9505\n",
      "Epoch 00274: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1597 - acc: 0.9505 - val_loss: 0.3263 - val_acc: 0.9185\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9534\n",
      "Epoch 00275: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1524 - acc: 0.9533 - val_loss: 0.5024 - val_acc: 0.8847\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9506\n",
      "Epoch 00276: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1591 - acc: 0.9506 - val_loss: 0.3716 - val_acc: 0.9108\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9539\n",
      "Epoch 00277: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1531 - acc: 0.9539 - val_loss: 0.3418 - val_acc: 0.9201\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9537\n",
      "Epoch 00278: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1548 - acc: 0.9537 - val_loss: 0.3589 - val_acc: 0.9194\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9536\n",
      "Epoch 00279: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1523 - acc: 0.9536 - val_loss: 0.5198 - val_acc: 0.8763\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9531\n",
      "Epoch 00280: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1537 - acc: 0.9531 - val_loss: 0.3318 - val_acc: 0.9189\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9520\n",
      "Epoch 00281: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1554 - acc: 0.9520 - val_loss: 0.3428 - val_acc: 0.9168\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9529\n",
      "Epoch 00282: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1517 - acc: 0.9529 - val_loss: 0.5449 - val_acc: 0.8626\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9538\n",
      "Epoch 00283: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1513 - acc: 0.9538 - val_loss: 0.3607 - val_acc: 0.9152\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9545\n",
      "Epoch 00284: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1487 - acc: 0.9545 - val_loss: 0.3623 - val_acc: 0.9143\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9520\n",
      "Epoch 00285: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1525 - acc: 0.9520 - val_loss: 0.3455 - val_acc: 0.9175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9524\n",
      "Epoch 00286: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1542 - acc: 0.9524 - val_loss: 0.3966 - val_acc: 0.9008\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9554\n",
      "Epoch 00287: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1456 - acc: 0.9554 - val_loss: 0.3298 - val_acc: 0.9166\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9541\n",
      "Epoch 00288: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1508 - acc: 0.9540 - val_loss: 0.3534 - val_acc: 0.9122\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9519\n",
      "Epoch 00289: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1575 - acc: 0.9519 - val_loss: 0.4260 - val_acc: 0.8966\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9549\n",
      "Epoch 00290: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1462 - acc: 0.9550 - val_loss: 0.3352 - val_acc: 0.9222\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9531\n",
      "Epoch 00291: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1498 - acc: 0.9531 - val_loss: 0.4390 - val_acc: 0.9017\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9541\n",
      "Epoch 00292: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1487 - acc: 0.9541 - val_loss: 0.3372 - val_acc: 0.9192\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9543\n",
      "Epoch 00293: val_loss did not improve from 0.30838\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1479 - acc: 0.9543 - val_loss: 0.3957 - val_acc: 0.9078\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 631us/sample - loss: 0.3849 - acc: 0.8914\n",
      "Loss: 0.3848740494511209 Accuracy: 0.8913811\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0035 - acc: 0.3674\n",
      "Epoch 00001: val_loss improved from inf to 1.66865, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/001-1.6686.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 2.0036 - acc: 0.3674 - val_loss: 1.6686 - val_acc: 0.5656\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4070 - acc: 0.5724\n",
      "Epoch 00002: val_loss improved from 1.66865 to 1.08677, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/002-1.0868.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 1.4071 - acc: 0.5723 - val_loss: 1.0868 - val_acc: 0.7191\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1505 - acc: 0.6655\n",
      "Epoch 00003: val_loss did not improve from 1.08677\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 1.1505 - acc: 0.6655 - val_loss: 1.1489 - val_acc: 0.6143\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9830 - acc: 0.7249\n",
      "Epoch 00004: val_loss improved from 1.08677 to 0.77652, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/004-0.7765.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.9830 - acc: 0.7248 - val_loss: 0.7765 - val_acc: 0.8134\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8790 - acc: 0.7557\n",
      "Epoch 00005: val_loss improved from 0.77652 to 0.69619, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/005-0.6962.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.8790 - acc: 0.7557 - val_loss: 0.6962 - val_acc: 0.8369\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7748 - acc: 0.7887\n",
      "Epoch 00006: val_loss improved from 0.69619 to 0.62560, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/006-0.6256.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.7749 - acc: 0.7887 - val_loss: 0.6256 - val_acc: 0.8295\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7092 - acc: 0.8067\n",
      "Epoch 00007: val_loss did not improve from 0.62560\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.7092 - acc: 0.8067 - val_loss: 0.6343 - val_acc: 0.8351\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6537 - acc: 0.8222\n",
      "Epoch 00008: val_loss improved from 0.62560 to 0.53831, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/008-0.5383.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.6536 - acc: 0.8222 - val_loss: 0.5383 - val_acc: 0.8644\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6064 - acc: 0.8342\n",
      "Epoch 00009: val_loss improved from 0.53831 to 0.46548, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/009-0.4655.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.6065 - acc: 0.8342 - val_loss: 0.4655 - val_acc: 0.8891\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5695 - acc: 0.8423\n",
      "Epoch 00010: val_loss improved from 0.46548 to 0.44292, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/010-0.4429.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5696 - acc: 0.8422 - val_loss: 0.4429 - val_acc: 0.8854\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5365 - acc: 0.8504\n",
      "Epoch 00011: val_loss did not improve from 0.44292\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5365 - acc: 0.8504 - val_loss: 0.4710 - val_acc: 0.8714\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.8577\n",
      "Epoch 00012: val_loss improved from 0.44292 to 0.42827, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/012-0.4283.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5080 - acc: 0.8577 - val_loss: 0.4283 - val_acc: 0.8945\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4864 - acc: 0.8638\n",
      "Epoch 00013: val_loss did not improve from 0.42827\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4864 - acc: 0.8638 - val_loss: 0.4435 - val_acc: 0.8852\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8708\n",
      "Epoch 00014: val_loss improved from 0.42827 to 0.37790, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/014-0.3779.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4629 - acc: 0.8708 - val_loss: 0.3779 - val_acc: 0.8989\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4454 - acc: 0.8748\n",
      "Epoch 00015: val_loss improved from 0.37790 to 0.32423, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/015-0.3242.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4457 - acc: 0.8747 - val_loss: 0.3242 - val_acc: 0.9166\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8791\n",
      "Epoch 00016: val_loss did not improve from 0.32423\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4283 - acc: 0.8791 - val_loss: 0.3494 - val_acc: 0.9101\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4163 - acc: 0.8835\n",
      "Epoch 00017: val_loss did not improve from 0.32423\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4164 - acc: 0.8834 - val_loss: 0.3718 - val_acc: 0.8982\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8863\n",
      "Epoch 00018: val_loss did not improve from 0.32423\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3967 - acc: 0.8862 - val_loss: 0.3253 - val_acc: 0.9180\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.8908\n",
      "Epoch 00019: val_loss improved from 0.32423 to 0.29672, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/019-0.2967.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3874 - acc: 0.8908 - val_loss: 0.2967 - val_acc: 0.9294\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3753 - acc: 0.8929\n",
      "Epoch 00020: val_loss did not improve from 0.29672\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3753 - acc: 0.8929 - val_loss: 0.3318 - val_acc: 0.9159\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3607 - acc: 0.8987\n",
      "Epoch 00021: val_loss did not improve from 0.29672\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3607 - acc: 0.8987 - val_loss: 0.3112 - val_acc: 0.9175\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.9002\n",
      "Epoch 00022: val_loss did not improve from 0.29672\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3545 - acc: 0.9002 - val_loss: 0.3239 - val_acc: 0.9103\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.9007\n",
      "Epoch 00023: val_loss improved from 0.29672 to 0.28086, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/023-0.2809.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3463 - acc: 0.9007 - val_loss: 0.2809 - val_acc: 0.9273\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.9034\n",
      "Epoch 00024: val_loss did not improve from 0.28086\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3345 - acc: 0.9034 - val_loss: 0.2842 - val_acc: 0.9299\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.9079\n",
      "Epoch 00025: val_loss improved from 0.28086 to 0.27834, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/025-0.2783.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3266 - acc: 0.9078 - val_loss: 0.2783 - val_acc: 0.9248\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.9075\n",
      "Epoch 00026: val_loss did not improve from 0.27834\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3206 - acc: 0.9075 - val_loss: 0.3172 - val_acc: 0.9143\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9104\n",
      "Epoch 00027: val_loss did not improve from 0.27834\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3106 - acc: 0.9104 - val_loss: 0.2898 - val_acc: 0.9243\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9114\n",
      "Epoch 00028: val_loss did not improve from 0.27834\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3035 - acc: 0.9113 - val_loss: 0.3046 - val_acc: 0.9136\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9133\n",
      "Epoch 00029: val_loss did not improve from 0.27834\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2950 - acc: 0.9132 - val_loss: 0.2920 - val_acc: 0.9238\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.9160\n",
      "Epoch 00030: val_loss did not improve from 0.27834\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2884 - acc: 0.9159 - val_loss: 0.2826 - val_acc: 0.9231\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9180\n",
      "Epoch 00031: val_loss did not improve from 0.27834\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2812 - acc: 0.9180 - val_loss: 0.3159 - val_acc: 0.9138\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9177\n",
      "Epoch 00032: val_loss did not improve from 0.27834\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2810 - acc: 0.9177 - val_loss: 0.2988 - val_acc: 0.9189\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9200\n",
      "Epoch 00033: val_loss did not improve from 0.27834\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2734 - acc: 0.9199 - val_loss: 0.2998 - val_acc: 0.9152\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9246\n",
      "Epoch 00034: val_loss improved from 0.27834 to 0.25478, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/034-0.2548.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2614 - acc: 0.9246 - val_loss: 0.2548 - val_acc: 0.9341\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9245\n",
      "Epoch 00035: val_loss did not improve from 0.25478\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2595 - acc: 0.9245 - val_loss: 0.3076 - val_acc: 0.9143\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9257\n",
      "Epoch 00036: val_loss did not improve from 0.25478\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2538 - acc: 0.9257 - val_loss: 0.2669 - val_acc: 0.9294\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9290\n",
      "Epoch 00037: val_loss did not improve from 0.25478\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2482 - acc: 0.9289 - val_loss: 0.2603 - val_acc: 0.9338\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9282\n",
      "Epoch 00038: val_loss improved from 0.25478 to 0.23590, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/038-0.2359.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2451 - acc: 0.9281 - val_loss: 0.2359 - val_acc: 0.9390\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9301\n",
      "Epoch 00039: val_loss did not improve from 0.23590\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2370 - acc: 0.9301 - val_loss: 0.3065 - val_acc: 0.9087\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9309\n",
      "Epoch 00040: val_loss did not improve from 0.23590\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2360 - acc: 0.9309 - val_loss: 0.2447 - val_acc: 0.9366\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9321\n",
      "Epoch 00041: val_loss did not improve from 0.23590\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2295 - acc: 0.9321 - val_loss: 0.2443 - val_acc: 0.9357\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9312\n",
      "Epoch 00042: val_loss did not improve from 0.23590\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2340 - acc: 0.9312 - val_loss: 0.2374 - val_acc: 0.9350\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9333\n",
      "Epoch 00043: val_loss did not improve from 0.23590\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2267 - acc: 0.9333 - val_loss: 0.2811 - val_acc: 0.9294\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9378\n",
      "Epoch 00044: val_loss did not improve from 0.23590\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2142 - acc: 0.9378 - val_loss: 0.4291 - val_acc: 0.8717\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9360\n",
      "Epoch 00045: val_loss improved from 0.23590 to 0.21581, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/045-0.2158.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2158 - acc: 0.9360 - val_loss: 0.2158 - val_acc: 0.9408\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9395\n",
      "Epoch 00046: val_loss did not improve from 0.21581\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2089 - acc: 0.9395 - val_loss: 0.2345 - val_acc: 0.9376\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9410\n",
      "Epoch 00047: val_loss did not improve from 0.21581\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2029 - acc: 0.9410 - val_loss: 0.2794 - val_acc: 0.9264\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9410\n",
      "Epoch 00048: val_loss did not improve from 0.21581\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2003 - acc: 0.9410 - val_loss: 0.2633 - val_acc: 0.9273\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9415\n",
      "Epoch 00049: val_loss did not improve from 0.21581\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1974 - acc: 0.9415 - val_loss: 0.2207 - val_acc: 0.9427\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9433\n",
      "Epoch 00050: val_loss did not improve from 0.21581\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1929 - acc: 0.9433 - val_loss: 0.2321 - val_acc: 0.9373\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9415\n",
      "Epoch 00051: val_loss improved from 0.21581 to 0.20312, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/051-0.2031.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1952 - acc: 0.9415 - val_loss: 0.2031 - val_acc: 0.9469\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9435\n",
      "Epoch 00052: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1889 - acc: 0.9435 - val_loss: 0.2626 - val_acc: 0.9278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9455\n",
      "Epoch 00053: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1846 - acc: 0.9454 - val_loss: 0.2254 - val_acc: 0.9376\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9440\n",
      "Epoch 00054: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1857 - acc: 0.9440 - val_loss: 0.2125 - val_acc: 0.9434\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9458\n",
      "Epoch 00055: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1831 - acc: 0.9458 - val_loss: 0.2245 - val_acc: 0.9415\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9476\n",
      "Epoch 00056: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1778 - acc: 0.9476 - val_loss: 0.2110 - val_acc: 0.9448\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9476\n",
      "Epoch 00057: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1756 - acc: 0.9476 - val_loss: 0.2268 - val_acc: 0.9434\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9490\n",
      "Epoch 00058: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1736 - acc: 0.9490 - val_loss: 0.2123 - val_acc: 0.9446\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9483\n",
      "Epoch 00059: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1730 - acc: 0.9482 - val_loss: 0.2138 - val_acc: 0.9450\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9475\n",
      "Epoch 00060: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1747 - acc: 0.9475 - val_loss: 0.2834 - val_acc: 0.9231\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9500\n",
      "Epoch 00061: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1660 - acc: 0.9500 - val_loss: 0.2165 - val_acc: 0.9406\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9522\n",
      "Epoch 00062: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1615 - acc: 0.9522 - val_loss: 0.2407 - val_acc: 0.9420\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9535\n",
      "Epoch 00063: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1577 - acc: 0.9534 - val_loss: 0.2281 - val_acc: 0.9399\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9513\n",
      "Epoch 00064: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1664 - acc: 0.9513 - val_loss: 0.2235 - val_acc: 0.9401\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9543\n",
      "Epoch 00065: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1541 - acc: 0.9542 - val_loss: 0.2541 - val_acc: 0.9350\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9547\n",
      "Epoch 00066: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1519 - acc: 0.9547 - val_loss: 0.2423 - val_acc: 0.9336\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9548\n",
      "Epoch 00067: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1507 - acc: 0.9548 - val_loss: 0.2699 - val_acc: 0.9299\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9547\n",
      "Epoch 00068: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1508 - acc: 0.9547 - val_loss: 0.2406 - val_acc: 0.9376\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9561\n",
      "Epoch 00069: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1455 - acc: 0.9561 - val_loss: 0.2518 - val_acc: 0.9385\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9559\n",
      "Epoch 00070: val_loss did not improve from 0.20312\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1462 - acc: 0.9559 - val_loss: 0.2620 - val_acc: 0.9278\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9577\n",
      "Epoch 00071: val_loss improved from 0.20312 to 0.20042, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/071-0.2004.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1422 - acc: 0.9577 - val_loss: 0.2004 - val_acc: 0.9488\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9585\n",
      "Epoch 00072: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1401 - acc: 0.9584 - val_loss: 0.2146 - val_acc: 0.9415\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9570\n",
      "Epoch 00073: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1420 - acc: 0.9570 - val_loss: 0.2038 - val_acc: 0.9464\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9590\n",
      "Epoch 00074: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1389 - acc: 0.9590 - val_loss: 0.2204 - val_acc: 0.9443\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9596\n",
      "Epoch 00075: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1345 - acc: 0.9596 - val_loss: 0.2272 - val_acc: 0.9441\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9602\n",
      "Epoch 00076: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1348 - acc: 0.9601 - val_loss: 0.2129 - val_acc: 0.9439\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9583\n",
      "Epoch 00077: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1348 - acc: 0.9583 - val_loss: 0.2063 - val_acc: 0.9481\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9613\n",
      "Epoch 00078: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1286 - acc: 0.9613 - val_loss: 0.2181 - val_acc: 0.9443\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9603\n",
      "Epoch 00079: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1328 - acc: 0.9603 - val_loss: 0.2025 - val_acc: 0.9518\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9613\n",
      "Epoch 00080: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1262 - acc: 0.9613 - val_loss: 0.2184 - val_acc: 0.9441\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9627\n",
      "Epoch 00081: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1242 - acc: 0.9627 - val_loss: 0.2385 - val_acc: 0.9413\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9635\n",
      "Epoch 00082: val_loss did not improve from 0.20042\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1207 - acc: 0.9635 - val_loss: 0.2229 - val_acc: 0.9432\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9627\n",
      "Epoch 00083: val_loss improved from 0.20042 to 0.19877, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_6_conv_checkpoint/083-0.1988.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1246 - acc: 0.9627 - val_loss: 0.1988 - val_acc: 0.9502\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9648\n",
      "Epoch 00084: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1172 - acc: 0.9648 - val_loss: 0.2412 - val_acc: 0.9376\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9642\n",
      "Epoch 00085: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1220 - acc: 0.9641 - val_loss: 0.2252 - val_acc: 0.9448\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9651\n",
      "Epoch 00086: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1169 - acc: 0.9651 - val_loss: 0.2221 - val_acc: 0.9387\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9636\n",
      "Epoch 00087: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1191 - acc: 0.9636 - val_loss: 0.2554 - val_acc: 0.9345\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9670\n",
      "Epoch 00088: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1089 - acc: 0.9670 - val_loss: 0.2385 - val_acc: 0.9366\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9658\n",
      "Epoch 00089: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1125 - acc: 0.9658 - val_loss: 0.2390 - val_acc: 0.9415\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9657\n",
      "Epoch 00090: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1137 - acc: 0.9657 - val_loss: 0.2117 - val_acc: 0.9432\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9670\n",
      "Epoch 00091: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1094 - acc: 0.9670 - val_loss: 0.2305 - val_acc: 0.9450\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9668\n",
      "Epoch 00092: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1107 - acc: 0.9668 - val_loss: 0.2132 - val_acc: 0.9469\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9677\n",
      "Epoch 00093: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1066 - acc: 0.9677 - val_loss: 0.2737 - val_acc: 0.9343\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9680\n",
      "Epoch 00094: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1058 - acc: 0.9680 - val_loss: 0.2796 - val_acc: 0.9329\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9677\n",
      "Epoch 00095: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1058 - acc: 0.9677 - val_loss: 0.2301 - val_acc: 0.9429\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9684\n",
      "Epoch 00096: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1040 - acc: 0.9684 - val_loss: 0.1988 - val_acc: 0.9485\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9683\n",
      "Epoch 00097: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1013 - acc: 0.9683 - val_loss: 0.2784 - val_acc: 0.9264\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9711\n",
      "Epoch 00098: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0994 - acc: 0.9711 - val_loss: 0.2417 - val_acc: 0.9366\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9679\n",
      "Epoch 00099: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1062 - acc: 0.9679 - val_loss: 0.2207 - val_acc: 0.9462\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9715\n",
      "Epoch 00100: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0972 - acc: 0.9715 - val_loss: 0.2620 - val_acc: 0.9383\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9706\n",
      "Epoch 00101: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0967 - acc: 0.9705 - val_loss: 0.2434 - val_acc: 0.9406\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9717\n",
      "Epoch 00102: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0928 - acc: 0.9717 - val_loss: 0.2154 - val_acc: 0.9481\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9709\n",
      "Epoch 00103: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0947 - acc: 0.9709 - val_loss: 0.2361 - val_acc: 0.9436\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9715\n",
      "Epoch 00104: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0928 - acc: 0.9715 - val_loss: 0.2451 - val_acc: 0.9460\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9722\n",
      "Epoch 00105: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0915 - acc: 0.9722 - val_loss: 0.2237 - val_acc: 0.9462\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9715\n",
      "Epoch 00106: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0939 - acc: 0.9715 - val_loss: 0.2311 - val_acc: 0.9432\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9715\n",
      "Epoch 00107: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0919 - acc: 0.9715 - val_loss: 0.2556 - val_acc: 0.9373\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9728\n",
      "Epoch 00108: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0913 - acc: 0.9728 - val_loss: 0.2186 - val_acc: 0.9485\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9736\n",
      "Epoch 00109: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0912 - acc: 0.9736 - val_loss: 0.2543 - val_acc: 0.9362\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9729\n",
      "Epoch 00110: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0877 - acc: 0.9729 - val_loss: 0.2509 - val_acc: 0.9404\n",
      "Epoch 111/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9734\n",
      "Epoch 00111: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0864 - acc: 0.9733 - val_loss: 0.2171 - val_acc: 0.9441\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9712\n",
      "Epoch 00112: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0922 - acc: 0.9713 - val_loss: 0.2690 - val_acc: 0.9362\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9749\n",
      "Epoch 00113: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0834 - acc: 0.9748 - val_loss: 0.2407 - val_acc: 0.9392\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9748\n",
      "Epoch 00114: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0837 - acc: 0.9748 - val_loss: 0.3193 - val_acc: 0.9220\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9752\n",
      "Epoch 00115: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0804 - acc: 0.9752 - val_loss: 0.2472 - val_acc: 0.9399\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9749\n",
      "Epoch 00116: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0795 - acc: 0.9749 - val_loss: 0.2285 - val_acc: 0.9425\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9754\n",
      "Epoch 00117: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0814 - acc: 0.9754 - val_loss: 0.2500 - val_acc: 0.9411\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9774\n",
      "Epoch 00118: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0763 - acc: 0.9774 - val_loss: 0.2233 - val_acc: 0.9478\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9761\n",
      "Epoch 00119: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0800 - acc: 0.9761 - val_loss: 0.2174 - val_acc: 0.9455\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9770\n",
      "Epoch 00120: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0771 - acc: 0.9770 - val_loss: 0.2595 - val_acc: 0.9422\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9762\n",
      "Epoch 00121: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0781 - acc: 0.9762 - val_loss: 0.3153 - val_acc: 0.9271\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9760\n",
      "Epoch 00122: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0793 - acc: 0.9760 - val_loss: 0.2380 - val_acc: 0.9432\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9776\n",
      "Epoch 00123: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0755 - acc: 0.9776 - val_loss: 0.2122 - val_acc: 0.9483\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9780\n",
      "Epoch 00124: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0746 - acc: 0.9779 - val_loss: 0.2162 - val_acc: 0.9513\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9758\n",
      "Epoch 00125: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0817 - acc: 0.9758 - val_loss: 0.3007 - val_acc: 0.9255\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9787\n",
      "Epoch 00126: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0707 - acc: 0.9787 - val_loss: 0.2052 - val_acc: 0.9497\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9784\n",
      "Epoch 00127: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0747 - acc: 0.9784 - val_loss: 0.2243 - val_acc: 0.9441\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9798\n",
      "Epoch 00128: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0692 - acc: 0.9798 - val_loss: 0.2445 - val_acc: 0.9425\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9798\n",
      "Epoch 00129: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0674 - acc: 0.9798 - val_loss: 0.2306 - val_acc: 0.9434\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9792\n",
      "Epoch 00130: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0707 - acc: 0.9792 - val_loss: 0.2583 - val_acc: 0.9336\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9779\n",
      "Epoch 00131: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0754 - acc: 0.9779 - val_loss: 0.2618 - val_acc: 0.9362\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9796\n",
      "Epoch 00132: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0693 - acc: 0.9796 - val_loss: 0.2486 - val_acc: 0.9436\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9779\n",
      "Epoch 00133: val_loss did not improve from 0.19877\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0723 - acc: 0.9779 - val_loss: 0.2546 - val_acc: 0.9380\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 691us/sample - loss: 0.2411 - acc: 0.9354\n",
      "Loss: 0.24108471978738177 Accuracy: 0.9354102\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8063 - acc: 0.4338\n",
      "Epoch 00001: val_loss improved from inf to 1.38310, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/001-1.3831.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 1.8062 - acc: 0.4338 - val_loss: 1.3831 - val_acc: 0.6348\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0183 - acc: 0.7051\n",
      "Epoch 00002: val_loss improved from 1.38310 to 0.73132, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/002-0.7313.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.0183 - acc: 0.7051 - val_loss: 0.7313 - val_acc: 0.8157\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7316 - acc: 0.7984\n",
      "Epoch 00003: val_loss improved from 0.73132 to 0.51322, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/003-0.5132.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.7317 - acc: 0.7984 - val_loss: 0.5132 - val_acc: 0.8682\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.8423\n",
      "Epoch 00004: val_loss improved from 0.51322 to 0.43379, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/004-0.4338.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5816 - acc: 0.8422 - val_loss: 0.4338 - val_acc: 0.8980\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4905 - acc: 0.8680\n",
      "Epoch 00005: val_loss improved from 0.43379 to 0.36694, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/005-0.3669.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4906 - acc: 0.8680 - val_loss: 0.3669 - val_acc: 0.9066\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4354 - acc: 0.8811\n",
      "Epoch 00006: val_loss improved from 0.36694 to 0.34585, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/006-0.3458.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4355 - acc: 0.8810 - val_loss: 0.3458 - val_acc: 0.9005\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8942\n",
      "Epoch 00007: val_loss improved from 0.34585 to 0.31318, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/007-0.3132.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3845 - acc: 0.8942 - val_loss: 0.3132 - val_acc: 0.9131\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.9044\n",
      "Epoch 00008: val_loss improved from 0.31318 to 0.23745, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/008-0.2375.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3480 - acc: 0.9044 - val_loss: 0.2375 - val_acc: 0.9371\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.9075\n",
      "Epoch 00009: val_loss improved from 0.23745 to 0.22411, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/009-0.2241.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3291 - acc: 0.9075 - val_loss: 0.2241 - val_acc: 0.9411\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9153\n",
      "Epoch 00010: val_loss did not improve from 0.22411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3007 - acc: 0.9153 - val_loss: 0.2421 - val_acc: 0.9308\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9204\n",
      "Epoch 00011: val_loss did not improve from 0.22411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2779 - acc: 0.9204 - val_loss: 0.2290 - val_acc: 0.9352\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9257\n",
      "Epoch 00012: val_loss did not improve from 0.22411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2618 - acc: 0.9257 - val_loss: 0.2345 - val_acc: 0.9329\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9293\n",
      "Epoch 00013: val_loss improved from 0.22411 to 0.20010, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/013-0.2001.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2493 - acc: 0.9293 - val_loss: 0.2001 - val_acc: 0.9411\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9327\n",
      "Epoch 00014: val_loss did not improve from 0.20010\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2341 - acc: 0.9327 - val_loss: 0.2054 - val_acc: 0.9385\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9340\n",
      "Epoch 00015: val_loss improved from 0.20010 to 0.19458, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/015-0.1946.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2275 - acc: 0.9340 - val_loss: 0.1946 - val_acc: 0.9441\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9409\n",
      "Epoch 00016: val_loss improved from 0.19458 to 0.18558, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/016-0.1856.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2103 - acc: 0.9409 - val_loss: 0.1856 - val_acc: 0.9455\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9400\n",
      "Epoch 00017: val_loss improved from 0.18558 to 0.16411, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/017-0.1641.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2055 - acc: 0.9400 - val_loss: 0.1641 - val_acc: 0.9522\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9455\n",
      "Epoch 00018: val_loss did not improve from 0.16411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1934 - acc: 0.9456 - val_loss: 0.1649 - val_acc: 0.9522\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9482\n",
      "Epoch 00019: val_loss did not improve from 0.16411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1802 - acc: 0.9482 - val_loss: 0.1831 - val_acc: 0.9476\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9486\n",
      "Epoch 00020: val_loss did not improve from 0.16411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1786 - acc: 0.9486 - val_loss: 0.1750 - val_acc: 0.9525\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9496\n",
      "Epoch 00021: val_loss did not improve from 0.16411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1716 - acc: 0.9496 - val_loss: 0.1921 - val_acc: 0.9432\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9529\n",
      "Epoch 00022: val_loss improved from 0.16411 to 0.15479, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/022-0.1548.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1634 - acc: 0.9529 - val_loss: 0.1548 - val_acc: 0.9571\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9552\n",
      "Epoch 00023: val_loss did not improve from 0.15479\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1551 - acc: 0.9553 - val_loss: 0.1570 - val_acc: 0.9567\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9565\n",
      "Epoch 00024: val_loss did not improve from 0.15479\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1509 - acc: 0.9565 - val_loss: 0.1716 - val_acc: 0.9522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9591\n",
      "Epoch 00025: val_loss did not improve from 0.15479\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1421 - acc: 0.9591 - val_loss: 0.1627 - val_acc: 0.9529\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9591\n",
      "Epoch 00026: val_loss did not improve from 0.15479\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1398 - acc: 0.9591 - val_loss: 0.1739 - val_acc: 0.9485\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9599\n",
      "Epoch 00027: val_loss did not improve from 0.15479\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1362 - acc: 0.9599 - val_loss: 0.1718 - val_acc: 0.9495\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9608\n",
      "Epoch 00028: val_loss improved from 0.15479 to 0.14583, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/028-0.1458.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1331 - acc: 0.9608 - val_loss: 0.1458 - val_acc: 0.9541\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9617\n",
      "Epoch 00029: val_loss did not improve from 0.14583\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1302 - acc: 0.9617 - val_loss: 0.1697 - val_acc: 0.9495\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9644\n",
      "Epoch 00030: val_loss did not improve from 0.14583\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1230 - acc: 0.9644 - val_loss: 0.2546 - val_acc: 0.9306\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9677\n",
      "Epoch 00031: val_loss did not improve from 0.14583\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1127 - acc: 0.9677 - val_loss: 0.1886 - val_acc: 0.9448\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9674\n",
      "Epoch 00032: val_loss did not improve from 0.14583\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1132 - acc: 0.9675 - val_loss: 0.1599 - val_acc: 0.9555\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9679\n",
      "Epoch 00033: val_loss did not improve from 0.14583\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1080 - acc: 0.9679 - val_loss: 0.1677 - val_acc: 0.9492\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9672\n",
      "Epoch 00034: val_loss did not improve from 0.14583\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1095 - acc: 0.9672 - val_loss: 0.1916 - val_acc: 0.9455\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9701\n",
      "Epoch 00035: val_loss did not improve from 0.14583\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1039 - acc: 0.9701 - val_loss: 0.1513 - val_acc: 0.9555\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9721\n",
      "Epoch 00036: val_loss did not improve from 0.14583\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0968 - acc: 0.9722 - val_loss: 0.1653 - val_acc: 0.9527\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9715\n",
      "Epoch 00037: val_loss did not improve from 0.14583\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0960 - acc: 0.9715 - val_loss: 0.1458 - val_acc: 0.9602\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9714\n",
      "Epoch 00038: val_loss improved from 0.14583 to 0.14233, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/038-0.1423.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0951 - acc: 0.9713 - val_loss: 0.1423 - val_acc: 0.9548\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9711\n",
      "Epoch 00039: val_loss improved from 0.14233 to 0.14126, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/039-0.1413.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0973 - acc: 0.9711 - val_loss: 0.1413 - val_acc: 0.9583\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9743\n",
      "Epoch 00040: val_loss did not improve from 0.14126\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0863 - acc: 0.9743 - val_loss: 0.1477 - val_acc: 0.9609\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9749\n",
      "Epoch 00041: val_loss did not improve from 0.14126\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0853 - acc: 0.9749 - val_loss: 0.1494 - val_acc: 0.9590\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9767\n",
      "Epoch 00042: val_loss did not improve from 0.14126\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0806 - acc: 0.9767 - val_loss: 0.1439 - val_acc: 0.9592\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9765\n",
      "Epoch 00043: val_loss did not improve from 0.14126\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0812 - acc: 0.9765 - val_loss: 0.1803 - val_acc: 0.9497\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9779\n",
      "Epoch 00044: val_loss did not improve from 0.14126\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0758 - acc: 0.9779 - val_loss: 0.1428 - val_acc: 0.9574\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9774\n",
      "Epoch 00045: val_loss improved from 0.14126 to 0.13194, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_7_conv_checkpoint/045-0.1319.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0750 - acc: 0.9774 - val_loss: 0.1319 - val_acc: 0.9611\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9785\n",
      "Epoch 00046: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0740 - acc: 0.9784 - val_loss: 0.1471 - val_acc: 0.9597\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9771\n",
      "Epoch 00047: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0755 - acc: 0.9771 - val_loss: 0.1700 - val_acc: 0.9541\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9802\n",
      "Epoch 00048: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0675 - acc: 0.9802 - val_loss: 0.1343 - val_acc: 0.9595\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9820\n",
      "Epoch 00049: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0631 - acc: 0.9820 - val_loss: 0.1918 - val_acc: 0.9485\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9805\n",
      "Epoch 00050: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0681 - acc: 0.9805 - val_loss: 0.1528 - val_acc: 0.9611\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9831\n",
      "Epoch 00051: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0602 - acc: 0.9830 - val_loss: 0.1737 - val_acc: 0.9550\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9796\n",
      "Epoch 00052: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0680 - acc: 0.9796 - val_loss: 0.1858 - val_acc: 0.9532\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9823\n",
      "Epoch 00053: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0588 - acc: 0.9823 - val_loss: 0.1664 - val_acc: 0.9534\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9795\n",
      "Epoch 00054: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0671 - acc: 0.9795 - val_loss: 0.2134 - val_acc: 0.9464\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9849\n",
      "Epoch 00055: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0540 - acc: 0.9849 - val_loss: 0.1771 - val_acc: 0.9471\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9836\n",
      "Epoch 00056: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0545 - acc: 0.9836 - val_loss: 0.1609 - val_acc: 0.9560\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9845\n",
      "Epoch 00057: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0536 - acc: 0.9845 - val_loss: 0.1757 - val_acc: 0.9555\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9860\n",
      "Epoch 00058: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0506 - acc: 0.9860 - val_loss: 0.1581 - val_acc: 0.9606\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9836\n",
      "Epoch 00059: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0564 - acc: 0.9835 - val_loss: 0.1639 - val_acc: 0.9576\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9862\n",
      "Epoch 00060: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0483 - acc: 0.9862 - val_loss: 0.2411 - val_acc: 0.9406\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9870\n",
      "Epoch 00061: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0457 - acc: 0.9870 - val_loss: 0.1616 - val_acc: 0.9606\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9846\n",
      "Epoch 00062: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0531 - acc: 0.9846 - val_loss: 0.1683 - val_acc: 0.9597\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9835\n",
      "Epoch 00063: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0571 - acc: 0.9835 - val_loss: 0.2765 - val_acc: 0.9345\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9892\n",
      "Epoch 00064: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0411 - acc: 0.9892 - val_loss: 0.1624 - val_acc: 0.9595\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9853\n",
      "Epoch 00065: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0509 - acc: 0.9853 - val_loss: 0.1735 - val_acc: 0.9576\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9880\n",
      "Epoch 00066: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0441 - acc: 0.9879 - val_loss: 0.2046 - val_acc: 0.9490\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9842\n",
      "Epoch 00067: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0542 - acc: 0.9842 - val_loss: 0.1481 - val_acc: 0.9627\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9877\n",
      "Epoch 00068: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0437 - acc: 0.9877 - val_loss: 0.1565 - val_acc: 0.9599\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9873\n",
      "Epoch 00069: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0435 - acc: 0.9873 - val_loss: 0.1363 - val_acc: 0.9655\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9896\n",
      "Epoch 00070: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0369 - acc: 0.9896 - val_loss: 0.1498 - val_acc: 0.9595\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9864\n",
      "Epoch 00071: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0470 - acc: 0.9864 - val_loss: 0.1418 - val_acc: 0.9634\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9907\n",
      "Epoch 00072: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0360 - acc: 0.9907 - val_loss: 0.1626 - val_acc: 0.9564\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9905\n",
      "Epoch 00073: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0364 - acc: 0.9905 - val_loss: 0.1926 - val_acc: 0.9543\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9886\n",
      "Epoch 00074: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0388 - acc: 0.9886 - val_loss: 0.1567 - val_acc: 0.9604\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9907\n",
      "Epoch 00075: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0343 - acc: 0.9907 - val_loss: 0.1668 - val_acc: 0.9576\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9898\n",
      "Epoch 00076: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0352 - acc: 0.9897 - val_loss: 0.1780 - val_acc: 0.9599\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9883\n",
      "Epoch 00077: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0415 - acc: 0.9882 - val_loss: 0.2174 - val_acc: 0.9522\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9884\n",
      "Epoch 00078: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0399 - acc: 0.9884 - val_loss: 0.1955 - val_acc: 0.9536\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9906\n",
      "Epoch 00079: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0331 - acc: 0.9906 - val_loss: 0.1639 - val_acc: 0.9597\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9912\n",
      "Epoch 00080: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0326 - acc: 0.9912 - val_loss: 0.1782 - val_acc: 0.9576\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9920\n",
      "Epoch 00081: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0303 - acc: 0.9920 - val_loss: 0.1834 - val_acc: 0.9562\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9905\n",
      "Epoch 00082: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0341 - acc: 0.9905 - val_loss: 0.1609 - val_acc: 0.9625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9920\n",
      "Epoch 00083: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0299 - acc: 0.9920 - val_loss: 0.2339 - val_acc: 0.9450\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9916\n",
      "Epoch 00084: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0299 - acc: 0.9916 - val_loss: 0.1714 - val_acc: 0.9606\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9889\n",
      "Epoch 00085: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0377 - acc: 0.9889 - val_loss: 0.1547 - val_acc: 0.9606\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9914\n",
      "Epoch 00086: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0300 - acc: 0.9914 - val_loss: 0.1850 - val_acc: 0.9578\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9914\n",
      "Epoch 00087: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0297 - acc: 0.9914 - val_loss: 0.1774 - val_acc: 0.9597\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9922\n",
      "Epoch 00088: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0279 - acc: 0.9922 - val_loss: 0.1522 - val_acc: 0.9632\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9910\n",
      "Epoch 00089: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0321 - acc: 0.9910 - val_loss: 0.1735 - val_acc: 0.9588\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9933\n",
      "Epoch 00090: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0266 - acc: 0.9933 - val_loss: 0.1715 - val_acc: 0.9569\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9923\n",
      "Epoch 00091: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0267 - acc: 0.9923 - val_loss: 0.1742 - val_acc: 0.9546\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9917\n",
      "Epoch 00092: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0298 - acc: 0.9917 - val_loss: 0.1639 - val_acc: 0.9641\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9910\n",
      "Epoch 00093: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0326 - acc: 0.9910 - val_loss: 0.1445 - val_acc: 0.9646\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9933\n",
      "Epoch 00094: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0252 - acc: 0.9933 - val_loss: 0.1657 - val_acc: 0.9613\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9933\n",
      "Epoch 00095: val_loss did not improve from 0.13194\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0253 - acc: 0.9933 - val_loss: 0.1801 - val_acc: 0.9560\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPmclknewJCYQdUXaChM0NLe4LUje0LnWpdrFaax+forZqbevjY+1Ta6v1hxW3qmhFXOqCIiAuoOyLsu8JkD0hy2SZme/vjzOThSyEkCGBfN+v10DmLueee2fmfO8599xzjYiglFJKHYqjszOglFLq2KABQymlVJtowFBKKdUmGjCUUkq1iQYMpZRSbaIBQymlVJtowFBKKdUmGjCUUkq1iQYMpZRSbRLW2RnoSCkpKdK/f//OzoZSSh0zVqxYUSAiqW1Z9rgKGP3792f58uWdnQ2llDpmGGN2tXVZbZJSSinVJhowlFJKtYkGDKWUUm1yXF3DaE5tbS3Z2dlUVVV1dlaOSZGRkfTu3RuXy9XZWVFKdbLjPmBkZ2cTGxtL//79McZ0dnaOKSJCYWEh2dnZDBgwoLOzo5TqZMd9k1RVVRXJyckaLNrBGENycrLWzpRSQDcIGIAGiyOgx04pFdQtAsahVFfvxest7exsKKVUl6YBA6ip2Y/XeyAkaZeUlPD000+3a90LL7yQkpKSNi//0EMP8fjjj7drW0opdSgaMABjHIA/JGm3FjC8Xm+r637wwQckJCSEIltKKXXYNGAA4EAkNAFjxowZbNu2jczMTO655x4WLVrE6aefztSpUxk2bBgA06ZNY+zYsQwfPpyZM2fWrdu/f38KCgrYuXMnQ4cO5dZbb2X48OGce+65eDyeVre7evVqJk6cyKhRo/j+979PcXExAE8++STDhg1j1KhRXH311QB89tlnZGZmkpmZyZgxYygrKwvJsVBKHdtC1q3WGDMLuBjIE5ERzcy/B7i2QT6GAqkiUmSM2QmUAT7AKyJZHZGnLVvuorx8dZPpfn8F4MDhiDrsNN3uTAYPfqLF+Y8++ijr169n9Wq73UWLFrFy5UrWr19f11V11qxZJCUl4fF4GDduHJdffjnJyckH5X0Lr732Gs8++yxXXXUVc+bM4brrrmtxuzfccAN/+9vfmDx5Mg888AC/+93veOKJJ3j00UfZsWMHERERdc1djz/+OE899RSnnnoq5eXlREZGHvZxUEod/0JZw3gBOL+lmSLyJxHJFJFM4F7gMxEparDIWYH5HRIsWnd0ewKNHz++0X0NTz75JKNHj2bixIns2bOHLVu2NFlnwIABZGZmAjB27Fh27tzZYvqlpaWUlJQwefJkAH74wx+yePFiAEaNGsW1117Lv/71L8LC7PnCqaeeyt13382TTz5JSUlJ3XSllGooZCWDiCw2xvRv4+LXAK+FKi9BLdUEKio2YowhOvqkUGcBgJiYmLq/Fy1axPz581myZAnR0dGceeaZzd73EBERUfe30+k8ZJNUS95//30WL17Me++9xx//+EfWrVvHjBkzuOiii/jggw849dRTmTdvHkOGDGlX+kqp41enX8MwxkRjayJzGkwW4GNjzApjzG2HWP82Y8xyY8zy/Pz8duYhdNcwYmNjW70mUFpaSmJiItHR0WzcuJGlS5ce8Tbj4+NJTEzk888/B+Dll19m8uTJ+P1+9uzZw1lnncX//u//UlpaSnl5Odu2bWPkyJH8+te/Zty4cWzcuPGI86CUOv50hbaHS4AvD2qOOk1EcowxPYBPjDEbRWRxcyuLyExgJkBWVpa0LwsOoLZ9qx5CcnIyp556KiNGjOCCCy7goosuajT//PPP55lnnmHo0KGcdNJJTJw4sUO2++KLL/KTn/yEyspKBg4cyPPPP4/P5+O6666jtLQUEeHOO+8kISGB3/72tyxcuBCHw8Hw4cO54IILOiQPSqnjixFpZxnblsRtk9R/mrvo3WCZucC/ReTVFuY/BJSLyCFvMMjKypKDH6C0YcMGhg4d2up6Hs82fD4PbneL2ezW2nIMlVLHJmPMirZeK+7UJiljTDwwGXinwbQYY0xs8G/gXGB9aHPiJFT3YSil1PEilN1qXwPOBFKMMdnAg4ALQESeCSz2feBjEalosGoaMDcwhlEY8KqIfBSqfAbyGrJrGEopdbwIZS+pa9qwzAvY7rcNp20HRocmVy0J3Z3eSil1vOj0XlJdQXBokFBez1FKqWOdBgyg/jBowFBKqZZowCBYw0CvYyilVCs0YAD1h6FrBAy3231Y05VS6mjQgIHWMJRSqi00YAChrGHMmDGDp556qu598CFH5eXlTJkyhZNPPpmRI0fyzjvvtJJKYyLCPffcw4gRIxg5ciSvv/46APv27eOMM84gMzOTESNG8Pnnn+Pz+bjxxhvrlv3LX/7S4fuolOoeusLQIEfPXXfB6qbDm4eJlyi/B4cjGozz8NLMzIQnWh7efPr06dx1113cfvvtALzxxhvMmzePyMhI5s6dS1xcHAUFBUycOJGpU6e26Rnab731FqtXr2bNmjUUFBQwbtw4zjjjDF599VXOO+887r//fnw+H5WVlaxevZqcnBzWr7f3Ph7OE/yUUqqh7hUwWhS64c3HjBlDXl4ee/fuJT8/n8TERPr06UNtbS333XcfixcvxuFwkJOTQ25uLunp6YdM84svvuCaa67B6XSSlpbG5MmTWbZsGePGjePmm2+mtraWadOmkZmZycCBA9m+fTt33HEHF110Eeeee27I9lUpdXzrXgGjhZqA31eBp3IDUVEnEBbW8Y9EvfLKK3nzzTfZv38/06dPB+CVV14hPz+fFStW4HK56N+/f7PDmh+OM844g8WLF/P+++9z4403cvfdd3PDDTewZs0a5s2bxzPPPMMbb7zBrFmzOmK3lFLdjF7DAII1jFBd9J4+fTqzZ8/mzTff5MorrwTssOY9evTA5XKxcOFCdu3a1eb0Tj/9dF5//XV8Ph/5+fksXryY8ePHs2vXLtLS0rj11lv50Y9+xMqVKykoKMDv93P55Zfzhz/8gZUrV4ZkH5VSx7/uVcNoQbCXVKi61Q4fPpyysjIyMjLo2bMnANdeey2XXHIJI0eOJCsr67AeWPT973+fJUuWMHr0aIwxPPbYY6Snp/Piiy/ypz/9CZfLhdvt5qWXXiInJ4ebbroJv9/u2//8z/+EZB+VUse/kA5vfrS1d3hzv7+Gioq1RET0JTy8RyizeEzS4c2VOn4dM8Obdx06NIhSSh2KBgz0xj2llGoLDRhAfbdaDRhKKdUSDRgQuFnOoTUMpZRqhQaMgOAzMZRSSjVPA0YdrWEopVRrNGAEhKqGUVJSwtNPP92udS+88EId+0kp1WWELGAYY2YZY/KMMetbmH+mMabUGLM68HqgwbzzjTGbjDFbjTEzQpXHxkJTw2gtYHi93lbX/eCDD0hI6PihSpRSqj1CWcN4ATj/EMt8LiKZgdfDAMYYJ/AUcAEwDLjGGDMshPkMCE0NY8aMGWzbto3MzEzuueceFi1axOmnn87UqVMZNszu1rRp0xg7dizDhw9n5syZdev279+fgoICdu7cydChQ7n11lsZPnw45557Lh6Pp8m23nvvPSZMmMCYMWM4++yzyc3NBaC8vJybbrqJkSNHMmrUKObMmQPARx99xMknn8zo0aOZMmVKh++7Uur4ErKhQURksTGmfztWHQ9sFZHtAMaY2cClwHdHmqcWRjcHwO/vi4jg7NjRzXn00UdZv349qwMbXrRoEStXrmT9+vUMGDAAgFmzZpGUlITH42HcuHFcfvnlJCcnN0pny5YtvPbaazz77LNcddVVzJkzh+uuu67RMqeddhpLly7FGMM///lPHnvsMf785z/z+9//nvj4eNatWwdAcXEx+fn53HrrrSxevJgBAwZQVFR0eDuulOp2OnssqUnGmDXAXuC/RORbIAPY02CZbGBCSwkYY24DbgPo27fvEWTFcLTu9B4/fnxdsAB48sknmTt3LgB79uxhy5YtTQLGgAEDyMzMBGDs2LHs3LmzSbrZ2dlMnz6dffv2UVNTU7eN+fPnM3v27LrlEhMTee+99zjjjDPqlklKSurQfVRKHX86M2CsBPqJSLkx5kLgbWDw4SYiIjOBmWDHkmpt2dZqAh7PXnw+D273iMPNwmGLiYmp+3vRokXMnz+fJUuWEB0dzZlnntnsMOcRERF1fzudzmabpO644w7uvvtupk6dyqJFi3jooYdCkn+lVPfUab2kROSAiJQH/v4AcBljUoAcoE+DRXsHpoWYk1Bcw4iNjaWsrKzF+aWlpSQmJhIdHc3GjRtZunRpu7dVWlpKRkYGAC+++GLd9HPOOafRY2KLi4uZOHEiixcvZseOHQDaJKWUOqROCxjGmHQTeB6pMWZ8IC+FwDJgsDFmgDEmHLgaePco5CckvaSSk5M59dRTGTFiBPfcc0+T+eeffz5er5ehQ4cyY8YMJk6c2O5tPfTQQ1x55ZWMHTuWlJSUuum/+c1vKC4uZsSIEYwePZqFCxeSmprKzJkzueyyyxg9enTdg52UUqolIRve3BjzGnAmkALkAg8CLgARecYY83Pgp4AX8AB3i8hXgXUvBJ7AnvbPEpE/tmWb7R3eHKCqag+1tfnExp7cpv3rTnR4c6WOX4czvHkoe0ldc4j5fwf+3sK8D4APQpGvlgRv3BORwNhSSimlGtI7vevoMzGUUqo1GjAC9JkYSinVOg0YdUL7XG+llDrWacAI0BqGUkq1TgNGHa1hKKVUazRgBHSlGobb7e7sLCilVBOdPZZU15CdjYl2Bu4S6fyAoZRSXZHWMADy8jDlwbGZOjZgzJgxo9GwHA899BCPP/445eXlTJkyhZNPPpmRI0fyzjvvHDKtloZBb26Y8paGNFdKqfbqVjWMuz66i9X7mxnfvLwcwpz4XF4cjiiMafthyUzP5InzWx7VcPr06dx1113cfvvtALzxxhvMmzePyMhI5s6dS1xcHAUFBUycOJGpU6e2etNgc8Og+/3+Zocpb25Ic6WUOhLdKmC0yJgG9+t17I17Y8aMIS8vj71795Kfn09iYiJ9+vShtraW++67j8WLF+NwOMjJySE3N5f09PQW02puGPT8/PxmhylvbkhzpZQ6Et0qYLRYE/j2WyTcRXn6ASIi+hIe3qNDt3vllVfy5ptvsn///rpB/l555RXy8/NZsWIFLpeL/v37NzuseVBbh0FXSqlQ0WsYAE4n+IM1i44fGmT69OnMnj2bN998kyuvvBKwQ5H36NEDl8vFwoUL2bVrV6tptDQMekvDlDc3pLlSSh0JDRgADgf47cXuUHSrHT58OGVlZWRkZNCzZ08Arr32WpYvX87IkSN56aWXGDJkSKtptDQMekvDlDc3pLlSSh2JkA1v3hnaPbz51q1IdTXlfT2Eh/ckIiIjhLk89ujw5kodvw5neHOtYQA4nRi/H3B0iRv3lFKqK9KAAbZJyuereyaGUkqpprpFwDhks1vdNQytYRzseGqyVEodmeM+YERGRlJYWNh6wed0gt+PQWsYDYkIhYWFREZGdnZWlFJdQMjuwzDGzAIuBvJEZEQz868Ffg0YoAz4qYisCczbGZjmA7xtvSDTnN69e5OdnU1+fn7LCx04AMXF1GxxgSOM8PCa9m7uuBMZGUnv3r07OxtKqS4glDfuvYB9ZvdLLczfAUwWkWJjzAXATGBCg/lniUjBkWbC5XLV3QXdon/8A372M9bOG4e/RyxDh356pJtVSqnjTsiapERkMVDUyvyvRCR4N9lSoPNOYwPDibtqXPj9nkMsrJRS3VNXuYZxC/Bhg/cCfGyMWWGMuS3kW4+JASCsyoXPVxnyzSml1LGo08eSMsachQ0YpzWYfJqI5BhjegCfGGM2Bmosza1/G3AbQN++fduXiUANw1kVpjUMpZRqQafWMIwxo4B/ApeKSGFwuojkBP7PA+YC41tKQ0RmikiWiGSlpqa2LyPBGka1E79faxhKKdWcTgsYxpi+wFvA9SKyucH0GGNMbPBv4FxgfUgzE6hhhFU58Pm0hqGUUs0JZbfa14AzgRRjTDbwIIGHoIrIM8ADQDLwdOChQcHus2nA3MC0MOBVEfkoVPkE6moYjiqjNQyllGpByAKGiFxziPk/An7UzPTtwOhQ5atZwRqGx+D3exCRVp98p5RS3VFX6SXVuepqGPZucL9fH0yklFIH04ABEB0NgNMTDBh6HUMppQ6mAQPsWFJRUTg9dhwpvRdDKaWa0oARFBODo9IHaA1DKaWaowEjyO3G4fECaE8ppZRqhgaMoJgYTGUtgN6LoZRSzdCAEeR24wgEDK1hKKVUUxowgmJiMB77HAy9hqGUUk1pwAhyuzEV1YD2klJKqeZowAiKicFU2hv2tIahlFJNacAIcrsxFTZQaA1DKaWa0oARFBMDFTZQaA1DKaWa0oAR5HZDeQWI9pJSSqnmaMAIionB+P04ao3eh6GUUs3QgBFU91zvKK1hKKVUMzRgBAWeiRFeG6HXMJRSqhkaMILqahgR2iSllFLN0IARFHzqXnW4NkkppVQzNGAEBWoYrppwbZJSSqlmhDRgGGNmGWPyjDHrW5hvjDFPGmO2GmPWGmNObjDvh8aYLYHXD0OZT6C+hlEVpjfuKaVUM0Jdw3gBOL+V+RcAgwOv24B/ABhjkoAHgQnAeOBBY0xiSHMavIZRHYbPVxHSTSml1LEopAFDRBYDRa0scinwklhLgQRjTE/gPOATESkSkWLgE1oPPEcuUMNwVUdTW5sX0k0ppdSxKKyTt58B7GnwPjswraXpoROoYYTXxlBdvQYRwRgT0k0qpTqPzwc1NVBbC9XV9u+aGggPh9jYunNIKiqgrAw8HnA6ISzMLpOUZP9uqKrKLi9i3/v99Wl7vRAZaYua6GibZn4+FBTUz4uKsv+7XHYbDV8uF5SW2uXz8+22gsLDYcqU0B+zzg4YR8wYcxu2OYu+ffu2P6G6JqlIRKrxeotxuZI6IotK1fH7bUHlctVPE7GFTEEBJCZCfHzjdbxeW6hFRdVP8/lg82bYsMG+d7vtV1jEFiRVVVBZCeXltmCqrYXkZEhNtQVdSQns3w+5uXbbwUItLMwuk5pql4+Ksi+HA779FlatgjVrbIE3cKB9RUTYdHJz7Xb79oX+/aFnT8jLg927Yc8emwen074qK6Gw0O5zbS306gW9e9v937kTtmyBbdtsnhwO+4qOhoQE+woLg6Ii+zpwwOYhmFe32xb4sbF2ORF73D0eu83CQrv/fv+hPy9j6gv/5uYlJ0Namt3vvDx7rDtDWpr9PEOtswNGDtCnwfvegWk5wJkHTV/UXAIiMhOYCZCVldXCR9sGgRDuqg4HoLp6rwaM41htLezbB8XF9WeZVVX2DK6kxBZCIragMsbOKyuzBXBtrS3wXS47z+Ox82tqbIEVPIMsLobsbMjJsWeEJSU2fb/fFnCxsfZssrDQphEUF2cLXbCFcEGBzUtcHKSn27Q3bWq8zpFwOGx+IiJs4Ggt3YQEGD3aFvjvvmsLSbDHIj3dpjF3rj0WQU4nZGTYeT6ffUVFQUoKDBpkC/W9e2HRIhsA+vWDE0+ECy+0ywXX8XjsMSwpsUF06FAb/GJj7fY8nvogGQyU1dX1n6HbbdNOTrb7ETyTd7nq99/lsmmVldmX32+Pe2yszYvfb7ddXW0/l2DQjYqCHj1soHW77faCr4gIW7w4nfWBvLLSLpeaao9DeHj996iqyn7HamrsdoJ/19TYvKSk2PWio+uPccMTkFBqU8AwxvwCeB4oA/4JjAFmiMjHR7j9d4GfG2NmYy9wl4rIPmPMPOCRBhe6zwXuPcJtHZrbTViVPSQ1NXuBESHfZHclYgvP7Gx79pmdbc90g2eTYH+YPp/9kQar6E6nLWD37bOFTEFB/ZlmZWV91d3lsssaU59msOAoKrI/9JbOHFvjdtu0a2tt/vz++jNbl8v+2MvL7Y8/Pt6eNWdk2AIweHYcHl5foHk8tgDr0cP+X1xsz8h37bL5PfVUWxCHh9uCaf9+G8zOOgsyM2HECLufFRU2TWNsQRhs3gg2rbhc9rjl59v9T0iwZ6VpaY0LHrDHMbhcsBD2euGkk2zNoWFLbTCAJiTUT/f77eezf79Nv2dPm0f7uQu7SnfhcrjoFdtLm30PobK2kmpvNV6/F5/4SIpKItwZ3mn5aWsN42YR+asx5jwgEbgeeBloNWAYY17D1hRSjDHZ2J5PLgAReQb4ALgQ2ApUAjcF5hUZY34PLAsk9bCItHbxvGPExOCssl/g6uq9Id9cV7U+bz255bmEOcIQfxhhtUn0CBtEbVU4paX1zQ8FBY2r9cG24OCrqqr+/+CrosKele7fb+c1y/jBWQ3eqBYWsIVxWkYVPdKEwYOjSEqCqGgh37+R3c6FFDs2EVtzEvFVo4n1DKPaUUKFczflYbvoHbeffvG5SHQezvAaYl0JxIcn4g53ExkphEf6iYp0ckafs8hKmwTisIVwlJ9v89exJncNO0t2sqNkB4WVhcSEx+B2uYmPjGd46nDG9BzDkORhlFYXsyZ3DWtz15ISncLlQy8nNiIWgGpvNbPXz+aDrR/QO+lEhmSMY1yvcaS709tdiBZ7iimuKibMEUaYI4wiTxFf5a5j3dZ1ZB/IJi0mjV6xvejbty+TB19IlKv++PrFz8wVM/l89+dEOiOJDIukX0I/7pp4V6MCyi9+nls5i31l+0iMSiQhMoETk08kKyELE+hDU1RVwMzNf2P2t7Nxh7vp6e5Jakwqu0p2sWr/KkqqSgBId6czrtc4esf1ZkfJDrYXbyevIo8x6WM4ve/pnN7vdPon9Cc5Kpn4yHhyy3P5Jucblu1dRl5FHmkxafSM7UliZCLFVcXkV+RTUlXCqLRRTBk4hb7xjZuoa321rM9bz7K9y9hWtI34yHiSo5JJjEokLiIOd7ibaFc024q28XXO13yd8zXJUck8evajDEkZUpfONznf8MzyZ6isre9+Hx8RT8/YnqS70wl3hlNSVUKxp5j8ynx2l+5mV+ku8iry6BvflxOTT+Sk5JNId6eTEJlAYmQiSVFJpESnkBqTSn5FPnM2zGHOhjkszV7aaB8MhjR3Gn3i+jA4eTCj00bbV/po0t3p7freHA4jbTjNMsasFZFRxpi/AotEZK4xZpWIjAl5Dg9DVlaWLF++vP0JDBmCf9QIFv9sDgMG/JF+/e7ruMyFWHlNOUuzl/Ll7i9Zvm851d760jgpIo2M8KEky1C8VeHsqPiWnZXfUl5bwrCI8xjmmEaYpydf5y3kM/kjeTELmm7A74TiAXCgNzi84KwBI1DSDwpPgsITwReBK7IKZ5QHR3wOkrwBX+IG/OGlRFWeiLtqKEk1oxltriejRxRpafYMvE8f+39OzXe8vvFl/r3pX+wrz2FI8nDG9ZzA2PRx9Is9gZ5RA4gP68Gq0k+Yu3U27216D4/XQ1xEHD3dPSmtLmV/uW3IjQyLpMpb1XQ/AqJd0aTFpNX/uKuKqfHVNFmup7sn04ZMo8hTxKc7PqWgsqBuXq/YXqRGp+LxeiirLqO4qrhumw7jwC+NG8mjXdFcPvRyesf15rlVz9UVevmV+XXLOoyDhMgEEiITCHeGIyL4xU98ZDyn9TmN0/udTlavLBzGQY2vhtKqUuZvn8+7m9/lqz1fNdkmQJgjjJ7unuRX5tflr298X/587p+5fOjl7Dmwh5veuYkFOxbQJ64PglDlraKgsoAz+5/JW1e9RWJUIp5aDze8fQNvfvdmk230iOnBhYMvxO1yM2v1LCprKzln4Dm4nC72le0jtyKX3nG9GZM+hjHpY6j119YV/rnluQxIHMDAxIEkRSaxbO8y1uSuabQvBoNgyyqncZIcnUxBZUGT/Y0Ki8LjtW1qg5MGk+5Op6ymjPKacrIPZNftv8vhotZf2+L3I8IZwZieY9iQv4HK2kp+OfGXTB8xnUc+f4Q5G+YQHxFfV0ALUhccGjIYkqOT6Rvfl37x/UiJTmFX6S42F25mV8muuv1pyck9T+biwReTGJWIy+HCGEN+RT57Duxhd+luNhZsZM8B2zcoPiKe4l8Xt+tkwxizQkSy2rRsGwPG89heSgOA0YATGzjGHnbuQuiIA0ZWFqSn88WMr+jR4weceOLfjzhPpVWlbC3ayrbibewp3cPJPU9mcv/JOEzrPZr94mf1/tV8tvMzwp3hpLnTSI1OJbcil1X7VrFq/yp2luykrLqcUk85lb4DCILB0DNsGFIVR0UFVFT68cVkQ1xO4w0c6AXeSEjabt+X9IOEXTg96fTb+ysGuMYTl+AlNqEWic6j2LGJfNlEmewjOjIcd2Q4YS5hZ+l2dhTvwCe+Rsk7jZNBSYMYmjKU+Mh4NhduZkP+BkqrSxmcNJhnL3mWyf0nIyJ8sOUDHvniEb7a8xVO4+T8E84nMz2TFftW8HX21xRXFTc5PqnRqVwx7Ar6xPVhX/k+9pfvJ9wZzpn9z+Ss/mcxMHEgew7sYc3+NWws2EhSVBL9EvrRN74vGbEZxITHNEpPRPD6vTiMA4dxUFZTxvub3+fNDW/y4ZYPSYpKYsrAKUwZMIUJGRPol9CPyLDIJp/Z1qKtrNq3inV560iJTmF02mhGpY1iS9EWXlj9ArPXz+ZA9QEuOvEifjHhF0wZMAWP18OqfatYuW8luRW5dQHM6/diMDiMg33l+1iavbTFIJiZnsklJ17CCUkn4PV78fq9uMPdjOgxgiEpQ+qCT0lVCcv2LuOeT+5hbe5aTulzCuvz1uMXP0+c9wQ3j7m5rtD519p/cfM7NzMoaRDPX/o8v/joFyzLWcbj5z7OHePvqMvn8r3L+c/m//Dh1g8prynn2pHX8t+n/jfDUoe1+h1vTWlVKd/kfMO+8n0UeYoorCwkJTqFcRnjyEzPJNoVjc/vI78yn2JPMUlRSSRFJRHmCOPb/G/5dPunLNi5gLLqMtzh7rqazrhATW5g4kCqfdUUe4op9BRSXlNOWbUNLH3i+zAqbRThznDyKvL49fxf88LqFwBwh7v51aRf8atJv6qrLQbV+mrJrcgVjBWqAAAgAElEQVTF6/eSGJlIbERsi7/zam81RZ4iiquKKfYUU+QpoqCygPzKfCKcEVxy0iUMTBx4yONU5Clibe5a8ivyuXL4le061qEIGA4gE9guIiWBG+t6i8jaduUwRI44YEyeDMbwzWMFREefyIgRb7U7KRHhj5//kQcXPdjkLCgjNoOrR1xNr9hebCrYxKbCTRRXFZMSnUJKdAoAC3csbHLGEmQkjNiq4ZjCwZTmx0G1GzzJkD0BsidCdTxRUTBqlG3nHjQI4lLLqHJvJCqmlhMShpIYmYjLBTk1G1iw7y1W5n/JZcOnctOYG5sUhIdS46upCxpRYVFEhkWSFJVERFhEk2Py6Y5P+fF/fsz24u1cP+p6vs3/lpX7VtIvvh+/mPALfjDyB6S50xqts6t0FzuKd7CzZCc5ZTlMyJjAWQPOIsxxdPpseP1enMbZIe3tnloPZTVl9Ijpcdjr1vhqWLF3BWtz1+J0OAl3hhMZFsnE3hObNL8citfv5dkVz/Lbhb9leI/hvHDpCwxIHNBkuc92fsb3X/8+xVXFRLuieeWyV5g2ZFqLaVbWVhIXEXfY+9bVLdmzhK/2fMX1o69v12fXlYUiYJwKrBaRCmPMdcDJwF9FZNeRZbVjHXHAuOgiyMtjzT8T8XoPMHbs0kOv04zK2kpueucm3vj2DaYPn8704dMZmDiQXrG9WLBjAa+uf5UPt3xIrb+W5KhkBsadhLMmmf2lhRRVFdizyN2nU/PdebBjim0OcufiiMsj3pVEfPVwYqMj6NXLXhQ99VQYM8ZexA32qsjIqL/Q2NVU1FTwwMIHeOLrJxiYOJD7TruP60Zdh8t5lLp6qDptCYabCjbx8OKH+eXEX5LVq03lijqGhCJgrMU2RY3CDvfxT+AqEZl8BPnscEccMK66CtavZ8Oc8ZSULGDSpN2HncTGgo1c+9a1rNq3ikfPfpR7Trmn0Y+xshI+/xzen1/KyjW1bFqVQkF9szi9etmeLwMH2h4p/frZV9++trdMVw0C7VFaVUpMeMxRqykopZo6nIDR1l+qV0TEGHMp8HcRec4Yc0v7s9hFud1QXk5ERC9qavYh4scc1AZ5oPoAH2/7mPnb55MUlcS4XuMYlzGO9XnrefLrJ/lw64fEhsfy3jXvMTTsIv7xD9tNMjvbdpX85pvg3aTxZGbCpZfCyJE2SIwaZftXdxfxkfGHXkgp1WW0NWCUGWPuxXanPT1wTeP4az+IiYGKCsLDeyHipba2gPBw2165s2QnP/7Pj1m4YyG1/lpiw2PxeD14/d661dPd6fx6/O9wb/oxf/hhGksDLVrh4bbmkJEBP/85nHMOnH563c3lSil1TGhrwJgO/AB7P8Z+Y0xf4E+hy1YnaVDDAHsvRnh4D3aX7uasF8+ipKqEX078JRefeDGT+kyi1lfLmtw1LN+7nNrSFDbOvYwnfxuOx2Pvhn30UbjyStu05Gi9U5RSSnV5bQoYgSDxCjDOGHMx8I2IvBTarHWCmBioqSHc2FpFTc1ecg6k8r0Xv0exp5hPb/iUsb3qexI7TRjenRP59M8TeecdezftddfBL35hm5eUUup40qbzXmPMVcA3wJXAVcDXxpgrQpmxThEYnjLCa9vW9xR/x/de+h55FXnMu25eo2DxzjswcaJtWlq8GO67z16jeO45DRZKqeNTW5uk7gfGiUgegDEmFZgPNL3l81hWN8S5DRz3fjGLPaV7mH/DfCb0ngDYoTDuv982Nw0eDE8/DT/8YdPxeJRS6njT1oDhCAaLgEKOx+eBB2oYjsoavqtI4KNdG3j4zIc5pc8pgO3ddPPN8Mor8OMfw9//3nQ8fKWUOl61tbj7KDCC7GuB99OxAwceXwI1DCkv5+ktNfSIjODuSXcDdtC8qVNhwQJ45BGYMaPxqJ1KKXW8a+tF73uMMZcDpwYmzRSRuaHLVicJBIzXt7/Lt6WVPDCqX92YQ/fea4PFiy/CDTd0ZiaVUqpztLlBRUTmAHNCmJfO53ZTFQYztj7DkIQkzu5hB3r77DP429/gzjs1WCiluq9WA4YxpgyaHYPXACIix9coYzEx/H087KrJ4+XJ1+GreYWyMi833xzGoEG2KUoppbqrVgOGiMS2Nv+443bz72EwKWIQ3+t/Gps3/4t7761i+3Y3n32md2Yrpbq346+n0xGQ6Gg2pMJYk0F4eC/WrTuFp55yc+edcMYZnZ07pZTqXBowGsihlLIIGOpNJCKiF++++1OSkqq1KUoppdCA0ciGAzsAGFodhzG9WLLkYs45Z7s2RSmlFCEOGMaY840xm4wxW40xM5qZ/xdjzOrAa7MxpqTBPF+Dee+GMp9BGwo3AjC0IoolS3pQUZHAlCkrj8amlVKqywvZfcrGGCfwFHAOkA0sM8a8KyLfBZcRkV82WP4OYEyDJDwikhmq/DVnQ/4GEqsMaeXCH95xEhlZybhxnwPXHs1sKKVUlxTKGsZ4YKuIbBeRGmA2cGkry19D/Z3kneK7gu8YeiACyip4+22YNGkpDkeXegqtUkp1mlAGjAxgT4P32YFpTRhj+gEDgAUNJkcaY5YbY5YaY5p/6rxd97bAcsvz8/OPKMMb8jcwtDKa5Tk9ycmBs89eQ3X13iNKUymljhdd5aL31cCbIuJrMK1f4DmzPwCeMMYMam5FEZkpIlkikpV6BM83LawsJL8yn6E1cby9ewxOJ5x9djY1NRowlFIKQhswcoA+Dd73DkxrztUc1BwlIjmB/7cDi2h8faPDbSjYAMAwfzJv75/E5MmQlpZEbW0BXm95KDetlFLHhFAGjGXAYGPMAGNMODYoNOntZIwZAiQCSxpMSzTGRAT+TsEOevjdwet2pO/ybfIRzkl8VzWQadMgJmYkABUVa0O5aaWUOiaELGCIiBf4OTAP2AC8ISLfGmMeNsZMbbDo1cBsEWk4ZtVQYLkxZg2wEHi0Ye+qUNiQv4FoVzTLiuzlkkunlON2nwxAefmqUG5aKaWOCSF9/I+IfMBBz80QkQcOev9QM+t9BYwMZd4OtqFgAycln8SCvcMZyVr6ekEiRuJypVBWpvdiKKVUV7no3ek2FGxgWOowdpcmcCKbYft2jDG43SdTXq4BQymlNGAA5TXl7C7dzdCUoeQURpBBDmzbBkBs7MlUVKzH76/u5FwqpVTn0oABbCywQ4L0ixlKWZmhd1QRbN8OgNt9MiJeKiq+7cwsKqVUp9OAgb3gDZDgHQpARrq3LmDExtoL33odQynV3WnAwF6/CHOE4Sw5AYDe/V11ASMycgBOZ5xex1BKdXsaMLD3YAxOGkzuPhcAvU+Khh07wOfDGAdu9xitYSiluj0NGNgaxtDUoWRn2/e9RqZAbS3k2BvT7YXvNfj93k7MpVJKda5uHzC8fi97y/YyLGUYOTmQkgKRJ/WzMxtc+Pb7q/B4NnViTpVSqnN1+4AR5gij5Ncl3Hv6vWRnQ0YGMCgwzmHdhW87jJU2SymlurNuHzAAnA4n0a5ocnKgd2/sP2FhdfdiREWdhMMRpRe+lVLdmgaMBupqGGFh0K9fXQ3D4QjD7R6tNQylVLemASOguhry8wM1DICBA+sCBhAYImQ1Iv7OyaBSSnUyDRgBewPPScoIPhNw0KCDAsYYfL4DeDzbjn7mlFKqC9CAERDoQdu4hlFQAAcOABAfPwmAkpIFzaytlFLHPw0YAcF7MBoFDKirZURHDyMqajD5+W8d/cwppVQXoAEjIFjDqGuSOihgGGNISbmMkpIF1NYWH/0MKqVUJ9OAEZCdDW43xMUFJhwUMABSUy9DxEth4X+OfgaVUqqTacAICHapNSYwIT4ekpPr7sUAiI3NIiKiNwUF2iyllOp+NGAE1N2019BBXWuNcZCS8n2Kij7C56s4uhlUSqlOFtKAYYw53xizyRiz1Rgzo5n5Nxpj8o0xqwOvHzWY90NjzJbA64ehzCc0uGmvoYMCBkBKymX4/VUUFX0U6iwppVSXErKAYYxxAk8BFwDDgGuMMcOaWfR1EckMvP4ZWDcJeBCYAIwHHjTGJIYqrz4f7NvXQg1j507weOomxcefhsuVor2llFLdTihrGOOBrSKyXURqgNnApW1c9zzgExEpEpFi4BPg/BDlk7w88HqbqWGcc46d8e67dZMcjjCSky+lsPA/+pxvpVS3EsqAkQHsafA+OzDtYJcbY9YaY940xvQ5zHUxxtxmjFlujFmen5/frow2uWkvaPJk6NMHXnqp0eTU1Mvw+Q5QXPxpu7anlFLHos6+6P0e0F9ERmFrES8ebgIiMlNEskQkKzU1tV2ZaHLTXpDDAddfD/Pmwf79dZMTE6cQFpZMdvaT7dqeUkodi0IZMHKAPg3e9w5MqyMihSISbNf5JzC2ret2pCY37TV0/fX2Isdrr9VNcjgi6NfvXoqL51FcvChU2VJKqS4llAFjGTDYGDPAGBMOXA2823ABY0zPBm+nAhsCf88DzjXGJAYudp8bmBYS2dngckGzFZQhQ2D8eHixceWnV6+fER6ewY4d9yIiocqaUkp1GSELGCLiBX6OLeg3AG+IyLfGmIeNMVMDi91pjPnWGLMGuBO4MbBuEfB7bNBZBjwcmBYSOTnQq5dtgWrWDTfAmjX2FeB0RtG//0McOLCUwsJ3W1hRKaWOH+Z4OjvOysqS5cuXH/Z63/se1NTAF1+0sEBBgY0od94Jjz9eN9nv97Js2XCMCWPcuLXYnsRKKXXsMMasEJGstizb2Re9u4ScnBauXwSlpMBFF8Err9hutgEORxgDBvyBysrvyM19JfQZVUqpTtTtA4aIvYbRpIfUwW64wfaU+vDDRpNTUy/H7R7Ljh2/weerDF1GlVKqk2nAEJgzB2688RALXnyxjSpPNu5Ka4yDE074P6qr97Bnz59Dlk+llOps3T5gOBxw/vkwcuQhFnS54PbbYf58WL++0ayEhDNITb2C3bsfpbo6ZL1/lVKqU3X7gHFYbr0VoqLgr39tMmvgwMcQ8bF9+72dkDGllAo9DRiHIznZXst4+WU4aBiSqKgB9OlzN7m5L3PgwDedlEGllAodDRiH6847oboaZs5sMqtv33txudLYsuVO/H5vMysrpdSxSwPG4Ro2DM49F55+2t680UBYWCwnnPAXysq+Zvv2X3dSBpVSKjQ0YLTHXXfB3r3w5z/bblYNpKVdQ0bGz8nO/j9yc1/tpAwqpVTH04DRHuedZ2/ku+8+uOIKKGo8asmgQf9HfPzpbNr0I8rKVnVSJpVSqmNpwGgPh8M+VOlPf4L33oPRo+GrrxrMdjF8+L9xuZJZv/77VFR814mZVUqpjqEBo70cDviv/7KBIjLSXtdYsaJudnh4GsOHz8XnK2P58jHs3Pl7/P6aVhJUSqmuTQPGkcrKgs8+s+NNXXghbN9eNysuLovx4zeQmnoZO3c+wIoVWVrbUEodszRgdIReveCjj+zAhOed1+gejfDwHgwb9hojRrxLTU0eq1dPprx8TSuJKaVU16QBo6MMGWKvZ2Rnw7RpTXpPpaRcwpgxn+NwRLJ69VmUla1oISGllOqaNGB0pFNOgb/8xV7XaObhGtHRg8nMXExYWDyrV0+htHRJJ2RSKaXaRwNGR7v+enC7YdasZmdHRQ0gM/MzwsNTWb36TPbufVYf8aqUOiZowOhoMTFw9dXwxhtw4ECzi0RG9uXkk5eSkHAWmzffxqZNP8Ln8xzljCql1OEJacAwxpxvjNlkjNlqjJnRzPy7jTHfGWPWGmM+Ncb0azDPZ4xZHXgdWw/NvuUWqKy0QaMFLlcyo0a9T79+v2X//lmsXDmeoqL5RzGTSrVRaSncdJN9gJjq1kIWMIx9wPVTwAXAMOAaY8ywgxZbBWSJyCjgTeCxBvM8IpIZeE0NVT5DYsIEO+bUc8+1upgxTgYMeJiRI9/H6y1j7dpzWLv2AsrL1x2ljCrVBu+8Ay+8ALNnd3ZOVCcLZQ1jPLBVRLaLSA0wG7i04QIislBEgs81XQoc6kGpxwZj4OabYelS+O7Q910kJ1/I+PEbGTjwTxw4sJTly0ezdu3FFBV9cnxf31i3rskAjqoL+uQT+/+CBZ2bD9XpQhkwMoA9Dd5nB6a15Bag4QOzI40xy40xS40x00KRwZC6/noIC2vx4vfBnM5I+vb9LyZM2Eq/fr+hrGwZa9eey7Jlw9m//2VEfCHO8FG2axeMGQMPPtjZOVGtEbFPmQR7g6pXh+3vzrrERW9jzHVAFvCnBpP7iUgW8APgCWPMoBbWvS0QWJbnH/RQo07Vowdccgm89JJtA25o3Tq44AJ7Z/i8eY3u2XCFJTGgzwNMmrSbIUNewuGIYOPGG1i2bBT5+XOOnxrH66+DzwfPPAPl5Z2dm+7r9tvhzTdbnr9+vb12cd55thPHKh1MszsLZcDIAfo0eN87MK0RY8zZwP3AVBGpDk4XkZzA/9uBRcCY5jYiIjNFJEtEslJTUzsu9x3h9tvtXd99+8J//zds3gz33GPPrJctg9Wr6x8ofvfd9keZng7JyTgWfkF6+vWMHbuCYcP+Dfj59tsr+Oqrnqxffxm7dz9ORcW3nb2H7Td7NqSlQUkJvPhiZ+eme1qzxj7X5cEHm9xoWifYHPWHP9j/Fy48Onnral57DU44AYqLOzsnnUtEQvICwoDtwAAgHFgDDD9omTHANmDwQdMTgYjA3ynAFmDYobY5duxY6XKWLxeZPl3E4RCxP0uRH/1IpKBApLpa5MUXRUaNEnG5RDIzRW68UWTIEJGEBJENG+qS8flqZf/+V+S7766XJUsGycKFyMKFRjZuvFWqq/M6cQfbYeNGexz+7/9Exo8XGTxYxOfr7Fx1P7/4Rf13cvny5pc5/3z7fRQRGTZM5Lzzjl7+jlRpqf2NHSm/X2T0aHuc/vjHI0+viwGWS1vL9bYu2J4XcCGwORAU7g9MexhbmwCYD+QCqwOvdwPTTwHWBYLMOuCWtmyvSwaMoO3bRR55ROTLL5uf7/XW/71jh0iPHiKDBonk5ze7eFXVPtmy5W5ZtChMPv88QXbtekxKS7+R2tqyjs97R/vd70SMEcnOFpk9234N3323s3N1bNi4UeTWW1v8XrRZdbVISorI2WeLhIeL3Hln02WqqkSiokTuuMO+v/12kZiYjimEQ23fPpGePUVOOklk7dojS+ubb+x3NDZWJDVVpLKyY/LYRXSZgHG0X106YByur74SiYgQOe00kaeeErntNpFTTrG1lfffF6mtFRGR8vLvZPXqswM1Dvv66qt+snXrPVJZubWTd6IZfr89Y5082b6vrRXp00fkrLM6NVvHBJ9PZOJE+7P93vfqvgPt8tZbNp333xe54gpbENbUNF5mwYLGwXzOHPv+iy+aT/M//7FBpeHJT2fwekWmTLHBLj1dJDJS5Nln7XevPW65RSQ62h4HEHn66Y7NbyfTgHG8CJ59g0hiosjpp4skJ9v3aWm2SWHhQvHX1EhFxWbJy5srO3f+UdaunSoLFzpl4UIja9acLwUFH4i/tR9LdbXIxx83LTBCYc0am/9//KN+2mOP2WmrVh16/RUrjk4+Q6WiQuS779q37jPP2ON0xRX2/7vvbn8+LrnEnoHX1tYXhO+913iZe+8VcTpFDhyw7wsKbM3w979vmt68ebZZFURefrn9+WrJ0qUimza1bdk//MHm47nnRPbvt7UoELnppsMPZqWlNljccosNOBMnigwYcGTBuovRgHE82b7dNt0EC/zqapG5c0WmTbM1EBBJSrJf6NzcutWqqrJlx46H5Msve8rChcjXXw+X3K8eFe/H7zeuUn/4oa22g8hPfxr6/QkWQnkNrrsUF9umjnPOsc0gzamuFvnxj20+b7kl9PlsqCOvr1xxhd3/lpomRUS2bBG5/36R//f/6re9f7+9rnXWWfa7cMcd7S+c9++3efjv/7bva2ps89SVVzZeLitL5NRTG0/LzGxaG/zyS1uojh5tr8cNGtSxQX3uXJvf1FSRXbtaX3bxYnu98Ac/qP/NeL0iv/mNPV4//nHzNY3qatt09frrjbfx9NN2va+/rs8LiLz6avPbnzfPngB9883Rq2n5/SI5Oe1eXQNGd1FWJvLmmyLXX2+DR8+eIp9+2mgRn69a9m3/p+Tc1EN8YYiAeMORovEuKZ7kFgGpHZgm3isutl+HF15oeXt+f/MFem2tyCef2OaNpUtFNm9u/gzM7xcZOLD5C6czZ9rtX3ihiMfTeF5ensgZZ9j548bZ/19/vQ0HqAPs3i3Sr5/IL3955GmtWmXz7nSK9O5tz9gb+uADe2yCtUqwTZKbN9sCMDzcXsMQsQXy5Mm2ueX11xsXgj6fyNtv2w4VzX0Ojz9u027QqULuvNN+h4qK7PtgbeKhhxqv+8tf2uWCn9HSpSLx8bbjwv79tpYCtgmoI7z/vq25jB0rEhcncvLJtpZ2sKIikb//3TZBnXBCfa2ooV//2ubtN7+x72tqRJ5/3qYdrB2B3c7bb9df7M7MrD++Pp9tUh09umngefVVe8yC6cTHi1x3nT0hOtiKFbbJb/VqG6C2bbPN0HPn2n1u6cSpIb/fHu9x42ytp51BWgNGd7Rmjf0iG2PP4j/80H7pZ82yZ3wgVdPPkf3PXivFN46VqoEJ4o0Nk+0/jZRFHyOL5iMlY6PEF+GU4gV/E6+3wY8y+MUcO9YWdpMn2x5OX35pz1LT0xsXciAyfLgtbBt65RU77/nnm9+HYNA491xbKKxda7fTr58tpP71L/ujmDDB/hh37Khfd/16e1bXkSor7Vl2cJ9eeqlt661aZXt/zZzZePq0aTbfn3xiC/+LLrIFUEmJyLXX2m306mU7BeTk2OAdH19fk3zggcbp5eXZwgxEJk2yn8fbb9f36AGRkSNFPv+8fp2yMvvZTJzYOK3ly+3yjzwi8sYbItdcY98fXBMKNl/ddVf99ZTevevPyv1++/n06dO2Qq81H39s933sWFvovvee/X5fc43dTk2NLVyvuab+GGVmtnyR2++3PRRB5OabRfr3t3+PHi0yY4b9fi5eXP+ZB4/Bwdcsnn/eTv/BD2ztX8Re33E6Rc48U2TnTpHXXrM1YZfL9i4LflcrK21N/uDfy8Gv+HibxwULmq8RffihDZ5g9+PZZzVgHO6rWwcMEZHycttOe/CXb/DgJjUPERHx+8Xv90lZ2RrZs+cJWb/ge1KVilSmI+sedcnOP46UogemiXfMMJvOgAH2usmIEfVpO50iU6faM6MlS+wP+B//sF/4Xr1sIPP57Jkq2MKkvLzlfXjuOVsoREbWb2PYsMbBYNs222PllFPsBdiLL65f9sYbmz+jO1x+v8gPf2jTnDPHBsmoKLs/rXnpJZt3h8Mem/nz7fRggfy739n3f/ubff+Tn9gfvNMp8vDDTX/0OTk20EyY0LTmJWKbPZ57ztYug8fghBNsPubMsQU32Hb8YcPqz4Cfe67p/g4bVp9GVJQtMA+uoZSU2LwGP5fHH2/aY+uTT+z8v/3Nvt+71wahYOF6KH6/rS1ERNgmrsLC+nmPPGLTPucc2xQbvL53++0iK1ceOm2vV+Syy+x648fbIHRwgVxVZT8XsE1tJSVN07j/fpu/6Gi7rMtlg3bZQb0UFyywTYlpaTaIjBxp0/3Vr0QWLrSf0bPP2iD0wQe25vHhhyI33GC/42A//337bHqVlXZfwZ4Izpp1xM1/GjC6u7VrbeG9YoXIunWH1Q3S++Ui8bvCGgWcyp7IhnuQLxelyqpV35Ndu/4kles/sT+A4Be5uTxkZNjq/bnn2rRuuKH5Qu9gc+bYwPfcc/ZsrTmvvlqfx6QkW9jef78tzDIybFPd3r0tX3/Ys0fk3/8WefRRkfvusz/Cn/3Mnk1+9ZXIn/9s037wQbt8sJvmoEHNB6Tycrs+2LPMLVtsgZqUJLJ1qw1qiYn1hY/fX19wDRhgt3kkystFnnjCBoqGhXx5uT17HjzY5uHBB22B1NxZ69dfi/z1r/b/1gqhTz+1TVEtdaTw+22ATU4WGTOm/nOKj7e1xOB6FRU2MPzkJ/a7VFBgr8MFTwAuuKBpMPL7bW0sJsae4b/77uF3862psYH/UL2m3nrLfhdbsn17fQeEsWObBpag776znzHY6zAffti2fFZW2ushERH2e/TXv9YHnLvvPvIaXIAGDHVktm2zTREbN0rl7uWSt//fsmvXo7Jhwy3yzTej6rrvLlkyUFat+p6sX3+VbNp0u+zc+Yjs3/+KlJR8IR7PHvHt3G6bPxwO27TU3m6NLXn8cZtuw7O6ZcvsNoOFVHi4vW6SmWnPAM86yzahNKyBOZ32Bxkf33j6JZc0DjhffCESFmabMGbOtAVEVZX9IaelSd2ZY7DA3rrVBom+fe28g2/6Ki21PZ9KSzv2uHQFS5aIuN22Z9///I+taZ1yij0OV15pa5wpKVJXmwFb+3G7bQH55JOtB6Su1Etp3bqmNYuD5eba3lt79x5++hs21Df/pabamkgHOpyAYezyx4esrCxZvnx5Z2fjuFdVtYvCwv9QXLyAmppcamvzqa3Nw+stOWhJB5G+HrhLU5HBA4mI6ElERB/i4iYRFzcRpzMqNBmsrrYjq+7YYQc53LPHjlfl8djnlPTrBxMnwqRJdhj66Gg7wrCIXXbVKrvuzTdDXFzjtP/9bzuUxoYNEBkJCQl2rKUzz4RHHrFpNvTpp3bIl4QEm2ZsbGj2+Vjg88Fjj8EDD9hBDC+5xA6ZM2mSHSrn449hyxY7beTIzs5t1+Lzwdy5cNppdvigDmSMWSF23L5DL6sBQ3UUr7ec6urdVFXtpLo6u8Erh5qafVRX78XrLQTAmHDi4saTkjKN1NTpREYeQyPbi9gC7sUXbUC66y6YMsUGnebMmwdRUXDGGUc3n13Vjh02YAwe3Nk5UWjA6OxsqFZ4vaWUln5BSclnFBd/Snn5SsCQkDCZmJjRiNQiUucgIBEAAA14SURBVIvTGUNMzAhiYkYRFTUYkWq83jL8fg+Rkf0JC+vGZ+pKdaDDCRhhoc6MUg2FhcWTnHwRyckXAVBZuYW8vNfIy5tNWdnzGOPC4XDh9R7A769sIRVDVNQJuN1jiIubSELCGcTEjMbh0K+zUqGkNQzVJYn4qaraQXn5WqqqtuNwROF0unE4Iqis3Ex5+SrKylZSXb0LAKczlujoYTgckTgcEYFXJA5HJMa48HpL8XoLqa0tJCZmBOnpN5GYOAVjusQjYZTqNFrDUMc8YxxERQ0iKqrZ52bVqa7OoaTkc0pLP8Pj2YrfX4PXW4rfX4VINX5/NX5/DWFhcbhcKURE9KWo6EPy8l4jIqIviYln43TG4HBE4nTGER19ItHRw4iOHowxYfj9tYh4cTpjMC1do1Cqm9CAoY5pEREZpKVdTVra1W1ex+erorDwHfbte56iog8CQaUKv9/T4jphYQnExIzC7R5NZGS/utqLfUXV1WT8/kq83gP4fOXExAwjLm4SDkd4R+yqUp1OA4bqdpzOSHr0mE6PHtMbTff5Kqms3Exl5QY8nq2AYEwYxjjxeHZQUbGWfftm4fdXHMa23CQknEVc3ClER59EdPQQnE43lZXfUVGxnurqbGJjx5OYeDbh4an4/bUcOPA1JSULcDrdpKRMIypqIAB+fzUlJZ/j8WwiJeUyIiJ6duRhUeqQ9BqGUodBxI/PVx6okdhaSf3f1TidbpzOOByOSMrKllNcPI+ioo+pqtrebHrGRBB8MnF09HCqq3fh85UDBrC/zZiYUURE9Pn/7d15kFxlucfx76/X6e5ZeoYMkw1NMBggghAoC4laIKh4r6VYLqBg6a271C2xXErrXrC0rpequ1XdcvnDEi3UQi+lCBeU0ipRY4zGBRIWt2CUTcgC05NM90xm6fXxj/POOARNTgKzMOf5/JM557x9+j0nb/fT5z3veR+q1R/NBispy9DQVaxd+yG6utbPji6DNOn0zBVP+hnvV68fYHLyd5RKm8jlTj7msdbr+5mefphGY5iBgdeRyfQe9TVHvt7vES19PqzWuSWm1aqFq5c9tNvjlEqbKJU2kcmUGR+/l9HR71Ot/oRC4VT6+y+lXL6YVmuUkZFvMjJyB43GMP39lzIw8HoKhfXs339DuNr5ayPJIJUqks+vIZdbTTpd4PDhX9JoHJjd3t29mYGB11IqnUUuN0Q2O0SzOUK1uo1qdRvj4zvpdKZny2ezg6xb9wlWrfpHpDRjY7+gUrmVdnuCvr4t9PW9knS6h0rlNoaHv06t9lPK5VcxNHQ1g4NvIZPpO+o5arenmJx8kFLpbB/xtoA8YDiXAM3mIYaHb6HdniCVyiJlMWvPXvm0WqPU6wdoNPaHIHUWPT3nUShs5PDhezl06C7Gxn6OWeuIPafo6dlMb+8WisXTKRRehJTmsceup1bbTqGwgU6nTr3+BFI0Gq3drj1tD8XiGZTLFzM6+j2mph5CypPLDQIppBS53Gr6+i6kt/dCUqkcw8O3MDLyTdrtcXK5lQwNvYuVK99DPv+CUL82rVaNZrNCo1EBjGJxI11d60mlsrPva9ZhcvL3jI/fzdjYPXQ6U6TTvWQyveTza+nr20KxeMZzduVjZkxO7iGXO5lsduA52edC84DhnIslejr/iTDFy1OkUiX6+l5BNlt+Rlkz4+DB7/D44/9JNnsSg4NXsGLFG0mnu5mY2E2ttoNmc5gVKy6nVDoLSZgZ4+M7qVRupdk8BHQwazM19TDj47swawCQTvcxOPgW+vq2MDLyLQ4e/A7QPmb9pQz5/NrZQBl1F06FffaQyZTDIIQxZrr4Mpl+urs3k04Xw3M/OTKZfrLZFWQyA7RaVaanH2V6+lHM2hQKGygUNpDPnzLbzdduT1Cr/ZjR0a00m8OA6O4+l/7+SygWz0RKI2XCvstkMmVSqS6mp//I1NQfmJp6hFxuJd3d59DdfS75/Mpocj+MdnuSRmMf9fo+6vX9NBpP0mgcoNkcoVjcSLl8MT095z8tUD4bSyZgSLoM+AyQBm40s/8+Ynse+ApwHnAQuMLMHgvbrgP+nqjVvN/M7jrW+3nAcO75o9OpMz5+H61Wjf7+i0ml8rPbGo2nqFTuoN0enx14kE73hF/yg5h1mJraw+TkHqanHyeVys2OWCuVNtHbewHF4umzVxJR+Ueo1XZQq+1gYuLXmDUwa4Wh2KM0mweBDiDy+TV0dZ0KiOnph6nX9z6j/rncytB9eBH1+n5GR7cyNvazcC/p6FKp4lG7E59ZvkAmM0CjsQ+IBlNEgbIDGNnsCjZv/lns/c21JAKGolD8e+A1wF5gJ/AOM9s9p8x7gbPN7J8lXQm82cyukHQm8DXgZcBq4AfAi83sqD85PGA4506UWYdWqxqey8k/bVu7PRXu/0Tfl1I2XHHoiHKTNBpPEX1Vtel06uGh0Srt9gRdXadQKJxGNjtIq1VjYuJXHD78QAhWQhKpVBe53Bry+TXk86vJ5VaRTvcgiUajQrW6nWp1G81mhWhwhMhkymzceMMJHfdSCRgvBz5hZq8Ly9cBmNl/zSlzVyjzc0kZ4ElgELh2btm55Y72nh4wnHPu+BxPwJjPMW9rgCfmLO8N6/5iGYvubNWAk2K+1jnn3AJ63g+SlvRPknZJ2lWpVBa7Os45t2zNZ8DYB5wyZ3ltWPcXy4QuqT6im99xXguAmX3BzM43s/MHBwefo6o755w70nwGjJ3AaZLWS8oBVwJ3HlHmTuDd4e+3Aj8MKQPvBK6UlJe0HjgNuGce6+qcc+4Y5u1xSjNrSXofcBfRsNovmdlvJV1PlEP2TuCLwFclPQQcIgoqhHLfAHYDLeCaY42Qcs45N7/8wT3nnEuwpTJKyjnn3DLiAcM551wsy6pLSlIF+OMJvnwFMPIcVuf5KOnnIOnHD34OIHnn4IVmFmuI6bIKGM+GpF1x+/GWq6Sfg6QfP/g5AD8HR+NdUs4552LxgOGccy4WDxh/9oXFrsASkPRzkPTjBz8H4Ofgr/J7GM4552LxKwznnHOxJD5gSLpM0h5JD0m6drHrsxAknSJpm6Tdkn4r6QNh/YCk70v6Q/i3f7HrOp8kpSXdL+nbYXm9pLtDW7glzIG2rEkqS7pN0u8kPSjp5UlqB5I+FD4Dv5H0NUldSWwHcSU6YISsgJ8FXg+cCbwjZPtb7lrAh83sTOAC4Jpw3NcCW83sNGBrWF7OPgA8OGf5f4BPmdkGYJQoRfBy9xngu2Z2OvBSovORiHYgaQ3wfuB8M3sJ0Zx3V5LMdhBLogMGUQrYh8zsEYuy0X8deNMi12nemdkBM7sv/D1O9CWxhujYbwrFbgIuX5wazj9Ja4G/BW4MywJeDdwWiizr4weQ1Ae8imgSUMysYWZVEtQOiCZgLYT0CkXgAAlrB8cj6QEj8Zn9JK0DzgXuBobM7EDY9CQwtEjVWgifBv4F6ITlk4BqyPwIyWgL64EK8OXQNXejpBIJaQdmtg/4X+BxokBRA+4lee0gtqQHjEST1A38P/BBMxubuy3kJVmWQ+gkvQEYNrN7F7suiywDbAY+Z2bnAhMc0f20zNtBP9HV1HpgNVACLlvUSi1xSQ8YsTP7LTeSskTB4mYzuz2sfkrSqrB9FTC8WPWbZ1uAN0p6jKgb8tVEffnl0DUByWgLe4G9ZnZ3WL6NKIAkpR1cCjxqZhUzawK3E7WNpLWD2JIeMOJkBVx2Qn/9F4EHzeyTczbNzYD4buBbC123hWBm15nZWjNbR/R//kMzuwrYRpT5EZbx8c8wsyeBJyRtDKsuIUpaloh2QNQVdYGkYvhMzBx/otrB8Uj8g3uS/oaoP3smK+B/LHKV5p2kVwA/AX7Nn/vwP0p0H+MbwAuIZv19u5kdWpRKLhBJFwEfMbM3SDqV6IpjALgfuNrM6otZv/km6RyiG/854BHg74h+SCaiHUj6d+AKopGD9wP/QHTPIlHtIK7EBwznnHPxJL1LyjnnXEweMJxzzsXiAcM551wsHjCcc87F4gHDOedcLB4wnFsCJF00M2uuc0uVBwznnHOxeMBw7jhIulrSPZIekPT5kFPjsKRPhbwKWyUNhrLnSPqFpF9JumMmr4SkDZJ+IOmXku6T9KKw++45uSluDk8fO7dkeMBwLiZJZxA9FbzFzM4B2sBVRJPW7TKzTcB24N/CS74C/KuZnU30VP3M+puBz5rZS4ELiWZKhWjW4A8S5WY5lWheI+eWjMyxizjngkuA84Cd4cd/gWhivg5wSyjzf8DtIddE2cy2h/U3AbdK6gHWmNkdAGY2DRD2d4+Z7Q3LDwDrgB3zf1jOxeMBw7n4BNxkZtc9baX08SPKneh8O3PnK2rjn0+3xHiXlHPxbQXeKulkmM2B/kKiz9HM7KbvBHaYWQ0YlfTKsP5dwPaQ4XCvpMvDPvKSigt6FM6dIP8F41xMZrZb0seA70lKAU3gGqLEQy8L24aJ7nNANDX2DSEgzMwEC1Hw+Lyk68M+3raAh+HcCfPZap17liQdNrPuxa6Hc/PNu6Scc87F4lcYzjnnYvErDOecc7F4wHDOOReLBwznnHOxeMBwzjkXiwcM55xzsXjAcM45F8ufAP1t/1sZuaXXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 720us/sample - loss: 0.1709 - acc: 0.9495\n",
      "Loss: 0.1709458163129949 Accuracy: 0.9495327\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6653 - acc: 0.4864\n",
      "Epoch 00001: val_loss improved from inf to 1.01852, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/001-1.0185.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.6652 - acc: 0.4864 - val_loss: 1.0185 - val_acc: 0.7198\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7492 - acc: 0.7740\n",
      "Epoch 00002: val_loss improved from 1.01852 to 0.47875, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/002-0.4787.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7493 - acc: 0.7739 - val_loss: 0.4787 - val_acc: 0.8633\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8483\n",
      "Epoch 00003: val_loss improved from 0.47875 to 0.36961, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/003-0.3696.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.5202 - acc: 0.8482 - val_loss: 0.3696 - val_acc: 0.8947\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8785\n",
      "Epoch 00004: val_loss improved from 0.36961 to 0.26023, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/004-0.2602.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.4162 - acc: 0.8785 - val_loss: 0.2602 - val_acc: 0.9255\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.9012\n",
      "Epoch 00005: val_loss improved from 0.26023 to 0.25626, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/005-0.2563.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3442 - acc: 0.9012 - val_loss: 0.2563 - val_acc: 0.9259\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3030 - acc: 0.9116\n",
      "Epoch 00006: val_loss improved from 0.25626 to 0.20599, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/006-0.2060.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3029 - acc: 0.9116 - val_loss: 0.2060 - val_acc: 0.9457\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9232\n",
      "Epoch 00007: val_loss improved from 0.20599 to 0.18797, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/007-0.1880.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2611 - acc: 0.9232 - val_loss: 0.1880 - val_acc: 0.9476\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9298\n",
      "Epoch 00008: val_loss did not improve from 0.18797\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2400 - acc: 0.9297 - val_loss: 0.1935 - val_acc: 0.9460\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9381\n",
      "Epoch 00009: val_loss did not improve from 0.18797\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2173 - acc: 0.9381 - val_loss: 0.2037 - val_acc: 0.9406\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9407\n",
      "Epoch 00010: val_loss improved from 0.18797 to 0.16468, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/010-0.1647.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2014 - acc: 0.9407 - val_loss: 0.1647 - val_acc: 0.9527\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9471\n",
      "Epoch 00011: val_loss did not improve from 0.16468\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1804 - acc: 0.9471 - val_loss: 0.1765 - val_acc: 0.9492\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9487\n",
      "Epoch 00012: val_loss improved from 0.16468 to 0.15580, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/012-0.1558.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1726 - acc: 0.9487 - val_loss: 0.1558 - val_acc: 0.9541\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9532\n",
      "Epoch 00013: val_loss improved from 0.15580 to 0.14221, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/013-0.1422.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1576 - acc: 0.9532 - val_loss: 0.1422 - val_acc: 0.9616\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9588\n",
      "Epoch 00014: val_loss did not improve from 0.14221\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1441 - acc: 0.9588 - val_loss: 0.1689 - val_acc: 0.9522\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9605\n",
      "Epoch 00015: val_loss improved from 0.14221 to 0.14054, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/015-0.1405.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1345 - acc: 0.9605 - val_loss: 0.1405 - val_acc: 0.9569\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9628\n",
      "Epoch 00016: val_loss did not improve from 0.14054\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1280 - acc: 0.9628 - val_loss: 0.1539 - val_acc: 0.9499\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9636\n",
      "Epoch 00017: val_loss improved from 0.14054 to 0.13515, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/017-0.1352.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1208 - acc: 0.9636 - val_loss: 0.1352 - val_acc: 0.9602\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9673\n",
      "Epoch 00018: val_loss improved from 0.13515 to 0.13177, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/018-0.1318.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1123 - acc: 0.9673 - val_loss: 0.1318 - val_acc: 0.9599\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9685\n",
      "Epoch 00019: val_loss did not improve from 0.13177\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1050 - acc: 0.9685 - val_loss: 0.1643 - val_acc: 0.9536\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9671\n",
      "Epoch 00020: val_loss improved from 0.13177 to 0.13164, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/020-0.1316.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1087 - acc: 0.9671 - val_loss: 0.1316 - val_acc: 0.9590\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9735\n",
      "Epoch 00021: val_loss did not improve from 0.13164\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0914 - acc: 0.9735 - val_loss: 0.1364 - val_acc: 0.9578\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9744\n",
      "Epoch 00022: val_loss improved from 0.13164 to 0.13000, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/022-0.1300.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0866 - acc: 0.9744 - val_loss: 0.1300 - val_acc: 0.9590\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9749\n",
      "Epoch 00023: val_loss did not improve from 0.13000\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0825 - acc: 0.9748 - val_loss: 0.1544 - val_acc: 0.9513\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9774\n",
      "Epoch 00024: val_loss improved from 0.13000 to 0.12349, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/024-0.1235.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0778 - acc: 0.9774 - val_loss: 0.1235 - val_acc: 0.9632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9771\n",
      "Epoch 00025: val_loss improved from 0.12349 to 0.11583, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_DO_BN_8_conv_checkpoint/025-0.1158.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0786 - acc: 0.9770 - val_loss: 0.1158 - val_acc: 0.9634\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9769\n",
      "Epoch 00026: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0762 - acc: 0.9769 - val_loss: 0.1372 - val_acc: 0.9571\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9820\n",
      "Epoch 00027: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0637 - acc: 0.9820 - val_loss: 0.1758 - val_acc: 0.9534\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9834\n",
      "Epoch 00028: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0595 - acc: 0.9834 - val_loss: 0.1494 - val_acc: 0.9597\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9823\n",
      "Epoch 00029: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0602 - acc: 0.9823 - val_loss: 0.1366 - val_acc: 0.9571\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9817\n",
      "Epoch 00030: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0641 - acc: 0.9817 - val_loss: 0.1395 - val_acc: 0.9583\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9852\n",
      "Epoch 00031: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0522 - acc: 0.9852 - val_loss: 0.1364 - val_acc: 0.9604\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9844\n",
      "Epoch 00032: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0540 - acc: 0.9844 - val_loss: 0.1201 - val_acc: 0.9623\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9841\n",
      "Epoch 00033: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0544 - acc: 0.9841 - val_loss: 0.1325 - val_acc: 0.9618\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9848\n",
      "Epoch 00034: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0511 - acc: 0.9848 - val_loss: 0.1336 - val_acc: 0.9644\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9872\n",
      "Epoch 00035: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0455 - acc: 0.9872 - val_loss: 0.1551 - val_acc: 0.9581\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9860\n",
      "Epoch 00036: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0486 - acc: 0.9860 - val_loss: 0.1294 - val_acc: 0.9655\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9879\n",
      "Epoch 00037: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0420 - acc: 0.9879 - val_loss: 0.1408 - val_acc: 0.9604\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9889\n",
      "Epoch 00038: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0383 - acc: 0.9889 - val_loss: 0.1418 - val_acc: 0.9585\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9837\n",
      "Epoch 00039: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0531 - acc: 0.9837 - val_loss: 0.1228 - val_acc: 0.9669\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9907\n",
      "Epoch 00040: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0345 - acc: 0.9907 - val_loss: 0.1508 - val_acc: 0.9571\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9893\n",
      "Epoch 00041: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0364 - acc: 0.9893 - val_loss: 0.1574 - val_acc: 0.9597\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9856\n",
      "Epoch 00042: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0488 - acc: 0.9855 - val_loss: 0.1227 - val_acc: 0.9644\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9901\n",
      "Epoch 00043: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0364 - acc: 0.9901 - val_loss: 0.1487 - val_acc: 0.9574\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9922\n",
      "Epoch 00044: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0291 - acc: 0.9922 - val_loss: 0.1352 - val_acc: 0.9627\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9920\n",
      "Epoch 00045: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0287 - acc: 0.9920 - val_loss: 0.1507 - val_acc: 0.9639\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9882\n",
      "Epoch 00046: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0398 - acc: 0.9882 - val_loss: 0.1392 - val_acc: 0.9625\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9933\n",
      "Epoch 00047: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0256 - acc: 0.9933 - val_loss: 0.1315 - val_acc: 0.9653\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9889\n",
      "Epoch 00048: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0360 - acc: 0.9889 - val_loss: 0.1301 - val_acc: 0.9637\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9937\n",
      "Epoch 00049: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0249 - acc: 0.9938 - val_loss: 0.1396 - val_acc: 0.9632\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9925\n",
      "Epoch 00050: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0261 - acc: 0.9925 - val_loss: 0.1634 - val_acc: 0.9592\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9943\n",
      "Epoch 00051: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0222 - acc: 0.9943 - val_loss: 0.1299 - val_acc: 0.9672\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9889\n",
      "Epoch 00052: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0365 - acc: 0.9888 - val_loss: 0.1508 - val_acc: 0.9627\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9930\n",
      "Epoch 00053: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0248 - acc: 0.9930 - val_loss: 0.1371 - val_acc: 0.9627\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9923\n",
      "Epoch 00054: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0272 - acc: 0.9923 - val_loss: 0.1737 - val_acc: 0.9576\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9941\n",
      "Epoch 00055: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0213 - acc: 0.9941 - val_loss: 0.1384 - val_acc: 0.9641\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9924\n",
      "Epoch 00056: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0260 - acc: 0.9924 - val_loss: 0.1314 - val_acc: 0.9634\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9917\n",
      "Epoch 00057: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0293 - acc: 0.9917 - val_loss: 0.1325 - val_acc: 0.9637\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9958\n",
      "Epoch 00058: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0168 - acc: 0.9958 - val_loss: 0.1311 - val_acc: 0.9653\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9951\n",
      "Epoch 00059: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0192 - acc: 0.9951 - val_loss: 0.1675 - val_acc: 0.9597\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9940\n",
      "Epoch 00060: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0218 - acc: 0.9940 - val_loss: 0.1506 - val_acc: 0.9658\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9940\n",
      "Epoch 00061: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0223 - acc: 0.9940 - val_loss: 0.1511 - val_acc: 0.9620\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9904\n",
      "Epoch 00062: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0308 - acc: 0.9904 - val_loss: 0.1358 - val_acc: 0.9620\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9963\n",
      "Epoch 00063: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0145 - acc: 0.9963 - val_loss: 0.1778 - val_acc: 0.9564\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9931\n",
      "Epoch 00064: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0247 - acc: 0.9931 - val_loss: 0.1404 - val_acc: 0.9620\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9969\n",
      "Epoch 00065: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0139 - acc: 0.9969 - val_loss: 0.1605 - val_acc: 0.9611\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9955\n",
      "Epoch 00066: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0168 - acc: 0.9955 - val_loss: 0.2017 - val_acc: 0.9574\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9913\n",
      "Epoch 00067: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0284 - acc: 0.9913 - val_loss: 0.1510 - val_acc: 0.9609\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9955\n",
      "Epoch 00068: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0173 - acc: 0.9954 - val_loss: 0.1450 - val_acc: 0.9667\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9931\n",
      "Epoch 00069: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0231 - acc: 0.9931 - val_loss: 0.1261 - val_acc: 0.9681\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9953\n",
      "Epoch 00070: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0180 - acc: 0.9953 - val_loss: 0.1491 - val_acc: 0.9634\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9964\n",
      "Epoch 00071: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0142 - acc: 0.9964 - val_loss: 0.1494 - val_acc: 0.9625\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9967\n",
      "Epoch 00072: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0132 - acc: 0.9967 - val_loss: 0.1372 - val_acc: 0.9665\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9953\n",
      "Epoch 00073: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0166 - acc: 0.9953 - val_loss: 0.1721 - val_acc: 0.9613\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9953\n",
      "Epoch 00074: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0181 - acc: 0.9953 - val_loss: 0.1517 - val_acc: 0.9599\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9970\n",
      "Epoch 00075: val_loss did not improve from 0.11583\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0118 - acc: 0.9969 - val_loss: 0.1770 - val_acc: 0.9560\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 764us/sample - loss: 0.1592 - acc: 0.9522\n",
      "Loss: 0.15922518963011625 Accuracy: 0.9522326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GAP_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 44,304\n",
      "Trainable params: 43,920\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 709us/sample - loss: 0.7707 - acc: 0.7778\n",
      "Loss: 0.770710183526868 Accuracy: 0.7777778\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2064        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 65,104\n",
      "Trainable params: 64,592\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 748us/sample - loss: 0.5613 - acc: 0.8401\n",
      "Loss: 0.5612862356726626 Accuracy: 0.84008306\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 192)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           3088        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 107,728\n",
      "Trainable params: 106,960\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 781us/sample - loss: 0.3849 - acc: 0.8914\n",
      "Loss: 0.3848740494511209 Accuracy: 0.8913811\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           4112        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 191,312\n",
      "Trainable params: 190,288\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 821us/sample - loss: 0.2411 - acc: 0.9354\n",
      "Loss: 0.24108471978738177 Accuracy: 0.9354102\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           4112        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 273,872\n",
      "Trainable params: 272,592\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 834us/sample - loss: 0.1709 - acc: 0.9495\n",
      "Loss: 0.1709458163129949 Accuracy: 0.9495327\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 256)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4112        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 356,432\n",
      "Trainable params: 354,896\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 862us/sample - loss: 0.1592 - acc: 0.9522\n",
      "Loss: 0.15922518963011625 Accuracy: 0.9522326\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GAP_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 44,304\n",
      "Trainable params: 43,920\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 820us/sample - loss: 6.6507 - acc: 0.3219\n",
      "Loss: 6.6506658595670425 Accuracy: 0.3219107\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2064        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 65,104\n",
      "Trainable params: 64,592\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 874us/sample - loss: 7.1073 - acc: 0.2663\n",
      "Loss: 7.107347532868509 Accuracy: 0.2662513\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 192)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           3088        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 107,728\n",
      "Trainable params: 106,960\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 876us/sample - loss: 0.4429 - acc: 0.8887\n",
      "Loss: 0.44294568830066255 Accuracy: 0.88868123\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           4112        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 191,312\n",
      "Trainable params: 190,288\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 920us/sample - loss: 0.3021 - acc: 0.9190\n",
      "Loss: 0.3020774522426839 Accuracy: 0.9190031\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           4112        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 273,872\n",
      "Trainable params: 272,592\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 938us/sample - loss: 0.2007 - acc: 0.9464\n",
      "Loss: 0.2006719048110805 Accuracy: 0.94641745\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 256)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4112        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 356,432\n",
      "Trainable params: 354,896\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 959us/sample - loss: 0.2168 - acc: 0.9520\n",
      "Loss: 0.2168404757305119 Accuracy: 0.95202494\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
