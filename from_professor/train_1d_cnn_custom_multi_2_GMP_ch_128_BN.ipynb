{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 128\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 128)   768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 128)   512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 128)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 128)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 128)    512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 256)          1024        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           4112        batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 171,536\n",
      "Trainable params: 170,256\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 128)   768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 128)   512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 128)    512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 128)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 128)    512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 128)     82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 128)     512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 128)     0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 128)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 256)          1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           4112        batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 254,096\n",
      "Trainable params: 252,560\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 128)   768         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 128)   512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 128)    512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 128)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 128)    512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 128)     512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 128)     0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 128)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 256)     1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 256)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 256)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 384)          1536        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           6160        batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 421,776\n",
      "Trainable params: 419,472\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 128)   768         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 128)   512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 128)    512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 128)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 128)    512         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 128)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 128)     512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 128)     0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 128)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 256)     1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 256)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 256)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 256)      1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 256)      0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 256)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 256)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 256)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 512)          2048        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           8208        batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 753,296\n",
      "Trainable params: 750,224\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 128)   768         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 128)   512         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 128)    0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 128)    512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 128)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 128)    512         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 128)     0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 128)     512         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 128)     0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 128)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 256)     1024        conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 256)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 256)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 256)      1024        conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 256)      0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 256)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 256)      1024        conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 256)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 256)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 256)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 512)          2048        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           8208        batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,082,256\n",
      "Trainable params: 1,078,672\n",
      "Non-trainable params: 3,584\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 128)   768         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 128)   512         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 128)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 128)    512         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 128)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 128)    512         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 128)     0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 128)     512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 128)     0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 128)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 256)     1024        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 256)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 256)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 256)      1024        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 256)      0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 256)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 256)      1024        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 256)      0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 256)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 256)       1024        conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 256)       0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 256)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 256)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 512)          2048        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           8208        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,411,216\n",
      "Trainable params: 1,407,120\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7378 - acc: 0.4344\n",
      "Epoch 00001: val_loss improved from inf to 1.81954, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/001-1.8195.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 1.7377 - acc: 0.4344 - val_loss: 1.8195 - val_acc: 0.4410\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1914 - acc: 0.6262\n",
      "Epoch 00002: val_loss improved from 1.81954 to 1.11336, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/002-1.1134.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 1.1916 - acc: 0.6262 - val_loss: 1.1134 - val_acc: 0.6606\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9628 - acc: 0.7056\n",
      "Epoch 00003: val_loss improved from 1.11336 to 0.88844, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/003-0.8884.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.9630 - acc: 0.7056 - val_loss: 0.8884 - val_acc: 0.7275\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8215 - acc: 0.7544\n",
      "Epoch 00004: val_loss improved from 0.88844 to 0.81947, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/004-0.8195.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.8217 - acc: 0.7544 - val_loss: 0.8195 - val_acc: 0.7433\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7252 - acc: 0.7870\n",
      "Epoch 00005: val_loss improved from 0.81947 to 0.72145, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/005-0.7214.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.7253 - acc: 0.7870 - val_loss: 0.7214 - val_acc: 0.7848\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6541 - acc: 0.8063\n",
      "Epoch 00006: val_loss improved from 0.72145 to 0.66286, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/006-0.6629.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6542 - acc: 0.8063 - val_loss: 0.6629 - val_acc: 0.8027\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6011 - acc: 0.8242\n",
      "Epoch 00007: val_loss improved from 0.66286 to 0.60306, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/007-0.6031.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.6013 - acc: 0.8242 - val_loss: 0.6031 - val_acc: 0.8157\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5593 - acc: 0.8361\n",
      "Epoch 00008: val_loss improved from 0.60306 to 0.58807, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/008-0.5881.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5593 - acc: 0.8362 - val_loss: 0.5881 - val_acc: 0.8206\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.8480\n",
      "Epoch 00009: val_loss improved from 0.58807 to 0.57279, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/009-0.5728.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5191 - acc: 0.8480 - val_loss: 0.5728 - val_acc: 0.8309\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.8549\n",
      "Epoch 00010: val_loss did not improve from 0.57279\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4891 - acc: 0.8549 - val_loss: 0.5751 - val_acc: 0.8220\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.8656\n",
      "Epoch 00011: val_loss improved from 0.57279 to 0.53864, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/011-0.5386.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4593 - acc: 0.8656 - val_loss: 0.5386 - val_acc: 0.8358\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4404 - acc: 0.8709\n",
      "Epoch 00012: val_loss did not improve from 0.53864\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4404 - acc: 0.8709 - val_loss: 0.5687 - val_acc: 0.8178\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.8763\n",
      "Epoch 00013: val_loss did not improve from 0.53864\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4169 - acc: 0.8763 - val_loss: 0.5889 - val_acc: 0.8157\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8843\n",
      "Epoch 00014: val_loss improved from 0.53864 to 0.48260, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/014-0.4826.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3972 - acc: 0.8843 - val_loss: 0.4826 - val_acc: 0.8539\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8911\n",
      "Epoch 00015: val_loss did not improve from 0.48260\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3763 - acc: 0.8911 - val_loss: 0.5403 - val_acc: 0.8362\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3637 - acc: 0.8936\n",
      "Epoch 00016: val_loss did not improve from 0.48260\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3638 - acc: 0.8935 - val_loss: 0.5401 - val_acc: 0.8206\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8986\n",
      "Epoch 00017: val_loss did not improve from 0.48260\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3476 - acc: 0.8986 - val_loss: 0.5249 - val_acc: 0.8346\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.9045\n",
      "Epoch 00018: val_loss did not improve from 0.48260\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3304 - acc: 0.9045 - val_loss: 0.4834 - val_acc: 0.8500\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.9068\n",
      "Epoch 00019: val_loss did not improve from 0.48260\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3172 - acc: 0.9068 - val_loss: 0.5271 - val_acc: 0.8358\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9134\n",
      "Epoch 00020: val_loss did not improve from 0.48260\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.3018 - acc: 0.9134 - val_loss: 0.5479 - val_acc: 0.8330\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9171\n",
      "Epoch 00021: val_loss did not improve from 0.48260\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.2906 - acc: 0.9170 - val_loss: 0.4919 - val_acc: 0.8449\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.9196\n",
      "Epoch 00022: val_loss did not improve from 0.48260\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.2811 - acc: 0.9196 - val_loss: 0.5097 - val_acc: 0.8444\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9225\n",
      "Epoch 00023: val_loss did not improve from 0.48260\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.2686 - acc: 0.9225 - val_loss: 0.5549 - val_acc: 0.8346\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9266\n",
      "Epoch 00024: val_loss did not improve from 0.48260\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.2595 - acc: 0.9266 - val_loss: 0.5178 - val_acc: 0.8411\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9309\n",
      "Epoch 00025: val_loss improved from 0.48260 to 0.47716, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/025-0.4772.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.2441 - acc: 0.9309 - val_loss: 0.4772 - val_acc: 0.8498\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9333\n",
      "Epoch 00026: val_loss improved from 0.47716 to 0.46961, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/026-0.4696.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.2377 - acc: 0.9333 - val_loss: 0.4696 - val_acc: 0.8574\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9361\n",
      "Epoch 00027: val_loss improved from 0.46961 to 0.46190, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/027-0.4619.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.2257 - acc: 0.9361 - val_loss: 0.4619 - val_acc: 0.8602\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9393\n",
      "Epoch 00028: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.2167 - acc: 0.9392 - val_loss: 0.5106 - val_acc: 0.8451\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9403\n",
      "Epoch 00029: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.2130 - acc: 0.9403 - val_loss: 0.5091 - val_acc: 0.8491\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9462\n",
      "Epoch 00030: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1974 - acc: 0.9461 - val_loss: 0.4943 - val_acc: 0.8544\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9474\n",
      "Epoch 00031: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1949 - acc: 0.9474 - val_loss: 0.4716 - val_acc: 0.8572\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9508\n",
      "Epoch 00032: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1850 - acc: 0.9508 - val_loss: 0.4951 - val_acc: 0.8481\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9518\n",
      "Epoch 00033: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1778 - acc: 0.9518 - val_loss: 0.4885 - val_acc: 0.8537\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9535\n",
      "Epoch 00034: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1737 - acc: 0.9535 - val_loss: 0.4956 - val_acc: 0.8542\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9573\n",
      "Epoch 00035: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1645 - acc: 0.9573 - val_loss: 0.4919 - val_acc: 0.8556\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9548\n",
      "Epoch 00036: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1655 - acc: 0.9548 - val_loss: 0.4866 - val_acc: 0.8549\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9596\n",
      "Epoch 00037: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1542 - acc: 0.9596 - val_loss: 0.5974 - val_acc: 0.8218\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9581\n",
      "Epoch 00038: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1551 - acc: 0.9581 - val_loss: 0.4763 - val_acc: 0.8581\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9638\n",
      "Epoch 00039: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1384 - acc: 0.9638 - val_loss: 0.4624 - val_acc: 0.8679\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9664\n",
      "Epoch 00040: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1332 - acc: 0.9664 - val_loss: 0.5630 - val_acc: 0.8400\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9649\n",
      "Epoch 00041: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1343 - acc: 0.9650 - val_loss: 0.4980 - val_acc: 0.8495\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9684\n",
      "Epoch 00042: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1247 - acc: 0.9684 - val_loss: 0.4959 - val_acc: 0.8556\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9687\n",
      "Epoch 00043: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1249 - acc: 0.9686 - val_loss: 0.5198 - val_acc: 0.8500\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9688\n",
      "Epoch 00044: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1213 - acc: 0.9688 - val_loss: 0.5238 - val_acc: 0.8495\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9678\n",
      "Epoch 00045: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1225 - acc: 0.9678 - val_loss: 0.4855 - val_acc: 0.8584\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9749\n",
      "Epoch 00046: val_loss did not improve from 0.46190\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1042 - acc: 0.9749 - val_loss: 0.4795 - val_acc: 0.8665\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9679\n",
      "Epoch 00047: val_loss improved from 0.46190 to 0.45525, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv_checkpoint/047-0.4553.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1214 - acc: 0.9679 - val_loss: 0.4553 - val_acc: 0.8710\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9751\n",
      "Epoch 00048: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.1028 - acc: 0.9751 - val_loss: 0.4723 - val_acc: 0.8665\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9775\n",
      "Epoch 00049: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0953 - acc: 0.9775 - val_loss: 0.4658 - val_acc: 0.8644\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9785\n",
      "Epoch 00050: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0917 - acc: 0.9785 - val_loss: 0.5164 - val_acc: 0.8500\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9790\n",
      "Epoch 00051: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0888 - acc: 0.9790 - val_loss: 0.5941 - val_acc: 0.8330\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9790\n",
      "Epoch 00052: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0897 - acc: 0.9789 - val_loss: 0.5762 - val_acc: 0.8491\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9799\n",
      "Epoch 00053: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0854 - acc: 0.9799 - val_loss: 0.4906 - val_acc: 0.8572\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9780\n",
      "Epoch 00054: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0912 - acc: 0.9780 - val_loss: 0.5086 - val_acc: 0.8539\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9796\n",
      "Epoch 00055: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0845 - acc: 0.9796 - val_loss: 0.5383 - val_acc: 0.8535\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9784\n",
      "Epoch 00056: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0880 - acc: 0.9783 - val_loss: 0.4802 - val_acc: 0.8668\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9776\n",
      "Epoch 00057: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0871 - acc: 0.9776 - val_loss: 0.5564 - val_acc: 0.8532\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9821\n",
      "Epoch 00058: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0775 - acc: 0.9821 - val_loss: 0.5449 - val_acc: 0.8481\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9849\n",
      "Epoch 00059: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0684 - acc: 0.9849 - val_loss: 0.5303 - val_acc: 0.8581\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9797\n",
      "Epoch 00060: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0818 - acc: 0.9797 - val_loss: 0.4754 - val_acc: 0.8703\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9873\n",
      "Epoch 00061: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0618 - acc: 0.9873 - val_loss: 0.5005 - val_acc: 0.8642\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9842\n",
      "Epoch 00062: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0679 - acc: 0.9842 - val_loss: 0.5062 - val_acc: 0.8649\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9839\n",
      "Epoch 00063: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0661 - acc: 0.9839 - val_loss: 0.5136 - val_acc: 0.8633\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9877\n",
      "Epoch 00064: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0577 - acc: 0.9877 - val_loss: 0.5316 - val_acc: 0.8633\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9841\n",
      "Epoch 00065: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0664 - acc: 0.9840 - val_loss: 0.5324 - val_acc: 0.8539\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9845\n",
      "Epoch 00066: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0649 - acc: 0.9845 - val_loss: 0.5127 - val_acc: 0.8663\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9914\n",
      "Epoch 00067: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0468 - acc: 0.9914 - val_loss: 0.5120 - val_acc: 0.8647\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9886\n",
      "Epoch 00068: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0522 - acc: 0.9886 - val_loss: 0.6120 - val_acc: 0.8404\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9855\n",
      "Epoch 00069: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0607 - acc: 0.9855 - val_loss: 0.5332 - val_acc: 0.8633\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9898\n",
      "Epoch 00070: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0491 - acc: 0.9897 - val_loss: 0.6280 - val_acc: 0.8386\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9804\n",
      "Epoch 00071: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0721 - acc: 0.9804 - val_loss: 0.5295 - val_acc: 0.8628\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9914\n",
      "Epoch 00072: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0455 - acc: 0.9914 - val_loss: 0.4940 - val_acc: 0.8742\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9871\n",
      "Epoch 00073: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0540 - acc: 0.9871 - val_loss: 0.5900 - val_acc: 0.8584\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9897\n",
      "Epoch 00074: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0477 - acc: 0.9896 - val_loss: 0.5877 - val_acc: 0.8495\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9888\n",
      "Epoch 00075: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0506 - acc: 0.9888 - val_loss: 0.5478 - val_acc: 0.8616\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9880\n",
      "Epoch 00076: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0528 - acc: 0.9880 - val_loss: 0.5603 - val_acc: 0.8560\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9873\n",
      "Epoch 00077: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0529 - acc: 0.9873 - val_loss: 0.5735 - val_acc: 0.8574\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9904\n",
      "Epoch 00078: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0446 - acc: 0.9904 - val_loss: 0.5626 - val_acc: 0.8591\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9820\n",
      "Epoch 00079: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0699 - acc: 0.9819 - val_loss: 0.5413 - val_acc: 0.8656\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9885\n",
      "Epoch 00080: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0485 - acc: 0.9885 - val_loss: 0.5499 - val_acc: 0.8628\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9946\n",
      "Epoch 00081: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0322 - acc: 0.9946 - val_loss: 0.5376 - val_acc: 0.8623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9915\n",
      "Epoch 00082: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0401 - acc: 0.9916 - val_loss: 0.5522 - val_acc: 0.8677\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9930\n",
      "Epoch 00083: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0364 - acc: 0.9930 - val_loss: 0.5385 - val_acc: 0.8665\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9945\n",
      "Epoch 00084: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0333 - acc: 0.9944 - val_loss: 0.5687 - val_acc: 0.8605\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9867\n",
      "Epoch 00085: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0518 - acc: 0.9867 - val_loss: 0.5657 - val_acc: 0.8581\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9892\n",
      "Epoch 00086: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0463 - acc: 0.9891 - val_loss: 0.5191 - val_acc: 0.8730\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9839\n",
      "Epoch 00087: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0585 - acc: 0.9839 - val_loss: 0.5181 - val_acc: 0.8696\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9954\n",
      "Epoch 00088: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0286 - acc: 0.9954 - val_loss: 0.5165 - val_acc: 0.8733\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9955\n",
      "Epoch 00089: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0269 - acc: 0.9954 - val_loss: 0.5796 - val_acc: 0.8689\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9919\n",
      "Epoch 00090: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0379 - acc: 0.9919 - val_loss: 0.5551 - val_acc: 0.8649\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9951\n",
      "Epoch 00091: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0287 - acc: 0.9950 - val_loss: 0.5951 - val_acc: 0.8526\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9890\n",
      "Epoch 00092: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0440 - acc: 0.9891 - val_loss: 0.5640 - val_acc: 0.8626\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9930\n",
      "Epoch 00093: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0347 - acc: 0.9929 - val_loss: 0.5848 - val_acc: 0.8602\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9891\n",
      "Epoch 00094: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0436 - acc: 0.9891 - val_loss: 0.6004 - val_acc: 0.8600\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9912\n",
      "Epoch 00095: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0382 - acc: 0.9912 - val_loss: 0.5998 - val_acc: 0.8595\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9960\n",
      "Epoch 00096: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0247 - acc: 0.9959 - val_loss: 0.5734 - val_acc: 0.8647\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9896\n",
      "Epoch 00097: val_loss did not improve from 0.45525\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.0396 - acc: 0.9896 - val_loss: 0.5495 - val_acc: 0.8712\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.5681 - acc: 0.8384\n",
      "Loss: 0.5680644237363821 Accuracy: 0.8384216\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5367 - acc: 0.5124\n",
      "Epoch 00001: val_loss improved from inf to 1.52242, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/001-1.5224.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.5368 - acc: 0.5124 - val_loss: 1.5224 - val_acc: 0.5495\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9219 - acc: 0.7243\n",
      "Epoch 00002: val_loss improved from 1.52242 to 0.81172, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/002-0.8117.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.9219 - acc: 0.7244 - val_loss: 0.8117 - val_acc: 0.7473\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6936 - acc: 0.7998\n",
      "Epoch 00003: val_loss improved from 0.81172 to 0.64109, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/003-0.6411.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6939 - acc: 0.7997 - val_loss: 0.6411 - val_acc: 0.8164\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5691 - acc: 0.8401\n",
      "Epoch 00004: val_loss improved from 0.64109 to 0.62038, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/004-0.6204.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5692 - acc: 0.8401 - val_loss: 0.6204 - val_acc: 0.8234\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8622\n",
      "Epoch 00005: val_loss improved from 0.62038 to 0.47592, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/005-0.4759.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4859 - acc: 0.8622 - val_loss: 0.4759 - val_acc: 0.8640\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8782\n",
      "Epoch 00006: val_loss improved from 0.47592 to 0.44275, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/006-0.4428.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4275 - acc: 0.8782 - val_loss: 0.4428 - val_acc: 0.8733\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8926\n",
      "Epoch 00007: val_loss did not improve from 0.44275\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3824 - acc: 0.8926 - val_loss: 0.4437 - val_acc: 0.8707\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.9032\n",
      "Epoch 00008: val_loss improved from 0.44275 to 0.42178, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/008-0.4218.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3449 - acc: 0.9032 - val_loss: 0.4218 - val_acc: 0.8791\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9129\n",
      "Epoch 00009: val_loss did not improve from 0.42178\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3137 - acc: 0.9129 - val_loss: 0.4283 - val_acc: 0.8747\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9208\n",
      "Epoch 00010: val_loss did not improve from 0.42178\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2867 - acc: 0.9208 - val_loss: 0.4258 - val_acc: 0.8684\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9314\n",
      "Epoch 00011: val_loss improved from 0.42178 to 0.37851, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/011-0.3785.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2571 - acc: 0.9314 - val_loss: 0.3785 - val_acc: 0.8898\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9368\n",
      "Epoch 00012: val_loss improved from 0.37851 to 0.37130, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/012-0.3713.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2393 - acc: 0.9368 - val_loss: 0.3713 - val_acc: 0.8891\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9418\n",
      "Epoch 00013: val_loss improved from 0.37130 to 0.35281, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/013-0.3528.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.2197 - acc: 0.9418 - val_loss: 0.3528 - val_acc: 0.8984\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9478\n",
      "Epoch 00014: val_loss did not improve from 0.35281\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1991 - acc: 0.9478 - val_loss: 0.3815 - val_acc: 0.8873\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9532\n",
      "Epoch 00015: val_loss did not improve from 0.35281\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1819 - acc: 0.9532 - val_loss: 0.3575 - val_acc: 0.8921\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9594\n",
      "Epoch 00016: val_loss did not improve from 0.35281\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1643 - acc: 0.9594 - val_loss: 0.3686 - val_acc: 0.8877\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9655\n",
      "Epoch 00017: val_loss improved from 0.35281 to 0.33278, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv_checkpoint/017-0.3328.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1462 - acc: 0.9655 - val_loss: 0.3328 - val_acc: 0.9005\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9654\n",
      "Epoch 00018: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1419 - acc: 0.9654 - val_loss: 0.3559 - val_acc: 0.8915\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9718\n",
      "Epoch 00019: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1270 - acc: 0.9718 - val_loss: 0.3921 - val_acc: 0.8798\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9756\n",
      "Epoch 00020: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1137 - acc: 0.9755 - val_loss: 0.3540 - val_acc: 0.8947\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9781\n",
      "Epoch 00021: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1033 - acc: 0.9781 - val_loss: 0.3631 - val_acc: 0.8966\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9799\n",
      "Epoch 00022: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0973 - acc: 0.9799 - val_loss: 0.3477 - val_acc: 0.8968\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9821\n",
      "Epoch 00023: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0905 - acc: 0.9821 - val_loss: 0.3570 - val_acc: 0.8980\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9862\n",
      "Epoch 00024: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0747 - acc: 0.9861 - val_loss: 0.3712 - val_acc: 0.8968\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9824\n",
      "Epoch 00025: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0826 - acc: 0.9824 - val_loss: 0.3340 - val_acc: 0.9005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9907\n",
      "Epoch 00026: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0616 - acc: 0.9907 - val_loss: 0.3828 - val_acc: 0.8898\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9879\n",
      "Epoch 00027: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0654 - acc: 0.9879 - val_loss: 0.3641 - val_acc: 0.8961\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9863\n",
      "Epoch 00028: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0667 - acc: 0.9863 - val_loss: 0.3673 - val_acc: 0.8949\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9932\n",
      "Epoch 00029: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0465 - acc: 0.9932 - val_loss: 0.3478 - val_acc: 0.8984\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9932\n",
      "Epoch 00030: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0468 - acc: 0.9931 - val_loss: 0.3827 - val_acc: 0.8919\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9877\n",
      "Epoch 00031: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0624 - acc: 0.9877 - val_loss: 0.3434 - val_acc: 0.9005\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9950\n",
      "Epoch 00032: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0371 - acc: 0.9950 - val_loss: 0.3993 - val_acc: 0.8887\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9948\n",
      "Epoch 00033: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0363 - acc: 0.9948 - val_loss: 0.3926 - val_acc: 0.8903\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9927\n",
      "Epoch 00034: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0423 - acc: 0.9927 - val_loss: 0.3661 - val_acc: 0.8994\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9938\n",
      "Epoch 00035: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0389 - acc: 0.9938 - val_loss: 0.3585 - val_acc: 0.9008\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9971\n",
      "Epoch 00036: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0282 - acc: 0.9970 - val_loss: 0.3716 - val_acc: 0.9026\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9918\n",
      "Epoch 00037: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0426 - acc: 0.9918 - val_loss: 0.3694 - val_acc: 0.9022\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9935\n",
      "Epoch 00038: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0361 - acc: 0.9934 - val_loss: 0.3731 - val_acc: 0.9001\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9961\n",
      "Epoch 00039: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0277 - acc: 0.9961 - val_loss: 0.3899 - val_acc: 0.8952\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9980\n",
      "Epoch 00040: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0213 - acc: 0.9980 - val_loss: 0.3640 - val_acc: 0.9031\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9961\n",
      "Epoch 00041: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0284 - acc: 0.9961 - val_loss: 0.4133 - val_acc: 0.8945\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9907\n",
      "Epoch 00042: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0418 - acc: 0.9907 - val_loss: 0.3620 - val_acc: 0.9061\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9967\n",
      "Epoch 00043: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0230 - acc: 0.9967 - val_loss: 0.3623 - val_acc: 0.9052\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9968\n",
      "Epoch 00044: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0243 - acc: 0.9967 - val_loss: 0.3646 - val_acc: 0.9057\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9953\n",
      "Epoch 00045: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0284 - acc: 0.9953 - val_loss: 0.3402 - val_acc: 0.9087\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9974\n",
      "Epoch 00046: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0204 - acc: 0.9974 - val_loss: 0.3791 - val_acc: 0.9005\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9985\n",
      "Epoch 00047: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0159 - acc: 0.9985 - val_loss: 0.3833 - val_acc: 0.9012\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9961\n",
      "Epoch 00048: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0251 - acc: 0.9961 - val_loss: 0.3688 - val_acc: 0.9059\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9961\n",
      "Epoch 00049: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0223 - acc: 0.9961 - val_loss: 0.3634 - val_acc: 0.9082\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9986\n",
      "Epoch 00050: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0143 - acc: 0.9986 - val_loss: 0.3762 - val_acc: 0.9026\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9938\n",
      "Epoch 00051: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0287 - acc: 0.9938 - val_loss: 0.3756 - val_acc: 0.9059\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9988\n",
      "Epoch 00052: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0127 - acc: 0.9988 - val_loss: 0.3724 - val_acc: 0.9045\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9982\n",
      "Epoch 00053: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0135 - acc: 0.9982 - val_loss: 0.4022 - val_acc: 0.8966\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9962\n",
      "Epoch 00054: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0219 - acc: 0.9962 - val_loss: 0.3846 - val_acc: 0.9026\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9908\n",
      "Epoch 00055: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0384 - acc: 0.9908 - val_loss: 0.3782 - val_acc: 0.9050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9988\n",
      "Epoch 00056: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0118 - acc: 0.9988 - val_loss: 0.3841 - val_acc: 0.9031\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9993\n",
      "Epoch 00057: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0083 - acc: 0.9993 - val_loss: 0.3957 - val_acc: 0.9038\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9933\n",
      "Epoch 00058: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0276 - acc: 0.9933 - val_loss: 0.4001 - val_acc: 0.8968\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9970\n",
      "Epoch 00059: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0176 - acc: 0.9970 - val_loss: 0.3912 - val_acc: 0.9022\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9952\n",
      "Epoch 00060: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0228 - acc: 0.9952 - val_loss: 0.3616 - val_acc: 0.9106\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9996\n",
      "Epoch 00061: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0076 - acc: 0.9996 - val_loss: 0.4311 - val_acc: 0.8966\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9995\n",
      "Epoch 00062: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0078 - acc: 0.9995 - val_loss: 0.3735 - val_acc: 0.9061\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9972\n",
      "Epoch 00063: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0162 - acc: 0.9972 - val_loss: 0.4036 - val_acc: 0.9010\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9985\n",
      "Epoch 00064: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0126 - acc: 0.9985 - val_loss: 0.3923 - val_acc: 0.8977\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9993\n",
      "Epoch 00065: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0088 - acc: 0.9992 - val_loss: 0.4338 - val_acc: 0.8942\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9897\n",
      "Epoch 00066: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0388 - acc: 0.9896 - val_loss: 0.3809 - val_acc: 0.9075\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9981\n",
      "Epoch 00067: val_loss did not improve from 0.33278\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0123 - acc: 0.9981 - val_loss: 0.3818 - val_acc: 0.9122\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3745 - acc: 0.8833\n",
      "Loss: 0.37454007097122455 Accuracy: 0.8832814\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1971 - acc: 0.6341\n",
      "Epoch 00001: val_loss improved from inf to 1.13694, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/001-1.1369.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 1.1972 - acc: 0.6341 - val_loss: 1.1369 - val_acc: 0.6587\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.8476\n",
      "Epoch 00002: val_loss improved from 1.13694 to 0.48283, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/002-0.4828.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5389 - acc: 0.8476 - val_loss: 0.4828 - val_acc: 0.8640\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8918\n",
      "Epoch 00003: val_loss improved from 0.48283 to 0.47327, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/003-0.4733.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3857 - acc: 0.8918 - val_loss: 0.4733 - val_acc: 0.8579\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9144\n",
      "Epoch 00004: val_loss improved from 0.47327 to 0.35332, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/004-0.3533.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3093 - acc: 0.9144 - val_loss: 0.3533 - val_acc: 0.8959\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2556 - acc: 0.9293\n",
      "Epoch 00005: val_loss improved from 0.35332 to 0.29070, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/005-0.2907.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2556 - acc: 0.9293 - val_loss: 0.2907 - val_acc: 0.9145\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9415\n",
      "Epoch 00006: val_loss did not improve from 0.29070\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2152 - acc: 0.9415 - val_loss: 0.3318 - val_acc: 0.8968\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9501\n",
      "Epoch 00007: val_loss improved from 0.29070 to 0.29000, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/007-0.2900.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1878 - acc: 0.9500 - val_loss: 0.2900 - val_acc: 0.9115\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9560\n",
      "Epoch 00008: val_loss improved from 0.29000 to 0.26577, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/008-0.2658.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1645 - acc: 0.9560 - val_loss: 0.2658 - val_acc: 0.9171\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9620\n",
      "Epoch 00009: val_loss improved from 0.26577 to 0.22788, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/009-0.2279.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1445 - acc: 0.9620 - val_loss: 0.2279 - val_acc: 0.9292\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9689\n",
      "Epoch 00010: val_loss did not improve from 0.22788\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1245 - acc: 0.9689 - val_loss: 0.2444 - val_acc: 0.9257\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9758\n",
      "Epoch 00011: val_loss did not improve from 0.22788\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1040 - acc: 0.9757 - val_loss: 0.2490 - val_acc: 0.9262\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9761\n",
      "Epoch 00012: val_loss did not improve from 0.22788\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1002 - acc: 0.9761 - val_loss: 0.2577 - val_acc: 0.9257\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9830\n",
      "Epoch 00013: val_loss did not improve from 0.22788\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0786 - acc: 0.9830 - val_loss: 0.2456 - val_acc: 0.9238\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9870\n",
      "Epoch 00014: val_loss improved from 0.22788 to 0.20381, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/014-0.2038.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0666 - acc: 0.9870 - val_loss: 0.2038 - val_acc: 0.9408\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9898\n",
      "Epoch 00015: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0570 - acc: 0.9898 - val_loss: 0.2275 - val_acc: 0.9292\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9924\n",
      "Epoch 00016: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0490 - acc: 0.9924 - val_loss: 0.2522 - val_acc: 0.9276\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9920\n",
      "Epoch 00017: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0471 - acc: 0.9920 - val_loss: 0.2220 - val_acc: 0.9322\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9906\n",
      "Epoch 00018: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0483 - acc: 0.9906 - val_loss: 0.2104 - val_acc: 0.9376\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9965\n",
      "Epoch 00019: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0285 - acc: 0.9965 - val_loss: 0.2307 - val_acc: 0.9315\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9952\n",
      "Epoch 00020: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0320 - acc: 0.9952 - val_loss: 0.2389 - val_acc: 0.9308\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9980\n",
      "Epoch 00021: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0227 - acc: 0.9980 - val_loss: 0.2142 - val_acc: 0.9359\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9983\n",
      "Epoch 00022: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0195 - acc: 0.9982 - val_loss: 0.2236 - val_acc: 0.9348\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9906\n",
      "Epoch 00023: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0425 - acc: 0.9906 - val_loss: 0.2557 - val_acc: 0.9255\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9990\n",
      "Epoch 00024: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0157 - acc: 0.9989 - val_loss: 0.2474 - val_acc: 0.9273\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9926\n",
      "Epoch 00025: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0350 - acc: 0.9926 - val_loss: 0.2326 - val_acc: 0.9345\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9979\n",
      "Epoch 00026: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0180 - acc: 0.9979 - val_loss: 0.2248 - val_acc: 0.9350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9953\n",
      "Epoch 00027: val_loss did not improve from 0.20381\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0268 - acc: 0.9953 - val_loss: 0.2207 - val_acc: 0.9380\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9981\n",
      "Epoch 00028: val_loss improved from 0.20381 to 0.20139, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/028-0.2014.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0165 - acc: 0.9981 - val_loss: 0.2014 - val_acc: 0.9432\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9992\n",
      "Epoch 00029: val_loss did not improve from 0.20139\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0090 - acc: 0.9992 - val_loss: 0.2243 - val_acc: 0.9387\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9981\n",
      "Epoch 00030: val_loss did not improve from 0.20139\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0143 - acc: 0.9981 - val_loss: 0.2417 - val_acc: 0.9345\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9993\n",
      "Epoch 00031: val_loss did not improve from 0.20139\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0094 - acc: 0.9993 - val_loss: 0.2411 - val_acc: 0.9327\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9995\n",
      "Epoch 00032: val_loss did not improve from 0.20139\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0077 - acc: 0.9995 - val_loss: 0.2851 - val_acc: 0.9262\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9936\n",
      "Epoch 00033: val_loss did not improve from 0.20139\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0282 - acc: 0.9936 - val_loss: 0.2480 - val_acc: 0.9278\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9983\n",
      "Epoch 00034: val_loss did not improve from 0.20139\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0130 - acc: 0.9983 - val_loss: 0.2258 - val_acc: 0.9399\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9987\n",
      "Epoch 00035: val_loss did not improve from 0.20139\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0100 - acc: 0.9987 - val_loss: 0.2308 - val_acc: 0.9378\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9941\n",
      "Epoch 00036: val_loss did not improve from 0.20139\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0268 - acc: 0.9941 - val_loss: 0.2186 - val_acc: 0.9436\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9991\n",
      "Epoch 00037: val_loss did not improve from 0.20139\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0080 - acc: 0.9991 - val_loss: 0.2164 - val_acc: 0.9390\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9997\n",
      "Epoch 00038: val_loss did not improve from 0.20139\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0046 - acc: 0.9997 - val_loss: 0.2330 - val_acc: 0.9355\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9956\n",
      "Epoch 00039: val_loss improved from 0.20139 to 0.19975, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/039-0.1997.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0203 - acc: 0.9956 - val_loss: 0.1997 - val_acc: 0.9434\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9993\n",
      "Epoch 00040: val_loss did not improve from 0.19975\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0074 - acc: 0.9993 - val_loss: 0.2048 - val_acc: 0.9457\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9996\n",
      "Epoch 00041: val_loss did not improve from 0.19975\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0050 - acc: 0.9996 - val_loss: 0.2468 - val_acc: 0.9341\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9993\n",
      "Epoch 00042: val_loss did not improve from 0.19975\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0068 - acc: 0.9993 - val_loss: 0.2899 - val_acc: 0.9222\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9932\n",
      "Epoch 00043: val_loss did not improve from 0.19975\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0278 - acc: 0.9932 - val_loss: 0.2263 - val_acc: 0.9376\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9996\n",
      "Epoch 00044: val_loss did not improve from 0.19975\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0054 - acc: 0.9995 - val_loss: 0.2138 - val_acc: 0.9441\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9966\n",
      "Epoch 00045: val_loss did not improve from 0.19975\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0156 - acc: 0.9965 - val_loss: 0.2319 - val_acc: 0.9397\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9961\n",
      "Epoch 00046: val_loss improved from 0.19975 to 0.19807, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/046-0.1981.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0184 - acc: 0.9961 - val_loss: 0.1981 - val_acc: 0.9432\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9995\n",
      "Epoch 00047: val_loss improved from 0.19807 to 0.19787, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/047-0.1979.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0056 - acc: 0.9995 - val_loss: 0.1979 - val_acc: 0.9492\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9998\n",
      "Epoch 00048: val_loss did not improve from 0.19787\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0038 - acc: 0.9998 - val_loss: 0.2074 - val_acc: 0.9422\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9996\n",
      "Epoch 00049: val_loss did not improve from 0.19787\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0043 - acc: 0.9995 - val_loss: 0.2663 - val_acc: 0.9273\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9950\n",
      "Epoch 00050: val_loss did not improve from 0.19787\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0201 - acc: 0.9950 - val_loss: 0.2395 - val_acc: 0.9364\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9997\n",
      "Epoch 00051: val_loss did not improve from 0.19787\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0037 - acc: 0.9997 - val_loss: 0.2011 - val_acc: 0.9471\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 00052: val_loss did not improve from 0.19787\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0028 - acc: 0.9998 - val_loss: 0.2298 - val_acc: 0.9392\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9950\n",
      "Epoch 00053: val_loss did not improve from 0.19787\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0201 - acc: 0.9950 - val_loss: 0.2282 - val_acc: 0.9373\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9998\n",
      "Epoch 00054: val_loss improved from 0.19787 to 0.18942, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/054-0.1894.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0042 - acc: 0.9998 - val_loss: 0.1894 - val_acc: 0.9492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9998\n",
      "Epoch 00055: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0022 - acc: 0.9998 - val_loss: 0.2116 - val_acc: 0.9464\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9995\n",
      "Epoch 00056: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0039 - acc: 0.9995 - val_loss: 0.2593 - val_acc: 0.9306\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9950\n",
      "Epoch 00057: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0197 - acc: 0.9950 - val_loss: 0.1934 - val_acc: 0.9495\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9999\n",
      "Epoch 00058: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0027 - acc: 0.9999 - val_loss: 0.2208 - val_acc: 0.9397\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9987\n",
      "Epoch 00059: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0080 - acc: 0.9987 - val_loss: 0.2631 - val_acc: 0.9327\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9996\n",
      "Epoch 00060: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0039 - acc: 0.9996 - val_loss: 0.2115 - val_acc: 0.9441\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9982\n",
      "Epoch 00061: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0099 - acc: 0.9982 - val_loss: 0.2696 - val_acc: 0.9304\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9960\n",
      "Epoch 00062: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0160 - acc: 0.9960 - val_loss: 0.2243 - val_acc: 0.9392\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9985\n",
      "Epoch 00063: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0073 - acc: 0.9985 - val_loss: 0.2153 - val_acc: 0.9448\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 00064: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0029 - acc: 0.9997 - val_loss: 0.2170 - val_acc: 0.9432\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9954\n",
      "Epoch 00065: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0167 - acc: 0.9954 - val_loss: 0.2034 - val_acc: 0.9481\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00066: val_loss did not improve from 0.18942\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0025 - acc: 0.9998 - val_loss: 0.2302 - val_acc: 0.9418\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9984\n",
      "Epoch 00067: val_loss improved from 0.18942 to 0.18865, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv_checkpoint/067-0.1887.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0085 - acc: 0.9984 - val_loss: 0.1887 - val_acc: 0.9499\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9996\n",
      "Epoch 00068: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0040 - acc: 0.9996 - val_loss: 0.2146 - val_acc: 0.9422\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9998\n",
      "Epoch 00069: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0023 - acc: 0.9998 - val_loss: 0.1964 - val_acc: 0.9518\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9995\n",
      "Epoch 00070: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0041 - acc: 0.9995 - val_loss: 0.2569 - val_acc: 0.9331\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9995\n",
      "Epoch 00071: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0037 - acc: 0.9995 - val_loss: 0.2779 - val_acc: 0.9324\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9954\n",
      "Epoch 00072: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0173 - acc: 0.9953 - val_loss: 0.2395 - val_acc: 0.9399\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9964\n",
      "Epoch 00073: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0147 - acc: 0.9964 - val_loss: 0.2068 - val_acc: 0.9448\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 00074: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0024 - acc: 0.9999 - val_loss: 0.2249 - val_acc: 0.9427\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00075: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.2109 - val_acc: 0.9471\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9997\n",
      "Epoch 00076: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0021 - acc: 0.9997 - val_loss: 0.2220 - val_acc: 0.9439\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00077: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.2058 - val_acc: 0.9478\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 00078: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0031 - acc: 0.9997 - val_loss: 0.2684 - val_acc: 0.9338\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9964\n",
      "Epoch 00079: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0146 - acc: 0.9964 - val_loss: 0.2438 - val_acc: 0.9399\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9996\n",
      "Epoch 00080: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0032 - acc: 0.9996 - val_loss: 0.2132 - val_acc: 0.9446\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 00081: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0024 - acc: 0.9999 - val_loss: 0.2445 - val_acc: 0.9380\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 00082: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0034 - acc: 0.9996 - val_loss: 0.2339 - val_acc: 0.9450\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 00083: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0030 - acc: 0.9997 - val_loss: 0.3411 - val_acc: 0.9194\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9948\n",
      "Epoch 00084: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0186 - acc: 0.9948 - val_loss: 0.2222 - val_acc: 0.9439\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9972\n",
      "Epoch 00085: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0110 - acc: 0.9972 - val_loss: 0.2218 - val_acc: 0.9469\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00086: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0025 - acc: 0.9998 - val_loss: 0.2023 - val_acc: 0.9460\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 00087: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2244 - val_acc: 0.9478\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9984\n",
      "Epoch 00088: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0076 - acc: 0.9983 - val_loss: 0.2620 - val_acc: 0.9352\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9974\n",
      "Epoch 00089: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0105 - acc: 0.9974 - val_loss: 0.2215 - val_acc: 0.9448\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9993\n",
      "Epoch 00090: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0038 - acc: 0.9993 - val_loss: 0.2095 - val_acc: 0.9502\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00091: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.2159 - val_acc: 0.9495\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 00092: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0028 - acc: 0.9997 - val_loss: 0.5625 - val_acc: 0.8728\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9992\n",
      "Epoch 00093: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0043 - acc: 0.9992 - val_loss: 0.2261 - val_acc: 0.9411\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9994\n",
      "Epoch 00094: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0038 - acc: 0.9994 - val_loss: 0.2635 - val_acc: 0.9373\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9995\n",
      "Epoch 00095: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0035 - acc: 0.9995 - val_loss: 0.2980 - val_acc: 0.9278\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9998\n",
      "Epoch 00096: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0023 - acc: 0.9998 - val_loss: 0.2304 - val_acc: 0.9457\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00097: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.2855 - val_acc: 0.9313\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9990\n",
      "Epoch 00098: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0062 - acc: 0.9990 - val_loss: 0.3346 - val_acc: 0.9231\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9967\n",
      "Epoch 00099: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0125 - acc: 0.9967 - val_loss: 0.2392 - val_acc: 0.9411\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00100: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0031 - acc: 0.9996 - val_loss: 0.2306 - val_acc: 0.9469\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 00101: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2117 - val_acc: 0.9462\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 00102: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0017 - acc: 0.9999 - val_loss: 0.2183 - val_acc: 0.9467\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 00103: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0022 - acc: 0.9997 - val_loss: 0.4301 - val_acc: 0.9117\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9992\n",
      "Epoch 00104: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0047 - acc: 0.9992 - val_loss: 0.3042 - val_acc: 0.9285\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9982\n",
      "Epoch 00105: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0072 - acc: 0.9982 - val_loss: 0.2570 - val_acc: 0.9394\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9970\n",
      "Epoch 00106: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0115 - acc: 0.9970 - val_loss: 0.2342 - val_acc: 0.9427\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00107: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.2113 - val_acc: 0.9476\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 00108: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0022 - acc: 0.9999 - val_loss: 0.2206 - val_acc: 0.9481\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.5514e-04 - acc: 1.0000\n",
      "Epoch 00109: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 7.5514e-04 - acc: 1.0000 - val_loss: 0.2089 - val_acc: 0.9502\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 00110: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 0.2678 - val_acc: 0.9371\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.0856e-04 - acc: 1.0000\n",
      "Epoch 00111: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 6.0853e-04 - acc: 1.0000 - val_loss: 0.2122 - val_acc: 0.9485\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4342e-04 - acc: 1.0000\n",
      "Epoch 00112: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 3.4702e-04 - acc: 1.0000 - val_loss: 0.2164 - val_acc: 0.9506\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9958\n",
      "Epoch 00113: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0142 - acc: 0.9958 - val_loss: 0.2587 - val_acc: 0.9378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9991\n",
      "Epoch 00114: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0045 - acc: 0.9991 - val_loss: 0.2400 - val_acc: 0.9446\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9997\n",
      "Epoch 00115: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0021 - acc: 0.9997 - val_loss: 0.2282 - val_acc: 0.9457\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 00116: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.2312 - val_acc: 0.9471\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9981\n",
      "Epoch 00117: val_loss did not improve from 0.18865\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0075 - acc: 0.9981 - val_loss: 0.2475 - val_acc: 0.9415\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXl8FEX6h5+ayeROSEgCARIIIGcI4RZ/KKCsHKLgiggu4Hqirse67LqynqzHqqureOCBJ14gcsolgoKIgHIj900SAuQgCUkmyVz1+6MymSRMTjJJYOr5fCaT7q6uerunu771vlVdLaSUaDQajUbjxNDQBmg0Go2mcaGFQaPRaDRl0MKg0Wg0mjJoYdBoNBpNGbQwaDQajaYMWhg0Go1GUwYtDBqNRqMpgxYGjUaj0ZRBC4NGo9FoyuDT0AbUlMjISBkXF9fQZmg0Gs1FxdatWzOklFHVSXvRCUNcXBxbtmxpaDM0Go3mokIIcaK6aXUoSaPRaDRl0MKg0Wg0mjJoYdBoNBpNGS66PgZ3WK1WUlJSKCwsbGhTLlr8/f2JiYnBZDI1tCkajaaB8ZgwCCE+Bq4H0qSU3dxsnwA8BgggF7hfSrmzNmWlpKQQEhJCXFwcQogLMdsrkVKSmZlJSkoKbdu2bWhzNBpNA+PJUNKnwPBKth8DBkkpE4DngJm1LaiwsJCIiAgtCrVECEFERIT2uDQaDeBBj0FKuU4IEVfJ9g2lFjcBMRdSnhaFC0OfP41G46SxdD7fBazwZAF2ewFFRSdxOKyeLEaj0WguehpcGIQQV6OE4bFK0kwWQmwRQmxJT0+vVTkORwEWyymktNXS0orJzs7mnXfeqdW+1113HdnZ2dVOP23aNF599dValaXRaDTVoUGFQQjRHfgQGC2lzKwonZRyppSyj5SyT1RUtZ7odoPzUB213L9iKhMGm61yIVq+fDlhYWF1bpNGo9HUlgYTBiFEa2ABMElKebAeygPUCJy6ZurUqRw5coQePXrw6KOPsnbtWq666ipGjRpF165dAbjxxhvp3bs38fHxzJzp6mePi4sjIyOD48eP06VLF+655x7i4+MZOnQoBQUFlZa7Y8cO+vfvT/fu3fnjH/9IVlYWAG+++SZdu3ale/fujB8/HoCffvqJHj160KNHD3r27Elubm6dnweNRnNp4MnhqrOBwUCkECIFeAYwAUgp3wOeBiKAd4orbZuUss+Flnvo0CPk5e04b72UdhwOMwZDIEIYa5RncHAPOnSYXuH2l156id27d7Njhyp37dq1bNu2jd27d5cM//z4449p2rQpBQUF9O3blzFjxhAREVHO9kPMnj2bDz74gFtuuYX58+czceLECsu97bbbeOuttxg0aBBPP/00//73v5k+fTovvfQSx44dw8/PryRM9eqrrzJjxgwGDBhAXl4e/v7+NToHGo3Ge/CYxyClvFVK2UJKaZJSxkgpP5JSvlcsCkgp75ZShkspexR/LlgUGhP9+vUr80zAm2++SWJiIv379yc5OZlDhw6dt0/btm3p0aMHAL179+b48eMV5p+Tk0N2djaDBg0C4M9//jPr1q0DoHv37kyYMIEvvvgCHx+l/QMGDGDKlCm8+eabZGdnl6zXaDSa8lxytUNFLXu7PR+zeR8BAZfh4+P5mH5QUFDJ/2vXrmX16tVs3LiRwMBABg8e7PaZAT8/v5L/jUZjlaGkili2bBnr1q1jyZIlvPDCC/z+++9MnTqVkSNHsnz5cgYMGMDKlSvp3LlzrfLXaDSXNg0+Kqn+8FwfQ0hISKUx+5ycHMLDwwkMDGT//v1s2rTpgsts0qQJ4eHh/PzzzwB8/vnnDBo0CIfDQXJyMldffTUvv/wyOTk55OXlceTIERISEnjsscfo27cv+/fvv2AbNBrNpckl5zFUjPMBrroXhoiICAYMGEC3bt0YMWIEI0eOLLN9+PDhvPfee3Tp0oVOnTrRv3//Oil31qxZ3HfffZjNZtq1a8cnn3yC3W5n4sSJ5OTkIKXk4YcfJiwsjKeeeoo1a9ZgMBiIj49nxIgRdWKDRqO59BCeaEF7kj59+sjyL+rZt28fXbp0qXQ/h6OQ/Pzd+Pu3xWSKqDStt1Kd86jRaC5OhBBbq9uX6z2hJHMRfmcAq37yWaPRaCrDe4TBYsE3G7DZG9oSjUajadR4jTAIUXyojrp/8lmj0WguJbxGGHAKg9TCoNFoNJXhPcJgUE87X2yd7RqNRlPfeI8wON83oENJGo1GUyleIwzC4AwlNQ6PITg4uEbrNRqNpr7wGmEo8Rh0H4NGo9FUihcKg2em3Z4xY0bJsvNlOnl5eQwZMoRevXqRkJDA4sWLq52nlJJHH32Ubt26kZCQwNdffw3AqVOnGDhwID169KBbt278/PPP2O12br/99pK0r7/+ep0fo0aj8R4uvSkxHnkEdpw/7TYOB+Tn4+NnBN/AmuXZowdMr3ja7XHjxvHII4/wwAMPADB37lxWrlyJv78/CxcuJDQ0lIyMDPr378+oUaOq9X7lBQsWsGPHDnbu3ElGRgZ9+/Zl4MCBfPXVVwwbNownnngCu92O2Wxmx44dnDx5kt27dwPU6I1wGo1GU55LTxgqwoMvu+/ZsydpaWmkpqaSnp5OeHg4sbGxWK1WHn/8cdatW4fBYODkyZOcOXOG6OjoKvNcv349t956K0ajkebNmzNo0CA2b95M3759ufPOO7Fardx444306NGDdu3acfToUR566CFGjhzJ0KFDPXasGo3m0ufSE4aKWvZWK+zcia1FIL6tutZ5sWPHjmXevHmcPn2acePGAfDll1+Snp7O1q1bMZlMxMXFuZ1uuyYMHDiQdevWsWzZMm6//XamTJnCbbfdxs6dO1m5ciXvvfcec+fO5eOPP66Lw9JoNF6I7mOoI8aNG8ecOXOYN28eY8eOBdR0282aNcNkMrFmzRpOnDhR7fyuuuoqvv76a+x2O+np6axbt45+/fpx4sQJmjdvzj333MPdd9/Ntm3byMjIwOFwMGbMGJ5//nm2bdvmkWPUaDTewaXnMVSEc7iqwzPCEB8fT25uLq1ataJFixYATJgwgRtuuIGEhAT69OlToxfj/PGPf2Tjxo0kJiYihOC///0v0dHRzJo1i1deeQWTyURwcDCfffYZJ0+e5I477sBR/IzGiy++6JFj1Gg03oHXTLuNlLB1K5YoP3zbJHjQwosXPe22RnPpoqfddocQSEBcZEKo0Wg09Y33CAOol7hpYdBoNJpK8TJhEFoYNBqNpgq8SxgMeOKVzxqNRnNJ4VXCIIXw2KgkjUajuVTwmDAIIT4WQqQJIXZXsF0IId4UQhwWQuwSQvTylC2uQtGhJI1Go6kCT3oMnwLDK9k+AuhQ/JkMvOtBWxQe6mPIzs7mnXfeqdW+1113nZ7bSKPRNCo8JgxSynXA2UqSjAY+k4pNQJgQooWn7AFACIQHHIbKhMFms1W67/LlywkLC6t7ozQajaaWNOSTz62A5FLLKcXrTnmsRA95DFOnTuXIkSP06NGDa6+9lpEjR/LUU08RHh7O/v37OXjwIDfeeCPJyckUFhby17/+lcmTJwMQFxfHli1byMvLY8SIEVx55ZVs2LCBVq1asXjxYgICArDb4dw5Zf733y/hlVeep7DQQpMmEbz22pdERjbHbM7jpZceYufOLQgheOyxZ7jiijH88st3vPba4zgcdpo2jWT27B+w2SAgAPz8wGKBggIoKoIzZ+C228DHB5o2hSZNICcHMjMhLw9sNjVJ7eWXw5//DFddBRs3wvLlcPSoysNuh9694brrIDYWli2Db7+F5GTIz1fb+/aFIUMgIgI2bIBNmyArS01n5XCo8k0m6NwZ7rgDRoyA336D999XE+fGx0PPntCihToGux22b4dff4UTJ5SdNps6Xz4+EBoKV1+t8rHZYOlS+OEHZY+UEBYGw4fDyJHqWJcuhZ9/VscjpcrHZAJfX1Xu8OHKtlWrYMkSSE+HyEiVT0GBOmc2GzRvDtHR0Lo1tGunlo8fh/37ISkJzp6F7Gzo0gWuvRY6dYKffoLvv1d5NGumzlFurrLLma/dDkajssdkcl2HzZur4xw8WP2+Z8+q8/7rr+ocp6e7bgODQeUhhDrnzuM0GMDfHxISoE8flc/WrbBzp7LDalXl+/ioD6h9fXzU9TB6NCQmwuHDsG+f63P0qLrWHA6VZ8uW6vdzXmdGI6SkKHuzsqCwUJ1/UDb5+Kjf2t9frXOeBynVx+FQtjmvUYdD7RcSovJv0QI6dIBWreDIEdi1S/0GFov6lD5+o9F1fp1lhoZCeLj6REQou9PT1Xk5cECV7aT0+TQaVZ52u8te5zElJsIf/gAdO6rressWOHVKXUMFBa59/P0hJkbdT2PHqo+n8eiTz0KIOGCplLKbm21LgZeklOuLl38AHpNSbnGTdjIq3ETr1q17l59zqPQTuxXNug2AOQ+JRASG1Og4qph1m+PHj3P99deXTHu9du1aRo4cye7du2nbti0AZ8+epWnTphQUFNC3b1/Wrv2JoKAIOnWKY/nyLZw9m8c111zGokVb6NatBw89dAvDh49i5MiJnD3reiPpuXNZhISEIYRg8eIPOXFiH48++j/+97/HKCoq4umnp2MyQVJSFmDj1lt7MXPmOtq1a0ta2lmaNGnqVh/9/CA9fR+vvNIFu11VROfOqRsiIkLdYD4+6sb74QdVSRmN6sI1mdRN5+en7Pz997JvUG3fXlU0QUEq/fr1qhIAddH366cqUJNJ3TA2m6oU1q+HtDQIDASzWdlwxRWuirU0JpOqtDt2VDe00aiO0WaD06dVhVtQoNKGhKgbMipK3bzJyfDjj6oyArV+yBBVoTjPldWqbChtu8GgKsN27dT5OntW2RoWpso/c0bd6CdOqH2dREdDXJwSk+Bg2LYNDh5U23x8YMAAVYGlpal8Q0JU2tBQdZzO8+6s1IRQn6NHlYDa7WXPTbNm6rzFxqplZ0VaugJ1TiVmt6tGwI4d6jxLCW3aqHsgMlLZ5yzfWRkKoUR21Splb2mioqBrV7jsMvVbG40q/1OnIDVVCWNOjsrLWflFRqq0vr4u4bJa1TVRWOiqcJ3C5qzQTSaXfc7K+Nw5lX9KihIrs1n9romJ6rr081P7OX9nh8NVIVssqsyCApVPdrb6jc+eVSIZHKyu665dldg5z63zfJY+z07Bcc7MU1SkxHr7dtfvnpCgrouAAPXx8VHpCwrUNZqcDHfeCY89Rq2oyZPPDekxnARiSy3HFK87DynlTGAmqCkxLqhU6fzjuWm4Afr161ciCgDTp7/JggULcTggOTmZpUsP0a1bREnFZbVCq1ZtadOmBzk5EBfXm927j3PFFap10rSpurB27kzhscfGkZZ2CqvVQtu2bUlIgN9/X82MGXOwWtWF1LlzOL/9toRrrhlI375tycuD+PimhIerG6GoSH1MJnURGgyqZbdiRdXHVliovIBff4Urr1SVbEgprT17VlUSKSmqdd21a9lZz6WEQ4fUDZuYqCoAd1ityhtZsgT694fx49XN6CwjM9PVqu/YUd3kldn888/qHF555fll5ufDunWqRdi3r0rnDilh715VkQ8cqESzKqRUlfzp06qSdRc5TEpS56Rfv7LnsqacO6e8OCHUNdO8uapwazPrfG6uqhyrc4ygRPiXX+DYMfV7dOpU/X3rAymVNxIefuGz8FutZSv62pKRobzI+HiXuDQGGlIYvgUeFELMAS4HcqSUFxxGqqxl79h/DGkrwhDf25OvZwAgKCgIUJX04sVr+fbb1bz33kYCAgK5//7BBAYWEhenKuaEBJUuJMSPhOJpnFq3NpKbW0CPHmUvvscff4gpU6YwatQo1q5dy7Rp00q2RUSolpmz1eK8cFu2PN8+Z6ukNvj7wy23qI87mjaF4pnH3SKEqjiqwmRSoYnRo92X0bRp9ewFZfO111a8PShIhZqqQgh1E8fHV79sIVQF3bx5xWlat1afCyU0FIYNu/B8oOYC5eMDgwapT2PEKZZ1QekQ3oUQGak+jQ1PDledDWwEOgkhUoQQdwkh7hNC3FecZDlwFDgMfAD8xVO2OJEGUewx1O17n0NCQsjNzcVigZMnVQwzJ0fFZvfsgZMnc4iICCcxMZCAgP3s2rWJZs3UBeF0i8vjdI/Lt0hycnJo1aoVALNmzSpZf+211zJjxoyS/bKysujfvz/r1q3j2LFjgApnaTQaTVV4zGOQUt5axXYJPOCp8t0ilDBIKevUY/D3jyAxcQBdunTj//5vBMOGjcTXV7UQTSa4997hrF79Hv36daFTp07079+/1mVNmzaNsWPHEh4ezjXXXFNS6T/55JM88MADdOvWDaPRyDPPPMNNN93EzJkzuemmm3A4HDRr1oxVq1bV1WFrNJpLFO+ZdhuwH94LZjOiWyIGw4X5gna7ilemp6v4tMGgPIBmzVwjJy429LTbGs2ly8XS+Vz/CEPxcwy1E0Mp1YgK5wgUh0N1eDpHUlTUYanRaDQXE14mDLXrY3A4lGeQlqZGwRgMqhMrIkKNkvF0R7amavIseczbO488Sx4TEiYQHhBeafrTeaeJCozCaDhfzVNzU/lo20e0CGlBQrMEEqMT8fdxuYFHzh7h812fc3+f+2keXEmPcg04fPYwBzMPcnXc1QSY1KiA307+xmc7P2NI2yGM7jwagzCw5MAS/rn6nwxvP5zXhr2GqOHFZ3fYOZB5gNjQWEL8VO9yviWfbae2Ed8snqYBddM7K6Uk+VwyNoeNUL9Q/H38Sc1NJSknidZNWtMxouzoA5vDxppja5i3dx5n8s8Q5BtEREAEzwx6hojAuh/aVGAt4ETOCQJNgQSZgpBICm2F+Bp9aRbUrNr55Fvy2ZSyiXNF5yi0FXK24CxHs46SkpvCzV1uZmy866GDX1N+ZdmhZRzNOsrpvNPc3+d+xnQd4zZfKSVZhVmk5acR4BNAm7A2F3zMNcG7hMHg6mOoDlIq7yA1VQ3bCw5WI3yc49Rri9VuxcfgU+Ob2hPYHXayC7MpshdRaCvEardiMtY8zKb6bdTxnC04y9w9czmQcYBr2l7DNW2vIcg3qCStzWHjy11fsvzwcg5lHuJEzglu7nIzrw9/nUBToNv80/LT+Hj7x+xJ38OgNoMY1n4YFruFjSkb+eHYD3yz5xvyrfkATF09ldt73M6TA58kOji6TD45hTk8svIRPt3xKZ0iOvHUwKcY3218iUB8f+R7Ji6YSLo5vWSfNk3a8OOff6RdeDsyzZkM/3I4h88e5o1f3+DFIS8yufdkDMJQch6++v0rfj35K/8Z8h+CfYNL8imyFbHi8Apm757N8ezjtAhuQWRgJBtTNrI3fS8AYf5hTOo+iePZx1lycAkGYWDG5hnER8UTExrDyiMriQyMZPqv04ltEsuUK6acd64KrAWsPrqaxQcWs+vMLmJCY2gb1paU3BRWHVlFVmEWAkHnyM6E+oWy9dRWbA4bsaGxrJiwgvhm8SXHsjttN8sOLWPt8bX0btGbyb0nE9sklmUHl/HOlne4vNXlTBs8raTsdSfWMW3tNLaf3k52ofupXkwGE29f9zaTe0/GYrcwfdN0XtnwChnmDIJ9g2kX3o7colyOZR+jW7NuTO49uWTfM3ln8DH4uBWL/Rn7GTV7FBO7T+SpgU+5vb9+SfqFT3Z8wjd7v+Fc0bnzthuEga2Tt9Ijuodb24tsRew6s4vNqZv57vB3rDq6ikJbYZk0/j7+NPFrwtw9c5l6airTBk/j+XXP88LPLyCEoHWT1hiEgZu/uZnHr3ycZ69+lnUn1vHulnfZnbabzIJMzhacxeZwzZrQr1U//tTtT4zrNu68a9oTeFcfw4lDGDJzcCR2xWh0XwE5sdnU+OLsbPXQUkyMGgp4oZzJO0PyuWSaBjSlTZM2blusoG7urMIsACICIvDzKTtI3yEd5FnysDvs+Bh8MBlM+Pn4ub0ZpJRY7BYc0oFDOrDYLZitZvKt+eQW5SKLQ2sZJzK4Ze0tXN32av7Q9g9c3fZqLmt6WZnWcnmOZh1lwoIJ7Dqzi7ZhbYkKimJD8gYsdgsmgwmrw4qf0Y8rYq+gf6v+RAdH89Zvb3Ek6witm7QmPiqeUL9Q5u6ZS+fIznx989ckNHe9ejWnMIeHVjzEnN1zsDqsRAREkFlQ9imqUL9QxnYdyx097iDIN4g3fn2Dr37/ilYhrVh922rahbcDYPmh5dy79F5Sc1O5t/e9/JL8C7vO7KJ5UHM6RHQgzD+MZQeX0TWqK1/f/DX+Pv5sSd3CX5b/hSBTEKsmreK+ZfexIXkDH97wIZ/s+IQ1x9eQ0CyBv/X/G9d1uI6/rfwbs3fPBqBndE+W/mkp4f7hJZVfVmEWUYFRJEYncjrvNGfyzpDQPIHRnUZzWdPL+GLXF8zfN59AUyD/uOIfPNDvAZYfWs4LP79AUk4S0wZN48F+DzJhwQQW7FvAwnELGd1Zjec9nXea6Zum8+6WdzlXdI4Q3xD6tOzD6bzTHMs+Rrh/OMMuG8ZVra8i5VwKm1M3k12YzZWxV9KtWTceXfUoZquZj0Z9xMHMg3y681MOZqon7zpGdOTw2cMARAdHk5qbSoBPAIW2QjbctYH+Mf05W3CWrjO6YjKaGNlhJD2iexBoCiS7MJtCWyEtglvQKrQV//3lv6w8spJbu93K9tPb2Z+xn+s6XMfdPe9m+GXDCTAFIKUk8pVIbup8Ex+M+gBQjZiWr7UkLT+NLpFdGBw3mMcGPEabsDacyj3FFR9dQWpuKlaHlft638fb171d5v56Z/M7PLD8AYJMQdzc9WaGtB2CxW4hz5KHQRjwNfry6KpHub7j9Xw15qvz7scnfnyCGZtnYLFbAGjdpDWjO41mZIeRRAdH4+fjRxO/JkQHR2N1WHlo+UPM3Daz5Jq9o8cdvDH8DUL8QiiyFfHg8gf5cPuHRAZGkmHOICIggkFxg4gMiKRpQFOaBzenWVAzTp47yVe7v2LH6R083O9h3hjxRoX3Y2XUpI/Bq4TBkXQEkZ6Fo0cXjMagCtOZzWrIqcWiBKFZM/fhIiklBbYC/Ix+FVbwpckwZ3A8+zgBPgEl+7Vu0poAnwB8jD6YrWZyCnPILsymwFZQZt8Q3xD8ffwxCANWh5WcwhzssuwjrqF+obQObY2/SVXkDukgqyCLM/lnMFvNlCfAJ4BQv1DCA8Lx9/Fn5+6dzDwxk9XHVnM062hJuuZBzUmMTuSeXvcwutPoEo9i0f5F3L7odoQQTEyYSEpuCinnUrgy9kpuS7yNrlFd+TnpZ5YfWs76pPVsP70dm8NGz+ieTBs8jRs63lAiZD8c/YGJCyeSXZjNG8Pf4J5e95BVmMWwL4ax4/QO/tLnL9zX5z46R3ZmT/oeVh9dTYBPAFfEXkF8VPx55/+3k78x4ssR+Bn9eO/693h/6/ssP7SczpGdmXXjLPq16odDOli0fxGL9i8iKSeJlHMpXNvuWv437H9lPJcdp3cw5LMh5BblYnVY+fyPnzOx+0SklMzZPYcX17/I72m/IxAYhIF/D/433Zt359b5txIeEI5AkHwumes7Xs8DfR/gD+3+gI+hYmf9XNE5jMJYxsty3qfO82W2mhn86WB2nN5B6yatCfIN4kDGAawOKzd3vZm7et7F4LjB+Bp93e7vjqScJEZ8OaLEexnYZiATEyYysuNIWoa0JCkniQ+2fsCOMzuYmDCRoe2HkvBuAhGBEWydvJW7v72bL3//ki33bCExOrHCcuwOO8+sfYYXfn6B9uHteXPEm1zX4brz0g37Yhin806z876dJb9Dz/d7ckv8LeRZ8lhzbA0Aj1/1OAv2LeBg5kHW3r6WeXvn8fIvL3Nj5xt5a8RbxITGsGj/Im76+iau73g9s8fMLnNuS/OP7//B9E3TOfLwkZLwzdbUrUxaOIl9Gfu4vcftjOwwkj4t+9CmSZtKz6eUkne3vMvrm17n+aufZ1y3cedt/2DbByzav4ixXccyvtv4kjCiO/am7yXQFEhcWFyFaSpDC0MFOFKOYjh9FluPTvj4uH9659w59ei80agemQ8OdpsMq93K0ayj5FpyAQg0BeJr9MUhHSVhFYHAaDDia/RFIDiVd4pQv1Aua3oZ+ZZ8jmUfK2l9lCbYN5imAU0J9w9HSklGQQZZBVnYHDbs0o5BGGji14TwgHB8jb7YHDbMVjOnck/hkA5C/EKwOWwU2YqwSzv+Pv5EBUZhMpowCAMmg4kAnwAM5R6SKH0ej2UdY33Seo5nH+dEzglWHV1FUk4S0cHRNA1oSmpuKtmF2fRu0Ztvxn5D2/C25x1HeQqsBSTlJNExoqPbGyotP41JCyfx/ZHvGd9tPPsz9rM3fS/zb5nP9R2vrzL/8uxO283Qz4eWnPenBz7Ng/0ePM/7qg47Tu/ghtk3MLnXZJ4a9FSZbVJKfjz2I4sPLObWbrdyRewVAGw/tZ1Rc0bRPKg5rw59lcFxg2tcbmWcyTvDi+tfJN2cTr4ln5jQGB7p/wiXNb2s1nlmFWQxe/dshrYfWq18FuxbwJi5Y7ipy00s2LeAJ656gueveb5aZR3IOECbsDYVeqRP/vgkL61/iXP/OkegKZAZv83gwRUPcuyvx4gLiyMpJ4kpK6cwf998jMLIkluXMKKDekpx+qbp/HPVPzEIAxMSJvDV7q9IbJ7Ij3/+scJwJUByTjLt3mzHg30f5PXhr/Pd4e8YPWc0kYGRfDL6E4a2H1qtY2uMaGGoAMfJ4xhOZWBL7ICPqcl527Ozlafg76/m/nE3VYOUklxLLseyjmGXdlqGtMTusJNvzcdqt2IQBoQQSCmRSOwOOxa7BYkk2DeYDk07YDQYCQ4OJvtcNnmWPKx2K1aHFX8ff0L9QittTVaG1W7lZO5J8i35+Bp98TX60sS/CU38mlSrP6Oy82h32FlxeAWzds5SLn1ISzpGdGRy78mVhppqikM6eGn9Szy15in8jH4sGr/ogm7GY1nHmLN7Dnf1uqtGnYruKN2PUl3sDnvJNXEpIqVk1JxRLD24lM6Rndl+7/Y6ux4W71/MjV/fyPo71jOg9QBunX8rP5/4meS/JZc5n2uOrcEhHQweAYXiAAAgAElEQVRpN6TM/sezj/PcT88xa+cs2oa3ZcOdG4gKiqqy3EkLJ7Fw30K+GvMV4+eNp1NkJ1ZPWu2RTvD6RAtDBThSkzCkpmFLaI+PX9lRK9nZylMICpL4Ryfh7+NLixDXLOBWu5V0czpnC85SaCvEz+hH+6btK219OJFSYnPYynQ4BwcHk5eXV51Drjca03MM205tw8fgQ/fm3RvaFE0VnMg+wd1L7uY/1/yHvq361lm+p3JP0fK1lrw+7HUe6f8IrV9vzRWxV/D1zV/XKJ/knGSCfIOqPeJq5+md9HhfdT53juzMT7f/dMGNisZATYTBq17tiaG4lSHLDle1WFRHc2AgBLZIJrMgnZO5JzmTdwaAQmsh+zL2kZqbislgok2TNnSN6loiClOnTmXGjBkl+U2bNo1XX32VvLw8hgwZQu/evenVoxfffvttlSbeeOON9O7dm/j4eGbOnFmy/rvvvqNXr14kJiYyZIhqGeXl5XHHHXeQkJBA9+7dmT9//oWcnUZFrxa9tChcJLQJa8OqSavqVBQAWoS0oFVIKzanbiYpJ4nkc8lcGXtljfOJbRJbo2G4idGJjO40mnbh7Vg1adUlIQo15ZIbrvrId4+w43QF825biqDIgtzljyj15LNz7nOTvxWLoxBfgy8OHNgcNvyMfrQNa8ujAx6lS2QXt51W48aN45FHHuGBB9QMH3PnzmXlypX4+/uzcOFCQkNDycjIoH///owaNarSsMLHH39cZnruMWPG4HA4uOeee1i3bh1t27YtmfPoueeeo0mTJvz++++Amh9Jo7mU6NuqL5tPbuaXpF8AuLJ1zYWhNnwz9huAWg3dvhS45IShUpwVcqnwmcWihqb6+tmxOArxET4lnZNmq5kiexFCqDHfFcVOe/bsSVpaGqmpqaSnpxMeHk5sbCxWq5XHH3+cdevWYTAYOHnyJGfOnCE6uuJxyG+++SYLFy4E1PTchw4dIj09nYEDB5ZM4920eIrI1atXM2fOnJJ9w8Mrf6hLo7nY6NuyL4v2L2LpoaUE+waXGcrsSbxVEJxccsIwfXjF8247Ms5gOJ6MtXMrTMEtsFrVm5xCQ8ERfoAie1GZoY82h420/DQiAyNLhv1VxNixY5k3bx6nT59mXPGc019++SXp6els3boVk8lEXFwchYWFFeaxdu1aVq9ezcaNGwkMDGTw4MGVptdoLnX6tlThqbl75nJ13NW1HpihqRne1cdQ/HSq8/ViZ88q5yGqRQG5ltzzpkjwMfjQMqRllaIAKpw0Z84c5s2bx9jid+/l5OTQrFkzTCYTa9asofyb58qTk5NDeHg4gYGB7N+/n02bNgFUOH22c6ptJzqUpLnU6NNS9ZXaHDYGxA5oYGu8B68SBuEcty9dwhAYCOfs6QgEkYG1f2NGfHw8ubm5tGrVihYt1GimCRMmsGXLFhISEvjss8/o3LlzpXkMHz4cm81Gly5dmDp1asn03FFRUSXTZycmJpZ4JE8++SRZWVl069aNxMRE1qxZU2v7NZrGSHhAeMnzFPXVv6C5BENJlVLiMUgKC9V02a1i7Jw2ZxIeEH7BcUVnJ7CTyMhINm7c6Datu6Gqfn5+rKjg3ZojRoxgRLlXjAUHB5d5WY9GcynSr1U/jmUd4/KYyxvaFK/Bu4Sh2GMosFtIzzwLIgwCM7Hn2b1ySJpGczHwxFVPcH2H68tMSKjxLN4lDMUewylbPrkiGxFt5IxZlEy9q9FoGh9do7rSNaprQ5vhVVwyfQzVeYLb2cdgsQPWQIKMYTikg+jg6Et2yoLqcrE9Aa/RaDzHJeEx+Pv7k5mZSUREROUVfPE2q5RgDaRDTBxGY9WTv13qSCnJzMzE/2J9J6lGo6lTLglhiImJISUlhfT09MoTWq2QkUGaVWC0mzloLag8vRfh7+9PTExMQ5uh0WgaAZeEMJhMppKngivlyBHMo0bQ9QmIT32J3e8/5nnjNBqN5iLjkuljqBa+vmQUT4ba1L/2zyxoNBrNpYzXCUN6sTBEBWlh0Gg0Gnd4VBiEEMOFEAeEEIeFEFPdbG8thFgjhNguhNglhDj//X51ia8vyYGqg7V56MX90g2NRqPxFB4TBiGEEZgBjAC6ArcKIcoPRn4SmCul7AmMB97xlD0A+PlxIlA9JNMqXHsMGo1G4w5Pegz9gMNSyqNSSgswBxhdLo0EQov/bwKketAe8PUlJUB5DK0jtTBoNBqNOzw5KqkVkFxqOQUoP9nJNOB7IcRDQBDwBw/aA0YjpwJ9QQpaN9PvLtBoNBp3NHTn863Ap1LKGOA64HMhxHk2CSEmCyG2CCG2VPmsQmUIwZkgIxQ0pVmkser0Go1G44V4UhhOArGllmOK15XmLmAugJRyI+APnBfjkVLOlFL2kVL2iYqKuiCjMoMAcwQRuu9Zo9Fo3OJJYdgMdBBCtBVC+KI6l78tlyYJGAIghOiCEoYLcAmqJivQBuZImlb/3eAajUbjVXhMGKSUNuBBYCWwDzX6aI8Q4lkhxKjiZH8H7hFC7ARmA7dLD8/mlhtgxaegCT6XxDPfGo1GU/d4tHqUUi4Hlpdb93Sp//cC9fq+PnOgGb8zel53jUajqYiG7nyuV6SUFAXkElykZxHVaDSaivAqYci35uPwsRJSdGGv8NRoNJpLGa8ShgxzBgBhRXqoqkaj0VSEVwlDpjkTgIgi/bYyjUajqQivEoZT55TH0LzQ2sCWaDQaTePFq4ThRLoShhZFFqS0N7A1Go1G0zjxKmFIzlTCEFNoxuHQXoNGo9G4w6uE4VR2BjgMxFhyURO+ajQajaY8XiUMZ/Iy1QR6jrM4HEUNbY5Go9E0SrxKGDLMGWCOJNKeqT0GjUajqQCvEoasomJhcGTicGhh0Gg0Gnd4lTCcs2VgKAwn2GrWHoNGo9FUgFcJQ74jA7+iUAx2tMeg0Wg0FeA1wiClpMiQSZA1BIMV3fms0Wg0FeA1wpBnycNhsBBiC8ZgBamFQaPRaNziNcJQMoEegQA4LAUNaY5Go9E0WrxOGCKEEgZZZG5IczQajabR4jXCkJ6vhKGZTwCghUGj0WgqwmuEISu3CMxNaeFf7DEU5jewRRqNRtM48Rph6B92I/w3k05+zQHtMWg0Gk1FeI0wZKp39BARVjzdtu581mg0GrdUSxiEEH8VQoQKxUdCiG1CiKGeNq4ucQmDAwBZpIVBo9Fo3FFdj+FOKeU5YCgQDkwCXvKYVR7g3Dn1HRlRvEILg0aj0bilusIgir+vAz6XUu4pte6iYNw4sNmgY1unx1DYwBZpNBpN46S6wrBVCPE9ShhWCiFCAEdVOwkhhgshDgghDgshplaQ5hYhxF4hxB4hxFfVN73mGI1gcI5K0h6DRqPRuMWnmunuAnoAR6WUZiFEU+COynYQQhiBGcC1QAqwWQjxrZRyb6k0HYB/AQOklFlCiGa1OYia4BQGLHpKDI1Go3FHdT2GK4ADUspsIcRE4Ekgp4p9+gGHpZRHpZrjeg4wulyae4AZUsosACllWvVNrx3Cz/mAmw4laTQajTuqKwzvAmYhRCLwd+AI8FkV+7QCkkstpxSvK01HoKMQ4hchxCYhxPBq2lN7/PzUt/YYNBqNxi3VFQablFKiWvxvSylnACF1UL4P0AEYDNwKfCCECCufSAgxWQixRQixJT09/cJK9PVV30VaGDQajcYd1RWGXCHEv1DDVJcJIQyAqYp9TgKxpZZjiteVJgX4VkpplVIeAw6ihKIMUsqZUso+Uso+UVFR1TS5ApzCYNEv6tFoNBp3VFcYxgFFqOcZTqMq+Veq2Gcz0EEI0VYI4QuMB74tl2YRyltACBGJCi0draZNtaNEGLTHoNFoNO6oljAUi8GXQBMhxPVAoZSy0j4GKaUNeBBYCewD5kop9wghnhVCjCpOthLIFELsBdYAj0opM2t5LNVDewwajUZTKdUariqEuAXlIaxFPdj2lhDiUSnlvMr2k1IuB5aXW/d0qf8lMKX4Uz84O5+LtDBoNBqNO6r7HMMTQF/ncFIhRBSwGqhUGBolxR6DsFob2BCNRlMtfv0VNm2Cv/61oS3xGqrbx2Ao94xBZg32bVyUhJK0MGg0FwWzZsHjjze0FV5FdT2G74QQK4HZxcvjKBciumjwKT5kLQwazcVBXh6YzeBwgOHibI9ebFRLGKSUjwohxgADilfNlFIu9JxZHkQIHL4CoYVBo7k4yMtT32YzBAc3rC1eQnU9BqSU84H5HrSl3pAmg/YYNJqLhfzi1/Dm5mphqCcqFQYhRC4g3W1CDSoK9YhVHkaajEiLnitJo7kocHoMzm+Nx6lUGKSUdTHtRePD14QszENKiRAX1WslNBrvQwtDveOdPTm+vgirHbv9XENbotFoqkILQ73jncLg54+wQlFR+ambNBpNo8PZx6CFod7wSmEQvv4YrFBUlNrQpmg0mqrQHkO9453C4BeIsIHFoj0GjaZR43Boj6EB8FJhCNIeg0ZzMVBQ6t3sWhjqDe8UBv8AjDaj7mPQaBo7pcVAC0O94ZXCgK8vPgUmLIVaGDSaRo0WhgbBO4WhZ0+CDhQSd9tq2Lmzoa3RaDQVoYWhQfBOYXjxRVJfGIDfCTP06gVr1jS0RRqNxh3OjmfQwlCPeKcwGAwUjh/Er7NACgE//tjQFmk0GneUFoPc3Iazw8vwTmEA/PxaYgt1QExLOOrZ10xrNJpa4hQGX1/tMdQjXisMvr6tALDHRWth0GgaK04xiI7WwlCPeK0w+PkpYbDFhmth0GgaK84+hubNtTDUI14sDC0BKIoJgLQ0fdFpNI0R7TE0CF4rDCZTc8BAUUujWnHsWIPao9Fo3OAUg2bNtDDUI14rDAaDD76+zTFHF7/JTYeTNJrGR14eBAVBaGj9CsOHH8LLL9dfeY0MrxUGUP0M+c2LY5haGDSaxkd+vhKGkBAlDNLdCyU9wBdfwLvv1k9ZjRCPCoMQYrgQ4oAQ4rAQYmol6cYIIaQQoo8n7SmPr29LzP5pqjWihUGjaXzk5an3PAcHK1EoPameJ8nMhJQUsNnqp7xGhseEQQhhBGYAI4CuwK1CiK5u0oUAfwV+9ZQtFeHn14oiyylo1073MWg0jZHSwuBcrg8yM8FuV+LghXjSY+gHHJZSHpVSWoA5wGg36Z4DXgYKPWiLW3x9W2KzZSLbttEeg0bTGMnPr39hkFIJA3htg9GTwtAKSC61nFK8rgQhRC8gVkq5zIN2VEjJswxtmqkLwOFoCDM0Gk1FODufncJQH9Ni5OWBxaL+P37c8+U1Qhqs81kIYQBeA/5ejbSThRBbhBBb0tPT68wG57MM1pgQKCyE06frLG+NRlMHNEQoyektgBYGD3ASiC21HFO8zkkI0A1YK4Q4DvQHvnXXAS2lnCml7COl7BMVFVVnBjqnxbDE+KkVOpyk0TQuGloYdCipztkMdBBCtBVC+ALjgW+dG6WUOVLKSCllnJQyDtgEjJJSbvGgTWXw928NQH7z4pEOWhg0msaFc7hqQwiDn5/2GOoaKaUNeBBYCewD5kop9wghnhVCjPJUuTXBxyeUgICOZIUcBiG0MGg0jY2G9BgSE71WGHw8mbmUcjmwvNy6pytIO9iTtlRESEhfsrPXQEyMFgaNpjFht6vnFupbGDIy1HefPuohN4tFTfvtRXj1k88AoaF9sVhSccRpYdBoGhXOmVUbymPo2VMNXU1Orjz9JYjXC0NISF8ALDGBXtvRpNE0SpzCEBQE/v5gMNSfMISFwWWXqWUvDCd5vTAEB/cEjJhb2CA1Fc6da2iTNBoNuEQgOFj1ATrnS/I0mZkQEQFxcWpZC4P3YTQGEBycQFbH4gdnfvmlYQ3SaC5GnA+E1SWlhcH5XV/CEBmp+h2NRq+MJHi9MIAKJ51pfwRpMsGaNQ1tjkZzcXHihJqIcv36us23oYQhI0N5DD4+EBurPQZvJSSkLxafHBz9EssKQ2EhvPSSDi9pNJWxbRsUFcHmzXWbb+k+Bqi9MIwYAX+vcoIFF85QEqhwkhYG7yQ0tB8ABZfHqos8J0dt+Owz+Ne/4L33GtA6jaaRc/Cg+j5ypG7zdecx1HSupBMn4LvvYNWq6u9TWhjattWhJG8lMDAegyGAnF4GNZHeunVqw0cfqe+PP66/F4RoNBcbTmGo6+HedRFKWrBAfR84UL13K1gsqozSHkNqqvKIvAgtDKjXfAYH9yStXap6DH7NGti9G377DXr1UhfVpk0NbaZG0zjxlDCUfo7B+V1TYZg/X31bLNXzaJzPMERGqm/nyKSkpJqVe5GjhaGYkJC+5Fp3IP/vCiUMH38MJhN88w0EBsInnzS0iRpN48QpDHU9db1TBGrbx3DqFGzYAKOKZ+DZs6fqfZxPPZcOJYHXhZO0MBQTFjYIh6OAwv5tYedO+PRTGD1avd1t7FiYMwfM5oY2U6NpXGRnQ1qauk8sFjh5sup9qktennp+ISBALddUGBYtUiHgJ55Qy3v3Vr2P02MoHUoCl/h5CVoYigkP/wNC+JLZ3awupqwsuOsutfHOO1Wnl9Mt1Wg0ikOH1Pfw4eq7LsNJzgn0hFDLTmEo39/3l7/AuHHn7z9/PnTqBH37qgq+Oh5DeWGIiYH27WHJklofxsWIFoZifHxCCAsbTGrMdhU6io2Fa69VG6+6Sl0cH36oO6Hri7lztRBfDDhb0iNGqO+6FAbnaz2dBAerifVKdwQfPQrvvw8LF6oJ95xkZsLatTBmjBKW+PjaCYMQcMst8MMPrjCTF6CFoRQREddjth3E8uzf4fXX1VOPoC6Ohx5So5XmzWtYIxsSu12F1xYu9Gw5RUVw333w6KOeLUdz4Rw8qO6PwYPV/VLXHoOzfwHUlBjO9U5ee031a1itsKXUq1y+/VZdrzfdpJa7dq3eyKTywgBKGOx2z1/3jQgtDKWIiBgJQNq4CNXSKM0DD6hpeB94wKtaDmVYs0bdcE8+6VnPaelSFco7dkyNQ9dUzeLF8MEH9V/uwYMqTBMcDK1beyaU5KT8DKsZGWqQiLNzufST18uWQatWalQhKI+hOiOTMjJUxMDZrwHqvQwdOigv1kvQwlCKgIB2BAZ2ITNz6fkbfXzURZidDX/9a/0b1xj44gv1vXcv/Pij58r57DPXjfnTT54rpz44elRdM55k82Y1QOIvf1EjcS6U0iGZqjh4EDp2VP+3a1e3D7lVJQwzZihbX3xReQROYbBa4fvv4brrXP0T8fHqu6pwUumH25w4w0k//gh1+M75xowWhnJERFxPdvZP2GxupsFISFCt5a++Uq1ab8JsVjH/CRMgKgrefNMz5aSnw/LlqpILD1dx4gthzx6YOLHmT8zWhqNHy3pSNhv076+OxVNkZSlRiIxU5Tkfyqwt69apeY+qI/xSni8Mnu5jACUMZjO89RbccIMShSuvVBNgOhzqOzdXCYOTzp3Vd22EAZQwOBw17/fauFH1V9blaK16QAtDOSIiRiKllaysCh6hnzpVzdP+n/9UnVlGxqUzW+vixeqGvPtuuPdeNUrDEy82mj1bVXC33w6DBl24MHz0EXz5JTz//IXlI6WKZ1c0bHH7dnVdfPmla92WLUroFi3yjDBJCXfcoZ7MXbgQhg5VHbHVecK3IpYsUfvfc0/Vw7PPnFHXhFMY2rdXx1udY83MhOho+PzzitOU72NwCkNurvIqMzNd/VBXXqmmstmzRzUsTCYYMqTsvnFxVQ9ZrUgYEhLUCKeahpPmzIGUFBWCdUd6urpmGtmgFi0M5QgN/T98fMLIyFjsPoGvLzz4oGoJbNvmPo3ZrNzb9u3VBTt5spqQrzLWrKkbN/Vf/1I3wPvvK5e6rvjiC9XyGThQdQwbjcqVr2tmzVJx4W7dVIfmhfYzOOfIef111flYW3buVBOxTZ/ufrtz2pTSgxO+/159FxQoYa1rVqxQ+f73v3D55XD//aoSuhBvds0aFZs/ehSedvsWXhdOkezQQX23a6e+q/Mw2EcfKWF57bWKK8WKQkm5ufD22+oNa1deqdY5v9evV8IwcKCrs9pJdUYmOafcLo8QMH68aqj89luVh1eC8/pbscL99qlTlUf73HPVz7M+kFJeVJ/evXtLT7N//93yp58CpdWa4z5BVpaUgYFS3nmna53ZLOWiRVLec4+UzZtLCVKOHi3l3/6m/u/RQ8qjR93nt3atSjNs2IUZnpIipa+vlOHhKr927aT88ccLy1NKKU+fltJolHLqVNe6ceOkbNJEyrNnLzx/J7t3K7unT1fLO3ao5Vmzapdfaqraf8oUZeuwYVI6HLXL6/HHVV7x8edvKyyUsmlTKYWQMiBAyvx8tX7AACn79JEyLk7KESNqV25lPPywKq+wUC1brVLGxEh57bW1yy8rS0qDQcpnnpHy3nvV/7/9VjaNw6HSSSnlBx+oc+K8rrdsUcsLFlRejs0mZZs2ynaQcvNm9+kiIqR84AHX8uHDKv0dd6jvjz8ua1fLluqcg5T/+9/5+T36qLo/rNaKbWvaVMq//MX9tqwsKWNjpezQQcq8vMqPUUopk5OVLSEhqr5w/k5O8vKkDA5W20HK2bMrzstqlXLiRCm/+67qcisA2CKrWc82eEVf0099CENOzq9yzRpkSsq7FSe6914p/f2lzMyUMj1dym7dXBfBmDFS/vSTK+2SJapiGjLk/HzMZnWh+fio/deurdw4s1nK48fdb3voIZXP0aNSLlsmZefO6kao6kZ1x4kTUk6aJOUtt0g5eLCybfdu1/Zt25RYjBtX+8q2PP/4h7L/zBm1bLcrkbvjjtrl99lnyu6tW5XYgBLvmuJwqN9ItW2lzMgou33ePLX+H/9Q34sXS5mdrc7PE09I+a9/qf/T0mp3HBURHy/l0KFl1z37rLLh4MGa57d4sesazM5WFW3Pnup3cPLqq0owpk5VwuTrqyp6KVUjAVSayli0SKX76CMlDvfe6z6dn5+Ujz3mWj59Wu1nMinRMJvLpr/lFtdvtHfv+fl9+qnatm+f+/JsNiXuTz1Vse0//qjyqEg8SvPJJyrtf/6jvr//vux25/W5erWUV12ljnf9evd2jR+v0r7+etXlVoAWhgvE4XDI337rLjdvrqSsXbvU6Xv8ceUN+PurCqKoyH36f/9bpT92rOz6qVPV+iVLpGzVSsorrnBf0RYWSvnWW1JGR6sbY9eusttTU9WFVdqLOXtW5WcwlG1dVUV6upSdOqlWTufOUrZuLeVNN52f7vnnle2fflq9fB0OKY8ccb/NYlGe1ujRZdffeKOUbduen95mq1qQJk2SMjJSVWwWi5Rdu6rKtKZC5vRcJk50Ly7XX68q0YIC1QC4804pFy5UaX/6Scrff1f/z5hRs3Ir49QplefLL5ddn5qqro/qVFzleeQRdR0XFKjlzz9XZcydq5ZzcpRQR0e7KuCuXcvmERZWddl/+IPybKxWKW+7TTWmcnPLprFYVP7PPutal5fnKre0YDh56y21LS7O/W+8e7eq+Nu3l/Lrr89Pk5Ehy3isFTFlikq3fHnl6f70J3VN5+aqe/Nvfyu7/ZprlC0Oh7rnLrtMCe1bb7lss9nUdQxSvvRS5eVVgRaGOiA5+S25Zg3y3LltFSe66ip1Cn19pVyxovIMjx9XF+Uzz7jWbd+uWpLOFvH777tEojRJSSosBFIOHChlVJSU/fq5WmpSqpvaaFTudmny8lRoAdSFWNqTcUdurpR9+6oKYt26ytPabMqe4GDlBj/wgGphvv32+Tedw+G6ob755vy8vv3WfaXrbOmX9pJSUtQN9fDDFdvmcEjZooVqaTn56CNXZe1k9WopR42S8u67pXz6aXWuy/PEE0pck5PVDT5limvbqVNlw2zjx6vfZ/JkdV4sFrW+WzcV5qgO06ZJ+eablaf54gtZ4g2V5+67lZ2pqdUrz0liopRXX+1attlUxd+5s/rf6Y1s3aqu99jY81v7vXtLOXx4xWXs3avyeOEFtfzzz7LEeyhNVpZa/9prrnV2u7qHDAb3XvP27bLK1vyKFS7vfvBgV9hPSikPHFDrP/+84v2lVMIZH6+ur/LeY2lbo6KknDBBLQ8dqs6jk2PHVFnPPedal54u5ciRsiSsfPPNqlFUPl0t0cJQB1gsZ+VPP/nLAwfurzjR0qXq5q9ueOLaa1Vs1dmC7dFDtSgyM52FqlZD9+6ueGRhoZSXX67K+e47VeF99VXZm+bnn1VF/uc/uy+3qEjFXJ19H4mJKuyxYkXZMIHdLuV116kbb/Hi6h1TUpJqJYIKC8THq//Hjy/bCnz6aZeIJiaeLxx//KOUzZq5KlInTs/spptUfjk5an9nSMFdRS6lq5VeusLJz1ct3nHj1HJhobrxwsJUK1gIJXSlcTik7NjRFQYcOFAJp5NXX5VlwhOzZ6tlf38lOE6c4YRZs8qe8/KsW6fShYaWrbTKc8cdKh7uLq8jR5RYOVuoRUWqkvnTn8rmefCgq9WbmamOv3QLXUop58+XJa3oJk3KenQOx/nljx2rrrNXX1UeUkpK2e133aWuAWdYzeFQFWb//mWviZQUVe7MmWX3j4hQXqQ77HYVtjtwwP12JzablO++q4735ptdx7BhgyqzqkaelEqETCa1v9PuvDzX8TpFyulNv/66LBMx+Pe/VfknTpTN1+FQ92pQkGr8jBlTM2+/EhqNMADDgQPAYWCqm+1TgL3ALuAHoE1VedaXMEgp5d69E+W6daHSZquko6myjqzyOCv0H35QrUJQIYfSOOPVvXqpG/f++9XyvHmuNA6HalkEBrriqjExFXduOzGbVUt00CB1c4LK38mbb6p1b71V/WOSUvU3LFmiKh27XVWCBoOqbEeMUDcPqBDLxx+r/5cude2flqb6Fkq3xEvz3/+q/Lp2VS1ao1FVGKUrv/K89poqp7xwTJmiyjp1St2ApWO/L78sz+tLcYaR3ntPLT/5pCr33F+9iaAAABtXSURBVDmXsPTv70qfna0qDFCek5P0dCUooLy9d99VlcNDD7k6eK1W1SgIDpbntVxttrKVaWysqjQqYtIkJdRnzqhwDaiK6PLL1bq333Z1/r72muqHAtXIKI3DobwAZwhn+/aKy5TSdQ05P5dd5hqgsHSpWlf+N3PuM26cOq9Sqg5pkPLLL8um/e03dS7rAqeoP/GEuk7uvlstl+9wr4gXX1TpP/tMfaKjlaf29tvqmgWXUOzf7xLYn35SDUR3fY5O6qrfrhSNQhgAI3AEaAf4AjuBruXSXA0EFv9/P/B1VfnWpzBkZf0s16xBJiVV0ZlWXcxm1Trt319VTk43szyLFqnWoL+/+okeffT8NElJKjbr56cqq+qMkihNfr5rxNQ77ygX399feQx1cVGuWaNEq1cv1Uq/6y5VuVksqs+idF+KM1z0++8V57d6teovKD0aZdIk1bJy586PGKH6Scpz8KCrcgoLKxv2SEtTgvngg651//ynEiVnpbxypdp/5UpXxbBqVdkynKG78h3AdrvyGFq0cFWcfn7qM2eOqlBAxb/btSsb1pk4Uf0+Gze6juHdSgZH7NunhKBjR1kSq1+wQOURGKjWDR+uPDVw9Sm56yNbsUKlqUyISlNYqCr4H35QIjl0qKogo6KU8Dn7MEqflxdfVOe5Y0fldfj4KPs3bKhembXB4XCJgcGgyhs1qnJPrTQ2m2sUFCjhHzZMlniMpftfHA5XWAjU8VXVR1HHNBZhuAJYWWr5X8C/KknfE/ilqnzrUxiklHLnzhFy3bpQWVR0pm4ydHoA0dGuEJI7kpPVDTVqVMVeyb5957uiNcFmU56H0ahadhERqiXtaZwV4OrVSkA6dlTDOqvi5Em1jxNnuOjf/3ats9mU0Pn5la3gSzN0qKsyKC9GEyaoME5urvKETCZX6ElKVeEZjcr7CQo6v7NcSuWB3H9/xQJbUKBi5IWFqvXrrFwCAlQr0uFQMWVQXuCqVbIkDBcZKeXf/+5eeMrj9Cbvustly4YNygOYMUOts1jUMUDFw1wdDiXGJ09WXp47PvxQ5R0RoSrLPXsqTrt2rRLN8HDl2VUVEqoLioqkvO8+1UdUlcftjqNHVWf6Rx8pgbPblSfi41N2eLeUqm/t4YdVw8855LceaSzCcDPwYanlScDblaR/G3iyqnzrWxjy8vbJtWt95P7999RNhjt2qJZZ+Q7mhiInR7VsQMWT6wOzWfUnCOFqXZUPqVWXG25Qlc7zz6tWZ8+esqSjvaKKzDlc8u67z9+2fr0scfk7d1ajjcp7JM6QkK/v+Z39taGwUHkEQUGuijMpSZ2fxx5Twtm+vepviYhQZcfGVu3ZpaZK+cYb5/fblKeoSPU5/fDDhR+LOx58UJZ4plVht1c8su9i4syZRnccF50wABOBTYBfBdsnA1uALa1bt/bMWauEQ4emyDVrROUjlGpCZZ2PDUFqqhoVVJ98843qLJ83r+ZhsNL8+qvyDpwuekyM+6GIpbHb1cNZ7h7OczjUqBVnaKG0h+LEObqqfIvwQik/Ln/YMJd4Oh9s2rBBCak7UWus2GyqQeSBuLmm+jQWYahWKAn4A7APaFadfOvbY5BSSoslS65fHyW3bh0gHY5GVqlrXKO8zOa6Ed133lG3xj//6X77jh0qDu7sKPUUX3+t7Bg7tuz6I0dUJ7dGUwNqIgxCpa97hBA+wEFgCHAS2Az8SUq5p1SansA8YLiU8lB18u3Tp4/cUvqFHPXEqVOfcuDAHbRv/zqxsY/Ue/maesRmg5Ur1aR0JlPD2WG1qrmZbr9dzWir0VwAQoitUso+1UrrKWEoNuQ6YDpqhNLHUsoXhBDPopTrWyHEaiABcE4inySlHFVZng0lDFJKdu8eRVbWanr33k5QUOd6t0Gj0WhqS6MRBk/QUMIAUFR0ms2b4wkIuIyePX/BYPBpEDs0Go2mptREGPS02zXAzy+ajh3fJTf3N06ceLahzdFoNBqPoIWhhjRrdgvNm/+ZEyeeIyOjgpdvaDQazUWMFoZa0LHjuwQH92bfvkmYzRfw8heNRqNphGhhqAVGYwDdui3AYPBl9+4b3b8fWqPRaC5StDDUEn//1nTtOhez+RD799+OlI6GNkmj0WjqBC0MF0B4+NW0b/8KGRkLSUp6qaHN0Wg0mjpBC8MFEhPzCM2a/Yljx54kM/O7hjZHo9FoLhgtDBeIEIJOnT4gKKg7+/b9iYKC4w1tkkaj0VwQWhjqAKMxkG7d5iOlgz17bsZuL2xokzQajabWaGGoIwIC2tOly2fk5W3l8OG/NrQ5Go1GU2v0nA51SGTkKFq3nkpS0kvk5++iSZOBRERcT1jYVQ1tmkaj0fx/e/ceX0dZJnD895x7ctI0SZukTdI0LS21aYDS8tEiioC6FBVRREGRZRFRF11E3VXxiuyyK+u6uOx6gQ+goCwoCFpZVARcXMRCWyht2jQ2bZM0bW7NvTk5t5ln/zjTkNMmbVpM06TP9/Ppp2dm3jPzPuc9mWfmnTPzjpudMfyFVVX9I1VVtwBCS8vtbNx4Lt3dv53sahljzLhZYvgL8/kCVFV9lRUrnuecc7qIRmuoq/trEom2ya6aMcaMiyWGCRQIzKC6+iEcZ4Bt266ym+CMMVOCJYYJFo0uY9GiO+jpeYodOz6PqjPZVTLGmMOyxHAczJ17LWVln6Cl5du88sqFJJPtk10lY4wZkyWG40BEWLz4eyxZcjf9/X9k/frldHY+ylQbJMkYc3KwxHCciAhz517LihUvEgyWsGXL+9i8+R3EYg2TXTVjjMliieE4y8s7jZUrN3DKKbfT1/dH1q1bSn39dQwN7ZrsqhljDGA3uE0Kny/AvHk3UlJyOc3N/8zevXfR1vYj8vKWE4lUEQyWkk73kkrto7DwfCorvzDZVTbGnETsjGEShcNzWbz4P1m1aicVFZ8lEJjF/v2b6eh4gP7+54nHG9m584vs3Xv3uNfZ1vYTGho+Y9cvjDHHzM4YTgDhcDmnnHLbIfNVHTZtegfbt3+SaLSGmTNXeTt8FxH/IeX3799Eff21qCbJzz+bkpIPAJBI7KWj40HKyq7H78+Z6HCMMVOcnTGcwET8VFc/SDhcwZYtl7J58yU8//wcnnuukObmf8V1k8NlXTdBXd1VBAKFRKOnsWPH50in9+M4MTZvfhc7dvw9W7dejuumht+TTO475vsq0ukBOjsfG/P9qg7J5L5jWnf2epTe3udw3cRrXpcxZnwsMZzggsEiamp+gesmiMXqKCpazcyZ57Jz5xdYt+40Wlr+i76+tezc+SUGBzfxutfdw6mnfp9EooXm5lvZtu0a9u/fyJw519LV9Svq6z/C4OA2amsv5fnni3n55TcxOLj1qOqk6rJ16wfZsuVS6uquxnXTWctdN8Hmze9i7dp59PX9aYx1ODjO0BG3tXfvD9i48c38+c9/e1R1HI3rpg+bCF03Ne4uOOuqM9OZTOQXXERWA/8B+IG7VfWbBy0PA/cDK4Eu4HJVbTzcOs866yxdv379xFT4BKaqiMjwdFfXr9mx47PEYtuG582dex1LltwFQF3d1bS33w/AwoX/SmXlP9DUdCu7dn0FAL8/j9LSq+noeAjH6aey8ovMm/c5AoGZw9tLp7sJBIqytgvQ2PhPNDZ+laKi1XR3/4bZsy+luvpBfL4Qrptm69Yr2Lfv5wSDJYCyYsWL5ORUDb8/ne5n8+Z3EovVs2zZwxQUvGXUmAcGNvDSS28kEJhJKtVJdfXPKCl5PwB9fc/T2/t7BgdrSSY7qaq6mYKCN435+SUSbWza9HZEgpxxxlMEg0VZy4eGGtm48S1Eo9UsW/boYbvcmppupaXlP6mp+QUzZ64as9xI8Xgz/f1rKSq6iEBgxiHLh4Z20tX1OGVlH8fnC49rnaNxnBjNzbcBLrm51cyYsZLc3FOPeV2umyAYLBz3e3bvvp2ursepqLiBWbMuRsSOPQ9HVXGcQQKBvAnflohsUNWzxlV2ohKDZDrB/wy8HWgB1gEfVNWtI8pcD5yuqp8QkSuA96rq5Ydb78maGEajqiQSLQwMbCAeb6Ss7Dr8/iiQ2RGuX386s2a9kyVL7kVEUFWam28jmWxj/vybCIVKSSY7aGi4kY6OB/H78ykvvx6RAJ2djxCLbSMSWcisWRdTWHg+4fB84vFGtmy5lJKSD7F06Y/Zs+cOGhpuJBJZwIwZr8dxBujufoJTTrmdWbMu4qWXVhEKlbNixR8JBGaSTvexadNqBgbWEw5XkEi0sGjRdygruz4rAaVSvWzYsALVFCtWrKO29j0MDdWzfPn/snv37bS33wdAJFKF66ZIpdpZtOgOyso+cUgiSyTaeOWV84nHm1F1iEZrWL786eEkmEy28/LLbyKZbMNxBiksfBs1Nb8cNTk0Nn6Dxsab8fki+HwRli9/lry80wFwnEF8vtzh7btugn371tDaeg89PU8CSjBYTFXVzcydex0+XxCAvr611NZeTCq1j4KCC6ipeYxAIN9r32ZCobLhsoeTSLRSW/tuBgY2kOkMcAChqurrzJ//1aPaSff2/h91dVeSTvezZMmdlJQc9s/S+2xuobHx6/j9+ThOP9FoDfPnf43i4ssOaZMDn8/gYB2Dg6+QTvdRXPwBwuE5467j0Ugk9tDaei89Pb+jpORDlJVdN+p1uiNJp/vo6fk9vb3PkJOzmLKyT4yrbUbjODHq6q6iq+txKis/T2Xllw57QOI4cVRTox5YjMeJkhjOBm5W1Qu96ZsAVPVfRpT5rVfmTyISANqAYj1MpSwxjJ/jDI37YvPAwEs0N3+Tzs5HAKGg4DwKCs6jv/8FenqeQvXVPv7c3GWsXPnCcBLq6PgZ7e3/zeDgKyQSLVRV3cz8+V8GoKfnGTZtuhCRAHl5K3CcfmKxbVRX/4zCwguoq/swXV2PEwrNITd3KeFwOYlEK7HYNlKpdpYv/wMzZ57N0NAO1q9fjuPsRyTAvHlfoLLy8wQC+aRSvdTVXUl39xMUFr6daLSGcLgCkQCuG6et7YfE47s5/fQncJwBamvfy4wZKykv/zSBQD67dn2FWKyeM854ilisnvr6j1BQcB5z536USKQKny+XZHIP3d1PsmfPHcyZcw3z53+Fl18+F9U0FRWfpqtrDf39awmHKygouAC/P0pHx0Ok0z2EwxXMmXMN+flvpLn5m/T1PUsoNJeiogvJyTmVpqZbCIXKKSv7OLt2fYnc3GWUll5JW9t9xGJb8PvzKSx8GwUFbyESqSIcrsB1h4jHm0km27wdnI/du28jleqhuvpBior+ilhsO7t3f4v29vspKlrNwoXfIhCYgc8XQdVFNXO9yecLIxLGcQZIpTrYt28NTU3/RCSygGBwNgMDL1BaejWlpVfi9+fh84VIp/txnH5U04gE6e39Ay0t36a09GqWLLmTzs5HaGr6Z2KxrcyYcRZVVTcTjZ5OMFjM4OAm9u69i46OB3Hd2PD3SiRIcfH7mDXrEiKR+YTD5bhuAsfpJ5XqJplsJZlsA3Q4MYsEEQni90cJBmcRCMzC74/i80VIp3vp6XmSrq4n6Ol5CnCJRE4hHt9BXt6ZVFV9nZycUwmF5uLzhVFNo5rGdROoJkgm2xgcrCMWqyMW20Ysto2hoR2Ag0gY1QTRaA2LFt1Bbu4SVFOIBAgGi/H5QsNxHXy2D5BMdrJ588UMDLxIQcH59PY+QySygLKy64lGl5KTs5hAoBC/P0osVk9b2720tz9ARcWNVFV97aj3BZnP98RIDJcBq1X1o970VcAbVPVTI8rUemVavOkdXpkxr1paYphYicQeREKEQsXD89Lp/cRiW0gk9pBMdjB79iWEw3NHfb/rpvH5sn/s1tv7HPv2PUp//wskEi0sXnwHs2dfAmSuV7S23k1//5+IxbaRSOwlHC4jHK6kpOSDFBe/Z3g9nZ0/p7X1hyxceCt5eWdkbUPVpanpVtrbHyCR2J21wwkECqmp+QUFBed663mMrVuvQDVz8V4kQE3Nr5g1azUAbW33UV//seHlI82Zcy1LltyFiI/BwW1s3PhmUql95OWdSVHRRQwNNdDb+wzp9ADFxe9lzpxrKCx86/DRqarS1fU/tLffT0/P06TT3eTnn01NzS8JhYrp7v4ttbXvw3UHyc9fRXHxZcRi9XR3/4ZEYvdh2y4UKue0037FjBlnjvhclNbWu9i+/YZR4xlLaemHWbz4e/h8EZqa/pGmpluBwz8deORnk9m2Q3v7T9i162skEs1ZZX2+KCUlV1BU9Hai0UxbtrbeSWvrD3GcvnHXczxycpZQXPxeL9EvpLPzYRoaPksyuWdc7xcJkpNzKrm5ryMaraGw8K3k57+B7u5fs337DYfEBhAIFACC48RQTeDzRfH78/D7cxAJkEp147oxli59gOLiS+np+V8aGv6OwcHaMeoQprj4UsrLP8nMmecc0+cw7RKDiHwM+BhAZWXlyqampgmps5keMtdHegF3xJFldrdBKtVLMtlGOt1LKFRCTs7CrOWOEyMebyQeb8R1hwiFygmHK4hEKrLKJRJ7cd141vsPHI0f6VqBqsPQUAORyMKs7oihoZ2opsjNXZIVUzLZTiKxm0SiBZ8vh0ikklBoDqC4bpJgsGjMbQ4ObmP//pdw3TiuGwd8w9t03QSuG8fvzyMUKiUSqTok8cbjTcTju3Gc/agm8PvzCQRmIhLwjpSDRKOnjdpl5Dhxenp+RzLZRiq1j2CwmJKSDxAI5I9SdoihoQYSiWYSib34fBECgZkEAgWEQnMJheZ4Z4JDuG6ma8V1UzjOftLpLlKpLu/aSByfL0RBwfnk5CwYZTsxBgbWk0jsJZnc6509+REJ4POF8fnCBIOzyc1dSiSy4JCDnVfXM0hHx8OoJhAJoZokmewgleoABJ8vF58viOPEcJxBXHfI25ZQUXED+flvyFpfKtVFLFbP0NAOHKcfx9lPIFBIcfH7j+paz2hOlMRgXUnGGHOCOJrEMJE/GVgHLBaRBSISAq4A1hxUZg1wtff6MuCZwyUFY4wxE2/C7nxW1bSIfAr4LZmfq96rqltE5BZgvaquAe4BfiwiDUA3meRhjDFmEk3oIzFU9QngiYPmfW3E6zjw/omsgzHGmKNjd58YY4zJYonBGGNMFksMxhhjslhiMMYYk8USgzHGmCwT+nTViSAincCx3vo8G3jtgwScWKZbTNMtHph+MU23eGD6xTRaPPNVtXi0wgebconhtRCR9eO982+qmG4xTbd4YPrFNN3igekX02uNx7qSjDHGZLHEYIwxJsvJlhjumuwKTIDpFtN0iwemX0zTLR6YfjG9pnhOqmsMxhhjjuxkO2MwxhhzBCdNYhCR1SJSLyINIvLFya7P0RKReSLyexHZKiJbROTT3vwiEfmdiGz3/n9to3lMAhHxi8jLIvK4N71ARF7w2uqn3mPbpwQRKRCRR0Rkm4jUicjZU72NROQz3neuVkQeFJHIVGojEblXRDq8gcEOzBu1TSTjDi+uTSKyYvJqPrYxYvqW973bJCKPiUjBiGU3eTHVi8iFR1r/SZEYJDN813eBi4Bq4IMiUj25tTpqaeBzqloNrAI+6cXwReBpVV0MPO1NTzWfBupGTN8G3K6qi4Ae4NpJqdWx+Q/gN6r6OuAMMnFN2TYSkXLgBuAsVa0h8wj9K5habfQjYPVB88Zqk4uAxd6/jwHfP051PFo/4tCYfgfUqOrpwJ+BmwC8/cQVwDLvPd+Tg4c0PMhJkRiA1wMNqrpTMwPfPgRcMsl1Oiqq2qqqL3mvB8jscMrJxHGfV+w+4D2jr+HEJCIVwDuBu71pAS4AHvGKTJmYRGQmcC6ZcUZQ1aSq9jLF24jM4/lzvFEWc4FWplAbqeofyIz3MtJYbXIJcL9mrAUKRGT0Ac4n0WgxqeqTqpr2JtcCB8ahvQR4SFUTqroLaCCzTxzTyZIYyoGRI6m3ePOmJBGpAs4EXgBKVbXVW9QGlE5StY7Vd4DP8+pI87OA3hFf8KnUVguATuCHXtfY3SISZQq3karuAf4NaCaTEPqADUzdNjpgrDaZLvuKjwC/9l4fdUwnS2KYNkQkD/g5cKOq9o9c5g2LOmV+ZiYi7wI6VHXDZNflLyQArAC+r6pnAoMc1G00BduokMwR5wKgDIhyaBfGlDbV2uRIROTLZLqeHzjWdZwsiWEPMG/EdIU3b0oRkSCZpPCAqj7qzW4/cKrr/d8xWfU7BucA7xaRRjLdexeQ6aMv8LotYGq1VQvQoqoveNOPkEkUU7mN3gbsUtVOVU0Bj5Jpt6naRgeM1SZTel8hIn8DvAu4Ul+9F+GoYzpZEsM6YLH3S4oQmQsxaya5TkfF63u/B6hT1X8fsWgNcLX3+mrgl8e7bsdKVW9S1QpVrSLTJs+o6pXA74HLvGJTJiZVbQN2i8gSb9Zbga1M4TYi04W0SkRyve/ggZimZBuNMFabrAH+2vt10iqgb0SX0wlNRFaT6ZZ9t6rGRixaA1whImERWUDmwvqLh12Zqp4U/4B3kLlSvwP48mTX5xjq/yYyp7ubgI3ev3eQ6ZN/GtgOPAUUTXZdjzG+84DHvdcLvS9uA/AwEJ7s+h1FHMuB9V47/QIonOptBHwD2AbUAj8GwlOpjYAHyVwfSZE5q7t2rDYBhMwvGHcAm8n8GmvSYxhnTA1kriUc2D/8YET5L3sx1QMXHWn9duezMcaYLCdLV5IxxphxssRgjDEmiyUGY4wxWSwxGGOMyWKJwRhjTBZLDMYcRyJy3oGnyBpzorLEYIwxJoslBmNGISIfFpEXRWSjiNzpjRmxX0Ru98YmeFpEir2yy0Vk7Yjn4B94tv8iEXlKRF4RkZdE5BRv9Xkjxmx4wLuj2JgThiUGYw4iIkuBy4FzVHU54ABXknmA3HpVXQY8C3zde8v9wBc08xz8zSPmPwB8V1XPAN5I5k5VyDwZ90YyY4MsJPPsIWNOGIEjFzHmpPNWYCWwzjuYzyHzkDUX+KlX5ifAo94YDAWq+qw3/z7gYRGZAZSr6mMAqhoH8Nb3oqq2eNMbgSrguYkPy5jxscRgzKEEuE9Vb8qaKfLVg8od6/NkEiNeO9jfoTnBWFeSMYd6GrhMREpgeHzg+WT+Xg48UfRDwHOq2gf0iMibvflXAc9qZpS9FhF5j7eOsIjkHtcojDlGdqRizEFUdauIfAV4UkR8ZJ5g+UkyA++83lvWQeY6BGQe2/wDb8e/E7jGm38VcKeI3OKt4/3HMQxjjpk9XdWYcRKR/aqaN9n1MGaiWVeSMcaYLHbGYIwxJoudMRhjjMliicEYY0wWSwzGGGOyWGIwxhiTxRKDMcaYLJYYjDHGZPl/bjCak9SUydUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2572 - acc: 0.9348\n",
      "Loss: 0.2571553029750521 Accuracy: 0.9347871\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8915 - acc: 0.7333\n",
      "Epoch 00001: val_loss improved from inf to 0.66023, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/001-0.6602.hdf5\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.8915 - acc: 0.7333 - val_loss: 0.6602 - val_acc: 0.8274\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.9028\n",
      "Epoch 00002: val_loss improved from 0.66023 to 0.35303, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/002-0.3530.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3453 - acc: 0.9028 - val_loss: 0.3530 - val_acc: 0.8940\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9303\n",
      "Epoch 00003: val_loss improved from 0.35303 to 0.24184, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/003-0.2418.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2478 - acc: 0.9303 - val_loss: 0.2418 - val_acc: 0.9285\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9479\n",
      "Epoch 00004: val_loss improved from 0.24184 to 0.21755, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/004-0.2176.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1887 - acc: 0.9479 - val_loss: 0.2176 - val_acc: 0.9338\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9568\n",
      "Epoch 00005: val_loss improved from 0.21755 to 0.19659, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/005-0.1966.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1589 - acc: 0.9568 - val_loss: 0.1966 - val_acc: 0.9394\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9616\n",
      "Epoch 00006: val_loss did not improve from 0.19659\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1373 - acc: 0.9616 - val_loss: 0.1978 - val_acc: 0.9401\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9712\n",
      "Epoch 00007: val_loss improved from 0.19659 to 0.17128, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/007-0.1713.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1085 - acc: 0.9712 - val_loss: 0.1713 - val_acc: 0.9469\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9766\n",
      "Epoch 00008: val_loss did not improve from 0.17128\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0932 - acc: 0.9766 - val_loss: 0.2584 - val_acc: 0.9224\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9803\n",
      "Epoch 00009: val_loss did not improve from 0.17128\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0796 - acc: 0.9803 - val_loss: 0.1760 - val_acc: 0.9504\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9842\n",
      "Epoch 00010: val_loss did not improve from 0.17128\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0648 - acc: 0.9842 - val_loss: 0.1800 - val_acc: 0.9469\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9882\n",
      "Epoch 00011: val_loss did not improve from 0.17128\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0526 - acc: 0.9882 - val_loss: 0.2304 - val_acc: 0.9299\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9875\n",
      "Epoch 00012: val_loss improved from 0.17128 to 0.17040, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/012-0.1704.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0546 - acc: 0.9875 - val_loss: 0.1704 - val_acc: 0.9476\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9915\n",
      "Epoch 00013: val_loss did not improve from 0.17040\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0434 - acc: 0.9915 - val_loss: 0.2031 - val_acc: 0.9397\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9959\n",
      "Epoch 00014: val_loss improved from 0.17040 to 0.14866, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/014-0.1487.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0276 - acc: 0.9959 - val_loss: 0.1487 - val_acc: 0.9539\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9961\n",
      "Epoch 00015: val_loss did not improve from 0.14866\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0250 - acc: 0.9961 - val_loss: 0.2412 - val_acc: 0.9311\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9915\n",
      "Epoch 00016: val_loss did not improve from 0.14866\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0394 - acc: 0.9915 - val_loss: 0.1688 - val_acc: 0.9511\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9986\n",
      "Epoch 00017: val_loss did not improve from 0.14866\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0151 - acc: 0.9985 - val_loss: 0.1792 - val_acc: 0.9481\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9939\n",
      "Epoch 00018: val_loss did not improve from 0.14866\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0302 - acc: 0.9939 - val_loss: 0.1701 - val_acc: 0.9506\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9967\n",
      "Epoch 00019: val_loss did not improve from 0.14866\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0205 - acc: 0.9967 - val_loss: 0.1615 - val_acc: 0.9534\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9926\n",
      "Epoch 00020: val_loss did not improve from 0.14866\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0321 - acc: 0.9926 - val_loss: 0.1625 - val_acc: 0.9534\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9991\n",
      "Epoch 00021: val_loss did not improve from 0.14866\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0101 - acc: 0.9990 - val_loss: 0.1559 - val_acc: 0.9543\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9953\n",
      "Epoch 00022: val_loss did not improve from 0.14866\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0234 - acc: 0.9953 - val_loss: 0.1653 - val_acc: 0.9534\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9991\n",
      "Epoch 00023: val_loss improved from 0.14866 to 0.14808, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/023-0.1481.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0084 - acc: 0.9991 - val_loss: 0.1481 - val_acc: 0.9585\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9992\n",
      "Epoch 00024: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0082 - acc: 0.9992 - val_loss: 0.1889 - val_acc: 0.9518\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9959\n",
      "Epoch 00025: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0205 - acc: 0.9959 - val_loss: 0.1832 - val_acc: 0.9539\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9987\n",
      "Epoch 00026: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0098 - acc: 0.9987 - val_loss: 0.1595 - val_acc: 0.9546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9996\n",
      "Epoch 00027: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0059 - acc: 0.9996 - val_loss: 0.1651 - val_acc: 0.9546\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9957\n",
      "Epoch 00028: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0212 - acc: 0.9957 - val_loss: 0.1777 - val_acc: 0.9539\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9996\n",
      "Epoch 00029: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0056 - acc: 0.9996 - val_loss: 0.1525 - val_acc: 0.9578\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9989\n",
      "Epoch 00030: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0070 - acc: 0.9989 - val_loss: 0.2091 - val_acc: 0.9420\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9968\n",
      "Epoch 00031: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0157 - acc: 0.9968 - val_loss: 0.1817 - val_acc: 0.9515\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9992\n",
      "Epoch 00032: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0066 - acc: 0.9991 - val_loss: 0.2464 - val_acc: 0.9392\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9955\n",
      "Epoch 00033: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0184 - acc: 0.9955 - val_loss: 0.1778 - val_acc: 0.9515\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9995\n",
      "Epoch 00034: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0044 - acc: 0.9995 - val_loss: 0.2036 - val_acc: 0.9490\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9997\n",
      "Epoch 00035: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0033 - acc: 0.9997 - val_loss: 0.1530 - val_acc: 0.9597\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9968\n",
      "Epoch 00036: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0154 - acc: 0.9968 - val_loss: 0.1664 - val_acc: 0.9555\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9971\n",
      "Epoch 00037: val_loss did not improve from 0.14808\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0135 - acc: 0.9971 - val_loss: 0.1950 - val_acc: 0.9502\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9946\n",
      "Epoch 00038: val_loss improved from 0.14808 to 0.14801, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/038-0.1480.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0200 - acc: 0.9946 - val_loss: 0.1480 - val_acc: 0.9602\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9997\n",
      "Epoch 00039: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0036 - acc: 0.9997 - val_loss: 0.1584 - val_acc: 0.9581\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 00040: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0026 - acc: 0.9998 - val_loss: 0.1485 - val_acc: 0.9590\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9978\n",
      "Epoch 00041: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0105 - acc: 0.9977 - val_loss: 0.1749 - val_acc: 0.9532\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9952\n",
      "Epoch 00042: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0191 - acc: 0.9952 - val_loss: 0.1571 - val_acc: 0.9571\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9999\n",
      "Epoch 00043: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0027 - acc: 0.9998 - val_loss: 0.1681 - val_acc: 0.9597\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9985\n",
      "Epoch 00044: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0082 - acc: 0.9985 - val_loss: 0.1570 - val_acc: 0.9581\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00045: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0026 - acc: 0.9998 - val_loss: 0.2502 - val_acc: 0.9336\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9929\n",
      "Epoch 00046: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0258 - acc: 0.9929 - val_loss: 0.1579 - val_acc: 0.9599\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9980\n",
      "Epoch 00047: val_loss improved from 0.14801 to 0.13555, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/047-0.1355.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0096 - acc: 0.9980 - val_loss: 0.1355 - val_acc: 0.9639\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9993\n",
      "Epoch 00048: val_loss did not improve from 0.13555\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0051 - acc: 0.9993 - val_loss: 0.1514 - val_acc: 0.9625\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9975\n",
      "Epoch 00049: val_loss did not improve from 0.13555\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0115 - acc: 0.9974 - val_loss: 0.1457 - val_acc: 0.9588\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9971\n",
      "Epoch 00050: val_loss did not improve from 0.13555\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0120 - acc: 0.9971 - val_loss: 0.1439 - val_acc: 0.9623\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00051: val_loss improved from 0.13555 to 0.13419, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/051-0.1342.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0031 - acc: 0.9996 - val_loss: 0.1342 - val_acc: 0.9651\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9998\n",
      "Epoch 00052: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0023 - acc: 0.9998 - val_loss: 0.1431 - val_acc: 0.9634\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9999\n",
      "Epoch 00053: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0016 - acc: 0.9999 - val_loss: 0.2067 - val_acc: 0.9506\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9989\n",
      "Epoch 00054: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0058 - acc: 0.9989 - val_loss: 0.2891 - val_acc: 0.9280\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9957\n",
      "Epoch 00055: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0156 - acc: 0.9957 - val_loss: 0.1408 - val_acc: 0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9982\n",
      "Epoch 00056: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0076 - acc: 0.9982 - val_loss: 0.1487 - val_acc: 0.9623\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00057: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1353 - val_acc: 0.9634\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9959\n",
      "Epoch 00058: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0152 - acc: 0.9959 - val_loss: 0.1573 - val_acc: 0.9588\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00059: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0025 - acc: 0.9997 - val_loss: 0.1402 - val_acc: 0.9632\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9973\n",
      "Epoch 00060: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0108 - acc: 0.9973 - val_loss: 0.1503 - val_acc: 0.9611\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00061: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0019 - acc: 0.9999 - val_loss: 0.1578 - val_acc: 0.9592\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9969\n",
      "Epoch 00062: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0116 - acc: 0.9969 - val_loss: 0.1500 - val_acc: 0.9653\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9991\n",
      "Epoch 00063: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0053 - acc: 0.9991 - val_loss: 0.1446 - val_acc: 0.9644\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 00064: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0013 - acc: 0.9999 - val_loss: 0.1415 - val_acc: 0.9653\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 00065: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0021 - acc: 0.9998 - val_loss: 0.2140 - val_acc: 0.9464\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9993\n",
      "Epoch 00066: val_loss did not improve from 0.13419\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0041 - acc: 0.9993 - val_loss: 0.1755 - val_acc: 0.9571\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9964\n",
      "Epoch 00067: val_loss improved from 0.13419 to 0.12921, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv_checkpoint/067-0.1292.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0134 - acc: 0.9964 - val_loss: 0.1292 - val_acc: 0.9660\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 00068: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.1336 - val_acc: 0.9693\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00069: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1378 - val_acc: 0.9681\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9994\n",
      "Epoch 00070: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0040 - acc: 0.9994 - val_loss: 0.2766 - val_acc: 0.9327\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9983\n",
      "Epoch 00071: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0069 - acc: 0.9983 - val_loss: 0.1970 - val_acc: 0.9509\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9982\n",
      "Epoch 00072: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0082 - acc: 0.9982 - val_loss: 0.1408 - val_acc: 0.9648\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9992\n",
      "Epoch 00073: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0045 - acc: 0.9992 - val_loss: 0.1460 - val_acc: 0.9648\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 00074: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1446 - val_acc: 0.9679\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9965\n",
      "Epoch 00075: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0124 - acc: 0.9965 - val_loss: 0.1469 - val_acc: 0.9620\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 00076: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0046 - acc: 0.9989 - val_loss: 0.1352 - val_acc: 0.9660\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00077: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1326 - val_acc: 0.9667\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.8465e-04 - acc: 1.0000\n",
      "Epoch 00078: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 8.8665e-04 - acc: 1.0000 - val_loss: 0.1395 - val_acc: 0.9653\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.4881e-04 - acc: 1.0000\n",
      "Epoch 00079: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 5.4882e-04 - acc: 1.0000 - val_loss: 0.1418 - val_acc: 0.9669\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 00080: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0023 - acc: 0.9995 - val_loss: 0.3537 - val_acc: 0.9206\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9964\n",
      "Epoch 00081: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0135 - acc: 0.9964 - val_loss: 0.1671 - val_acc: 0.9597\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9995\n",
      "Epoch 00082: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0035 - acc: 0.9994 - val_loss: 0.1483 - val_acc: 0.9653\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9969\n",
      "Epoch 00083: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0116 - acc: 0.9969 - val_loss: 0.1615 - val_acc: 0.9616\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 00084: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.1584 - val_acc: 0.9630\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 00085: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1497 - val_acc: 0.9637\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9999\n",
      "Epoch 00086: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0010 - acc: 0.9999 - val_loss: 0.1659 - val_acc: 0.9585\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00087: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0043 - acc: 0.9991 - val_loss: 0.1564 - val_acc: 0.9658\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00088: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0031 - acc: 0.9994 - val_loss: 0.1596 - val_acc: 0.9609\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9977\n",
      "Epoch 00089: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0087 - acc: 0.9977 - val_loss: 0.1673 - val_acc: 0.9599\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9999\n",
      "Epoch 00090: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1650 - val_acc: 0.9599\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 00091: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1617 - val_acc: 0.9611\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9997\n",
      "Epoch 00092: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1670 - val_acc: 0.9602\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 00093: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0013 - acc: 0.9999 - val_loss: 0.1600 - val_acc: 0.9637\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9988\n",
      "Epoch 00094: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0062 - acc: 0.9988 - val_loss: 0.2264 - val_acc: 0.9476\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9987\n",
      "Epoch 00095: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0056 - acc: 0.9987 - val_loss: 0.1675 - val_acc: 0.9588\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00096: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0035 - acc: 0.9992 - val_loss: 0.1433 - val_acc: 0.9653\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9999\n",
      "Epoch 00097: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1610 - val_acc: 0.9606\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 00098: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.1795 - val_acc: 0.9583\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00099: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0037 - acc: 0.9992 - val_loss: 0.1825 - val_acc: 0.9613\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 00100: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1954 - val_acc: 0.9602\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9986\n",
      "Epoch 00101: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0065 - acc: 0.9986 - val_loss: 0.1571 - val_acc: 0.9620\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 00102: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1501 - val_acc: 0.9646\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.0194e-04 - acc: 0.9999\n",
      "Epoch 00103: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 9.1775e-04 - acc: 0.9999 - val_loss: 0.1549 - val_acc: 0.9648\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00104: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0043 - acc: 0.9991 - val_loss: 0.1463 - val_acc: 0.9630\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00105: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0034 - acc: 0.9993 - val_loss: 0.1735 - val_acc: 0.9606\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 00106: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1462 - val_acc: 0.9662\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00107: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1746 - val_acc: 0.9609\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.2635e-04 - acc: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 4.2640e-04 - acc: 1.0000 - val_loss: 0.1515 - val_acc: 0.9669\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00109: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0037 - acc: 0.9992 - val_loss: 0.2724 - val_acc: 0.9352\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9986\n",
      "Epoch 00110: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0058 - acc: 0.9986 - val_loss: 0.1563 - val_acc: 0.9653\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 00111: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1632 - val_acc: 0.9641\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9991\n",
      "Epoch 00112: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0031 - acc: 0.9991 - val_loss: 0.1947 - val_acc: 0.9585\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00113: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0033 - acc: 0.9992 - val_loss: 0.1564 - val_acc: 0.9651\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.4018e-04 - acc: 1.0000\n",
      "Epoch 00114: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 4.4384e-04 - acc: 1.0000 - val_loss: 0.1482 - val_acc: 0.9683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 00115: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0026 - acc: 0.9994 - val_loss: 0.2181 - val_acc: 0.9525\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9977\n",
      "Epoch 00116: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0081 - acc: 0.9977 - val_loss: 0.1488 - val_acc: 0.9655\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 00117: val_loss did not improve from 0.12921\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1532 - val_acc: 0.9648\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1607 - acc: 0.9564\n",
      "Loss: 0.1607014153388678 Accuracy: 0.95638627\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7155 - acc: 0.7833\n",
      "Epoch 00001: val_loss improved from inf to 0.55079, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv_checkpoint/001-0.5508.hdf5\n",
      "36805/36805 [==============================] - 122s 3ms/sample - loss: 0.7154 - acc: 0.7833 - val_loss: 0.5508 - val_acc: 0.8397\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9224\n",
      "Epoch 00002: val_loss improved from 0.55079 to 0.21374, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv_checkpoint/002-0.2137.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2620 - acc: 0.9223 - val_loss: 0.2137 - val_acc: 0.9380\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9434\n",
      "Epoch 00003: val_loss improved from 0.21374 to 0.19909, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv_checkpoint/003-0.1991.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1928 - acc: 0.9434 - val_loss: 0.1991 - val_acc: 0.9401\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9579\n",
      "Epoch 00004: val_loss did not improve from 0.19909\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1435 - acc: 0.9578 - val_loss: 0.2048 - val_acc: 0.9385\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9633\n",
      "Epoch 00005: val_loss improved from 0.19909 to 0.17576, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv_checkpoint/005-0.1758.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1248 - acc: 0.9632 - val_loss: 0.1758 - val_acc: 0.9474\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9701\n",
      "Epoch 00006: val_loss improved from 0.17576 to 0.15048, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv_checkpoint/006-0.1505.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1041 - acc: 0.9701 - val_loss: 0.1505 - val_acc: 0.9546\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9779\n",
      "Epoch 00007: val_loss did not improve from 0.15048\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0807 - acc: 0.9779 - val_loss: 0.1572 - val_acc: 0.9492\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9827\n",
      "Epoch 00008: val_loss did not improve from 0.15048\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0644 - acc: 0.9826 - val_loss: 0.1540 - val_acc: 0.9509\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9813\n",
      "Epoch 00009: val_loss did not improve from 0.15048\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0646 - acc: 0.9813 - val_loss: 0.1551 - val_acc: 0.9515\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9831\n",
      "Epoch 00010: val_loss improved from 0.15048 to 0.12224, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv_checkpoint/010-0.1222.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0632 - acc: 0.9831 - val_loss: 0.1222 - val_acc: 0.9595\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9879\n",
      "Epoch 00011: val_loss did not improve from 0.12224\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0469 - acc: 0.9879 - val_loss: 0.1376 - val_acc: 0.9578\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9935\n",
      "Epoch 00012: val_loss did not improve from 0.12224\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0310 - acc: 0.9935 - val_loss: 0.2215 - val_acc: 0.9315\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9966\n",
      "Epoch 00013: val_loss did not improve from 0.12224\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0221 - acc: 0.9966 - val_loss: 0.1328 - val_acc: 0.9578\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9930\n",
      "Epoch 00014: val_loss improved from 0.12224 to 0.11833, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv_checkpoint/014-0.1183.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0329 - acc: 0.9930 - val_loss: 0.1183 - val_acc: 0.9646\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9971\n",
      "Epoch 00015: val_loss did not improve from 0.11833\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0169 - acc: 0.9971 - val_loss: 0.1717 - val_acc: 0.9448\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9961\n",
      "Epoch 00016: val_loss did not improve from 0.11833\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0203 - acc: 0.9961 - val_loss: 0.1614 - val_acc: 0.9506\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9883\n",
      "Epoch 00017: val_loss did not improve from 0.11833\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0421 - acc: 0.9883 - val_loss: 0.1413 - val_acc: 0.9569\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9987\n",
      "Epoch 00018: val_loss did not improve from 0.11833\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0101 - acc: 0.9987 - val_loss: 0.1205 - val_acc: 0.9630\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9985\n",
      "Epoch 00019: val_loss did not improve from 0.11833\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0104 - acc: 0.9984 - val_loss: 0.1500 - val_acc: 0.9553\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9907\n",
      "Epoch 00020: val_loss did not improve from 0.11833\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0345 - acc: 0.9907 - val_loss: 0.1506 - val_acc: 0.9574\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9986\n",
      "Epoch 00021: val_loss did not improve from 0.11833\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0096 - acc: 0.9986 - val_loss: 0.1422 - val_acc: 0.9569\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9935\n",
      "Epoch 00022: val_loss improved from 0.11833 to 0.11307, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv_checkpoint/022-0.1131.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0259 - acc: 0.9935 - val_loss: 0.1131 - val_acc: 0.9658\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9992\n",
      "Epoch 00023: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0067 - acc: 0.9992 - val_loss: 0.1356 - val_acc: 0.9611\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9989\n",
      "Epoch 00024: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0075 - acc: 0.9989 - val_loss: 0.1226 - val_acc: 0.9648\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9987\n",
      "Epoch 00025: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0068 - acc: 0.9988 - val_loss: 0.2272 - val_acc: 0.9371\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9957\n",
      "Epoch 00026: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0166 - acc: 0.9957 - val_loss: 0.1970 - val_acc: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9979\n",
      "Epoch 00027: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0112 - acc: 0.9978 - val_loss: 0.1621 - val_acc: 0.9525\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9958\n",
      "Epoch 00028: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0162 - acc: 0.9958 - val_loss: 0.1136 - val_acc: 0.9679\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9996\n",
      "Epoch 00029: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0039 - acc: 0.9996 - val_loss: 0.1177 - val_acc: 0.9662\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9995\n",
      "Epoch 00030: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0044 - acc: 0.9995 - val_loss: 0.1556 - val_acc: 0.9599\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9920\n",
      "Epoch 00031: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0266 - acc: 0.9920 - val_loss: 0.1177 - val_acc: 0.9648\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9997\n",
      "Epoch 00032: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0034 - acc: 0.9997 - val_loss: 0.1450 - val_acc: 0.9616\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9964\n",
      "Epoch 00033: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0142 - acc: 0.9964 - val_loss: 0.1899 - val_acc: 0.9506\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9993\n",
      "Epoch 00034: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0046 - acc: 0.9993 - val_loss: 0.1323 - val_acc: 0.9641\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9980\n",
      "Epoch 00035: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0089 - acc: 0.9980 - val_loss: 0.1354 - val_acc: 0.9618\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9961\n",
      "Epoch 00036: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0150 - acc: 0.9961 - val_loss: 0.1306 - val_acc: 0.9644\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9985\n",
      "Epoch 00037: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0077 - acc: 0.9985 - val_loss: 0.1191 - val_acc: 0.9651\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00038: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0025 - acc: 0.9998 - val_loss: 0.1443 - val_acc: 0.9639\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9974\n",
      "Epoch 00039: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0105 - acc: 0.9974 - val_loss: 0.1269 - val_acc: 0.9679\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9971\n",
      "Epoch 00040: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0112 - acc: 0.9971 - val_loss: 0.1336 - val_acc: 0.9662\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9998\n",
      "Epoch 00041: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0022 - acc: 0.9998 - val_loss: 0.1207 - val_acc: 0.9688\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 00042: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1981 - val_acc: 0.9550\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9986\n",
      "Epoch 00043: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0069 - acc: 0.9986 - val_loss: 0.2615 - val_acc: 0.9308\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9954\n",
      "Epoch 00044: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0168 - acc: 0.9954 - val_loss: 0.1248 - val_acc: 0.9658\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00045: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1323 - val_acc: 0.9632\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9988\n",
      "Epoch 00046: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0059 - acc: 0.9988 - val_loss: 0.1221 - val_acc: 0.9651\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 00047: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0027 - acc: 0.9997 - val_loss: 0.1368 - val_acc: 0.9660\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9968\n",
      "Epoch 00048: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0118 - acc: 0.9968 - val_loss: 0.1344 - val_acc: 0.9630\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9972\n",
      "Epoch 00049: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0122 - acc: 0.9972 - val_loss: 0.1430 - val_acc: 0.9606\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00050: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0037 - acc: 0.9993 - val_loss: 0.1207 - val_acc: 0.9690\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 00051: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1176 - val_acc: 0.9695\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 00052: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1358 - val_acc: 0.9669\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9990\n",
      "Epoch 00053: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0050 - acc: 0.9990 - val_loss: 0.1534 - val_acc: 0.9581\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9955\n",
      "Epoch 00054: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0162 - acc: 0.9955 - val_loss: 0.1178 - val_acc: 0.9672\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9998\n",
      "Epoch 00055: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0023 - acc: 0.9997 - val_loss: 0.1275 - val_acc: 0.9660\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9987\n",
      "Epoch 00056: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0060 - acc: 0.9987 - val_loss: 0.1337 - val_acc: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00057: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1424 - val_acc: 0.9644\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 00058: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0024 - acc: 0.9996 - val_loss: 0.1319 - val_acc: 0.9667\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9973\n",
      "Epoch 00059: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0100 - acc: 0.9973 - val_loss: 0.1215 - val_acc: 0.9690\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00060: val_loss did not improve from 0.11307\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1186 - val_acc: 0.9690\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9972\n",
      "Epoch 00061: val_loss improved from 0.11307 to 0.11203, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv_checkpoint/061-0.1120.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0106 - acc: 0.9972 - val_loss: 0.1120 - val_acc: 0.9688\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00062: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1152 - val_acc: 0.9700\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00063: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1494 - val_acc: 0.9646\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9979\n",
      "Epoch 00064: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0076 - acc: 0.9979 - val_loss: 0.2245 - val_acc: 0.9485\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9982\n",
      "Epoch 00065: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0069 - acc: 0.9982 - val_loss: 0.1221 - val_acc: 0.9648\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9979\n",
      "Epoch 00066: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0072 - acc: 0.9978 - val_loss: 0.1247 - val_acc: 0.9693\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
      "Epoch 00067: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.1415 - val_acc: 0.9646\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 00068: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1123 - val_acc: 0.9700\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.5058e-04 - acc: 0.9999\n",
      "Epoch 00069: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 7.5567e-04 - acc: 0.9999 - val_loss: 0.1219 - val_acc: 0.9679\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9977\n",
      "Epoch 00070: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0087 - acc: 0.9977 - val_loss: 0.1176 - val_acc: 0.9693\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00071: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1296 - val_acc: 0.9667\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00072: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1250 - val_acc: 0.9702\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9999\n",
      "Epoch 00073: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0010 - acc: 0.9999 - val_loss: 0.2170 - val_acc: 0.9506\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9986\n",
      "Epoch 00074: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.1776 - val_acc: 0.9527\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9982\n",
      "Epoch 00075: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0063 - acc: 0.9982 - val_loss: 0.1351 - val_acc: 0.9676\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9980\n",
      "Epoch 00076: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0080 - acc: 0.9980 - val_loss: 0.1187 - val_acc: 0.9709\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00077: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1141 - val_acc: 0.9737\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 00078: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0015 - acc: 0.9996 - val_loss: 0.1241 - val_acc: 0.9706\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.9473e-04 - acc: 0.9999\n",
      "Epoch 00079: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 5.9787e-04 - acc: 0.9999 - val_loss: 0.1239 - val_acc: 0.9690\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 00080: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1434 - val_acc: 0.9639\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9976\n",
      "Epoch 00081: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0088 - acc: 0.9976 - val_loss: 0.1298 - val_acc: 0.9676\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00082: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1209 - val_acc: 0.9723\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.6990e-04 - acc: 0.9999\n",
      "Epoch 00083: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 4.7204e-04 - acc: 0.9999 - val_loss: 0.1221 - val_acc: 0.9723\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.0672e-04 - acc: 0.9999\n",
      "Epoch 00084: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 9.0681e-04 - acc: 0.9999 - val_loss: 0.1168 - val_acc: 0.9716\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9972\n",
      "Epoch 00085: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0100 - acc: 0.9972 - val_loss: 0.1923 - val_acc: 0.9581\n",
      "Epoch 86/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9991\n",
      "Epoch 00086: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0035 - acc: 0.9991 - val_loss: 0.1203 - val_acc: 0.9706\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 00087: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1190 - val_acc: 0.9732\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.1649e-04 - acc: 0.9999\n",
      "Epoch 00088: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 6.1650e-04 - acc: 0.9999 - val_loss: 0.1198 - val_acc: 0.9706\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9999\n",
      "Epoch 00089: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0010 - acc: 0.9999 - val_loss: 0.1858 - val_acc: 0.9515\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9982\n",
      "Epoch 00090: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0073 - acc: 0.9982 - val_loss: 0.1462 - val_acc: 0.9634\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00091: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0029 - acc: 0.9993 - val_loss: 0.1361 - val_acc: 0.9676\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 00092: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.1246 - val_acc: 0.9683\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.1149e-04 - acc: 0.9999\n",
      "Epoch 00093: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 6.9186e-04 - acc: 0.9999 - val_loss: 0.1489 - val_acc: 0.9653\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9971\n",
      "Epoch 00094: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0087 - acc: 0.9971 - val_loss: 0.1215 - val_acc: 0.9683\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.4741e-04 - acc: 0.9999\n",
      "Epoch 00095: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0010 - acc: 0.9999 - val_loss: 0.1259 - val_acc: 0.9679\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9981\n",
      "Epoch 00096: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0069 - acc: 0.9981 - val_loss: 0.1342 - val_acc: 0.9690\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.1350e-04 - acc: 0.9999\n",
      "Epoch 00097: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 9.3213e-04 - acc: 0.9999 - val_loss: 0.1282 - val_acc: 0.9693\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9991\n",
      "Epoch 00098: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0042 - acc: 0.9991 - val_loss: 0.1295 - val_acc: 0.9709\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.2740e-04 - acc: 0.9999\n",
      "Epoch 00099: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 7.2730e-04 - acc: 0.9999 - val_loss: 0.1169 - val_acc: 0.9734\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.8084e-04 - acc: 0.9999\n",
      "Epoch 00100: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 4.8080e-04 - acc: 0.9999 - val_loss: 0.1183 - val_acc: 0.9725\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6523e-04 - acc: 1.0000\n",
      "Epoch 00101: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 2.6542e-04 - acc: 1.0000 - val_loss: 0.1259 - val_acc: 0.9709\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4638e-04 - acc: 1.0000\n",
      "Epoch 00102: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 1.4636e-04 - acc: 1.0000 - val_loss: 0.1173 - val_acc: 0.9730\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1649e-04 - acc: 1.0000\n",
      "Epoch 00103: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 1.1649e-04 - acc: 1.0000 - val_loss: 0.1173 - val_acc: 0.9732\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.1795e-05 - acc: 1.0000\n",
      "Epoch 00104: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 9.2858e-05 - acc: 1.0000 - val_loss: 0.1182 - val_acc: 0.9727\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9954\n",
      "Epoch 00105: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0140 - acc: 0.9954 - val_loss: 0.1395 - val_acc: 0.9655\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 00106: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1329 - val_acc: 0.9686\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.6976e-04 - acc: 0.9999\n",
      "Epoch 00107: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1256 - val_acc: 0.9686\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9976\n",
      "Epoch 00108: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0081 - acc: 0.9976 - val_loss: 0.1305 - val_acc: 0.9706\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.4759e-04 - acc: 1.0000\n",
      "Epoch 00109: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 9.4761e-04 - acc: 1.0000 - val_loss: 0.1238 - val_acc: 0.9706\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.8877e-04 - acc: 0.9999\n",
      "Epoch 00110: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 5.8902e-04 - acc: 0.9999 - val_loss: 0.1292 - val_acc: 0.9688\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3557e-04 - acc: 1.0000\n",
      "Epoch 00111: val_loss did not improve from 0.11203\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 3.7006e-04 - acc: 1.0000 - val_loss: 0.1245 - val_acc: 0.9693\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1444 - acc: 0.9643\n",
      "Loss: 0.14437985515370888 Accuracy: 0.9642783\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7116 - acc: 0.7799\n",
      "Epoch 00001: val_loss improved from inf to 0.48537, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/001-0.4854.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/sample - loss: 0.7115 - acc: 0.7799 - val_loss: 0.4854 - val_acc: 0.8528\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9184\n",
      "Epoch 00002: val_loss improved from 0.48537 to 0.23338, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/002-0.2334.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2613 - acc: 0.9184 - val_loss: 0.2334 - val_acc: 0.9245\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9418\n",
      "Epoch 00003: val_loss did not improve from 0.23338\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1906 - acc: 0.9418 - val_loss: 0.2493 - val_acc: 0.9217\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9552\n",
      "Epoch 00004: val_loss improved from 0.23338 to 0.19562, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/004-0.1956.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1460 - acc: 0.9551 - val_loss: 0.1956 - val_acc: 0.9392\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9651\n",
      "Epoch 00005: val_loss improved from 0.19562 to 0.17307, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/005-0.1731.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1165 - acc: 0.9651 - val_loss: 0.1731 - val_acc: 0.9432\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9721\n",
      "Epoch 00006: val_loss did not improve from 0.17307\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0939 - acc: 0.9721 - val_loss: 0.2214 - val_acc: 0.9350\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9753\n",
      "Epoch 00007: val_loss improved from 0.17307 to 0.16081, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/007-0.1608.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0821 - acc: 0.9753 - val_loss: 0.1608 - val_acc: 0.9471\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9795\n",
      "Epoch 00008: val_loss improved from 0.16081 to 0.15488, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/008-0.1549.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0691 - acc: 0.9795 - val_loss: 0.1549 - val_acc: 0.9525\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9821\n",
      "Epoch 00009: val_loss did not improve from 0.15488\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0607 - acc: 0.9821 - val_loss: 0.2034 - val_acc: 0.9397\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9859\n",
      "Epoch 00010: val_loss improved from 0.15488 to 0.14769, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/010-0.1477.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0509 - acc: 0.9859 - val_loss: 0.1477 - val_acc: 0.9555\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9878\n",
      "Epoch 00011: val_loss did not improve from 0.14769\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0426 - acc: 0.9878 - val_loss: 0.1667 - val_acc: 0.9499\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9858\n",
      "Epoch 00012: val_loss did not improve from 0.14769\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0490 - acc: 0.9858 - val_loss: 0.1608 - val_acc: 0.9488\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9886\n",
      "Epoch 00013: val_loss did not improve from 0.14769\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0419 - acc: 0.9886 - val_loss: 0.1647 - val_acc: 0.9504\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9916\n",
      "Epoch 00014: val_loss did not improve from 0.14769\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0325 - acc: 0.9916 - val_loss: 0.1577 - val_acc: 0.9564\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9951\n",
      "Epoch 00015: val_loss did not improve from 0.14769\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0216 - acc: 0.9951 - val_loss: 0.1553 - val_acc: 0.9562\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9955\n",
      "Epoch 00016: val_loss did not improve from 0.14769\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0182 - acc: 0.9955 - val_loss: 0.2328 - val_acc: 0.9390\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9933\n",
      "Epoch 00017: val_loss did not improve from 0.14769\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0246 - acc: 0.9933 - val_loss: 0.3056 - val_acc: 0.9231\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9953\n",
      "Epoch 00018: val_loss did not improve from 0.14769\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0188 - acc: 0.9953 - val_loss: 0.1544 - val_acc: 0.9585\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9956\n",
      "Epoch 00019: val_loss did not improve from 0.14769\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0176 - acc: 0.9956 - val_loss: 0.2009 - val_acc: 0.9441\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9964\n",
      "Epoch 00020: val_loss did not improve from 0.14769\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0142 - acc: 0.9964 - val_loss: 0.1633 - val_acc: 0.9546\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9912\n",
      "Epoch 00021: val_loss improved from 0.14769 to 0.14481, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/021-0.1448.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0295 - acc: 0.9913 - val_loss: 0.1448 - val_acc: 0.9625\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9988\n",
      "Epoch 00022: val_loss did not improve from 0.14481\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0074 - acc: 0.9988 - val_loss: 0.1623 - val_acc: 0.9583\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9983\n",
      "Epoch 00023: val_loss did not improve from 0.14481\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0082 - acc: 0.9983 - val_loss: 0.2372 - val_acc: 0.9373\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9929\n",
      "Epoch 00024: val_loss did not improve from 0.14481\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0237 - acc: 0.9929 - val_loss: 0.1602 - val_acc: 0.9578\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9975\n",
      "Epoch 00025: val_loss improved from 0.14481 to 0.11658, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/025-0.1166.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0115 - acc: 0.9975 - val_loss: 0.1166 - val_acc: 0.9690\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9990\n",
      "Epoch 00026: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0055 - acc: 0.9990 - val_loss: 0.1661 - val_acc: 0.9553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9917\n",
      "Epoch 00027: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0267 - acc: 0.9917 - val_loss: 0.1308 - val_acc: 0.9646\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9934\n",
      "Epoch 00028: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0228 - acc: 0.9934 - val_loss: 0.1405 - val_acc: 0.9611\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9985\n",
      "Epoch 00029: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0073 - acc: 0.9985 - val_loss: 0.1516 - val_acc: 0.9588\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9968\n",
      "Epoch 00030: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0132 - acc: 0.9968 - val_loss: 0.1509 - val_acc: 0.9606\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9988\n",
      "Epoch 00031: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0055 - acc: 0.9988 - val_loss: 0.1809 - val_acc: 0.9543\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9988\n",
      "Epoch 00032: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0056 - acc: 0.9988 - val_loss: 0.1695 - val_acc: 0.9578\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9968\n",
      "Epoch 00033: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0115 - acc: 0.9968 - val_loss: 0.1410 - val_acc: 0.9611\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9987\n",
      "Epoch 00034: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0062 - acc: 0.9986 - val_loss: 0.2320 - val_acc: 0.9418\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9932\n",
      "Epoch 00035: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0234 - acc: 0.9932 - val_loss: 0.1344 - val_acc: 0.9630\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9989\n",
      "Epoch 00036: val_loss did not improve from 0.11658\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0063 - acc: 0.9989 - val_loss: 0.1332 - val_acc: 0.9641\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00037: val_loss improved from 0.11658 to 0.11433, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/037-0.1143.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0031 - acc: 0.9996 - val_loss: 0.1143 - val_acc: 0.9693\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00038: val_loss did not improve from 0.11433\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0035 - acc: 0.9993 - val_loss: 0.2163 - val_acc: 0.9488\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9941\n",
      "Epoch 00039: val_loss did not improve from 0.11433\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0197 - acc: 0.9941 - val_loss: 0.1392 - val_acc: 0.9653\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9979\n",
      "Epoch 00040: val_loss did not improve from 0.11433\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0082 - acc: 0.9979 - val_loss: 0.1952 - val_acc: 0.9492\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9982\n",
      "Epoch 00041: val_loss did not improve from 0.11433\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0070 - acc: 0.9982 - val_loss: 0.1499 - val_acc: 0.9616\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9964\n",
      "Epoch 00042: val_loss did not improve from 0.11433\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0123 - acc: 0.9964 - val_loss: 0.1301 - val_acc: 0.9681\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00043: val_loss did not improve from 0.11433\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0034 - acc: 0.9993 - val_loss: 0.1594 - val_acc: 0.9604\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00044: val_loss did not improve from 0.11433\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.2209 - val_acc: 0.9453\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9973\n",
      "Epoch 00045: val_loss did not improve from 0.11433\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0102 - acc: 0.9973 - val_loss: 0.2105 - val_acc: 0.9513\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9957\n",
      "Epoch 00046: val_loss did not improve from 0.11433\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0146 - acc: 0.9957 - val_loss: 0.1215 - val_acc: 0.9669\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9961\n",
      "Epoch 00047: val_loss improved from 0.11433 to 0.10899, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv_checkpoint/047-0.1090.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0141 - acc: 0.9961 - val_loss: 0.1090 - val_acc: 0.9686\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00048: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.1264 - val_acc: 0.9700\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00049: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0030 - acc: 0.9994 - val_loss: 0.1441 - val_acc: 0.9627\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9950\n",
      "Epoch 00050: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0187 - acc: 0.9950 - val_loss: 0.1281 - val_acc: 0.9646\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9972\n",
      "Epoch 00051: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0109 - acc: 0.9972 - val_loss: 0.1145 - val_acc: 0.9693\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00052: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1127 - val_acc: 0.9711\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00053: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1292 - val_acc: 0.9674\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9976\n",
      "Epoch 00054: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0097 - acc: 0.9976 - val_loss: 0.1500 - val_acc: 0.9644\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9971\n",
      "Epoch 00055: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0111 - acc: 0.9971 - val_loss: 0.1202 - val_acc: 0.9690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00056: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1180 - val_acc: 0.9693\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 00057: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1261 - val_acc: 0.9704\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9990\n",
      "Epoch 00058: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0044 - acc: 0.9990 - val_loss: 0.2558 - val_acc: 0.9394\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9955\n",
      "Epoch 00059: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0157 - acc: 0.9955 - val_loss: 0.1353 - val_acc: 0.9674\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9972\n",
      "Epoch 00060: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0102 - acc: 0.9972 - val_loss: 0.1297 - val_acc: 0.9672\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 00061: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1280 - val_acc: 0.9690\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 00062: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1217 - val_acc: 0.9695\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 00063: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1308 - val_acc: 0.9676\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00064: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0030 - acc: 0.9994 - val_loss: 0.3392 - val_acc: 0.9220\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9949\n",
      "Epoch 00065: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0173 - acc: 0.9949 - val_loss: 0.1337 - val_acc: 0.9674\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 00066: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1218 - val_acc: 0.9706\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00067: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1173 - val_acc: 0.9697\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9967\n",
      "Epoch 00068: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0115 - acc: 0.9967 - val_loss: 0.1125 - val_acc: 0.9704\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00069: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1242 - val_acc: 0.9723\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 00070: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 0.1474 - val_acc: 0.9613\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9986\n",
      "Epoch 00071: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0059 - acc: 0.9986 - val_loss: 0.1410 - val_acc: 0.9679\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9981\n",
      "Epoch 00072: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.1326 - val_acc: 0.9672\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00073: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1123 - val_acc: 0.9725\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9989\n",
      "Epoch 00074: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0042 - acc: 0.9989 - val_loss: 0.1438 - val_acc: 0.9672\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9989\n",
      "Epoch 00075: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0041 - acc: 0.9989 - val_loss: 0.1624 - val_acc: 0.9632\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9986\n",
      "Epoch 00076: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.1456 - val_acc: 0.9665\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
      "Epoch 00077: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.1440 - val_acc: 0.9662\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9989\n",
      "Epoch 00078: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0039 - acc: 0.9989 - val_loss: 0.1610 - val_acc: 0.9625\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9986\n",
      "Epoch 00079: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.1555 - val_acc: 0.9646\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9974\n",
      "Epoch 00080: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0095 - acc: 0.9974 - val_loss: 0.1322 - val_acc: 0.9695\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 00081: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1212 - val_acc: 0.9723\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9980\n",
      "Epoch 00082: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0068 - acc: 0.9980 - val_loss: 0.1329 - val_acc: 0.9683\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 00083: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0015 - acc: 0.9997 - val_loss: 0.1458 - val_acc: 0.9676\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.6715e-04 - acc: 0.9999\n",
      "Epoch 00084: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 8.9515e-04 - acc: 0.9999 - val_loss: 0.1274 - val_acc: 0.9709\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9973\n",
      "Epoch 00085: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0087 - acc: 0.9973 - val_loss: 0.1343 - val_acc: 0.9676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00086: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.1340 - val_acc: 0.9681\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9991\n",
      "Epoch 00087: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0034 - acc: 0.9991 - val_loss: 0.1411 - val_acc: 0.9686\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9989\n",
      "Epoch 00088: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0038 - acc: 0.9989 - val_loss: 0.1426 - val_acc: 0.9693\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9991\n",
      "Epoch 00089: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0034 - acc: 0.9991 - val_loss: 0.1377 - val_acc: 0.9679\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9991\n",
      "Epoch 00090: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0034 - acc: 0.9991 - val_loss: 0.1644 - val_acc: 0.9632\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9976\n",
      "Epoch 00091: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0083 - acc: 0.9976 - val_loss: 0.1476 - val_acc: 0.9674\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9997\n",
      "Epoch 00092: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1633 - val_acc: 0.9655\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 00093: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1559 - val_acc: 0.9653\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9981\n",
      "Epoch 00094: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0064 - acc: 0.9981 - val_loss: 0.1430 - val_acc: 0.9669\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 00095: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1573 - val_acc: 0.9646\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9996\n",
      "Epoch 00096: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1426 - val_acc: 0.9693\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9975\n",
      "Epoch 00097: val_loss did not improve from 0.10899\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0093 - acc: 0.9975 - val_loss: 0.1333 - val_acc: 0.9709\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1451 - acc: 0.9618\n",
      "Loss: 0.14509350173697572 Accuracy: 0.9617861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GMP_ch_128_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 256)          1024        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 171,536\n",
      "Trainable params: 170,256\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5681 - acc: 0.8384\n",
      "Loss: 0.5680644237363821 Accuracy: 0.8384216\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 128)   512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 128)    512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 128)     512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 256)          1024        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 254,096\n",
      "Trainable params: 252,560\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3745 - acc: 0.8833\n",
      "Loss: 0.37454007097122455 Accuracy: 0.8832814\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 128)   512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 128)    512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 128)    512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 128)     512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 256)     1024        conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 384)          1536        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 421,776\n",
      "Trainable params: 419,472\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2572 - acc: 0.9348\n",
      "Loss: 0.2571553029750521 Accuracy: 0.9347871\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 128)   512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 128)    512         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 128)    512         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 128)     512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 256)     1024        conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 256)      1024        conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 512)          2048        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 753,296\n",
      "Trainable params: 750,224\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1607 - acc: 0.9564\n",
      "Loss: 0.1607014153388678 Accuracy: 0.95638627\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 128)   512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 128)    512         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 128)    512         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 128)     512         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 256)     1024        conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 256)      1024        conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 256)      1024        conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 512)          2048        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,082,256\n",
      "Trainable params: 1,078,672\n",
      "Non-trainable params: 3,584\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1444 - acc: 0.9643\n",
      "Loss: 0.14437985515370888 Accuracy: 0.9642783\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 128)   512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 128)    512         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 128)    512         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 128)     512         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 256)     1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 256)      1024        conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 256)      1024        conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 256)       1024        conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 512)          2048        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,411,216\n",
      "Trainable params: 1,407,120\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1451 - acc: 0.9618\n",
      "Loss: 0.14509350173697572 Accuracy: 0.9617861\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GMP_ch_128_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 256)          1024        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 171,536\n",
      "Trainable params: 170,256\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.6989 - acc: 0.8386\n",
      "Loss: 0.6988842990663316 Accuracy: 0.8386293\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 128)   512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 128)    512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 128)     512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 256)          1024        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 254,096\n",
      "Trainable params: 252,560\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.4360 - acc: 0.8897\n",
      "Loss: 0.43597502226893897 Accuracy: 0.8897196\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 128)   512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 128)    512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 128)    512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 128)     512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 256)     1024        conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 384)          1536        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 421,776\n",
      "Trainable params: 419,472\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3042 - acc: 0.9292\n",
      "Loss: 0.30424274902322707 Accuracy: 0.92917967\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 128)   512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 128)    512         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 128)    512         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 128)     512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 256)     1024        conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 256)      1024        conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 512)          2048        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 753,296\n",
      "Trainable params: 750,224\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1668 - acc: 0.9583\n",
      "Loss: 0.16680799333779053 Accuracy: 0.95825547\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 128)   512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 128)    512         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 128)    512         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 128)     512         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 256)     1024        conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 256)      1024        conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 256)      1024        conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 512)          2048        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,082,256\n",
      "Trainable params: 1,078,672\n",
      "Non-trainable params: 3,584\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.1535 - acc: 0.9634\n",
      "Loss: 0.15345669925176111 Accuracy: 0.9634476\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 128)   512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 128)    512         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 128)    512         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 128)     512         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 256)     1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 256)      1024        conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 256)      1024        conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 256)       1024        conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 512)          2048        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,411,216\n",
      "Trainable params: 1,407,120\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.1677 - acc: 0.9599\n",
      "Loss: 0.1676736310158125 Accuracy: 0.95991695\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
