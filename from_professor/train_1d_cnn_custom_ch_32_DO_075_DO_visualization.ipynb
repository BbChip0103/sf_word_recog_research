{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_32_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=32, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=32*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 318,768\n",
      "Trainable params: 318,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 227,696\n",
      "Trainable params: 227,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 113,072\n",
      "Trainable params: 113,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 88,560\n",
      "Trainable params: 88,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 132,784\n",
      "Trainable params: 132,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3724 - acc: 0.2485\n",
      "Epoch 00001: val_loss improved from inf to 2.05899, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/001-2.0590.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 2.3722 - acc: 0.2485 - val_loss: 2.0590 - val_acc: 0.3527\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9163 - acc: 0.4064\n",
      "Epoch 00002: val_loss improved from 2.05899 to 1.80020, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/002-1.8002.hdf5\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 1.9164 - acc: 0.4063 - val_loss: 1.8002 - val_acc: 0.4442\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7114 - acc: 0.4710\n",
      "Epoch 00003: val_loss improved from 1.80020 to 1.65093, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/003-1.6509.hdf5\n",
      "36805/36805 [==============================] - 23s 624us/sample - loss: 1.7115 - acc: 0.4711 - val_loss: 1.6509 - val_acc: 0.5006\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5657 - acc: 0.5152\n",
      "Epoch 00004: val_loss improved from 1.65093 to 1.56435, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/004-1.5644.hdf5\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 1.5656 - acc: 0.5153 - val_loss: 1.5644 - val_acc: 0.5253\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4649 - acc: 0.5490\n",
      "Epoch 00005: val_loss improved from 1.56435 to 1.52403, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/005-1.5240.hdf5\n",
      "36805/36805 [==============================] - 24s 639us/sample - loss: 1.4650 - acc: 0.5490 - val_loss: 1.5240 - val_acc: 0.5267\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3829 - acc: 0.5724\n",
      "Epoch 00006: val_loss improved from 1.52403 to 1.48796, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/006-1.4880.hdf5\n",
      "36805/36805 [==============================] - 24s 639us/sample - loss: 1.3829 - acc: 0.5723 - val_loss: 1.4880 - val_acc: 0.5427\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3243 - acc: 0.5887\n",
      "Epoch 00007: val_loss improved from 1.48796 to 1.47030, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/007-1.4703.hdf5\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 1.3242 - acc: 0.5887 - val_loss: 1.4703 - val_acc: 0.5458\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2663 - acc: 0.6091\n",
      "Epoch 00008: val_loss improved from 1.47030 to 1.43386, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/008-1.4339.hdf5\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 1.2662 - acc: 0.6091 - val_loss: 1.4339 - val_acc: 0.5586\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2173 - acc: 0.6208\n",
      "Epoch 00009: val_loss improved from 1.43386 to 1.40693, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/009-1.4069.hdf5\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 1.2174 - acc: 0.6208 - val_loss: 1.4069 - val_acc: 0.5749\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1811 - acc: 0.6315\n",
      "Epoch 00010: val_loss improved from 1.40693 to 1.40090, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/010-1.4009.hdf5\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 1.1811 - acc: 0.6315 - val_loss: 1.4009 - val_acc: 0.5705\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1339 - acc: 0.6479\n",
      "Epoch 00011: val_loss improved from 1.40090 to 1.38503, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/011-1.3850.hdf5\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 1.1339 - acc: 0.6479 - val_loss: 1.3850 - val_acc: 0.5744\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1056 - acc: 0.6537\n",
      "Epoch 00012: val_loss improved from 1.38503 to 1.38150, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/012-1.3815.hdf5\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 1.1056 - acc: 0.6537 - val_loss: 1.3815 - val_acc: 0.5795\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0719 - acc: 0.6637\n",
      "Epoch 00013: val_loss improved from 1.38150 to 1.37506, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/013-1.3751.hdf5\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 1.0718 - acc: 0.6637 - val_loss: 1.3751 - val_acc: 0.5812\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0427 - acc: 0.6704\n",
      "Epoch 00014: val_loss improved from 1.37506 to 1.35130, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/014-1.3513.hdf5\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 1.0427 - acc: 0.6705 - val_loss: 1.3513 - val_acc: 0.5910\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0088 - acc: 0.6859\n",
      "Epoch 00015: val_loss improved from 1.35130 to 1.33335, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/015-1.3333.hdf5\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 1.0089 - acc: 0.6859 - val_loss: 1.3333 - val_acc: 0.6033\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9861 - acc: 0.6921\n",
      "Epoch 00016: val_loss did not improve from 1.33335\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.9861 - acc: 0.6921 - val_loss: 1.3458 - val_acc: 0.5991\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9657 - acc: 0.6949\n",
      "Epoch 00017: val_loss did not improve from 1.33335\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.9656 - acc: 0.6949 - val_loss: 1.3343 - val_acc: 0.6035\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9349 - acc: 0.7051\n",
      "Epoch 00018: val_loss improved from 1.33335 to 1.32246, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/018-1.3225.hdf5\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.9349 - acc: 0.7051 - val_loss: 1.3225 - val_acc: 0.6133\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9085 - acc: 0.7121\n",
      "Epoch 00019: val_loss did not improve from 1.32246\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.9084 - acc: 0.7121 - val_loss: 1.3387 - val_acc: 0.6028\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8872 - acc: 0.7160\n",
      "Epoch 00020: val_loss improved from 1.32246 to 1.31831, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/020-1.3183.hdf5\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.8872 - acc: 0.7160 - val_loss: 1.3183 - val_acc: 0.6140\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8676 - acc: 0.7264\n",
      "Epoch 00021: val_loss improved from 1.31831 to 1.30705, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/021-1.3071.hdf5\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 0.8676 - acc: 0.7264 - val_loss: 1.3071 - val_acc: 0.6166\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8428 - acc: 0.7340\n",
      "Epoch 00022: val_loss did not improve from 1.30705\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.8428 - acc: 0.7341 - val_loss: 1.3196 - val_acc: 0.6112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8351 - acc: 0.7364\n",
      "Epoch 00023: val_loss did not improve from 1.30705\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.8351 - acc: 0.7364 - val_loss: 1.3119 - val_acc: 0.6173\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8158 - acc: 0.7422\n",
      "Epoch 00024: val_loss improved from 1.30705 to 1.29349, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_3_conv_checkpoint/024-1.2935.hdf5\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.8158 - acc: 0.7422 - val_loss: 1.2935 - val_acc: 0.6210\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7947 - acc: 0.7466\n",
      "Epoch 00025: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 638us/sample - loss: 0.7947 - acc: 0.7466 - val_loss: 1.3252 - val_acc: 0.6131\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7790 - acc: 0.7523\n",
      "Epoch 00026: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.7789 - acc: 0.7523 - val_loss: 1.3180 - val_acc: 0.6131\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7674 - acc: 0.7568\n",
      "Epoch 00027: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.7675 - acc: 0.7568 - val_loss: 1.3151 - val_acc: 0.6177\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7508 - acc: 0.7602\n",
      "Epoch 00028: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.7508 - acc: 0.7602 - val_loss: 1.3285 - val_acc: 0.6182\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7372 - acc: 0.7633\n",
      "Epoch 00029: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.7376 - acc: 0.7633 - val_loss: 1.3049 - val_acc: 0.6292\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7244 - acc: 0.7704\n",
      "Epoch 00030: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.7244 - acc: 0.7704 - val_loss: 1.3118 - val_acc: 0.6257\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7129 - acc: 0.7739\n",
      "Epoch 00031: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.7129 - acc: 0.7739 - val_loss: 1.3267 - val_acc: 0.6210\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6981 - acc: 0.7737\n",
      "Epoch 00032: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.6981 - acc: 0.7736 - val_loss: 1.3019 - val_acc: 0.6278\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6849 - acc: 0.7810\n",
      "Epoch 00033: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.6850 - acc: 0.7810 - val_loss: 1.3061 - val_acc: 0.6294\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6747 - acc: 0.7833\n",
      "Epoch 00034: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 24s 639us/sample - loss: 0.6747 - acc: 0.7833 - val_loss: 1.3335 - val_acc: 0.6240\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6598 - acc: 0.7891\n",
      "Epoch 00035: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.6598 - acc: 0.7891 - val_loss: 1.3075 - val_acc: 0.6359\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6490 - acc: 0.7916\n",
      "Epoch 00036: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.6491 - acc: 0.7916 - val_loss: 1.2945 - val_acc: 0.6406\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6452 - acc: 0.7929\n",
      "Epoch 00037: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.6452 - acc: 0.7929 - val_loss: 1.2996 - val_acc: 0.6364\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6352 - acc: 0.7950\n",
      "Epoch 00038: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.6352 - acc: 0.7950 - val_loss: 1.3266 - val_acc: 0.6287\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6205 - acc: 0.7977\n",
      "Epoch 00039: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.6205 - acc: 0.7977 - val_loss: 1.3171 - val_acc: 0.6343\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6136 - acc: 0.8010\n",
      "Epoch 00040: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 0.6137 - acc: 0.8009 - val_loss: 1.3177 - val_acc: 0.6366\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5985 - acc: 0.8059\n",
      "Epoch 00041: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.5984 - acc: 0.8059 - val_loss: 1.3076 - val_acc: 0.6436\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5962 - acc: 0.8083\n",
      "Epoch 00042: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.5963 - acc: 0.8083 - val_loss: 1.3243 - val_acc: 0.6341\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5873 - acc: 0.8077\n",
      "Epoch 00043: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.5873 - acc: 0.8077 - val_loss: 1.3142 - val_acc: 0.6455\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5843 - acc: 0.8087\n",
      "Epoch 00044: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.5843 - acc: 0.8087 - val_loss: 1.3238 - val_acc: 0.6394\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5667 - acc: 0.8149\n",
      "Epoch 00045: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.5668 - acc: 0.8149 - val_loss: 1.3228 - val_acc: 0.6420\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5563 - acc: 0.8191\n",
      "Epoch 00046: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.5563 - acc: 0.8192 - val_loss: 1.3329 - val_acc: 0.6373\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.8192\n",
      "Epoch 00047: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.5520 - acc: 0.8192 - val_loss: 1.3271 - val_acc: 0.6455\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.8245\n",
      "Epoch 00048: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.5422 - acc: 0.8245 - val_loss: 1.3225 - val_acc: 0.6406\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.8252\n",
      "Epoch 00049: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.5400 - acc: 0.8252 - val_loss: 1.3380 - val_acc: 0.6399\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.8272\n",
      "Epoch 00050: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.5304 - acc: 0.8272 - val_loss: 1.3457 - val_acc: 0.6410\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5198 - acc: 0.8296\n",
      "Epoch 00051: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 636us/sample - loss: 0.5198 - acc: 0.8296 - val_loss: 1.3192 - val_acc: 0.6464\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.8332\n",
      "Epoch 00052: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.5157 - acc: 0.8332 - val_loss: 1.3331 - val_acc: 0.6445\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.8346\n",
      "Epoch 00053: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.5083 - acc: 0.8346 - val_loss: 1.3358 - val_acc: 0.6455\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.8359\n",
      "Epoch 00054: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 631us/sample - loss: 0.5033 - acc: 0.8359 - val_loss: 1.3194 - val_acc: 0.6506\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4998 - acc: 0.8371\n",
      "Epoch 00055: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 0.4998 - acc: 0.8371 - val_loss: 1.3268 - val_acc: 0.6557\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4887 - acc: 0.8422\n",
      "Epoch 00056: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 627us/sample - loss: 0.4886 - acc: 0.8422 - val_loss: 1.3405 - val_acc: 0.6490\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.8393\n",
      "Epoch 00057: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 629us/sample - loss: 0.4848 - acc: 0.8393 - val_loss: 1.3459 - val_acc: 0.6434\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4770 - acc: 0.8433\n",
      "Epoch 00058: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 625us/sample - loss: 0.4771 - acc: 0.8433 - val_loss: 1.3544 - val_acc: 0.6501\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4779 - acc: 0.8426\n",
      "Epoch 00059: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 626us/sample - loss: 0.4778 - acc: 0.8427 - val_loss: 1.3806 - val_acc: 0.6401\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8474\n",
      "Epoch 00060: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.4654 - acc: 0.8475 - val_loss: 1.3394 - val_acc: 0.6541\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.8467\n",
      "Epoch 00061: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 637us/sample - loss: 0.4643 - acc: 0.8467 - val_loss: 1.3392 - val_acc: 0.6527\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4659 - acc: 0.8463\n",
      "Epoch 00062: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.4659 - acc: 0.8462 - val_loss: 1.3241 - val_acc: 0.6539\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4590 - acc: 0.8489\n",
      "Epoch 00063: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.4589 - acc: 0.8490 - val_loss: 1.3367 - val_acc: 0.6587\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4451 - acc: 0.8533\n",
      "Epoch 00064: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.4451 - acc: 0.8533 - val_loss: 1.3402 - val_acc: 0.6555\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8564\n",
      "Epoch 00065: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.4408 - acc: 0.8564 - val_loss: 1.3518 - val_acc: 0.6548\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8585\n",
      "Epoch 00066: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.4355 - acc: 0.8584 - val_loss: 1.3253 - val_acc: 0.6650\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4378 - acc: 0.8561\n",
      "Epoch 00067: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.4377 - acc: 0.8561 - val_loss: 1.3567 - val_acc: 0.6555\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8575\n",
      "Epoch 00068: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.4283 - acc: 0.8575 - val_loss: 1.3429 - val_acc: 0.6578\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8610\n",
      "Epoch 00069: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 634us/sample - loss: 0.4254 - acc: 0.8610 - val_loss: 1.3302 - val_acc: 0.6622\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8636\n",
      "Epoch 00070: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.4145 - acc: 0.8636 - val_loss: 1.3551 - val_acc: 0.6613\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.8620\n",
      "Epoch 00071: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 633us/sample - loss: 0.4229 - acc: 0.8620 - val_loss: 1.3420 - val_acc: 0.6625\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4101 - acc: 0.8659\n",
      "Epoch 00072: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.4100 - acc: 0.8659 - val_loss: 1.3467 - val_acc: 0.6620\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.8646\n",
      "Epoch 00073: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 635us/sample - loss: 0.4111 - acc: 0.8646 - val_loss: 1.3459 - val_acc: 0.6653\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8666\n",
      "Epoch 00074: val_loss did not improve from 1.29349\n",
      "36805/36805 [==============================] - 23s 632us/sample - loss: 0.4074 - acc: 0.8666 - val_loss: 1.3781 - val_acc: 0.6583\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 356us/sample - loss: 1.3766 - acc: 0.5836\n",
      "Loss: 1.376645899041791 Accuracy: 0.58359295\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3993 - acc: 0.2274\n",
      "Epoch 00001: val_loss improved from inf to 1.87964, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/001-1.8796.hdf5\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 2.3992 - acc: 0.2274 - val_loss: 1.8796 - val_acc: 0.4253\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7943 - acc: 0.4271\n",
      "Epoch 00002: val_loss improved from 1.87964 to 1.60622, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/002-1.6062.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 1.7943 - acc: 0.4271 - val_loss: 1.6062 - val_acc: 0.5076\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6043 - acc: 0.4882\n",
      "Epoch 00003: val_loss improved from 1.60622 to 1.47446, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/003-1.4745.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 1.6043 - acc: 0.4882 - val_loss: 1.4745 - val_acc: 0.5584\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4879 - acc: 0.5319\n",
      "Epoch 00004: val_loss improved from 1.47446 to 1.38748, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/004-1.3875.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 1.4879 - acc: 0.5319 - val_loss: 1.3875 - val_acc: 0.5900\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4040 - acc: 0.5615\n",
      "Epoch 00005: val_loss improved from 1.38748 to 1.31813, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/005-1.3181.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 1.4039 - acc: 0.5615 - val_loss: 1.3181 - val_acc: 0.5991\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3350 - acc: 0.5832\n",
      "Epoch 00006: val_loss improved from 1.31813 to 1.25172, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/006-1.2517.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 1.3349 - acc: 0.5832 - val_loss: 1.2517 - val_acc: 0.6271\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2690 - acc: 0.6075\n",
      "Epoch 00007: val_loss improved from 1.25172 to 1.20188, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/007-1.2019.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 1.2690 - acc: 0.6075 - val_loss: 1.2019 - val_acc: 0.6420\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2167 - acc: 0.6220\n",
      "Epoch 00008: val_loss improved from 1.20188 to 1.16747, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/008-1.1675.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 1.2166 - acc: 0.6220 - val_loss: 1.1675 - val_acc: 0.6443\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1676 - acc: 0.6412\n",
      "Epoch 00009: val_loss improved from 1.16747 to 1.13405, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/009-1.1341.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 1.1675 - acc: 0.6412 - val_loss: 1.1341 - val_acc: 0.6660\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1238 - acc: 0.6547\n",
      "Epoch 00010: val_loss improved from 1.13405 to 1.12184, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/010-1.1218.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 1.1238 - acc: 0.6547 - val_loss: 1.1218 - val_acc: 0.6585\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0814 - acc: 0.6708\n",
      "Epoch 00011: val_loss improved from 1.12184 to 1.07909, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/011-1.0791.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 1.0814 - acc: 0.6708 - val_loss: 1.0791 - val_acc: 0.6783\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0434 - acc: 0.6806\n",
      "Epoch 00012: val_loss improved from 1.07909 to 1.04763, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/012-1.0476.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 1.0434 - acc: 0.6806 - val_loss: 1.0476 - val_acc: 0.6874\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0134 - acc: 0.6889\n",
      "Epoch 00013: val_loss improved from 1.04763 to 1.04750, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/013-1.0475.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 1.0135 - acc: 0.6889 - val_loss: 1.0475 - val_acc: 0.6860\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9846 - acc: 0.6984\n",
      "Epoch 00014: val_loss improved from 1.04750 to 0.98750, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/014-0.9875.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.9846 - acc: 0.6984 - val_loss: 0.9875 - val_acc: 0.7102\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9506 - acc: 0.7076\n",
      "Epoch 00015: val_loss improved from 0.98750 to 0.98369, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/015-0.9837.hdf5\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.9506 - acc: 0.7076 - val_loss: 0.9837 - val_acc: 0.7116\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9270 - acc: 0.7180\n",
      "Epoch 00016: val_loss improved from 0.98369 to 0.97629, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/016-0.9763.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.9269 - acc: 0.7180 - val_loss: 0.9763 - val_acc: 0.7135\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9026 - acc: 0.7245\n",
      "Epoch 00017: val_loss improved from 0.97629 to 0.94543, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/017-0.9454.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.9026 - acc: 0.7245 - val_loss: 0.9454 - val_acc: 0.7216\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8852 - acc: 0.7284\n",
      "Epoch 00018: val_loss improved from 0.94543 to 0.92602, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/018-0.9260.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.8851 - acc: 0.7284 - val_loss: 0.9260 - val_acc: 0.7310\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8615 - acc: 0.7374\n",
      "Epoch 00019: val_loss improved from 0.92602 to 0.91809, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/019-0.9181.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.8615 - acc: 0.7374 - val_loss: 0.9181 - val_acc: 0.7342\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8413 - acc: 0.7408\n",
      "Epoch 00020: val_loss improved from 0.91809 to 0.91396, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/020-0.9140.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.8414 - acc: 0.7408 - val_loss: 0.9140 - val_acc: 0.7384\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8238 - acc: 0.7482\n",
      "Epoch 00021: val_loss improved from 0.91396 to 0.89886, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/021-0.8989.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.8238 - acc: 0.7482 - val_loss: 0.8989 - val_acc: 0.7428\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8107 - acc: 0.7525\n",
      "Epoch 00022: val_loss improved from 0.89886 to 0.88205, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/022-0.8820.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.8106 - acc: 0.7525 - val_loss: 0.8820 - val_acc: 0.7461\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7893 - acc: 0.7564\n",
      "Epoch 00023: val_loss did not improve from 0.88205\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.7892 - acc: 0.7564 - val_loss: 0.8878 - val_acc: 0.7407\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7700 - acc: 0.7637\n",
      "Epoch 00024: val_loss did not improve from 0.88205\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.7699 - acc: 0.7637 - val_loss: 0.8854 - val_acc: 0.7484\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7585 - acc: 0.7655\n",
      "Epoch 00025: val_loss did not improve from 0.88205\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.7585 - acc: 0.7655 - val_loss: 0.8864 - val_acc: 0.7454\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7422 - acc: 0.7721\n",
      "Epoch 00026: val_loss improved from 0.88205 to 0.86797, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/026-0.8680.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.7421 - acc: 0.7721 - val_loss: 0.8680 - val_acc: 0.7522\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7321 - acc: 0.7733\n",
      "Epoch 00027: val_loss improved from 0.86797 to 0.85871, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/027-0.8587.hdf5\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.7321 - acc: 0.7733 - val_loss: 0.8587 - val_acc: 0.7522\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7196 - acc: 0.7776\n",
      "Epoch 00028: val_loss improved from 0.85871 to 0.85735, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/028-0.8574.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.7196 - acc: 0.7776 - val_loss: 0.8574 - val_acc: 0.7531\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7053 - acc: 0.7817\n",
      "Epoch 00029: val_loss improved from 0.85735 to 0.85015, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/029-0.8501.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.7053 - acc: 0.7817 - val_loss: 0.8501 - val_acc: 0.7568\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6950 - acc: 0.7874\n",
      "Epoch 00030: val_loss did not improve from 0.85015\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.6950 - acc: 0.7874 - val_loss: 0.8559 - val_acc: 0.7543\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6840 - acc: 0.7889\n",
      "Epoch 00031: val_loss improved from 0.85015 to 0.82260, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/031-0.8226.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.6839 - acc: 0.7889 - val_loss: 0.8226 - val_acc: 0.7678\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6712 - acc: 0.7914\n",
      "Epoch 00032: val_loss did not improve from 0.82260\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.6714 - acc: 0.7913 - val_loss: 0.8518 - val_acc: 0.7526\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6592 - acc: 0.7972\n",
      "Epoch 00033: val_loss did not improve from 0.82260\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.6592 - acc: 0.7972 - val_loss: 0.8314 - val_acc: 0.7636\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6540 - acc: 0.7960\n",
      "Epoch 00034: val_loss did not improve from 0.82260\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.6541 - acc: 0.7960 - val_loss: 0.8241 - val_acc: 0.7678\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6418 - acc: 0.7995\n",
      "Epoch 00035: val_loss did not improve from 0.82260\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.6420 - acc: 0.7995 - val_loss: 0.8241 - val_acc: 0.7598\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6393 - acc: 0.8010\n",
      "Epoch 00036: val_loss did not improve from 0.82260\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.6393 - acc: 0.8010 - val_loss: 0.8266 - val_acc: 0.7605\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6321 - acc: 0.8034\n",
      "Epoch 00037: val_loss did not improve from 0.82260\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.6322 - acc: 0.8034 - val_loss: 0.8253 - val_acc: 0.7598\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6174 - acc: 0.8073\n",
      "Epoch 00038: val_loss did not improve from 0.82260\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.6173 - acc: 0.8073 - val_loss: 0.8228 - val_acc: 0.7629\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6112 - acc: 0.8092\n",
      "Epoch 00039: val_loss improved from 0.82260 to 0.79959, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/039-0.7996.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.6112 - acc: 0.8092 - val_loss: 0.7996 - val_acc: 0.7734\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.8104\n",
      "Epoch 00040: val_loss did not improve from 0.79959\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.6059 - acc: 0.8104 - val_loss: 0.8199 - val_acc: 0.7689\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5932 - acc: 0.8128\n",
      "Epoch 00041: val_loss did not improve from 0.79959\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.5931 - acc: 0.8128 - val_loss: 0.8065 - val_acc: 0.7687\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5958 - acc: 0.8106\n",
      "Epoch 00042: val_loss improved from 0.79959 to 0.79383, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/042-0.7938.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.5958 - acc: 0.8106 - val_loss: 0.7938 - val_acc: 0.7766\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5853 - acc: 0.8146\n",
      "Epoch 00043: val_loss did not improve from 0.79383\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.5854 - acc: 0.8146 - val_loss: 0.7968 - val_acc: 0.7729\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5805 - acc: 0.8146\n",
      "Epoch 00044: val_loss did not improve from 0.79383\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.5805 - acc: 0.8146 - val_loss: 0.7954 - val_acc: 0.7692\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.8202\n",
      "Epoch 00045: val_loss improved from 0.79383 to 0.79352, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/045-0.7935.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.5688 - acc: 0.8203 - val_loss: 0.7935 - val_acc: 0.7673\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5633 - acc: 0.8216\n",
      "Epoch 00046: val_loss improved from 0.79352 to 0.79110, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/046-0.7911.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.5633 - acc: 0.8216 - val_loss: 0.7911 - val_acc: 0.7729\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5550 - acc: 0.8249\n",
      "Epoch 00047: val_loss improved from 0.79110 to 0.78741, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/047-0.7874.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.5550 - acc: 0.8249 - val_loss: 0.7874 - val_acc: 0.7787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5548 - acc: 0.8218\n",
      "Epoch 00048: val_loss did not improve from 0.78741\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.5548 - acc: 0.8218 - val_loss: 0.7951 - val_acc: 0.7729\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.8263\n",
      "Epoch 00049: val_loss did not improve from 0.78741\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.5440 - acc: 0.8263 - val_loss: 0.7925 - val_acc: 0.7773\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.8275\n",
      "Epoch 00050: val_loss improved from 0.78741 to 0.78327, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/050-0.7833.hdf5\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.5430 - acc: 0.8276 - val_loss: 0.7833 - val_acc: 0.7757\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.8286\n",
      "Epoch 00051: val_loss did not improve from 0.78327\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.5420 - acc: 0.8286 - val_loss: 0.7923 - val_acc: 0.7775\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8310\n",
      "Epoch 00052: val_loss did not improve from 0.78327\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.5316 - acc: 0.8311 - val_loss: 0.8059 - val_acc: 0.7736\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.8324\n",
      "Epoch 00053: val_loss improved from 0.78327 to 0.77632, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/053-0.7763.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.5259 - acc: 0.8324 - val_loss: 0.7763 - val_acc: 0.7794\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5205 - acc: 0.8361\n",
      "Epoch 00054: val_loss did not improve from 0.77632\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.5206 - acc: 0.8361 - val_loss: 0.7798 - val_acc: 0.7841\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.8345\n",
      "Epoch 00055: val_loss did not improve from 0.77632\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.5122 - acc: 0.8345 - val_loss: 0.7777 - val_acc: 0.7815\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8361\n",
      "Epoch 00056: val_loss improved from 0.77632 to 0.77538, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/056-0.7754.hdf5\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.5116 - acc: 0.8361 - val_loss: 0.7754 - val_acc: 0.7836\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.8366\n",
      "Epoch 00057: val_loss did not improve from 0.77538\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.5074 - acc: 0.8366 - val_loss: 0.7785 - val_acc: 0.7873\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.8385\n",
      "Epoch 00058: val_loss improved from 0.77538 to 0.77228, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/058-0.7723.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.5069 - acc: 0.8385 - val_loss: 0.7723 - val_acc: 0.7850\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4941 - acc: 0.8415\n",
      "Epoch 00059: val_loss did not improve from 0.77228\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4941 - acc: 0.8415 - val_loss: 0.7756 - val_acc: 0.7845\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4977 - acc: 0.8393\n",
      "Epoch 00060: val_loss improved from 0.77228 to 0.75730, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/060-0.7573.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.4978 - acc: 0.8393 - val_loss: 0.7573 - val_acc: 0.7929\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.8413\n",
      "Epoch 00061: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.4918 - acc: 0.8412 - val_loss: 0.7930 - val_acc: 0.7792\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.8425\n",
      "Epoch 00062: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.4874 - acc: 0.8425 - val_loss: 0.7829 - val_acc: 0.7827\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4782 - acc: 0.8455\n",
      "Epoch 00063: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4782 - acc: 0.8455 - val_loss: 0.7820 - val_acc: 0.7850\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8420\n",
      "Epoch 00064: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4871 - acc: 0.8421 - val_loss: 0.7613 - val_acc: 0.7843\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4747 - acc: 0.8453\n",
      "Epoch 00065: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.4747 - acc: 0.8453 - val_loss: 0.7793 - val_acc: 0.7908\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4782 - acc: 0.8459\n",
      "Epoch 00066: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.4782 - acc: 0.8459 - val_loss: 0.7749 - val_acc: 0.7894\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.8476\n",
      "Epoch 00067: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4679 - acc: 0.8476 - val_loss: 0.7657 - val_acc: 0.7929\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.8500\n",
      "Epoch 00068: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.4702 - acc: 0.8500 - val_loss: 0.7719 - val_acc: 0.7883\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4601 - acc: 0.8504\n",
      "Epoch 00069: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.4602 - acc: 0.8504 - val_loss: 0.7789 - val_acc: 0.7855\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4574 - acc: 0.8537\n",
      "Epoch 00070: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.4573 - acc: 0.8537 - val_loss: 0.7683 - val_acc: 0.7915\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8555\n",
      "Epoch 00071: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4529 - acc: 0.8555 - val_loss: 0.7708 - val_acc: 0.7906\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4552 - acc: 0.8543\n",
      "Epoch 00072: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4551 - acc: 0.8544 - val_loss: 0.7762 - val_acc: 0.7911\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.8543\n",
      "Epoch 00073: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.4522 - acc: 0.8543 - val_loss: 0.7639 - val_acc: 0.7894\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8533\n",
      "Epoch 00074: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.4475 - acc: 0.8533 - val_loss: 0.7771 - val_acc: 0.7899\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.8586\n",
      "Epoch 00075: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4394 - acc: 0.8586 - val_loss: 0.7682 - val_acc: 0.7955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.8584\n",
      "Epoch 00076: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.4367 - acc: 0.8584 - val_loss: 0.7718 - val_acc: 0.7815\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4390 - acc: 0.8562\n",
      "Epoch 00077: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.4389 - acc: 0.8562 - val_loss: 0.7720 - val_acc: 0.7873\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4389 - acc: 0.8585\n",
      "Epoch 00078: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.4388 - acc: 0.8585 - val_loss: 0.7673 - val_acc: 0.7866\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.8594\n",
      "Epoch 00079: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.4293 - acc: 0.8594 - val_loss: 0.7682 - val_acc: 0.7906\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.8584\n",
      "Epoch 00080: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4323 - acc: 0.8584 - val_loss: 0.7670 - val_acc: 0.7922\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4331 - acc: 0.8603\n",
      "Epoch 00081: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.4331 - acc: 0.8603 - val_loss: 0.7630 - val_acc: 0.7899\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4290 - acc: 0.8603\n",
      "Epoch 00082: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.4289 - acc: 0.8603 - val_loss: 0.7831 - val_acc: 0.7887\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4275 - acc: 0.8604\n",
      "Epoch 00083: val_loss did not improve from 0.75730\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4275 - acc: 0.8604 - val_loss: 0.7894 - val_acc: 0.7943\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.8623\n",
      "Epoch 00084: val_loss improved from 0.75730 to 0.75699, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/084-0.7570.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.4271 - acc: 0.8623 - val_loss: 0.7570 - val_acc: 0.7997\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8625\n",
      "Epoch 00085: val_loss did not improve from 0.75699\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.4188 - acc: 0.8625 - val_loss: 0.7603 - val_acc: 0.7973\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8667\n",
      "Epoch 00086: val_loss did not improve from 0.75699\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.4121 - acc: 0.8667 - val_loss: 0.7576 - val_acc: 0.7978\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8617\n",
      "Epoch 00087: val_loss improved from 0.75699 to 0.74500, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/087-0.7450.hdf5\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.4191 - acc: 0.8617 - val_loss: 0.7450 - val_acc: 0.7966\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8668\n",
      "Epoch 00088: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.4126 - acc: 0.8668 - val_loss: 0.7659 - val_acc: 0.7950\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4115 - acc: 0.8646\n",
      "Epoch 00089: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.4115 - acc: 0.8647 - val_loss: 0.7797 - val_acc: 0.7962\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8665\n",
      "Epoch 00090: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4085 - acc: 0.8665 - val_loss: 0.7586 - val_acc: 0.7939\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8677\n",
      "Epoch 00091: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.4072 - acc: 0.8677 - val_loss: 0.7536 - val_acc: 0.8020\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8677\n",
      "Epoch 00092: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.4047 - acc: 0.8677 - val_loss: 0.7641 - val_acc: 0.7904\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8641\n",
      "Epoch 00093: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.4089 - acc: 0.8641 - val_loss: 0.7806 - val_acc: 0.7906\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8682\n",
      "Epoch 00094: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.4025 - acc: 0.8682 - val_loss: 0.7646 - val_acc: 0.8022\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3920 - acc: 0.8706\n",
      "Epoch 00095: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3920 - acc: 0.8706 - val_loss: 0.7648 - val_acc: 0.7955\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8667\n",
      "Epoch 00096: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.4041 - acc: 0.8667 - val_loss: 0.7797 - val_acc: 0.7948\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3975 - acc: 0.8697\n",
      "Epoch 00097: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3975 - acc: 0.8696 - val_loss: 0.7557 - val_acc: 0.8018\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8716\n",
      "Epoch 00098: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3930 - acc: 0.8716 - val_loss: 0.7675 - val_acc: 0.8032\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3957 - acc: 0.8688\n",
      "Epoch 00099: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.3957 - acc: 0.8688 - val_loss: 0.7612 - val_acc: 0.8025\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8718\n",
      "Epoch 00100: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3935 - acc: 0.8718 - val_loss: 0.7617 - val_acc: 0.7966\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8722\n",
      "Epoch 00101: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3891 - acc: 0.8722 - val_loss: 0.7566 - val_acc: 0.8032\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.8708\n",
      "Epoch 00102: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.3900 - acc: 0.8708 - val_loss: 0.8088 - val_acc: 0.7838\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8714\n",
      "Epoch 00103: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3914 - acc: 0.8714 - val_loss: 0.7592 - val_acc: 0.8004\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8729\n",
      "Epoch 00104: val_loss did not improve from 0.74500\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3879 - acc: 0.8729 - val_loss: 0.7616 - val_acc: 0.8008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8722\n",
      "Epoch 00105: val_loss improved from 0.74500 to 0.74002, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/105-0.7400.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3865 - acc: 0.8722 - val_loss: 0.7400 - val_acc: 0.8041\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8757\n",
      "Epoch 00106: val_loss did not improve from 0.74002\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3766 - acc: 0.8757 - val_loss: 0.7815 - val_acc: 0.7999\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8780\n",
      "Epoch 00107: val_loss did not improve from 0.74002\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3783 - acc: 0.8779 - val_loss: 0.7538 - val_acc: 0.8048\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8757\n",
      "Epoch 00108: val_loss did not improve from 0.74002\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3766 - acc: 0.8756 - val_loss: 0.7738 - val_acc: 0.7994\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8752\n",
      "Epoch 00109: val_loss did not improve from 0.74002\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3775 - acc: 0.8752 - val_loss: 0.7687 - val_acc: 0.7992\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3720 - acc: 0.8772\n",
      "Epoch 00110: val_loss did not improve from 0.74002\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3719 - acc: 0.8772 - val_loss: 0.7589 - val_acc: 0.8043\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8769\n",
      "Epoch 00111: val_loss did not improve from 0.74002\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3774 - acc: 0.8768 - val_loss: 0.7809 - val_acc: 0.7966\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8730\n",
      "Epoch 00112: val_loss did not improve from 0.74002\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3793 - acc: 0.8730 - val_loss: 0.7601 - val_acc: 0.8036\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8791\n",
      "Epoch 00113: val_loss improved from 0.74002 to 0.73661, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/113-0.7366.hdf5\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3704 - acc: 0.8791 - val_loss: 0.7366 - val_acc: 0.8076\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8769\n",
      "Epoch 00114: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3761 - acc: 0.8769 - val_loss: 0.7514 - val_acc: 0.8067\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8799\n",
      "Epoch 00115: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3645 - acc: 0.8799 - val_loss: 0.7559 - val_acc: 0.8048\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8788\n",
      "Epoch 00116: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3670 - acc: 0.8788 - val_loss: 0.7554 - val_acc: 0.8088\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3700 - acc: 0.8794\n",
      "Epoch 00117: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3701 - acc: 0.8794 - val_loss: 0.7426 - val_acc: 0.8046\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8815\n",
      "Epoch 00118: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3609 - acc: 0.8815 - val_loss: 0.7514 - val_acc: 0.8048\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8795\n",
      "Epoch 00119: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3665 - acc: 0.8794 - val_loss: 0.7639 - val_acc: 0.8029\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8808\n",
      "Epoch 00120: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3589 - acc: 0.8808 - val_loss: 0.7501 - val_acc: 0.8106\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8805\n",
      "Epoch 00121: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3626 - acc: 0.8805 - val_loss: 0.7556 - val_acc: 0.8062\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8790\n",
      "Epoch 00122: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3671 - acc: 0.8790 - val_loss: 0.7647 - val_acc: 0.8088\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8820\n",
      "Epoch 00123: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3554 - acc: 0.8820 - val_loss: 0.7521 - val_acc: 0.8120\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.8787\n",
      "Epoch 00124: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3600 - acc: 0.8787 - val_loss: 0.7566 - val_acc: 0.8116\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8843\n",
      "Epoch 00125: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3509 - acc: 0.8843 - val_loss: 0.7469 - val_acc: 0.8102\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8846\n",
      "Epoch 00126: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3516 - acc: 0.8846 - val_loss: 0.7589 - val_acc: 0.8055\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8837\n",
      "Epoch 00127: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3504 - acc: 0.8837 - val_loss: 0.7595 - val_acc: 0.8099\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.8848\n",
      "Epoch 00128: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3461 - acc: 0.8848 - val_loss: 0.7643 - val_acc: 0.8099\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8823\n",
      "Epoch 00129: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3549 - acc: 0.8823 - val_loss: 0.7688 - val_acc: 0.8106\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8840\n",
      "Epoch 00130: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3518 - acc: 0.8840 - val_loss: 0.7649 - val_acc: 0.8085\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8841\n",
      "Epoch 00131: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3508 - acc: 0.8841 - val_loss: 0.7771 - val_acc: 0.8020\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8832\n",
      "Epoch 00132: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3545 - acc: 0.8832 - val_loss: 0.7533 - val_acc: 0.8127\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8837\n",
      "Epoch 00133: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3495 - acc: 0.8837 - val_loss: 0.7835 - val_acc: 0.8020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8848\n",
      "Epoch 00134: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3473 - acc: 0.8848 - val_loss: 0.7389 - val_acc: 0.8146\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.8882\n",
      "Epoch 00135: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3397 - acc: 0.8882 - val_loss: 0.7518 - val_acc: 0.8060\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.8874\n",
      "Epoch 00136: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3457 - acc: 0.8874 - val_loss: 0.7829 - val_acc: 0.8025\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8874\n",
      "Epoch 00137: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3382 - acc: 0.8875 - val_loss: 0.7389 - val_acc: 0.8109\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8868\n",
      "Epoch 00138: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3435 - acc: 0.8868 - val_loss: 0.7726 - val_acc: 0.8029\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8860\n",
      "Epoch 00139: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 660us/sample - loss: 0.3407 - acc: 0.8860 - val_loss: 0.7536 - val_acc: 0.8076\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8870\n",
      "Epoch 00140: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3387 - acc: 0.8869 - val_loss: 0.7595 - val_acc: 0.8150\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8889\n",
      "Epoch 00141: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3343 - acc: 0.8889 - val_loss: 0.7727 - val_acc: 0.8015\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.8847\n",
      "Epoch 00142: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3418 - acc: 0.8847 - val_loss: 0.7504 - val_acc: 0.8064\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8896\n",
      "Epoch 00143: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3322 - acc: 0.8896 - val_loss: 0.7413 - val_acc: 0.8074\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8879\n",
      "Epoch 00144: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3370 - acc: 0.8879 - val_loss: 0.7445 - val_acc: 0.8111\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8892\n",
      "Epoch 00145: val_loss did not improve from 0.73661\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3354 - acc: 0.8891 - val_loss: 0.7574 - val_acc: 0.8069\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8876\n",
      "Epoch 00146: val_loss improved from 0.73661 to 0.73427, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/146-0.7343.hdf5\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.3382 - acc: 0.8876 - val_loss: 0.7343 - val_acc: 0.8132\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8907\n",
      "Epoch 00147: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3322 - acc: 0.8907 - val_loss: 0.7602 - val_acc: 0.8148\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8889\n",
      "Epoch 00148: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3310 - acc: 0.8888 - val_loss: 0.7754 - val_acc: 0.8053\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8881\n",
      "Epoch 00149: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3333 - acc: 0.8881 - val_loss: 0.7583 - val_acc: 0.8120\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8932\n",
      "Epoch 00150: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3251 - acc: 0.8932 - val_loss: 0.7353 - val_acc: 0.8160\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3326 - acc: 0.8923\n",
      "Epoch 00151: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3326 - acc: 0.8923 - val_loss: 0.7505 - val_acc: 0.8127\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.8924\n",
      "Epoch 00152: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3243 - acc: 0.8924 - val_loss: 0.7384 - val_acc: 0.8174\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8925\n",
      "Epoch 00153: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3301 - acc: 0.8925 - val_loss: 0.7429 - val_acc: 0.8139\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8927\n",
      "Epoch 00154: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3275 - acc: 0.8927 - val_loss: 0.7593 - val_acc: 0.8143\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3209 - acc: 0.8937\n",
      "Epoch 00155: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3209 - acc: 0.8937 - val_loss: 0.7474 - val_acc: 0.8153\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3209 - acc: 0.8940\n",
      "Epoch 00156: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3209 - acc: 0.8940 - val_loss: 0.7385 - val_acc: 0.8155\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3194 - acc: 0.8935\n",
      "Epoch 00157: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3194 - acc: 0.8934 - val_loss: 0.7556 - val_acc: 0.8109\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.8940\n",
      "Epoch 00158: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3183 - acc: 0.8940 - val_loss: 0.7482 - val_acc: 0.8090\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8951\n",
      "Epoch 00159: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3180 - acc: 0.8952 - val_loss: 0.7542 - val_acc: 0.8171\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.8932\n",
      "Epoch 00160: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3244 - acc: 0.8932 - val_loss: 0.7660 - val_acc: 0.8155\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.8959\n",
      "Epoch 00161: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3173 - acc: 0.8959 - val_loss: 0.7462 - val_acc: 0.8146\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8963\n",
      "Epoch 00162: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3147 - acc: 0.8963 - val_loss: 0.7463 - val_acc: 0.8139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.8939\n",
      "Epoch 00163: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3156 - acc: 0.8939 - val_loss: 0.7425 - val_acc: 0.8153\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.8959\n",
      "Epoch 00164: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3127 - acc: 0.8959 - val_loss: 0.7524 - val_acc: 0.8169\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.8980\n",
      "Epoch 00165: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3109 - acc: 0.8980 - val_loss: 0.7444 - val_acc: 0.8146\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.8962\n",
      "Epoch 00166: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3136 - acc: 0.8962 - val_loss: 0.7454 - val_acc: 0.8190\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8975\n",
      "Epoch 00167: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3120 - acc: 0.8975 - val_loss: 0.7584 - val_acc: 0.8127\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.8975\n",
      "Epoch 00168: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3106 - acc: 0.8975 - val_loss: 0.7901 - val_acc: 0.8069\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8944\n",
      "Epoch 00169: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3175 - acc: 0.8944 - val_loss: 0.7531 - val_acc: 0.8116\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.8956\n",
      "Epoch 00170: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3148 - acc: 0.8956 - val_loss: 0.7559 - val_acc: 0.8171\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.8974\n",
      "Epoch 00171: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3081 - acc: 0.8974 - val_loss: 0.7566 - val_acc: 0.8141\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.8962\n",
      "Epoch 00172: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3097 - acc: 0.8962 - val_loss: 0.7433 - val_acc: 0.8162\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.8968\n",
      "Epoch 00173: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3094 - acc: 0.8968 - val_loss: 0.7432 - val_acc: 0.8132\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.8957\n",
      "Epoch 00174: val_loss did not improve from 0.73427\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3089 - acc: 0.8956 - val_loss: 0.7442 - val_acc: 0.8153\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3073 - acc: 0.8994\n",
      "Epoch 00175: val_loss improved from 0.73427 to 0.73247, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/175-0.7325.hdf5\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.3073 - acc: 0.8994 - val_loss: 0.7325 - val_acc: 0.8192\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.8984\n",
      "Epoch 00176: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3049 - acc: 0.8984 - val_loss: 0.7556 - val_acc: 0.8171\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3073 - acc: 0.8968\n",
      "Epoch 00177: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.3075 - acc: 0.8968 - val_loss: 0.7388 - val_acc: 0.8213\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.8996\n",
      "Epoch 00178: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3032 - acc: 0.8996 - val_loss: 0.7549 - val_acc: 0.8088\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8976\n",
      "Epoch 00179: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3107 - acc: 0.8976 - val_loss: 0.7480 - val_acc: 0.8137\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.8971\n",
      "Epoch 00180: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3085 - acc: 0.8971 - val_loss: 0.7509 - val_acc: 0.8143\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3067 - acc: 0.8980\n",
      "Epoch 00181: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3067 - acc: 0.8981 - val_loss: 0.7535 - val_acc: 0.8197\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.8991\n",
      "Epoch 00182: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3070 - acc: 0.8991 - val_loss: 0.7365 - val_acc: 0.8204\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.9002\n",
      "Epoch 00183: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.3039 - acc: 0.9002 - val_loss: 0.7442 - val_acc: 0.8169\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.8975\n",
      "Epoch 00184: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3063 - acc: 0.8975 - val_loss: 0.7630 - val_acc: 0.8102\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.8985\n",
      "Epoch 00185: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3081 - acc: 0.8985 - val_loss: 0.7491 - val_acc: 0.8127\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.8987\n",
      "Epoch 00186: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3054 - acc: 0.8987 - val_loss: 0.7435 - val_acc: 0.8185\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.9006\n",
      "Epoch 00187: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2985 - acc: 0.9006 - val_loss: 0.7475 - val_acc: 0.8176\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9007\n",
      "Epoch 00188: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.3016 - acc: 0.9007 - val_loss: 0.7504 - val_acc: 0.8195\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9011\n",
      "Epoch 00189: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2949 - acc: 0.9011 - val_loss: 0.7542 - val_acc: 0.8171\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.9033\n",
      "Epoch 00190: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2962 - acc: 0.9033 - val_loss: 0.7435 - val_acc: 0.8169\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.8998\n",
      "Epoch 00191: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.3001 - acc: 0.8997 - val_loss: 0.7567 - val_acc: 0.8139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9014\n",
      "Epoch 00192: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2986 - acc: 0.9015 - val_loss: 0.7444 - val_acc: 0.8188\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2923 - acc: 0.9048\n",
      "Epoch 00193: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2923 - acc: 0.9048 - val_loss: 0.7523 - val_acc: 0.8206\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9016\n",
      "Epoch 00194: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2928 - acc: 0.9016 - val_loss: 0.7439 - val_acc: 0.8181\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9032\n",
      "Epoch 00195: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2918 - acc: 0.9031 - val_loss: 0.7383 - val_acc: 0.8232\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9042\n",
      "Epoch 00196: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2912 - acc: 0.9043 - val_loss: 0.7550 - val_acc: 0.8164\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9024\n",
      "Epoch 00197: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2951 - acc: 0.9024 - val_loss: 0.7361 - val_acc: 0.8178\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9038\n",
      "Epoch 00198: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2930 - acc: 0.9038 - val_loss: 0.7433 - val_acc: 0.8202\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.9047\n",
      "Epoch 00199: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2883 - acc: 0.9047 - val_loss: 0.7538 - val_acc: 0.8220\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9034\n",
      "Epoch 00200: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2914 - acc: 0.9034 - val_loss: 0.7596 - val_acc: 0.8204\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.9053\n",
      "Epoch 00201: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2905 - acc: 0.9053 - val_loss: 0.7423 - val_acc: 0.8237\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9053\n",
      "Epoch 00202: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2853 - acc: 0.9053 - val_loss: 0.7434 - val_acc: 0.8185\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9036\n",
      "Epoch 00203: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2913 - acc: 0.9036 - val_loss: 0.7700 - val_acc: 0.8176\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9069\n",
      "Epoch 00204: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2842 - acc: 0.9069 - val_loss: 0.7363 - val_acc: 0.8223\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9005\n",
      "Epoch 00205: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2934 - acc: 0.9006 - val_loss: 0.7494 - val_acc: 0.8220\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9063\n",
      "Epoch 00206: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2866 - acc: 0.9063 - val_loss: 0.7558 - val_acc: 0.8183\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9067\n",
      "Epoch 00207: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 651us/sample - loss: 0.2844 - acc: 0.9067 - val_loss: 0.7553 - val_acc: 0.8153\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9046\n",
      "Epoch 00208: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2899 - acc: 0.9046 - val_loss: 0.7419 - val_acc: 0.8169\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9040\n",
      "Epoch 00209: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2894 - acc: 0.9040 - val_loss: 0.7340 - val_acc: 0.8267\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9021\n",
      "Epoch 00210: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2914 - acc: 0.9021 - val_loss: 0.7338 - val_acc: 0.8199\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9046\n",
      "Epoch 00211: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2904 - acc: 0.9046 - val_loss: 0.7370 - val_acc: 0.8204\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.9080\n",
      "Epoch 00212: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2799 - acc: 0.9080 - val_loss: 0.7656 - val_acc: 0.8190\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9045\n",
      "Epoch 00213: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2806 - acc: 0.9045 - val_loss: 0.7434 - val_acc: 0.8218\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9080\n",
      "Epoch 00214: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2809 - acc: 0.9080 - val_loss: 0.7430 - val_acc: 0.8192\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9072\n",
      "Epoch 00215: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2817 - acc: 0.9072 - val_loss: 0.7470 - val_acc: 0.8225\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9032\n",
      "Epoch 00216: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2886 - acc: 0.9032 - val_loss: 0.7334 - val_acc: 0.8232\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9045\n",
      "Epoch 00217: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2846 - acc: 0.9045 - val_loss: 0.7540 - val_acc: 0.8197\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9066\n",
      "Epoch 00218: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2832 - acc: 0.9066 - val_loss: 0.7368 - val_acc: 0.8246\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9056\n",
      "Epoch 00219: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2826 - acc: 0.9056 - val_loss: 0.7468 - val_acc: 0.8244\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9072\n",
      "Epoch 00220: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.2827 - acc: 0.9072 - val_loss: 0.7444 - val_acc: 0.8218\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9076\n",
      "Epoch 00221: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2831 - acc: 0.9076 - val_loss: 0.7515 - val_acc: 0.8241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2829 - acc: 0.9044\n",
      "Epoch 00222: val_loss did not improve from 0.73247\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2829 - acc: 0.9044 - val_loss: 0.7409 - val_acc: 0.8290\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9073\n",
      "Epoch 00223: val_loss improved from 0.73247 to 0.72979, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/223-0.7298.hdf5\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2784 - acc: 0.9073 - val_loss: 0.7298 - val_acc: 0.8234\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9065\n",
      "Epoch 00224: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2775 - acc: 0.9065 - val_loss: 0.7718 - val_acc: 0.8162\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.9062\n",
      "Epoch 00225: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2797 - acc: 0.9062 - val_loss: 0.7380 - val_acc: 0.8232\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9068\n",
      "Epoch 00226: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2760 - acc: 0.9068 - val_loss: 0.7551 - val_acc: 0.8274\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9059\n",
      "Epoch 00227: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2815 - acc: 0.9059 - val_loss: 0.7406 - val_acc: 0.8237\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9078\n",
      "Epoch 00228: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2783 - acc: 0.9078 - val_loss: 0.7476 - val_acc: 0.8314\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9069\n",
      "Epoch 00229: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2832 - acc: 0.9069 - val_loss: 0.7313 - val_acc: 0.8260\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2722 - acc: 0.9101\n",
      "Epoch 00230: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2722 - acc: 0.9101 - val_loss: 0.7605 - val_acc: 0.8244\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9117\n",
      "Epoch 00231: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2765 - acc: 0.9117 - val_loss: 0.7606 - val_acc: 0.8267\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9109\n",
      "Epoch 00232: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2713 - acc: 0.9109 - val_loss: 0.7389 - val_acc: 0.8272\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9090\n",
      "Epoch 00233: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2760 - acc: 0.9090 - val_loss: 0.7326 - val_acc: 0.8286\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9095\n",
      "Epoch 00234: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2724 - acc: 0.9096 - val_loss: 0.7463 - val_acc: 0.8232\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9085\n",
      "Epoch 00235: val_loss did not improve from 0.72979\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2742 - acc: 0.9085 - val_loss: 0.7427 - val_acc: 0.8286\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9085\n",
      "Epoch 00236: val_loss improved from 0.72979 to 0.71585, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_4_conv_checkpoint/236-0.7159.hdf5\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2766 - acc: 0.9084 - val_loss: 0.7159 - val_acc: 0.8330\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9110\n",
      "Epoch 00237: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2726 - acc: 0.9110 - val_loss: 0.7544 - val_acc: 0.8251\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2729 - acc: 0.9093\n",
      "Epoch 00238: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2729 - acc: 0.9093 - val_loss: 0.7453 - val_acc: 0.8251\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9085\n",
      "Epoch 00239: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2746 - acc: 0.9085 - val_loss: 0.7308 - val_acc: 0.8267\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9092\n",
      "Epoch 00240: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2743 - acc: 0.9092 - val_loss: 0.7317 - val_acc: 0.8300\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9095\n",
      "Epoch 00241: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.2702 - acc: 0.9096 - val_loss: 0.7204 - val_acc: 0.8316\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9101\n",
      "Epoch 00242: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2705 - acc: 0.9101 - val_loss: 0.7489 - val_acc: 0.8286\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9095\n",
      "Epoch 00243: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2713 - acc: 0.9094 - val_loss: 0.7318 - val_acc: 0.8251\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9123\n",
      "Epoch 00244: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2634 - acc: 0.9123 - val_loss: 0.7407 - val_acc: 0.8302\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9112\n",
      "Epoch 00245: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2699 - acc: 0.9112 - val_loss: 0.7266 - val_acc: 0.8323\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9106\n",
      "Epoch 00246: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2721 - acc: 0.9106 - val_loss: 0.7294 - val_acc: 0.8239\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9103\n",
      "Epoch 00247: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2668 - acc: 0.9103 - val_loss: 0.7301 - val_acc: 0.8300\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9090\n",
      "Epoch 00248: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2699 - acc: 0.9090 - val_loss: 0.7267 - val_acc: 0.8353\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9121\n",
      "Epoch 00249: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2646 - acc: 0.9121 - val_loss: 0.7470 - val_acc: 0.8269\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9097\n",
      "Epoch 00250: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2676 - acc: 0.9097 - val_loss: 0.7241 - val_acc: 0.8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9125\n",
      "Epoch 00251: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2608 - acc: 0.9125 - val_loss: 0.7449 - val_acc: 0.8283\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9123\n",
      "Epoch 00252: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2657 - acc: 0.9123 - val_loss: 0.7414 - val_acc: 0.8309\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9129\n",
      "Epoch 00253: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2658 - acc: 0.9129 - val_loss: 0.7451 - val_acc: 0.8248\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.9116\n",
      "Epoch 00254: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2669 - acc: 0.9116 - val_loss: 0.7207 - val_acc: 0.8344\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9114\n",
      "Epoch 00255: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2654 - acc: 0.9113 - val_loss: 0.7505 - val_acc: 0.8241\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2665 - acc: 0.9137\n",
      "Epoch 00256: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2665 - acc: 0.9138 - val_loss: 0.7376 - val_acc: 0.8283\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9120\n",
      "Epoch 00257: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2647 - acc: 0.9120 - val_loss: 0.7365 - val_acc: 0.8293\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9140\n",
      "Epoch 00258: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2605 - acc: 0.9140 - val_loss: 0.7278 - val_acc: 0.8328\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9142\n",
      "Epoch 00259: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2595 - acc: 0.9142 - val_loss: 0.7446 - val_acc: 0.8288\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9135\n",
      "Epoch 00260: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 659us/sample - loss: 0.2609 - acc: 0.9135 - val_loss: 0.7504 - val_acc: 0.8283\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.9139\n",
      "Epoch 00261: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2600 - acc: 0.9139 - val_loss: 0.7413 - val_acc: 0.8316\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9126\n",
      "Epoch 00262: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2616 - acc: 0.9126 - val_loss: 0.7341 - val_acc: 0.8293\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9136\n",
      "Epoch 00263: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2631 - acc: 0.9136 - val_loss: 0.7419 - val_acc: 0.8246\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9165\n",
      "Epoch 00264: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2549 - acc: 0.9165 - val_loss: 0.7503 - val_acc: 0.8316\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9148\n",
      "Epoch 00265: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2554 - acc: 0.9148 - val_loss: 0.7425 - val_acc: 0.8337\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9144\n",
      "Epoch 00266: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2584 - acc: 0.9144 - val_loss: 0.7375 - val_acc: 0.8297\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9154\n",
      "Epoch 00267: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2553 - acc: 0.9154 - val_loss: 0.7278 - val_acc: 0.8341\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9147\n",
      "Epoch 00268: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 652us/sample - loss: 0.2599 - acc: 0.9147 - val_loss: 0.7420 - val_acc: 0.8281\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9150\n",
      "Epoch 00269: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 658us/sample - loss: 0.2574 - acc: 0.9150 - val_loss: 0.7217 - val_acc: 0.8353\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9133\n",
      "Epoch 00270: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2590 - acc: 0.9134 - val_loss: 0.7423 - val_acc: 0.8309\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2556 - acc: 0.9146\n",
      "Epoch 00271: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2556 - acc: 0.9147 - val_loss: 0.7607 - val_acc: 0.8265\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9145\n",
      "Epoch 00272: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2570 - acc: 0.9145 - val_loss: 0.7450 - val_acc: 0.8323\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9159\n",
      "Epoch 00273: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2583 - acc: 0.9159 - val_loss: 0.7497 - val_acc: 0.8293\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2601 - acc: 0.9149\n",
      "Epoch 00274: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2602 - acc: 0.9149 - val_loss: 0.7731 - val_acc: 0.8269\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.9156\n",
      "Epoch 00275: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2523 - acc: 0.9156 - val_loss: 0.7374 - val_acc: 0.8262\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2522 - acc: 0.9138\n",
      "Epoch 00276: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2522 - acc: 0.9138 - val_loss: 0.7408 - val_acc: 0.8290\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9135\n",
      "Epoch 00277: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2554 - acc: 0.9135 - val_loss: 0.7167 - val_acc: 0.8351\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9179\n",
      "Epoch 00278: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2491 - acc: 0.9179 - val_loss: 0.7312 - val_acc: 0.8321\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9148\n",
      "Epoch 00279: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2539 - acc: 0.9148 - val_loss: 0.7332 - val_acc: 0.8337\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2530 - acc: 0.9156\n",
      "Epoch 00280: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2531 - acc: 0.9156 - val_loss: 0.7178 - val_acc: 0.8334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9152\n",
      "Epoch 00281: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2566 - acc: 0.9153 - val_loss: 0.7320 - val_acc: 0.8321\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9175\n",
      "Epoch 00282: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 654us/sample - loss: 0.2534 - acc: 0.9175 - val_loss: 0.7234 - val_acc: 0.8376\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9173\n",
      "Epoch 00283: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 656us/sample - loss: 0.2515 - acc: 0.9173 - val_loss: 0.7269 - val_acc: 0.8351\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9176\n",
      "Epoch 00284: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2478 - acc: 0.9176 - val_loss: 0.8029 - val_acc: 0.8162\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9151\n",
      "Epoch 00285: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 655us/sample - loss: 0.2533 - acc: 0.9151 - val_loss: 0.7180 - val_acc: 0.8365\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9170\n",
      "Epoch 00286: val_loss did not improve from 0.71585\n",
      "36805/36805 [==============================] - 24s 657us/sample - loss: 0.2524 - acc: 0.9170 - val_loss: 0.7324 - val_acc: 0.8309\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 343us/sample - loss: 0.8170 - acc: 0.7985\n",
      "Loss: 0.8169891846142825 Accuracy: 0.7985462\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3952 - acc: 0.2125\n",
      "Epoch 00001: val_loss improved from inf to 1.84023, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/001-1.8402.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 2.3951 - acc: 0.2126 - val_loss: 1.8402 - val_acc: 0.4177\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7927 - acc: 0.4113\n",
      "Epoch 00002: val_loss improved from 1.84023 to 1.55220, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/002-1.5522.hdf5\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 1.7928 - acc: 0.4113 - val_loss: 1.5522 - val_acc: 0.5132\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6054 - acc: 0.4783\n",
      "Epoch 00003: val_loss improved from 1.55220 to 1.43780, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/003-1.4378.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 1.6055 - acc: 0.4783 - val_loss: 1.4378 - val_acc: 0.5493\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4971 - acc: 0.5150\n",
      "Epoch 00004: val_loss improved from 1.43780 to 1.34999, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/004-1.3500.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 1.4971 - acc: 0.5150 - val_loss: 1.3500 - val_acc: 0.5837\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4052 - acc: 0.5525\n",
      "Epoch 00005: val_loss improved from 1.34999 to 1.26579, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/005-1.2658.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 1.4053 - acc: 0.5525 - val_loss: 1.2658 - val_acc: 0.6175\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3259 - acc: 0.5778\n",
      "Epoch 00006: val_loss improved from 1.26579 to 1.20763, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/006-1.2076.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 1.3259 - acc: 0.5778 - val_loss: 1.2076 - val_acc: 0.6371\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2491 - acc: 0.6092\n",
      "Epoch 00007: val_loss improved from 1.20763 to 1.13817, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/007-1.1382.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 1.2491 - acc: 0.6092 - val_loss: 1.1382 - val_acc: 0.6590\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1883 - acc: 0.6271\n",
      "Epoch 00008: val_loss improved from 1.13817 to 1.08504, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/008-1.0850.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 1.1883 - acc: 0.6272 - val_loss: 1.0850 - val_acc: 0.6758\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1314 - acc: 0.6470\n",
      "Epoch 00009: val_loss improved from 1.08504 to 1.04159, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/009-1.0416.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 1.1314 - acc: 0.6470 - val_loss: 1.0416 - val_acc: 0.6867\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0862 - acc: 0.6609\n",
      "Epoch 00010: val_loss improved from 1.04159 to 1.00818, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/010-1.0082.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 1.0861 - acc: 0.6610 - val_loss: 1.0082 - val_acc: 0.7016\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0486 - acc: 0.6745\n",
      "Epoch 00011: val_loss improved from 1.00818 to 0.96886, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/011-0.9689.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 1.0485 - acc: 0.6745 - val_loss: 0.9689 - val_acc: 0.7077\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0042 - acc: 0.6904\n",
      "Epoch 00012: val_loss improved from 0.96886 to 0.94249, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/012-0.9425.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 1.0043 - acc: 0.6904 - val_loss: 0.9425 - val_acc: 0.7181\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9692 - acc: 0.7027\n",
      "Epoch 00013: val_loss improved from 0.94249 to 0.92408, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/013-0.9241.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.9693 - acc: 0.7027 - val_loss: 0.9241 - val_acc: 0.7228\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9411 - acc: 0.7119\n",
      "Epoch 00014: val_loss improved from 0.92408 to 0.87477, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/014-0.8748.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.9410 - acc: 0.7119 - val_loss: 0.8748 - val_acc: 0.7365\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9020 - acc: 0.7223\n",
      "Epoch 00015: val_loss improved from 0.87477 to 0.85707, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/015-0.8571.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.9019 - acc: 0.7223 - val_loss: 0.8571 - val_acc: 0.7428\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8823 - acc: 0.7275\n",
      "Epoch 00016: val_loss improved from 0.85707 to 0.84314, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/016-0.8431.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.8822 - acc: 0.7275 - val_loss: 0.8431 - val_acc: 0.7496\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8504 - acc: 0.7402\n",
      "Epoch 00017: val_loss improved from 0.84314 to 0.81086, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/017-0.8109.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.8505 - acc: 0.7402 - val_loss: 0.8109 - val_acc: 0.7603\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8404 - acc: 0.7434\n",
      "Epoch 00018: val_loss did not improve from 0.81086\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.8403 - acc: 0.7434 - val_loss: 0.8405 - val_acc: 0.7505\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8159 - acc: 0.7515\n",
      "Epoch 00019: val_loss improved from 0.81086 to 0.78291, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/019-0.7829.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.8161 - acc: 0.7515 - val_loss: 0.7829 - val_acc: 0.7759\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8018 - acc: 0.7557\n",
      "Epoch 00020: val_loss did not improve from 0.78291\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.8018 - acc: 0.7558 - val_loss: 0.7939 - val_acc: 0.7589\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7785 - acc: 0.7632\n",
      "Epoch 00021: val_loss improved from 0.78291 to 0.76712, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/021-0.7671.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.7784 - acc: 0.7632 - val_loss: 0.7671 - val_acc: 0.7796\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7648 - acc: 0.7660\n",
      "Epoch 00022: val_loss improved from 0.76712 to 0.74458, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/022-0.7446.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.7648 - acc: 0.7660 - val_loss: 0.7446 - val_acc: 0.7880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7429 - acc: 0.7734\n",
      "Epoch 00023: val_loss improved from 0.74458 to 0.73419, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/023-0.7342.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.7429 - acc: 0.7733 - val_loss: 0.7342 - val_acc: 0.7939\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7332 - acc: 0.7754\n",
      "Epoch 00024: val_loss did not improve from 0.73419\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.7331 - acc: 0.7755 - val_loss: 0.7414 - val_acc: 0.7801\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7207 - acc: 0.7790\n",
      "Epoch 00025: val_loss improved from 0.73419 to 0.71833, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/025-0.7183.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.7207 - acc: 0.7790 - val_loss: 0.7183 - val_acc: 0.7934\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7141 - acc: 0.7809\n",
      "Epoch 00026: val_loss improved from 0.71833 to 0.70300, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/026-0.7030.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.7140 - acc: 0.7809 - val_loss: 0.7030 - val_acc: 0.7997\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6945 - acc: 0.7883\n",
      "Epoch 00027: val_loss did not improve from 0.70300\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.6946 - acc: 0.7883 - val_loss: 0.7239 - val_acc: 0.7959\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6810 - acc: 0.7915\n",
      "Epoch 00028: val_loss improved from 0.70300 to 0.70044, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/028-0.7004.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.6810 - acc: 0.7915 - val_loss: 0.7004 - val_acc: 0.7997\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6691 - acc: 0.7954\n",
      "Epoch 00029: val_loss did not improve from 0.70044\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.6691 - acc: 0.7954 - val_loss: 0.7058 - val_acc: 0.8029\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6617 - acc: 0.7993\n",
      "Epoch 00030: val_loss improved from 0.70044 to 0.68755, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/030-0.6876.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.6618 - acc: 0.7993 - val_loss: 0.6876 - val_acc: 0.8053\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6526 - acc: 0.8011\n",
      "Epoch 00031: val_loss did not improve from 0.68755\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.6525 - acc: 0.8011 - val_loss: 0.6910 - val_acc: 0.8053\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6460 - acc: 0.8014\n",
      "Epoch 00032: val_loss did not improve from 0.68755\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.6460 - acc: 0.8014 - val_loss: 0.7007 - val_acc: 0.7899\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6287 - acc: 0.8079\n",
      "Epoch 00033: val_loss improved from 0.68755 to 0.67730, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/033-0.6773.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.6286 - acc: 0.8079 - val_loss: 0.6773 - val_acc: 0.8027\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6269 - acc: 0.8057\n",
      "Epoch 00034: val_loss did not improve from 0.67730\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.6268 - acc: 0.8057 - val_loss: 0.6965 - val_acc: 0.8020\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6170 - acc: 0.8103\n",
      "Epoch 00035: val_loss did not improve from 0.67730\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.6170 - acc: 0.8103 - val_loss: 0.6927 - val_acc: 0.7999\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.8143\n",
      "Epoch 00036: val_loss improved from 0.67730 to 0.66309, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/036-0.6631.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.6056 - acc: 0.8143 - val_loss: 0.6631 - val_acc: 0.8071\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.8163\n",
      "Epoch 00037: val_loss improved from 0.66309 to 0.65899, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/037-0.6590.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.6022 - acc: 0.8162 - val_loss: 0.6590 - val_acc: 0.8109\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5939 - acc: 0.8175\n",
      "Epoch 00038: val_loss did not improve from 0.65899\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.5942 - acc: 0.8175 - val_loss: 0.6681 - val_acc: 0.8043\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5810 - acc: 0.8227\n",
      "Epoch 00039: val_loss did not improve from 0.65899\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.5810 - acc: 0.8228 - val_loss: 0.6846 - val_acc: 0.8018\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5769 - acc: 0.8222\n",
      "Epoch 00040: val_loss improved from 0.65899 to 0.63968, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/040-0.6397.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.5769 - acc: 0.8222 - val_loss: 0.6397 - val_acc: 0.8137\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.8242\n",
      "Epoch 00041: val_loss did not improve from 0.63968\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.5725 - acc: 0.8242 - val_loss: 0.7083 - val_acc: 0.7990\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5659 - acc: 0.8245\n",
      "Epoch 00042: val_loss improved from 0.63968 to 0.63139, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/042-0.6314.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.5658 - acc: 0.8245 - val_loss: 0.6314 - val_acc: 0.8153\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.8272\n",
      "Epoch 00043: val_loss did not improve from 0.63139\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.5594 - acc: 0.8272 - val_loss: 0.6367 - val_acc: 0.8143\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.8263\n",
      "Epoch 00044: val_loss did not improve from 0.63139\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.5590 - acc: 0.8263 - val_loss: 0.6582 - val_acc: 0.8113\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.8323\n",
      "Epoch 00045: val_loss did not improve from 0.63139\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.5465 - acc: 0.8323 - val_loss: 0.6410 - val_acc: 0.8176\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5362 - acc: 0.8341\n",
      "Epoch 00046: val_loss did not improve from 0.63139\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.5362 - acc: 0.8342 - val_loss: 0.6497 - val_acc: 0.8164\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.8349\n",
      "Epoch 00047: val_loss improved from 0.63139 to 0.62300, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/047-0.6230.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.5352 - acc: 0.8348 - val_loss: 0.6230 - val_acc: 0.8209\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.8388\n",
      "Epoch 00048: val_loss did not improve from 0.62300\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.5277 - acc: 0.8387 - val_loss: 0.6416 - val_acc: 0.8150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8351\n",
      "Epoch 00049: val_loss did not improve from 0.62300\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.5316 - acc: 0.8351 - val_loss: 0.6391 - val_acc: 0.8167\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5226 - acc: 0.8376\n",
      "Epoch 00050: val_loss did not improve from 0.62300\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.5226 - acc: 0.8375 - val_loss: 0.6267 - val_acc: 0.8171\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8414\n",
      "Epoch 00051: val_loss improved from 0.62300 to 0.60462, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/051-0.6046.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.5130 - acc: 0.8414 - val_loss: 0.6046 - val_acc: 0.8281\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8392\n",
      "Epoch 00052: val_loss did not improve from 0.60462\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.5079 - acc: 0.8392 - val_loss: 0.6151 - val_acc: 0.8279\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5011 - acc: 0.8439\n",
      "Epoch 00053: val_loss did not improve from 0.60462\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.5011 - acc: 0.8439 - val_loss: 0.6178 - val_acc: 0.8272\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4977 - acc: 0.8448\n",
      "Epoch 00054: val_loss did not improve from 0.60462\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.4977 - acc: 0.8448 - val_loss: 0.6221 - val_acc: 0.8267\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4913 - acc: 0.8466\n",
      "Epoch 00055: val_loss did not improve from 0.60462\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4913 - acc: 0.8466 - val_loss: 0.6135 - val_acc: 0.8302\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8466\n",
      "Epoch 00056: val_loss improved from 0.60462 to 0.59483, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/056-0.5948.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.4859 - acc: 0.8466 - val_loss: 0.5948 - val_acc: 0.8346\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.8496\n",
      "Epoch 00057: val_loss did not improve from 0.59483\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4816 - acc: 0.8496 - val_loss: 0.6064 - val_acc: 0.8337\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4797 - acc: 0.8482\n",
      "Epoch 00058: val_loss did not improve from 0.59483\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.4798 - acc: 0.8481 - val_loss: 0.6047 - val_acc: 0.8311\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4750 - acc: 0.8513\n",
      "Epoch 00059: val_loss did not improve from 0.59483\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.4750 - acc: 0.8513 - val_loss: 0.6107 - val_acc: 0.8307\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4666 - acc: 0.8532\n",
      "Epoch 00060: val_loss did not improve from 0.59483\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4668 - acc: 0.8532 - val_loss: 0.6109 - val_acc: 0.8318\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8521\n",
      "Epoch 00061: val_loss did not improve from 0.59483\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.4647 - acc: 0.8521 - val_loss: 0.6374 - val_acc: 0.8216\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4656 - acc: 0.8522\n",
      "Epoch 00062: val_loss did not improve from 0.59483\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4656 - acc: 0.8522 - val_loss: 0.6065 - val_acc: 0.8260\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4597 - acc: 0.8558\n",
      "Epoch 00063: val_loss did not improve from 0.59483\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4598 - acc: 0.8558 - val_loss: 0.6086 - val_acc: 0.8316\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.8560\n",
      "Epoch 00064: val_loss improved from 0.59483 to 0.59364, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/064-0.5936.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.4590 - acc: 0.8560 - val_loss: 0.5936 - val_acc: 0.8300\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8569\n",
      "Epoch 00065: val_loss did not improve from 0.59364\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4549 - acc: 0.8569 - val_loss: 0.6015 - val_acc: 0.8362\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4545 - acc: 0.8573\n",
      "Epoch 00066: val_loss did not improve from 0.59364\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4544 - acc: 0.8573 - val_loss: 0.6040 - val_acc: 0.8314\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4465 - acc: 0.8600\n",
      "Epoch 00067: val_loss improved from 0.59364 to 0.58870, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/067-0.5887.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4465 - acc: 0.8600 - val_loss: 0.5887 - val_acc: 0.8404\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4369 - acc: 0.8608\n",
      "Epoch 00068: val_loss did not improve from 0.58870\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4368 - acc: 0.8608 - val_loss: 0.5982 - val_acc: 0.8353\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.8608\n",
      "Epoch 00069: val_loss did not improve from 0.58870\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.4394 - acc: 0.8608 - val_loss: 0.5919 - val_acc: 0.8381\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.8614\n",
      "Epoch 00070: val_loss did not improve from 0.58870\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4417 - acc: 0.8614 - val_loss: 0.5961 - val_acc: 0.8328\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.8604\n",
      "Epoch 00071: val_loss did not improve from 0.58870\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4369 - acc: 0.8605 - val_loss: 0.5937 - val_acc: 0.8383\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8635\n",
      "Epoch 00072: val_loss did not improve from 0.58870\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4285 - acc: 0.8635 - val_loss: 0.5953 - val_acc: 0.8346\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8636\n",
      "Epoch 00073: val_loss did not improve from 0.58870\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.4274 - acc: 0.8636 - val_loss: 0.5898 - val_acc: 0.8344\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4290 - acc: 0.8652\n",
      "Epoch 00074: val_loss improved from 0.58870 to 0.57884, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/074-0.5788.hdf5\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4290 - acc: 0.8652 - val_loss: 0.5788 - val_acc: 0.8421\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4238 - acc: 0.8646\n",
      "Epoch 00075: val_loss did not improve from 0.57884\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.4238 - acc: 0.8646 - val_loss: 0.5982 - val_acc: 0.8365\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.8664\n",
      "Epoch 00076: val_loss did not improve from 0.57884\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4171 - acc: 0.8663 - val_loss: 0.6032 - val_acc: 0.8369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8686\n",
      "Epoch 00077: val_loss did not improve from 0.57884\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4172 - acc: 0.8686 - val_loss: 0.5841 - val_acc: 0.8402\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.8661\n",
      "Epoch 00078: val_loss did not improve from 0.57884\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4233 - acc: 0.8661 - val_loss: 0.6067 - val_acc: 0.8330\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8681\n",
      "Epoch 00079: val_loss did not improve from 0.57884\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4171 - acc: 0.8681 - val_loss: 0.5797 - val_acc: 0.8425\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4141 - acc: 0.8674\n",
      "Epoch 00080: val_loss did not improve from 0.57884\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.4142 - acc: 0.8674 - val_loss: 0.5853 - val_acc: 0.8397\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8685\n",
      "Epoch 00081: val_loss did not improve from 0.57884\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4142 - acc: 0.8685 - val_loss: 0.6029 - val_acc: 0.8304\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8721\n",
      "Epoch 00082: val_loss did not improve from 0.57884\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.4008 - acc: 0.8721 - val_loss: 0.6065 - val_acc: 0.8353\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.8687\n",
      "Epoch 00083: val_loss improved from 0.57884 to 0.57393, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/083-0.5739.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.4108 - acc: 0.8687 - val_loss: 0.5739 - val_acc: 0.8409\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.8685\n",
      "Epoch 00084: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4039 - acc: 0.8685 - val_loss: 0.6062 - val_acc: 0.8379\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8707\n",
      "Epoch 00085: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.4027 - acc: 0.8707 - val_loss: 0.5951 - val_acc: 0.8358\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8751\n",
      "Epoch 00086: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.3957 - acc: 0.8751 - val_loss: 0.5766 - val_acc: 0.8395\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8731\n",
      "Epoch 00087: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3965 - acc: 0.8731 - val_loss: 0.5776 - val_acc: 0.8451\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8755\n",
      "Epoch 00088: val_loss did not improve from 0.57393\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3931 - acc: 0.8755 - val_loss: 0.5941 - val_acc: 0.8439\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8761\n",
      "Epoch 00089: val_loss improved from 0.57393 to 0.57335, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/089-0.5734.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3890 - acc: 0.8762 - val_loss: 0.5734 - val_acc: 0.8397\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8768\n",
      "Epoch 00090: val_loss improved from 0.57335 to 0.57317, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/090-0.5732.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3875 - acc: 0.8768 - val_loss: 0.5732 - val_acc: 0.8472\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8760\n",
      "Epoch 00091: val_loss did not improve from 0.57317\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.3854 - acc: 0.8760 - val_loss: 0.5784 - val_acc: 0.8423\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8776\n",
      "Epoch 00092: val_loss did not improve from 0.57317\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.3858 - acc: 0.8776 - val_loss: 0.5990 - val_acc: 0.8414\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8778\n",
      "Epoch 00093: val_loss did not improve from 0.57317\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3808 - acc: 0.8778 - val_loss: 0.5738 - val_acc: 0.8435\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8803\n",
      "Epoch 00094: val_loss did not improve from 0.57317\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3774 - acc: 0.8802 - val_loss: 0.6208 - val_acc: 0.8286\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.8761\n",
      "Epoch 00095: val_loss did not improve from 0.57317\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3826 - acc: 0.8761 - val_loss: 0.5899 - val_acc: 0.8402\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8790\n",
      "Epoch 00096: val_loss did not improve from 0.57317\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.3774 - acc: 0.8790 - val_loss: 0.6022 - val_acc: 0.8414\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8803\n",
      "Epoch 00097: val_loss improved from 0.57317 to 0.56547, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/097-0.5655.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.3783 - acc: 0.8803 - val_loss: 0.5655 - val_acc: 0.8474\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3764 - acc: 0.8791\n",
      "Epoch 00098: val_loss did not improve from 0.56547\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3763 - acc: 0.8791 - val_loss: 0.5716 - val_acc: 0.8477\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8815\n",
      "Epoch 00099: val_loss improved from 0.56547 to 0.55652, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/099-0.5565.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3696 - acc: 0.8815 - val_loss: 0.5565 - val_acc: 0.8512\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8821\n",
      "Epoch 00100: val_loss did not improve from 0.55652\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3667 - acc: 0.8822 - val_loss: 0.5647 - val_acc: 0.8453\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8822\n",
      "Epoch 00101: val_loss did not improve from 0.55652\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3649 - acc: 0.8822 - val_loss: 0.5804 - val_acc: 0.8467\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3669 - acc: 0.8820\n",
      "Epoch 00102: val_loss did not improve from 0.55652\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3668 - acc: 0.8821 - val_loss: 0.5569 - val_acc: 0.8519\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8821\n",
      "Epoch 00103: val_loss improved from 0.55652 to 0.54914, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/103-0.5491.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.3627 - acc: 0.8822 - val_loss: 0.5491 - val_acc: 0.8509\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8842\n",
      "Epoch 00104: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3617 - acc: 0.8842 - val_loss: 0.5707 - val_acc: 0.8449\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8837\n",
      "Epoch 00105: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3628 - acc: 0.8837 - val_loss: 0.5764 - val_acc: 0.8453\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8850\n",
      "Epoch 00106: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3583 - acc: 0.8850 - val_loss: 0.5696 - val_acc: 0.8458\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.8843\n",
      "Epoch 00107: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3552 - acc: 0.8843 - val_loss: 0.5751 - val_acc: 0.8474\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8840\n",
      "Epoch 00108: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3571 - acc: 0.8840 - val_loss: 0.5684 - val_acc: 0.8456\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8883\n",
      "Epoch 00109: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3517 - acc: 0.8882 - val_loss: 0.5669 - val_acc: 0.8474\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8854\n",
      "Epoch 00110: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3593 - acc: 0.8854 - val_loss: 0.5560 - val_acc: 0.8512\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8848\n",
      "Epoch 00111: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3540 - acc: 0.8848 - val_loss: 0.5668 - val_acc: 0.8556\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3525 - acc: 0.8845\n",
      "Epoch 00112: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3524 - acc: 0.8846 - val_loss: 0.5862 - val_acc: 0.8509\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8881\n",
      "Epoch 00113: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.3446 - acc: 0.8881 - val_loss: 0.5664 - val_acc: 0.8481\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8863\n",
      "Epoch 00114: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3435 - acc: 0.8863 - val_loss: 0.5598 - val_acc: 0.8519\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3450 - acc: 0.8874\n",
      "Epoch 00115: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3450 - acc: 0.8874 - val_loss: 0.5871 - val_acc: 0.8432\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8881\n",
      "Epoch 00116: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3436 - acc: 0.8881 - val_loss: 0.5687 - val_acc: 0.8484\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8866\n",
      "Epoch 00117: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3471 - acc: 0.8866 - val_loss: 0.5578 - val_acc: 0.8484\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.8890\n",
      "Epoch 00118: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3449 - acc: 0.8890 - val_loss: 0.5511 - val_acc: 0.8519\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3377 - acc: 0.8912\n",
      "Epoch 00119: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3377 - acc: 0.8912 - val_loss: 0.5655 - val_acc: 0.8519\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8896\n",
      "Epoch 00120: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3387 - acc: 0.8896 - val_loss: 0.5590 - val_acc: 0.8507\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8917\n",
      "Epoch 00121: val_loss did not improve from 0.54914\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3361 - acc: 0.8918 - val_loss: 0.5614 - val_acc: 0.8553\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8913\n",
      "Epoch 00122: val_loss improved from 0.54914 to 0.54429, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/122-0.5443.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3369 - acc: 0.8913 - val_loss: 0.5443 - val_acc: 0.8584\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3332 - acc: 0.8924\n",
      "Epoch 00123: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3332 - acc: 0.8925 - val_loss: 0.5468 - val_acc: 0.8556\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8947\n",
      "Epoch 00124: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3304 - acc: 0.8947 - val_loss: 0.5619 - val_acc: 0.8484\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.8933\n",
      "Epoch 00125: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.3285 - acc: 0.8932 - val_loss: 0.5498 - val_acc: 0.8577\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8938\n",
      "Epoch 00126: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3245 - acc: 0.8938 - val_loss: 0.5533 - val_acc: 0.8549\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.8941\n",
      "Epoch 00127: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3271 - acc: 0.8941 - val_loss: 0.5475 - val_acc: 0.8588\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8940\n",
      "Epoch 00128: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3277 - acc: 0.8940 - val_loss: 0.5567 - val_acc: 0.8558\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.8927\n",
      "Epoch 00129: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3291 - acc: 0.8927 - val_loss: 0.5541 - val_acc: 0.8565\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8957\n",
      "Epoch 00130: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.3253 - acc: 0.8957 - val_loss: 0.5823 - val_acc: 0.8474\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.8943\n",
      "Epoch 00131: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.3272 - acc: 0.8943 - val_loss: 0.5764 - val_acc: 0.8484\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8952\n",
      "Epoch 00132: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3208 - acc: 0.8952 - val_loss: 0.5530 - val_acc: 0.8577\n",
      "Epoch 133/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8959\n",
      "Epoch 00133: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3233 - acc: 0.8959 - val_loss: 0.5493 - val_acc: 0.8584\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8961\n",
      "Epoch 00134: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3177 - acc: 0.8961 - val_loss: 0.5673 - val_acc: 0.8542\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.8962\n",
      "Epoch 00135: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3203 - acc: 0.8962 - val_loss: 0.5609 - val_acc: 0.8514\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8945\n",
      "Epoch 00136: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3221 - acc: 0.8945 - val_loss: 0.5524 - val_acc: 0.8584\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8961\n",
      "Epoch 00137: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3202 - acc: 0.8961 - val_loss: 0.5487 - val_acc: 0.8549\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3174 - acc: 0.8967\n",
      "Epoch 00138: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3174 - acc: 0.8967 - val_loss: 0.5779 - val_acc: 0.8502\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.8964\n",
      "Epoch 00139: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3127 - acc: 0.8965 - val_loss: 0.5618 - val_acc: 0.8577\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9008\n",
      "Epoch 00140: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3070 - acc: 0.9008 - val_loss: 0.5548 - val_acc: 0.8584\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.8954\n",
      "Epoch 00141: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3200 - acc: 0.8954 - val_loss: 0.6950 - val_acc: 0.8225\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8971\n",
      "Epoch 00142: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3164 - acc: 0.8972 - val_loss: 0.5463 - val_acc: 0.8612\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.8986\n",
      "Epoch 00143: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3129 - acc: 0.8987 - val_loss: 0.5455 - val_acc: 0.8577\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.9004\n",
      "Epoch 00144: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.3115 - acc: 0.9004 - val_loss: 0.5603 - val_acc: 0.8584\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.8979\n",
      "Epoch 00145: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3134 - acc: 0.8979 - val_loss: 0.5464 - val_acc: 0.8602\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3124 - acc: 0.8972\n",
      "Epoch 00146: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3124 - acc: 0.8972 - val_loss: 0.5458 - val_acc: 0.8579\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9002\n",
      "Epoch 00147: val_loss did not improve from 0.54429\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.3071 - acc: 0.9001 - val_loss: 0.5568 - val_acc: 0.8581\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.8988\n",
      "Epoch 00148: val_loss improved from 0.54429 to 0.54234, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/148-0.5423.hdf5\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.3103 - acc: 0.8988 - val_loss: 0.5423 - val_acc: 0.8626\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.8987\n",
      "Epoch 00149: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3088 - acc: 0.8987 - val_loss: 0.5608 - val_acc: 0.8553\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.9015\n",
      "Epoch 00150: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.3026 - acc: 0.9015 - val_loss: 0.5546 - val_acc: 0.8602\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.8998\n",
      "Epoch 00151: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.3086 - acc: 0.8997 - val_loss: 0.5546 - val_acc: 0.8574\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9018\n",
      "Epoch 00152: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2977 - acc: 0.9018 - val_loss: 0.5722 - val_acc: 0.8570\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9024\n",
      "Epoch 00153: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2999 - acc: 0.9025 - val_loss: 0.5541 - val_acc: 0.8614\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.8991\n",
      "Epoch 00154: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.3093 - acc: 0.8991 - val_loss: 0.5467 - val_acc: 0.8591\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2991 - acc: 0.9027\n",
      "Epoch 00155: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2991 - acc: 0.9027 - val_loss: 0.5616 - val_acc: 0.8605\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9034\n",
      "Epoch 00156: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2944 - acc: 0.9034 - val_loss: 0.5854 - val_acc: 0.8507\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9025\n",
      "Epoch 00157: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2966 - acc: 0.9025 - val_loss: 0.5679 - val_acc: 0.8584\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9017\n",
      "Epoch 00158: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2953 - acc: 0.9017 - val_loss: 0.5537 - val_acc: 0.8584\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.8994\n",
      "Epoch 00159: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.3035 - acc: 0.8994 - val_loss: 0.5515 - val_acc: 0.8626\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2984 - acc: 0.9020\n",
      "Epoch 00160: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2984 - acc: 0.9019 - val_loss: 0.5521 - val_acc: 0.8621\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9046\n",
      "Epoch 00161: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2936 - acc: 0.9046 - val_loss: 0.5483 - val_acc: 0.8621\n",
      "Epoch 162/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9027\n",
      "Epoch 00162: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2968 - acc: 0.9027 - val_loss: 0.5600 - val_acc: 0.8609\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9047\n",
      "Epoch 00163: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2938 - acc: 0.9047 - val_loss: 0.5516 - val_acc: 0.8598\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.9033\n",
      "Epoch 00164: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2921 - acc: 0.9033 - val_loss: 0.5556 - val_acc: 0.8579\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9049\n",
      "Epoch 00165: val_loss did not improve from 0.54234\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2888 - acc: 0.9049 - val_loss: 0.5538 - val_acc: 0.8621\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9038\n",
      "Epoch 00166: val_loss improved from 0.54234 to 0.53573, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/166-0.5357.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2931 - acc: 0.9038 - val_loss: 0.5357 - val_acc: 0.8623\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9060\n",
      "Epoch 00167: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.2873 - acc: 0.9060 - val_loss: 0.5460 - val_acc: 0.8649\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9056\n",
      "Epoch 00168: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2895 - acc: 0.9056 - val_loss: 0.5485 - val_acc: 0.8593\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9047\n",
      "Epoch 00169: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2907 - acc: 0.9047 - val_loss: 0.5442 - val_acc: 0.8579\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9082\n",
      "Epoch 00170: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2826 - acc: 0.9082 - val_loss: 0.6144 - val_acc: 0.8484\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9056\n",
      "Epoch 00171: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2860 - acc: 0.9056 - val_loss: 0.5457 - val_acc: 0.8628\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9059\n",
      "Epoch 00172: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2871 - acc: 0.9059 - val_loss: 0.5518 - val_acc: 0.8591\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9062\n",
      "Epoch 00173: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2869 - acc: 0.9062 - val_loss: 0.5416 - val_acc: 0.8665\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9085\n",
      "Epoch 00174: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2812 - acc: 0.9085 - val_loss: 0.5571 - val_acc: 0.8572\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9073\n",
      "Epoch 00175: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2826 - acc: 0.9073 - val_loss: 0.5547 - val_acc: 0.8584\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9113\n",
      "Epoch 00176: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2754 - acc: 0.9113 - val_loss: 0.5496 - val_acc: 0.8621\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9066\n",
      "Epoch 00177: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2867 - acc: 0.9066 - val_loss: 0.5598 - val_acc: 0.8654\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9068\n",
      "Epoch 00178: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2851 - acc: 0.9068 - val_loss: 0.5586 - val_acc: 0.8647\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9070\n",
      "Epoch 00179: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2779 - acc: 0.9070 - val_loss: 0.5516 - val_acc: 0.8572\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9091\n",
      "Epoch 00180: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2798 - acc: 0.9091 - val_loss: 0.5636 - val_acc: 0.8630\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9107\n",
      "Epoch 00181: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2791 - acc: 0.9107 - val_loss: 0.5445 - val_acc: 0.8605\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9094\n",
      "Epoch 00182: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2766 - acc: 0.9094 - val_loss: 0.5533 - val_acc: 0.8633\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9073\n",
      "Epoch 00183: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2779 - acc: 0.9073 - val_loss: 0.5383 - val_acc: 0.8658\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9101\n",
      "Epoch 00184: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2743 - acc: 0.9101 - val_loss: 0.5518 - val_acc: 0.8670\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9115\n",
      "Epoch 00185: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2733 - acc: 0.9115 - val_loss: 0.5593 - val_acc: 0.8626\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9107\n",
      "Epoch 00186: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2734 - acc: 0.9107 - val_loss: 0.5440 - val_acc: 0.8651\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9122\n",
      "Epoch 00187: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2705 - acc: 0.9122 - val_loss: 0.5583 - val_acc: 0.8661\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9096\n",
      "Epoch 00188: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2703 - acc: 0.9096 - val_loss: 0.5561 - val_acc: 0.8635\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9098\n",
      "Epoch 00189: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2742 - acc: 0.9098 - val_loss: 0.5626 - val_acc: 0.8637\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9124\n",
      "Epoch 00190: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2687 - acc: 0.9124 - val_loss: 0.5516 - val_acc: 0.8693\n",
      "Epoch 191/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9101\n",
      "Epoch 00191: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2735 - acc: 0.9101 - val_loss: 0.5536 - val_acc: 0.8614\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9124\n",
      "Epoch 00192: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2656 - acc: 0.9124 - val_loss: 0.5458 - val_acc: 0.8691\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9114\n",
      "Epoch 00193: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2672 - acc: 0.9114 - val_loss: 0.5381 - val_acc: 0.8665\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.9133\n",
      "Epoch 00194: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.2670 - acc: 0.9133 - val_loss: 0.5726 - val_acc: 0.8621\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9146\n",
      "Epoch 00195: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2652 - acc: 0.9147 - val_loss: 0.5378 - val_acc: 0.8661\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9128\n",
      "Epoch 00196: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2680 - acc: 0.9128 - val_loss: 0.5426 - val_acc: 0.8661\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9138\n",
      "Epoch 00197: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2643 - acc: 0.9138 - val_loss: 0.5471 - val_acc: 0.8693\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9125\n",
      "Epoch 00198: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2674 - acc: 0.9125 - val_loss: 0.5445 - val_acc: 0.8677\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.9157\n",
      "Epoch 00199: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2601 - acc: 0.9156 - val_loss: 0.5406 - val_acc: 0.8630\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9161\n",
      "Epoch 00200: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2604 - acc: 0.9161 - val_loss: 0.5510 - val_acc: 0.8637\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9142\n",
      "Epoch 00201: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2610 - acc: 0.9142 - val_loss: 0.5616 - val_acc: 0.8651\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2627 - acc: 0.9137\n",
      "Epoch 00202: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2626 - acc: 0.9137 - val_loss: 0.5585 - val_acc: 0.8668\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9128\n",
      "Epoch 00203: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2613 - acc: 0.9128 - val_loss: 0.5458 - val_acc: 0.8626\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9151\n",
      "Epoch 00204: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2612 - acc: 0.9151 - val_loss: 0.5553 - val_acc: 0.8649\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9125\n",
      "Epoch 00205: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2619 - acc: 0.9125 - val_loss: 0.5401 - val_acc: 0.8682\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9151\n",
      "Epoch 00206: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2581 - acc: 0.9151 - val_loss: 0.5530 - val_acc: 0.8628\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9133\n",
      "Epoch 00207: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2589 - acc: 0.9133 - val_loss: 0.5559 - val_acc: 0.8628\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9153\n",
      "Epoch 00208: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2606 - acc: 0.9153 - val_loss: 0.5558 - val_acc: 0.8705\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9168\n",
      "Epoch 00209: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2541 - acc: 0.9168 - val_loss: 0.5414 - val_acc: 0.8665\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9146\n",
      "Epoch 00210: val_loss did not improve from 0.53573\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2632 - acc: 0.9146 - val_loss: 0.5437 - val_acc: 0.8637\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9154\n",
      "Epoch 00211: val_loss improved from 0.53573 to 0.53477, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/211-0.5348.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2577 - acc: 0.9154 - val_loss: 0.5348 - val_acc: 0.8656\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9136\n",
      "Epoch 00212: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2589 - acc: 0.9137 - val_loss: 0.5482 - val_acc: 0.8684\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9156\n",
      "Epoch 00213: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2572 - acc: 0.9156 - val_loss: 0.5533 - val_acc: 0.8633\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9164\n",
      "Epoch 00214: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2552 - acc: 0.9164 - val_loss: 0.5528 - val_acc: 0.8663\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9177\n",
      "Epoch 00215: val_loss improved from 0.53477 to 0.52949, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/215-0.5295.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2559 - acc: 0.9177 - val_loss: 0.5295 - val_acc: 0.8649\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9163\n",
      "Epoch 00216: val_loss improved from 0.52949 to 0.52918, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/216-0.5292.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2539 - acc: 0.9163 - val_loss: 0.5292 - val_acc: 0.8700\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9185\n",
      "Epoch 00217: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2485 - acc: 0.9185 - val_loss: 0.5299 - val_acc: 0.8710\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9194\n",
      "Epoch 00218: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2484 - acc: 0.9193 - val_loss: 0.5358 - val_acc: 0.8693\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9176\n",
      "Epoch 00219: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2566 - acc: 0.9176 - val_loss: 0.5478 - val_acc: 0.8686\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9185\n",
      "Epoch 00220: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2480 - acc: 0.9185 - val_loss: 0.5428 - val_acc: 0.8682\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9168\n",
      "Epoch 00221: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2545 - acc: 0.9169 - val_loss: 0.5329 - val_acc: 0.8700\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9177\n",
      "Epoch 00222: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2518 - acc: 0.9178 - val_loss: 0.5493 - val_acc: 0.8691\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9202\n",
      "Epoch 00223: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2447 - acc: 0.9202 - val_loss: 0.5479 - val_acc: 0.8689\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9198\n",
      "Epoch 00224: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2478 - acc: 0.9198 - val_loss: 0.5375 - val_acc: 0.8703\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9174\n",
      "Epoch 00225: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2512 - acc: 0.9174 - val_loss: 0.5570 - val_acc: 0.8661\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2437 - acc: 0.9205\n",
      "Epoch 00226: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2437 - acc: 0.9205 - val_loss: 0.5582 - val_acc: 0.8693\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9185\n",
      "Epoch 00227: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2512 - acc: 0.9185 - val_loss: 0.5362 - val_acc: 0.8717\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9173\n",
      "Epoch 00228: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2521 - acc: 0.9173 - val_loss: 0.5494 - val_acc: 0.8717\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9180\n",
      "Epoch 00229: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2498 - acc: 0.9180 - val_loss: 0.5446 - val_acc: 0.8684\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9181\n",
      "Epoch 00230: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2443 - acc: 0.9181 - val_loss: 0.5541 - val_acc: 0.8682\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9208\n",
      "Epoch 00231: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2444 - acc: 0.9208 - val_loss: 0.5540 - val_acc: 0.8677\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9212\n",
      "Epoch 00232: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.2405 - acc: 0.9212 - val_loss: 0.5347 - val_acc: 0.8717\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2437 - acc: 0.9190\n",
      "Epoch 00233: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2437 - acc: 0.9190 - val_loss: 0.5494 - val_acc: 0.8686\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.9188\n",
      "Epoch 00234: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2471 - acc: 0.9188 - val_loss: 0.5442 - val_acc: 0.8710\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9193\n",
      "Epoch 00235: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2444 - acc: 0.9193 - val_loss: 0.5384 - val_acc: 0.8710\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9190\n",
      "Epoch 00236: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2440 - acc: 0.9190 - val_loss: 0.5493 - val_acc: 0.8670\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9222\n",
      "Epoch 00237: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2400 - acc: 0.9222 - val_loss: 0.5443 - val_acc: 0.8698\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9218\n",
      "Epoch 00238: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2401 - acc: 0.9218 - val_loss: 0.5445 - val_acc: 0.8693\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9211\n",
      "Epoch 00239: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2394 - acc: 0.9210 - val_loss: 0.5501 - val_acc: 0.8735\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9192\n",
      "Epoch 00240: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2439 - acc: 0.9192 - val_loss: 0.5543 - val_acc: 0.8672\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9177\n",
      "Epoch 00241: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2458 - acc: 0.9177 - val_loss: 0.5680 - val_acc: 0.8642\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9177\n",
      "Epoch 00242: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2458 - acc: 0.9177 - val_loss: 0.5426 - val_acc: 0.8696\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9228\n",
      "Epoch 00243: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2331 - acc: 0.9228 - val_loss: 0.5603 - val_acc: 0.8730\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9201\n",
      "Epoch 00244: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2402 - acc: 0.9201 - val_loss: 0.5560 - val_acc: 0.8670\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9233\n",
      "Epoch 00245: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2344 - acc: 0.9233 - val_loss: 0.5458 - val_acc: 0.8724\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9246\n",
      "Epoch 00246: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2338 - acc: 0.9246 - val_loss: 0.5467 - val_acc: 0.8712\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9236\n",
      "Epoch 00247: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.2324 - acc: 0.9236 - val_loss: 0.5620 - val_acc: 0.8742\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9213\n",
      "Epoch 00248: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2411 - acc: 0.9213 - val_loss: 0.5611 - val_acc: 0.8630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9221\n",
      "Epoch 00249: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2350 - acc: 0.9221 - val_loss: 0.5414 - val_acc: 0.8730\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9234\n",
      "Epoch 00250: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2340 - acc: 0.9234 - val_loss: 0.5566 - val_acc: 0.8649\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9211\n",
      "Epoch 00251: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2391 - acc: 0.9211 - val_loss: 0.5705 - val_acc: 0.8614\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9204\n",
      "Epoch 00252: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2389 - acc: 0.9204 - val_loss: 0.5311 - val_acc: 0.8719\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9228\n",
      "Epoch 00253: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2346 - acc: 0.9228 - val_loss: 0.5400 - val_acc: 0.8742\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9258\n",
      "Epoch 00254: val_loss did not improve from 0.52918\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2272 - acc: 0.9258 - val_loss: 0.5552 - val_acc: 0.8693\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9232\n",
      "Epoch 00255: val_loss improved from 0.52918 to 0.52725, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/255-0.5273.hdf5\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2329 - acc: 0.9232 - val_loss: 0.5273 - val_acc: 0.8754\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9229\n",
      "Epoch 00256: val_loss did not improve from 0.52725\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2358 - acc: 0.9229 - val_loss: 0.5458 - val_acc: 0.8689\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9248\n",
      "Epoch 00257: val_loss did not improve from 0.52725\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2329 - acc: 0.9248 - val_loss: 0.5416 - val_acc: 0.8719\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9239\n",
      "Epoch 00258: val_loss did not improve from 0.52725\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2361 - acc: 0.9239 - val_loss: 0.5450 - val_acc: 0.8717\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9245\n",
      "Epoch 00259: val_loss did not improve from 0.52725\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2309 - acc: 0.9245 - val_loss: 0.5490 - val_acc: 0.8703\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9238\n",
      "Epoch 00260: val_loss did not improve from 0.52725\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2277 - acc: 0.9238 - val_loss: 0.5758 - val_acc: 0.8649\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9240\n",
      "Epoch 00261: val_loss did not improve from 0.52725\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2359 - acc: 0.9241 - val_loss: 0.5410 - val_acc: 0.8744\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9247\n",
      "Epoch 00262: val_loss improved from 0.52725 to 0.52662, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/262-0.5266.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2293 - acc: 0.9247 - val_loss: 0.5266 - val_acc: 0.8751\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9253\n",
      "Epoch 00263: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2309 - acc: 0.9253 - val_loss: 0.5558 - val_acc: 0.8677\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9245\n",
      "Epoch 00264: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2280 - acc: 0.9245 - val_loss: 0.5543 - val_acc: 0.8719\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9236\n",
      "Epoch 00265: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2317 - acc: 0.9236 - val_loss: 0.5319 - val_acc: 0.8754\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9253\n",
      "Epoch 00266: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2288 - acc: 0.9253 - val_loss: 0.5414 - val_acc: 0.8712\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9242\n",
      "Epoch 00267: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2296 - acc: 0.9242 - val_loss: 0.5419 - val_acc: 0.8705\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9242\n",
      "Epoch 00268: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2260 - acc: 0.9242 - val_loss: 0.5428 - val_acc: 0.8754\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9233\n",
      "Epoch 00269: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2348 - acc: 0.9233 - val_loss: 0.5288 - val_acc: 0.8698\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9262\n",
      "Epoch 00270: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.2258 - acc: 0.9262 - val_loss: 0.5397 - val_acc: 0.8758\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9238\n",
      "Epoch 00271: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2286 - acc: 0.9238 - val_loss: 0.5467 - val_acc: 0.8703\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9233\n",
      "Epoch 00272: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2320 - acc: 0.9233 - val_loss: 0.5477 - val_acc: 0.8724\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9252\n",
      "Epoch 00273: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2270 - acc: 0.9253 - val_loss: 0.5458 - val_acc: 0.8733\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9271\n",
      "Epoch 00274: val_loss did not improve from 0.52662\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2223 - acc: 0.9271 - val_loss: 0.5474 - val_acc: 0.8751\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9273\n",
      "Epoch 00275: val_loss improved from 0.52662 to 0.51715, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/275-0.5171.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2268 - acc: 0.9273 - val_loss: 0.5171 - val_acc: 0.8782\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9263\n",
      "Epoch 00276: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2235 - acc: 0.9263 - val_loss: 0.5513 - val_acc: 0.8735\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9279\n",
      "Epoch 00277: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2217 - acc: 0.9279 - val_loss: 0.5389 - val_acc: 0.8747\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9258\n",
      "Epoch 00278: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2272 - acc: 0.9259 - val_loss: 0.5293 - val_acc: 0.8742\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9250\n",
      "Epoch 00279: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2228 - acc: 0.9250 - val_loss: 0.5585 - val_acc: 0.8658\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9268\n",
      "Epoch 00280: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2220 - acc: 0.9267 - val_loss: 0.5342 - val_acc: 0.8772\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9268\n",
      "Epoch 00281: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2243 - acc: 0.9268 - val_loss: 0.5338 - val_acc: 0.8740\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9269\n",
      "Epoch 00282: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.2214 - acc: 0.9269 - val_loss: 0.5405 - val_acc: 0.8744\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9248\n",
      "Epoch 00283: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2280 - acc: 0.9248 - val_loss: 0.5312 - val_acc: 0.8735\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9279\n",
      "Epoch 00284: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2230 - acc: 0.9279 - val_loss: 0.5361 - val_acc: 0.8747\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9275\n",
      "Epoch 00285: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2219 - acc: 0.9275 - val_loss: 0.5298 - val_acc: 0.8733\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9268\n",
      "Epoch 00286: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2226 - acc: 0.9269 - val_loss: 0.5633 - val_acc: 0.8661\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9254\n",
      "Epoch 00287: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2273 - acc: 0.9254 - val_loss: 0.5244 - val_acc: 0.8737\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9263\n",
      "Epoch 00288: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2199 - acc: 0.9263 - val_loss: 0.5208 - val_acc: 0.8770\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9270\n",
      "Epoch 00289: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2234 - acc: 0.9270 - val_loss: 0.5299 - val_acc: 0.8742\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9276\n",
      "Epoch 00290: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2210 - acc: 0.9276 - val_loss: 0.5332 - val_acc: 0.8761\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9295\n",
      "Epoch 00291: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2136 - acc: 0.9295 - val_loss: 0.5244 - val_acc: 0.8756\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9295\n",
      "Epoch 00292: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2145 - acc: 0.9295 - val_loss: 0.5276 - val_acc: 0.8747\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9273\n",
      "Epoch 00293: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2203 - acc: 0.9273 - val_loss: 0.5271 - val_acc: 0.8749\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9286\n",
      "Epoch 00294: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2207 - acc: 0.9286 - val_loss: 0.5184 - val_acc: 0.8724\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9288\n",
      "Epoch 00295: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2198 - acc: 0.9288 - val_loss: 0.5359 - val_acc: 0.8761\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9278\n",
      "Epoch 00296: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2161 - acc: 0.9278 - val_loss: 0.5345 - val_acc: 0.8765\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9314\n",
      "Epoch 00297: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2128 - acc: 0.9314 - val_loss: 0.5329 - val_acc: 0.8786\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9295\n",
      "Epoch 00298: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2153 - acc: 0.9295 - val_loss: 0.5221 - val_acc: 0.8775\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9269\n",
      "Epoch 00299: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2222 - acc: 0.9269 - val_loss: 0.5312 - val_acc: 0.8737\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9281\n",
      "Epoch 00300: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2195 - acc: 0.9281 - val_loss: 0.5199 - val_acc: 0.8754\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9284\n",
      "Epoch 00301: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2154 - acc: 0.9284 - val_loss: 0.5340 - val_acc: 0.8779\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9277\n",
      "Epoch 00302: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2165 - acc: 0.9277 - val_loss: 0.5379 - val_acc: 0.8765\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9289\n",
      "Epoch 00303: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2159 - acc: 0.9289 - val_loss: 0.5247 - val_acc: 0.8807\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9289\n",
      "Epoch 00304: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2090 - acc: 0.9289 - val_loss: 0.5267 - val_acc: 0.8770\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9306\n",
      "Epoch 00305: val_loss did not improve from 0.51715\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2132 - acc: 0.9306 - val_loss: 0.5406 - val_acc: 0.8784\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9300\n",
      "Epoch 00306: val_loss improved from 0.51715 to 0.51549, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/306-0.5155.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2095 - acc: 0.9300 - val_loss: 0.5155 - val_acc: 0.8817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9297\n",
      "Epoch 00307: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2138 - acc: 0.9297 - val_loss: 0.5239 - val_acc: 0.8791\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9281\n",
      "Epoch 00308: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2166 - acc: 0.9281 - val_loss: 0.5399 - val_acc: 0.8803\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9293\n",
      "Epoch 00309: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2143 - acc: 0.9292 - val_loss: 0.5353 - val_acc: 0.8733\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9301\n",
      "Epoch 00310: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2148 - acc: 0.9300 - val_loss: 0.5241 - val_acc: 0.8733\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9326\n",
      "Epoch 00311: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2058 - acc: 0.9326 - val_loss: 0.5361 - val_acc: 0.8765\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9301\n",
      "Epoch 00312: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2100 - acc: 0.9301 - val_loss: 0.5318 - val_acc: 0.8768\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9328\n",
      "Epoch 00313: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2086 - acc: 0.9328 - val_loss: 0.5493 - val_acc: 0.8775\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9318\n",
      "Epoch 00314: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2048 - acc: 0.9318 - val_loss: 0.5250 - val_acc: 0.8754\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9303\n",
      "Epoch 00315: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2099 - acc: 0.9303 - val_loss: 0.5172 - val_acc: 0.8789\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9324\n",
      "Epoch 00316: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2043 - acc: 0.9325 - val_loss: 0.5185 - val_acc: 0.8798\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9290\n",
      "Epoch 00317: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2138 - acc: 0.9290 - val_loss: 0.5330 - val_acc: 0.8812\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9293\n",
      "Epoch 00318: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.2112 - acc: 0.9293 - val_loss: 0.5262 - val_acc: 0.8817\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9321\n",
      "Epoch 00319: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2117 - acc: 0.9321 - val_loss: 0.5301 - val_acc: 0.8779\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9309\n",
      "Epoch 00320: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2104 - acc: 0.9309 - val_loss: 0.5344 - val_acc: 0.8805\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9325\n",
      "Epoch 00321: val_loss did not improve from 0.51549\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2066 - acc: 0.9325 - val_loss: 0.5254 - val_acc: 0.8754\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9305\n",
      "Epoch 00322: val_loss improved from 0.51549 to 0.51312, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_5_conv_checkpoint/322-0.5131.hdf5\n",
      "36805/36805 [==============================] - 25s 675us/sample - loss: 0.2114 - acc: 0.9305 - val_loss: 0.5131 - val_acc: 0.8807\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9323\n",
      "Epoch 00323: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2028 - acc: 0.9323 - val_loss: 0.5319 - val_acc: 0.8712\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9324\n",
      "Epoch 00324: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2058 - acc: 0.9324 - val_loss: 0.5386 - val_acc: 0.8733\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9312\n",
      "Epoch 00325: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2103 - acc: 0.9312 - val_loss: 0.5280 - val_acc: 0.8807\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9313\n",
      "Epoch 00326: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2066 - acc: 0.9313 - val_loss: 0.5322 - val_acc: 0.8768\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9305\n",
      "Epoch 00327: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2090 - acc: 0.9306 - val_loss: 0.5245 - val_acc: 0.8828\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9352\n",
      "Epoch 00328: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2007 - acc: 0.9353 - val_loss: 0.5291 - val_acc: 0.8793\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9321\n",
      "Epoch 00329: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2072 - acc: 0.9321 - val_loss: 0.5276 - val_acc: 0.8756\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9342\n",
      "Epoch 00330: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2006 - acc: 0.9342 - val_loss: 0.5272 - val_acc: 0.8763\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9346\n",
      "Epoch 00331: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2019 - acc: 0.9346 - val_loss: 0.5512 - val_acc: 0.8726\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9315\n",
      "Epoch 00332: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2102 - acc: 0.9315 - val_loss: 0.5522 - val_acc: 0.8698\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9321\n",
      "Epoch 00333: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2096 - acc: 0.9321 - val_loss: 0.5302 - val_acc: 0.8791\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9321\n",
      "Epoch 00334: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2073 - acc: 0.9321 - val_loss: 0.5251 - val_acc: 0.8751\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9327\n",
      "Epoch 00335: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2032 - acc: 0.9327 - val_loss: 0.5469 - val_acc: 0.8803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9326\n",
      "Epoch 00336: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2039 - acc: 0.9326 - val_loss: 0.5399 - val_acc: 0.8782\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9340\n",
      "Epoch 00337: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2047 - acc: 0.9341 - val_loss: 0.5432 - val_acc: 0.8765\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9340\n",
      "Epoch 00338: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 677us/sample - loss: 0.2054 - acc: 0.9340 - val_loss: 0.5184 - val_acc: 0.8838\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9352\n",
      "Epoch 00339: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 0.2005 - acc: 0.9352 - val_loss: 0.5293 - val_acc: 0.8798\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9323\n",
      "Epoch 00340: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2060 - acc: 0.9323 - val_loss: 0.5340 - val_acc: 0.8768\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9347\n",
      "Epoch 00341: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.2023 - acc: 0.9347 - val_loss: 0.5189 - val_acc: 0.8761\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9333\n",
      "Epoch 00342: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2014 - acc: 0.9333 - val_loss: 0.5296 - val_acc: 0.8817\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9338\n",
      "Epoch 00343: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2018 - acc: 0.9338 - val_loss: 0.5347 - val_acc: 0.8786\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9326\n",
      "Epoch 00344: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 0.2065 - acc: 0.9326 - val_loss: 0.5654 - val_acc: 0.8754\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9346\n",
      "Epoch 00345: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.2024 - acc: 0.9345 - val_loss: 0.5324 - val_acc: 0.8812\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9347\n",
      "Epoch 00346: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2003 - acc: 0.9347 - val_loss: 0.5164 - val_acc: 0.8800\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9326\n",
      "Epoch 00347: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.2015 - acc: 0.9326 - val_loss: 0.5455 - val_acc: 0.8798\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9347\n",
      "Epoch 00348: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.1986 - acc: 0.9347 - val_loss: 0.5384 - val_acc: 0.8789\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9349\n",
      "Epoch 00349: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.2004 - acc: 0.9349 - val_loss: 0.5232 - val_acc: 0.8796\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9341\n",
      "Epoch 00350: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.1976 - acc: 0.9341 - val_loss: 0.5318 - val_acc: 0.8798\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9325\n",
      "Epoch 00351: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.2050 - acc: 0.9325 - val_loss: 0.5265 - val_acc: 0.8810\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9351\n",
      "Epoch 00352: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.1997 - acc: 0.9351 - val_loss: 0.5330 - val_acc: 0.8840\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9365\n",
      "Epoch 00353: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.1952 - acc: 0.9365 - val_loss: 0.5257 - val_acc: 0.8803\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9355\n",
      "Epoch 00354: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.1976 - acc: 0.9355 - val_loss: 0.5270 - val_acc: 0.8826\n",
      "Epoch 355/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9349\n",
      "Epoch 00355: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.2017 - acc: 0.9349 - val_loss: 0.5371 - val_acc: 0.8805\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9360\n",
      "Epoch 00356: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.1954 - acc: 0.9360 - val_loss: 0.5356 - val_acc: 0.8826\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9361\n",
      "Epoch 00357: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.1967 - acc: 0.9361 - val_loss: 0.5277 - val_acc: 0.8840\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9333\n",
      "Epoch 00358: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.1996 - acc: 0.9333 - val_loss: 0.5269 - val_acc: 0.8833\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9365\n",
      "Epoch 00359: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.1946 - acc: 0.9365 - val_loss: 0.5331 - val_acc: 0.8789\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9343\n",
      "Epoch 00360: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.2039 - acc: 0.9343 - val_loss: 0.5234 - val_acc: 0.8833\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9356\n",
      "Epoch 00361: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1961 - acc: 0.9356 - val_loss: 0.5251 - val_acc: 0.8810\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9375\n",
      "Epoch 00362: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1916 - acc: 0.9375 - val_loss: 0.5502 - val_acc: 0.8730\n",
      "Epoch 363/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9360\n",
      "Epoch 00363: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 674us/sample - loss: 0.1961 - acc: 0.9360 - val_loss: 0.5431 - val_acc: 0.8807\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9330\n",
      "Epoch 00364: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.2011 - acc: 0.9330 - val_loss: 0.5394 - val_acc: 0.8840\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9372\n",
      "Epoch 00365: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1951 - acc: 0.9372 - val_loss: 0.5199 - val_acc: 0.8847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 366/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9346\n",
      "Epoch 00366: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1945 - acc: 0.9345 - val_loss: 0.5378 - val_acc: 0.8798\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9343\n",
      "Epoch 00367: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.1983 - acc: 0.9344 - val_loss: 0.5266 - val_acc: 0.8828\n",
      "Epoch 368/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9350\n",
      "Epoch 00368: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 672us/sample - loss: 0.1958 - acc: 0.9350 - val_loss: 0.5319 - val_acc: 0.8810\n",
      "Epoch 369/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9363\n",
      "Epoch 00369: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.1972 - acc: 0.9363 - val_loss: 0.5327 - val_acc: 0.8805\n",
      "Epoch 370/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9374\n",
      "Epoch 00370: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1901 - acc: 0.9374 - val_loss: 0.5437 - val_acc: 0.8838\n",
      "Epoch 371/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9369\n",
      "Epoch 00371: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1934 - acc: 0.9369 - val_loss: 0.5328 - val_acc: 0.8800\n",
      "Epoch 372/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9347\n",
      "Epoch 00372: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 25s 673us/sample - loss: 0.1964 - acc: 0.9347 - val_loss: 0.5263 - val_acc: 0.8754\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 363us/sample - loss: 0.6188 - acc: 0.8434\n",
      "Loss: 0.6188230420693925 Accuracy: 0.843406\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6168 - acc: 0.1318\n",
      "Epoch 00001: val_loss improved from inf to 2.21888, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/001-2.2189.hdf5\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 2.6169 - acc: 0.1318 - val_loss: 2.2189 - val_acc: 0.3075\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9995 - acc: 0.3473\n",
      "Epoch 00002: val_loss improved from 2.21888 to 1.61326, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/002-1.6133.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 1.9994 - acc: 0.3473 - val_loss: 1.6133 - val_acc: 0.5073\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6974 - acc: 0.4421\n",
      "Epoch 00003: val_loss improved from 1.61326 to 1.46332, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/003-1.4633.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 1.6974 - acc: 0.4421 - val_loss: 1.4633 - val_acc: 0.5427\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5633 - acc: 0.4843\n",
      "Epoch 00004: val_loss improved from 1.46332 to 1.33638, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/004-1.3364.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 1.5633 - acc: 0.4843 - val_loss: 1.3364 - val_acc: 0.5882\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4653 - acc: 0.5234\n",
      "Epoch 00005: val_loss improved from 1.33638 to 1.27691, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/005-1.2769.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 1.4652 - acc: 0.5234 - val_loss: 1.2769 - val_acc: 0.6033\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3887 - acc: 0.5498\n",
      "Epoch 00006: val_loss improved from 1.27691 to 1.21869, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/006-1.2187.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 1.3886 - acc: 0.5498 - val_loss: 1.2187 - val_acc: 0.6399\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3084 - acc: 0.5790\n",
      "Epoch 00007: val_loss improved from 1.21869 to 1.10370, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/007-1.1037.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 1.3083 - acc: 0.5790 - val_loss: 1.1037 - val_acc: 0.6741\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2318 - acc: 0.6098\n",
      "Epoch 00008: val_loss improved from 1.10370 to 1.09434, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/008-1.0943.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 1.2318 - acc: 0.6098 - val_loss: 1.0943 - val_acc: 0.6734\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1727 - acc: 0.6313\n",
      "Epoch 00009: val_loss improved from 1.09434 to 0.99600, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/009-0.9960.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 1.1726 - acc: 0.6313 - val_loss: 0.9960 - val_acc: 0.6942\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1241 - acc: 0.6466\n",
      "Epoch 00010: val_loss improved from 0.99600 to 0.97909, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/010-0.9791.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 1.1243 - acc: 0.6465 - val_loss: 0.9791 - val_acc: 0.7049\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0791 - acc: 0.6618\n",
      "Epoch 00011: val_loss improved from 0.97909 to 0.91513, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/011-0.9151.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 1.0790 - acc: 0.6618 - val_loss: 0.9151 - val_acc: 0.7240\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0342 - acc: 0.6793\n",
      "Epoch 00012: val_loss improved from 0.91513 to 0.89861, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/012-0.8986.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 1.0343 - acc: 0.6793 - val_loss: 0.8986 - val_acc: 0.7319\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0066 - acc: 0.6876\n",
      "Epoch 00013: val_loss improved from 0.89861 to 0.83711, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/013-0.8371.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 1.0065 - acc: 0.6876 - val_loss: 0.8371 - val_acc: 0.7573\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9589 - acc: 0.7020\n",
      "Epoch 00014: val_loss improved from 0.83711 to 0.79950, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/014-0.7995.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.9590 - acc: 0.7020 - val_loss: 0.7995 - val_acc: 0.7692\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9296 - acc: 0.7162\n",
      "Epoch 00015: val_loss improved from 0.79950 to 0.79413, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/015-0.7941.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.9295 - acc: 0.7162 - val_loss: 0.7941 - val_acc: 0.7696\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8988 - acc: 0.7243\n",
      "Epoch 00016: val_loss did not improve from 0.79413\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.8989 - acc: 0.7242 - val_loss: 0.7984 - val_acc: 0.7647\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8756 - acc: 0.7325\n",
      "Epoch 00017: val_loss improved from 0.79413 to 0.73446, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/017-0.7345.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.8755 - acc: 0.7325 - val_loss: 0.7345 - val_acc: 0.7855\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8411 - acc: 0.7445\n",
      "Epoch 00018: val_loss improved from 0.73446 to 0.72288, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/018-0.7229.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.8410 - acc: 0.7445 - val_loss: 0.7229 - val_acc: 0.7932\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8184 - acc: 0.7498\n",
      "Epoch 00019: val_loss did not improve from 0.72288\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.8183 - acc: 0.7498 - val_loss: 0.7458 - val_acc: 0.7829\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7937 - acc: 0.7582\n",
      "Epoch 00020: val_loss improved from 0.72288 to 0.66369, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/020-0.6637.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.7937 - acc: 0.7581 - val_loss: 0.6637 - val_acc: 0.8137\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7768 - acc: 0.7635\n",
      "Epoch 00021: val_loss did not improve from 0.66369\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.7768 - acc: 0.7635 - val_loss: 0.6638 - val_acc: 0.8106\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7522 - acc: 0.7695\n",
      "Epoch 00022: val_loss improved from 0.66369 to 0.62383, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/022-0.6238.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.7523 - acc: 0.7695 - val_loss: 0.6238 - val_acc: 0.8197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7345 - acc: 0.7780\n",
      "Epoch 00023: val_loss did not improve from 0.62383\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.7345 - acc: 0.7780 - val_loss: 0.6696 - val_acc: 0.8104\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7228 - acc: 0.7811\n",
      "Epoch 00024: val_loss did not improve from 0.62383\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.7228 - acc: 0.7811 - val_loss: 0.6459 - val_acc: 0.8125\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7071 - acc: 0.7866\n",
      "Epoch 00025: val_loss improved from 0.62383 to 0.60486, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/025-0.6049.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.7074 - acc: 0.7866 - val_loss: 0.6049 - val_acc: 0.8262\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6912 - acc: 0.7924\n",
      "Epoch 00026: val_loss improved from 0.60486 to 0.58404, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/026-0.5840.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.6912 - acc: 0.7925 - val_loss: 0.5840 - val_acc: 0.8297\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.7949\n",
      "Epoch 00027: val_loss improved from 0.58404 to 0.56331, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/027-0.5633.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.6808 - acc: 0.7949 - val_loss: 0.5633 - val_acc: 0.8430\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6698 - acc: 0.7986\n",
      "Epoch 00028: val_loss did not improve from 0.56331\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.6698 - acc: 0.7986 - val_loss: 0.5773 - val_acc: 0.8409\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6510 - acc: 0.8025\n",
      "Epoch 00029: val_loss improved from 0.56331 to 0.54116, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/029-0.5412.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.6509 - acc: 0.8024 - val_loss: 0.5412 - val_acc: 0.8528\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6476 - acc: 0.8047\n",
      "Epoch 00030: val_loss improved from 0.54116 to 0.53178, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/030-0.5318.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.6476 - acc: 0.8047 - val_loss: 0.5318 - val_acc: 0.8549\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6285 - acc: 0.8106\n",
      "Epoch 00031: val_loss improved from 0.53178 to 0.51554, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/031-0.5155.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.6285 - acc: 0.8106 - val_loss: 0.5155 - val_acc: 0.8563\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6165 - acc: 0.8141\n",
      "Epoch 00032: val_loss did not improve from 0.51554\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.6165 - acc: 0.8141 - val_loss: 0.5358 - val_acc: 0.8502\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6145 - acc: 0.8129\n",
      "Epoch 00033: val_loss improved from 0.51554 to 0.50135, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/033-0.5013.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.6145 - acc: 0.8129 - val_loss: 0.5013 - val_acc: 0.8581\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.8184\n",
      "Epoch 00034: val_loss improved from 0.50135 to 0.48664, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/034-0.4866.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.6011 - acc: 0.8184 - val_loss: 0.4866 - val_acc: 0.8640\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5967 - acc: 0.8227\n",
      "Epoch 00035: val_loss improved from 0.48664 to 0.48346, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/035-0.4835.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.5966 - acc: 0.8227 - val_loss: 0.4835 - val_acc: 0.8656\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5868 - acc: 0.8221\n",
      "Epoch 00036: val_loss did not improve from 0.48346\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.5867 - acc: 0.8221 - val_loss: 0.5036 - val_acc: 0.8605\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5771 - acc: 0.8255\n",
      "Epoch 00037: val_loss improved from 0.48346 to 0.48271, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/037-0.4827.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.5771 - acc: 0.8255 - val_loss: 0.4827 - val_acc: 0.8693\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5655 - acc: 0.8311\n",
      "Epoch 00038: val_loss improved from 0.48271 to 0.46231, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/038-0.4623.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.5655 - acc: 0.8311 - val_loss: 0.4623 - val_acc: 0.8735\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5585 - acc: 0.8326\n",
      "Epoch 00039: val_loss improved from 0.46231 to 0.46103, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/039-0.4610.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.5585 - acc: 0.8326 - val_loss: 0.4610 - val_acc: 0.8726\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5564 - acc: 0.8338\n",
      "Epoch 00040: val_loss improved from 0.46103 to 0.45484, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/040-0.4548.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.5564 - acc: 0.8337 - val_loss: 0.4548 - val_acc: 0.8754\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.8349\n",
      "Epoch 00041: val_loss did not improve from 0.45484\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.5468 - acc: 0.8349 - val_loss: 0.4675 - val_acc: 0.8696\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.8369\n",
      "Epoch 00042: val_loss improved from 0.45484 to 0.44972, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/042-0.4497.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.5422 - acc: 0.8369 - val_loss: 0.4497 - val_acc: 0.8758\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.8361\n",
      "Epoch 00043: val_loss improved from 0.44972 to 0.44939, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/043-0.4494.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.5405 - acc: 0.8361 - val_loss: 0.4494 - val_acc: 0.8779\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5244 - acc: 0.8419\n",
      "Epoch 00044: val_loss did not improve from 0.44939\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.5243 - acc: 0.8419 - val_loss: 0.4892 - val_acc: 0.8607\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.8376\n",
      "Epoch 00045: val_loss improved from 0.44939 to 0.43411, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/045-0.4341.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.5267 - acc: 0.8376 - val_loss: 0.4341 - val_acc: 0.8803\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.8457\n",
      "Epoch 00046: val_loss did not improve from 0.43411\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.5146 - acc: 0.8457 - val_loss: 0.4422 - val_acc: 0.8805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5147 - acc: 0.8448\n",
      "Epoch 00047: val_loss improved from 0.43411 to 0.42430, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/047-0.4243.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.5146 - acc: 0.8448 - val_loss: 0.4243 - val_acc: 0.8863\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.8474\n",
      "Epoch 00048: val_loss did not improve from 0.42430\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.5083 - acc: 0.8474 - val_loss: 0.4304 - val_acc: 0.8845\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.8477\n",
      "Epoch 00049: val_loss did not improve from 0.42430\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.5029 - acc: 0.8477 - val_loss: 0.4248 - val_acc: 0.8868\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5011 - acc: 0.8466\n",
      "Epoch 00050: val_loss improved from 0.42430 to 0.40746, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/050-0.4075.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.5011 - acc: 0.8466 - val_loss: 0.4075 - val_acc: 0.8912\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4895 - acc: 0.8525\n",
      "Epoch 00051: val_loss did not improve from 0.40746\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4895 - acc: 0.8525 - val_loss: 0.4288 - val_acc: 0.8826\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8514\n",
      "Epoch 00052: val_loss improved from 0.40746 to 0.40274, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/052-0.4027.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.4898 - acc: 0.8514 - val_loss: 0.4027 - val_acc: 0.8887\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4819 - acc: 0.8546\n",
      "Epoch 00053: val_loss improved from 0.40274 to 0.38866, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/053-0.3887.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.4819 - acc: 0.8546 - val_loss: 0.3887 - val_acc: 0.8949\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4799 - acc: 0.8520\n",
      "Epoch 00054: val_loss did not improve from 0.38866\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.4798 - acc: 0.8520 - val_loss: 0.3930 - val_acc: 0.8949\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8547\n",
      "Epoch 00055: val_loss did not improve from 0.38866\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.4827 - acc: 0.8547 - val_loss: 0.4099 - val_acc: 0.8861\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.8561\n",
      "Epoch 00056: val_loss did not improve from 0.38866\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4700 - acc: 0.8561 - val_loss: 0.4182 - val_acc: 0.8835\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4692 - acc: 0.8574\n",
      "Epoch 00057: val_loss did not improve from 0.38866\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4692 - acc: 0.8575 - val_loss: 0.3892 - val_acc: 0.8905\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.8604\n",
      "Epoch 00058: val_loss did not improve from 0.38866\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.4589 - acc: 0.8603 - val_loss: 0.4138 - val_acc: 0.8824\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8578\n",
      "Epoch 00059: val_loss improved from 0.38866 to 0.38591, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/059-0.3859.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.4625 - acc: 0.8578 - val_loss: 0.3859 - val_acc: 0.8949\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4498 - acc: 0.8646\n",
      "Epoch 00060: val_loss did not improve from 0.38591\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4498 - acc: 0.8646 - val_loss: 0.4214 - val_acc: 0.8921\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.8634\n",
      "Epoch 00061: val_loss did not improve from 0.38591\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.4488 - acc: 0.8634 - val_loss: 0.3994 - val_acc: 0.8933\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.8639\n",
      "Epoch 00062: val_loss did not improve from 0.38591\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4479 - acc: 0.8640 - val_loss: 0.3973 - val_acc: 0.8884\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8626\n",
      "Epoch 00063: val_loss improved from 0.38591 to 0.37692, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/063-0.3769.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4467 - acc: 0.8626 - val_loss: 0.3769 - val_acc: 0.8959\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.8673\n",
      "Epoch 00064: val_loss improved from 0.37692 to 0.36405, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/064-0.3640.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4405 - acc: 0.8674 - val_loss: 0.3640 - val_acc: 0.9026\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4340 - acc: 0.8674\n",
      "Epoch 00065: val_loss did not improve from 0.36405\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.4340 - acc: 0.8675 - val_loss: 0.3979 - val_acc: 0.8928\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.8658\n",
      "Epoch 00066: val_loss did not improve from 0.36405\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.4369 - acc: 0.8658 - val_loss: 0.3735 - val_acc: 0.9038\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4319 - acc: 0.8683\n",
      "Epoch 00067: val_loss improved from 0.36405 to 0.35984, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/067-0.3598.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4319 - acc: 0.8683 - val_loss: 0.3598 - val_acc: 0.9052\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4282 - acc: 0.8672\n",
      "Epoch 00068: val_loss improved from 0.35984 to 0.35646, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/068-0.3565.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4281 - acc: 0.8672 - val_loss: 0.3565 - val_acc: 0.9031\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8715\n",
      "Epoch 00069: val_loss did not improve from 0.35646\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.4224 - acc: 0.8715 - val_loss: 0.3573 - val_acc: 0.9066\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4210 - acc: 0.8727\n",
      "Epoch 00070: val_loss did not improve from 0.35646\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.4210 - acc: 0.8727 - val_loss: 0.3690 - val_acc: 0.9031\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4207 - acc: 0.8716\n",
      "Epoch 00071: val_loss improved from 0.35646 to 0.35225, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/071-0.3522.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4207 - acc: 0.8716 - val_loss: 0.3522 - val_acc: 0.9031\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8747\n",
      "Epoch 00072: val_loss did not improve from 0.35225\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.4147 - acc: 0.8747 - val_loss: 0.3913 - val_acc: 0.8938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8745\n",
      "Epoch 00073: val_loss did not improve from 0.35225\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.4107 - acc: 0.8745 - val_loss: 0.3555 - val_acc: 0.9054\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8760\n",
      "Epoch 00074: val_loss did not improve from 0.35225\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.4040 - acc: 0.8760 - val_loss: 0.3635 - val_acc: 0.9015\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8733\n",
      "Epoch 00075: val_loss improved from 0.35225 to 0.34535, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/075-0.3454.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4078 - acc: 0.8734 - val_loss: 0.3454 - val_acc: 0.9087\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8746\n",
      "Epoch 00076: val_loss improved from 0.34535 to 0.34483, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/076-0.3448.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4050 - acc: 0.8746 - val_loss: 0.3448 - val_acc: 0.9082\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8750\n",
      "Epoch 00077: val_loss did not improve from 0.34483\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.4039 - acc: 0.8750 - val_loss: 0.3486 - val_acc: 0.9057\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8767\n",
      "Epoch 00078: val_loss did not improve from 0.34483\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3949 - acc: 0.8766 - val_loss: 0.3619 - val_acc: 0.8998\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8805\n",
      "Epoch 00079: val_loss did not improve from 0.34483\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3929 - acc: 0.8805 - val_loss: 0.3454 - val_acc: 0.9064\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8802\n",
      "Epoch 00080: val_loss did not improve from 0.34483\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3935 - acc: 0.8802 - val_loss: 0.3497 - val_acc: 0.9061\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8783\n",
      "Epoch 00081: val_loss did not improve from 0.34483\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3913 - acc: 0.8783 - val_loss: 0.3464 - val_acc: 0.9078\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8813\n",
      "Epoch 00082: val_loss improved from 0.34483 to 0.34222, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/082-0.3422.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3924 - acc: 0.8812 - val_loss: 0.3422 - val_acc: 0.9101\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8816\n",
      "Epoch 00083: val_loss did not improve from 0.34222\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3865 - acc: 0.8816 - val_loss: 0.3453 - val_acc: 0.9089\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8807\n",
      "Epoch 00084: val_loss improved from 0.34222 to 0.33914, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/084-0.3391.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3851 - acc: 0.8807 - val_loss: 0.3391 - val_acc: 0.9082\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8814\n",
      "Epoch 00085: val_loss did not improve from 0.33914\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3823 - acc: 0.8815 - val_loss: 0.3532 - val_acc: 0.9068\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8865\n",
      "Epoch 00086: val_loss did not improve from 0.33914\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3715 - acc: 0.8865 - val_loss: 0.3426 - val_acc: 0.9075\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8829\n",
      "Epoch 00087: val_loss did not improve from 0.33914\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3779 - acc: 0.8830 - val_loss: 0.3420 - val_acc: 0.9096\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8832\n",
      "Epoch 00088: val_loss improved from 0.33914 to 0.33106, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/088-0.3311.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.3757 - acc: 0.8832 - val_loss: 0.3311 - val_acc: 0.9129\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8863\n",
      "Epoch 00089: val_loss did not improve from 0.33106\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3719 - acc: 0.8863 - val_loss: 0.3378 - val_acc: 0.9161\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8855\n",
      "Epoch 00090: val_loss did not improve from 0.33106\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3702 - acc: 0.8855 - val_loss: 0.3353 - val_acc: 0.9110\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3698 - acc: 0.8865\n",
      "Epoch 00091: val_loss did not improve from 0.33106\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.3698 - acc: 0.8865 - val_loss: 0.3493 - val_acc: 0.9073\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8902\n",
      "Epoch 00092: val_loss did not improve from 0.33106\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3625 - acc: 0.8903 - val_loss: 0.3464 - val_acc: 0.9080\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8853\n",
      "Epoch 00093: val_loss did not improve from 0.33106\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3636 - acc: 0.8853 - val_loss: 0.3329 - val_acc: 0.9140\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3618 - acc: 0.8873\n",
      "Epoch 00094: val_loss improved from 0.33106 to 0.32445, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/094-0.3244.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3620 - acc: 0.8873 - val_loss: 0.3244 - val_acc: 0.9133\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8867\n",
      "Epoch 00095: val_loss did not improve from 0.32445\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3665 - acc: 0.8866 - val_loss: 0.3410 - val_acc: 0.9068\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3577 - acc: 0.8881\n",
      "Epoch 00096: val_loss did not improve from 0.32445\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3577 - acc: 0.8881 - val_loss: 0.3304 - val_acc: 0.9129\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8895\n",
      "Epoch 00097: val_loss did not improve from 0.32445\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3547 - acc: 0.8895 - val_loss: 0.3530 - val_acc: 0.9061\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.8879\n",
      "Epoch 00098: val_loss did not improve from 0.32445\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3615 - acc: 0.8879 - val_loss: 0.3261 - val_acc: 0.9143\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8901\n",
      "Epoch 00099: val_loss did not improve from 0.32445\n",
      "36805/36805 [==============================] - 25s 688us/sample - loss: 0.3589 - acc: 0.8901 - val_loss: 0.3373 - val_acc: 0.9147\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8921\n",
      "Epoch 00100: val_loss did not improve from 0.32445\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3526 - acc: 0.8922 - val_loss: 0.3532 - val_acc: 0.9075\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8914\n",
      "Epoch 00101: val_loss did not improve from 0.32445\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3520 - acc: 0.8915 - val_loss: 0.3330 - val_acc: 0.9092\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.8918\n",
      "Epoch 00102: val_loss did not improve from 0.32445\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3479 - acc: 0.8918 - val_loss: 0.3418 - val_acc: 0.9071\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8906\n",
      "Epoch 00103: val_loss improved from 0.32445 to 0.32313, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/103-0.3231.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3519 - acc: 0.8906 - val_loss: 0.3231 - val_acc: 0.9110\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8908\n",
      "Epoch 00104: val_loss did not improve from 0.32313\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3481 - acc: 0.8909 - val_loss: 0.3304 - val_acc: 0.9087\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8929\n",
      "Epoch 00105: val_loss did not improve from 0.32313\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3433 - acc: 0.8929 - val_loss: 0.3347 - val_acc: 0.9078\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.8938\n",
      "Epoch 00106: val_loss did not improve from 0.32313\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3413 - acc: 0.8938 - val_loss: 0.3379 - val_acc: 0.9092\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3463 - acc: 0.8926\n",
      "Epoch 00107: val_loss did not improve from 0.32313\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3462 - acc: 0.8926 - val_loss: 0.3244 - val_acc: 0.9129\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8936\n",
      "Epoch 00108: val_loss improved from 0.32313 to 0.31697, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/108-0.3170.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3401 - acc: 0.8936 - val_loss: 0.3170 - val_acc: 0.9140\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.8935\n",
      "Epoch 00109: val_loss did not improve from 0.31697\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3372 - acc: 0.8935 - val_loss: 0.3245 - val_acc: 0.9152\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8934\n",
      "Epoch 00110: val_loss did not improve from 0.31697\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3371 - acc: 0.8934 - val_loss: 0.3254 - val_acc: 0.9143\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.8961\n",
      "Epoch 00111: val_loss improved from 0.31697 to 0.31616, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/111-0.3162.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3340 - acc: 0.8962 - val_loss: 0.3162 - val_acc: 0.9154\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8927\n",
      "Epoch 00112: val_loss did not improve from 0.31616\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3387 - acc: 0.8927 - val_loss: 0.3223 - val_acc: 0.9136\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8968\n",
      "Epoch 00113: val_loss did not improve from 0.31616\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3312 - acc: 0.8968 - val_loss: 0.3186 - val_acc: 0.9180\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8954\n",
      "Epoch 00114: val_loss improved from 0.31616 to 0.31600, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/114-0.3160.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3325 - acc: 0.8954 - val_loss: 0.3160 - val_acc: 0.9185\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.8963\n",
      "Epoch 00115: val_loss improved from 0.31600 to 0.31502, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/115-0.3150.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3300 - acc: 0.8963 - val_loss: 0.3150 - val_acc: 0.9157\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.8961\n",
      "Epoch 00116: val_loss did not improve from 0.31502\n",
      "36805/36805 [==============================] - 25s 686us/sample - loss: 0.3317 - acc: 0.8961 - val_loss: 0.3535 - val_acc: 0.9043\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8966\n",
      "Epoch 00117: val_loss did not improve from 0.31502\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.3307 - acc: 0.8966 - val_loss: 0.3203 - val_acc: 0.9138\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3237 - acc: 0.8982\n",
      "Epoch 00118: val_loss improved from 0.31502 to 0.31281, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/118-0.3128.hdf5\n",
      "36805/36805 [==============================] - 25s 683us/sample - loss: 0.3238 - acc: 0.8981 - val_loss: 0.3128 - val_acc: 0.9178\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8977\n",
      "Epoch 00119: val_loss improved from 0.31281 to 0.31208, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/119-0.3121.hdf5\n",
      "36805/36805 [==============================] - 25s 682us/sample - loss: 0.3255 - acc: 0.8977 - val_loss: 0.3121 - val_acc: 0.9168\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3237 - acc: 0.8990\n",
      "Epoch 00120: val_loss did not improve from 0.31208\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3237 - acc: 0.8990 - val_loss: 0.3176 - val_acc: 0.9161\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.8980\n",
      "Epoch 00121: val_loss improved from 0.31208 to 0.31076, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/121-0.3108.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3260 - acc: 0.8980 - val_loss: 0.3108 - val_acc: 0.9201\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.9011\n",
      "Epoch 00122: val_loss did not improve from 0.31076\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.3147 - acc: 0.9011 - val_loss: 0.3130 - val_acc: 0.9173\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8990\n",
      "Epoch 00123: val_loss improved from 0.31076 to 0.31009, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/123-0.3101.hdf5\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.3231 - acc: 0.8990 - val_loss: 0.3101 - val_acc: 0.9187\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.9012\n",
      "Epoch 00124: val_loss improved from 0.31009 to 0.30764, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/124-0.3076.hdf5\n",
      "36805/36805 [==============================] - 25s 679us/sample - loss: 0.3191 - acc: 0.9012 - val_loss: 0.3076 - val_acc: 0.9203\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9039\n",
      "Epoch 00125: val_loss did not improve from 0.30764\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.3109 - acc: 0.9039 - val_loss: 0.3085 - val_acc: 0.9168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3111 - acc: 0.9010\n",
      "Epoch 00126: val_loss improved from 0.30764 to 0.30619, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/126-0.3062.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 0.3111 - acc: 0.9010 - val_loss: 0.3062 - val_acc: 0.9227\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3124 - acc: 0.9018\n",
      "Epoch 00127: val_loss did not improve from 0.30619\n",
      "36805/36805 [==============================] - 25s 678us/sample - loss: 0.3124 - acc: 0.9018 - val_loss: 0.3197 - val_acc: 0.9157\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8998\n",
      "Epoch 00128: val_loss did not improve from 0.30619\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 0.3180 - acc: 0.8998 - val_loss: 0.3107 - val_acc: 0.9189\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3095 - acc: 0.9020\n",
      "Epoch 00129: val_loss improved from 0.30619 to 0.30421, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/129-0.3042.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3097 - acc: 0.9020 - val_loss: 0.3042 - val_acc: 0.9210\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.9000\n",
      "Epoch 00130: val_loss did not improve from 0.30421\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3153 - acc: 0.9000 - val_loss: 0.3238 - val_acc: 0.9168\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.9033\n",
      "Epoch 00131: val_loss did not improve from 0.30421\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3128 - acc: 0.9033 - val_loss: 0.3566 - val_acc: 0.9075\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9023\n",
      "Epoch 00132: val_loss did not improve from 0.30421\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3105 - acc: 0.9022 - val_loss: 0.3181 - val_acc: 0.9143\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9028\n",
      "Epoch 00133: val_loss improved from 0.30421 to 0.29934, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/133-0.2993.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.3069 - acc: 0.9028 - val_loss: 0.2993 - val_acc: 0.9203\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.9058\n",
      "Epoch 00134: val_loss did not improve from 0.29934\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.3023 - acc: 0.9059 - val_loss: 0.3116 - val_acc: 0.9213\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.9030\n",
      "Epoch 00135: val_loss did not improve from 0.29934\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.3083 - acc: 0.9030 - val_loss: 0.3065 - val_acc: 0.9220\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.9026\n",
      "Epoch 00136: val_loss did not improve from 0.29934\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.3060 - acc: 0.9026 - val_loss: 0.3220 - val_acc: 0.9154\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9055\n",
      "Epoch 00137: val_loss did not improve from 0.29934\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2988 - acc: 0.9055 - val_loss: 0.3199 - val_acc: 0.9168\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.9058\n",
      "Epoch 00138: val_loss did not improve from 0.29934\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3009 - acc: 0.9057 - val_loss: 0.3062 - val_acc: 0.9234\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.9042\n",
      "Epoch 00139: val_loss improved from 0.29934 to 0.29904, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/139-0.2990.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.3027 - acc: 0.9042 - val_loss: 0.2990 - val_acc: 0.9213\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9056\n",
      "Epoch 00140: val_loss improved from 0.29904 to 0.29091, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/140-0.2909.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2969 - acc: 0.9056 - val_loss: 0.2909 - val_acc: 0.9234\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9045\n",
      "Epoch 00141: val_loss did not improve from 0.29091\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2976 - acc: 0.9045 - val_loss: 0.2915 - val_acc: 0.9227\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9047\n",
      "Epoch 00142: val_loss did not improve from 0.29091\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2969 - acc: 0.9047 - val_loss: 0.3074 - val_acc: 0.9241\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.9060\n",
      "Epoch 00143: val_loss did not improve from 0.29091\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2969 - acc: 0.9060 - val_loss: 0.2987 - val_acc: 0.9220\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9034\n",
      "Epoch 00144: val_loss did not improve from 0.29091\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2995 - acc: 0.9034 - val_loss: 0.2981 - val_acc: 0.9166\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9073\n",
      "Epoch 00145: val_loss did not improve from 0.29091\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2888 - acc: 0.9072 - val_loss: 0.3045 - val_acc: 0.9222\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.9056\n",
      "Epoch 00146: val_loss did not improve from 0.29091\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2969 - acc: 0.9056 - val_loss: 0.2962 - val_acc: 0.9245\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9055\n",
      "Epoch 00147: val_loss did not improve from 0.29091\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2968 - acc: 0.9055 - val_loss: 0.3052 - val_acc: 0.9189\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9065\n",
      "Epoch 00148: val_loss did not improve from 0.29091\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2942 - acc: 0.9066 - val_loss: 0.3026 - val_acc: 0.9238\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9073\n",
      "Epoch 00149: val_loss did not improve from 0.29091\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2871 - acc: 0.9073 - val_loss: 0.2922 - val_acc: 0.9264\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.9085\n",
      "Epoch 00150: val_loss did not improve from 0.29091\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2894 - acc: 0.9085 - val_loss: 0.2985 - val_acc: 0.9231\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9074\n",
      "Epoch 00151: val_loss improved from 0.29091 to 0.29077, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/151-0.2908.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.2932 - acc: 0.9074 - val_loss: 0.2908 - val_acc: 0.9194\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9079\n",
      "Epoch 00152: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2901 - acc: 0.9079 - val_loss: 0.2937 - val_acc: 0.9257\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9072\n",
      "Epoch 00153: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2917 - acc: 0.9072 - val_loss: 0.2987 - val_acc: 0.9203\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9112\n",
      "Epoch 00154: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2831 - acc: 0.9112 - val_loss: 0.3046 - val_acc: 0.9220\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9109\n",
      "Epoch 00155: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2779 - acc: 0.9109 - val_loss: 0.2979 - val_acc: 0.9245\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9103\n",
      "Epoch 00156: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2822 - acc: 0.9103 - val_loss: 0.3000 - val_acc: 0.9236\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.9113\n",
      "Epoch 00157: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2792 - acc: 0.9113 - val_loss: 0.3000 - val_acc: 0.9215\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9077\n",
      "Epoch 00158: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2894 - acc: 0.9077 - val_loss: 0.2957 - val_acc: 0.9236\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.9117\n",
      "Epoch 00159: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2758 - acc: 0.9117 - val_loss: 0.2979 - val_acc: 0.9241\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9107\n",
      "Epoch 00160: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2789 - acc: 0.9107 - val_loss: 0.3120 - val_acc: 0.9136\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9089\n",
      "Epoch 00161: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2788 - acc: 0.9089 - val_loss: 0.3234 - val_acc: 0.9194\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2834 - acc: 0.9092\n",
      "Epoch 00162: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2833 - acc: 0.9093 - val_loss: 0.3060 - val_acc: 0.9245\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2793 - acc: 0.9113\n",
      "Epoch 00163: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2793 - acc: 0.9112 - val_loss: 0.2952 - val_acc: 0.9227\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9107\n",
      "Epoch 00164: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2760 - acc: 0.9107 - val_loss: 0.2984 - val_acc: 0.9241\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.9111\n",
      "Epoch 00165: val_loss did not improve from 0.29077\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2763 - acc: 0.9112 - val_loss: 0.2996 - val_acc: 0.9266\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9123\n",
      "Epoch 00166: val_loss improved from 0.29077 to 0.28690, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/166-0.2869.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2760 - acc: 0.9123 - val_loss: 0.2869 - val_acc: 0.9262\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9105\n",
      "Epoch 00167: val_loss improved from 0.28690 to 0.28361, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/167-0.2836.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2755 - acc: 0.9105 - val_loss: 0.2836 - val_acc: 0.9257\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.9136\n",
      "Epoch 00168: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2693 - acc: 0.9136 - val_loss: 0.2885 - val_acc: 0.9266\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9130\n",
      "Epoch 00169: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2746 - acc: 0.9130 - val_loss: 0.2941 - val_acc: 0.9222\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9113\n",
      "Epoch 00170: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.2734 - acc: 0.9113 - val_loss: 0.3092 - val_acc: 0.9227\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9140\n",
      "Epoch 00171: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2676 - acc: 0.9140 - val_loss: 0.2907 - val_acc: 0.9210\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9132\n",
      "Epoch 00172: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2705 - acc: 0.9132 - val_loss: 0.2918 - val_acc: 0.9287\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9125\n",
      "Epoch 00173: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2709 - acc: 0.9125 - val_loss: 0.2937 - val_acc: 0.9278\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9149\n",
      "Epoch 00174: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2658 - acc: 0.9149 - val_loss: 0.2997 - val_acc: 0.9259\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9148\n",
      "Epoch 00175: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2674 - acc: 0.9148 - val_loss: 0.2937 - val_acc: 0.9259\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2677 - acc: 0.9140\n",
      "Epoch 00176: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2677 - acc: 0.9140 - val_loss: 0.3133 - val_acc: 0.9252\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9165\n",
      "Epoch 00177: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2606 - acc: 0.9165 - val_loss: 0.3121 - val_acc: 0.9126\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9132\n",
      "Epoch 00178: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2656 - acc: 0.9132 - val_loss: 0.3188 - val_acc: 0.9220\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9160\n",
      "Epoch 00179: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2616 - acc: 0.9160 - val_loss: 0.2908 - val_acc: 0.9241\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9162\n",
      "Epoch 00180: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2615 - acc: 0.9162 - val_loss: 0.2851 - val_acc: 0.9227\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9161\n",
      "Epoch 00181: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2614 - acc: 0.9161 - val_loss: 0.2850 - val_acc: 0.9273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.9152\n",
      "Epoch 00182: val_loss did not improve from 0.28361\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2616 - acc: 0.9152 - val_loss: 0.3146 - val_acc: 0.9252\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.9162\n",
      "Epoch 00183: val_loss improved from 0.28361 to 0.28126, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/183-0.2813.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.2591 - acc: 0.9162 - val_loss: 0.2813 - val_acc: 0.9283\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9166\n",
      "Epoch 00184: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2599 - acc: 0.9166 - val_loss: 0.2884 - val_acc: 0.9259\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9192\n",
      "Epoch 00185: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2533 - acc: 0.9193 - val_loss: 0.2836 - val_acc: 0.9276\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9170\n",
      "Epoch 00186: val_loss improved from 0.28126 to 0.27496, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/186-0.2750.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2591 - acc: 0.9170 - val_loss: 0.2750 - val_acc: 0.9315\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9172\n",
      "Epoch 00187: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2544 - acc: 0.9172 - val_loss: 0.2878 - val_acc: 0.9243\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9196\n",
      "Epoch 00188: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2533 - acc: 0.9196 - val_loss: 0.2825 - val_acc: 0.9248\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9187\n",
      "Epoch 00189: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2553 - acc: 0.9188 - val_loss: 0.2905 - val_acc: 0.9220\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2556 - acc: 0.9175\n",
      "Epoch 00190: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2556 - acc: 0.9175 - val_loss: 0.2814 - val_acc: 0.9236\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2556 - acc: 0.9168\n",
      "Epoch 00191: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2556 - acc: 0.9168 - val_loss: 0.2913 - val_acc: 0.9241\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.9165\n",
      "Epoch 00192: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2580 - acc: 0.9165 - val_loss: 0.2963 - val_acc: 0.9266\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9179\n",
      "Epoch 00193: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2545 - acc: 0.9179 - val_loss: 0.2821 - val_acc: 0.9259\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9169\n",
      "Epoch 00194: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2539 - acc: 0.9169 - val_loss: 0.2912 - val_acc: 0.9311\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9180\n",
      "Epoch 00195: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2572 - acc: 0.9180 - val_loss: 0.2880 - val_acc: 0.9257\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9198\n",
      "Epoch 00196: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2512 - acc: 0.9197 - val_loss: 0.2854 - val_acc: 0.9269\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.9193\n",
      "Epoch 00197: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2506 - acc: 0.9193 - val_loss: 0.2889 - val_acc: 0.9280\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9205\n",
      "Epoch 00198: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2483 - acc: 0.9205 - val_loss: 0.2965 - val_acc: 0.9273\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9201\n",
      "Epoch 00199: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2464 - acc: 0.9201 - val_loss: 0.2941 - val_acc: 0.9294\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9187\n",
      "Epoch 00200: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2478 - acc: 0.9187 - val_loss: 0.2838 - val_acc: 0.9292\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9207\n",
      "Epoch 00201: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2481 - acc: 0.9207 - val_loss: 0.3039 - val_acc: 0.9292\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9207\n",
      "Epoch 00202: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2484 - acc: 0.9207 - val_loss: 0.2932 - val_acc: 0.9255\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9199\n",
      "Epoch 00203: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2485 - acc: 0.9199 - val_loss: 0.2948 - val_acc: 0.9292\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9189\n",
      "Epoch 00204: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2498 - acc: 0.9189 - val_loss: 0.2933 - val_acc: 0.9259\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9198\n",
      "Epoch 00205: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2451 - acc: 0.9198 - val_loss: 0.2979 - val_acc: 0.9269\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9209\n",
      "Epoch 00206: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2460 - acc: 0.9209 - val_loss: 0.2979 - val_acc: 0.9269\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9207\n",
      "Epoch 00207: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2433 - acc: 0.9207 - val_loss: 0.2885 - val_acc: 0.9266\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9205\n",
      "Epoch 00208: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2458 - acc: 0.9206 - val_loss: 0.2870 - val_acc: 0.9301\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9189\n",
      "Epoch 00209: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2440 - acc: 0.9189 - val_loss: 0.2828 - val_acc: 0.9280\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9199\n",
      "Epoch 00210: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2477 - acc: 0.9199 - val_loss: 0.2889 - val_acc: 0.9283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9237\n",
      "Epoch 00211: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2363 - acc: 0.9237 - val_loss: 0.2804 - val_acc: 0.9266\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9218\n",
      "Epoch 00212: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2447 - acc: 0.9219 - val_loss: 0.2813 - val_acc: 0.9280\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9218\n",
      "Epoch 00213: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2395 - acc: 0.9218 - val_loss: 0.3057 - val_acc: 0.9273\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9210\n",
      "Epoch 00214: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2437 - acc: 0.9210 - val_loss: 0.2869 - val_acc: 0.9231\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9225\n",
      "Epoch 00215: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2412 - acc: 0.9225 - val_loss: 0.2967 - val_acc: 0.9236\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9221\n",
      "Epoch 00216: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2399 - acc: 0.9221 - val_loss: 0.2793 - val_acc: 0.9276\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9221\n",
      "Epoch 00217: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2362 - acc: 0.9221 - val_loss: 0.2801 - val_acc: 0.9280\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9238\n",
      "Epoch 00218: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2358 - acc: 0.9238 - val_loss: 0.2907 - val_acc: 0.9273\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9223\n",
      "Epoch 00219: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2345 - acc: 0.9223 - val_loss: 0.2815 - val_acc: 0.9280\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9229\n",
      "Epoch 00220: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2393 - acc: 0.9229 - val_loss: 0.2905 - val_acc: 0.9257\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9230\n",
      "Epoch 00221: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2341 - acc: 0.9230 - val_loss: 0.2921 - val_acc: 0.9241\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9236\n",
      "Epoch 00222: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2358 - acc: 0.9235 - val_loss: 0.2786 - val_acc: 0.9290\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9255\n",
      "Epoch 00223: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2274 - acc: 0.9255 - val_loss: 0.2812 - val_acc: 0.9304\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9236\n",
      "Epoch 00224: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2354 - acc: 0.9237 - val_loss: 0.2900 - val_acc: 0.9273\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9235\n",
      "Epoch 00225: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2353 - acc: 0.9235 - val_loss: 0.2974 - val_acc: 0.9271\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9247\n",
      "Epoch 00226: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2354 - acc: 0.9247 - val_loss: 0.2936 - val_acc: 0.9255\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9257\n",
      "Epoch 00227: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2294 - acc: 0.9257 - val_loss: 0.2835 - val_acc: 0.9294\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9239\n",
      "Epoch 00228: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2349 - acc: 0.9238 - val_loss: 0.2875 - val_acc: 0.9294\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9257\n",
      "Epoch 00229: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2294 - acc: 0.9257 - val_loss: 0.2789 - val_acc: 0.9297\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9262\n",
      "Epoch 00230: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2290 - acc: 0.9262 - val_loss: 0.2920 - val_acc: 0.9294\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9248\n",
      "Epoch 00231: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2334 - acc: 0.9247 - val_loss: 0.2833 - val_acc: 0.9297\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9261\n",
      "Epoch 00232: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2291 - acc: 0.9261 - val_loss: 0.2776 - val_acc: 0.9297\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9250\n",
      "Epoch 00233: val_loss did not improve from 0.27496\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2296 - acc: 0.9250 - val_loss: 0.2856 - val_acc: 0.9292\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9240\n",
      "Epoch 00234: val_loss improved from 0.27496 to 0.27326, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/234-0.2733.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2318 - acc: 0.9240 - val_loss: 0.2733 - val_acc: 0.9269\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9267\n",
      "Epoch 00235: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2288 - acc: 0.9267 - val_loss: 0.2990 - val_acc: 0.9248\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9251\n",
      "Epoch 00236: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2294 - acc: 0.9251 - val_loss: 0.2797 - val_acc: 0.9262\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9286\n",
      "Epoch 00237: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2217 - acc: 0.9286 - val_loss: 0.2822 - val_acc: 0.9287\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9256\n",
      "Epoch 00238: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2250 - acc: 0.9256 - val_loss: 0.3040 - val_acc: 0.9257\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9237\n",
      "Epoch 00239: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2324 - acc: 0.9238 - val_loss: 0.2892 - val_acc: 0.9297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9273\n",
      "Epoch 00240: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2243 - acc: 0.9273 - val_loss: 0.2824 - val_acc: 0.9248\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9287\n",
      "Epoch 00241: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2195 - acc: 0.9287 - val_loss: 0.2986 - val_acc: 0.9262\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9234\n",
      "Epoch 00242: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2276 - acc: 0.9234 - val_loss: 0.2842 - val_acc: 0.9210\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9269\n",
      "Epoch 00243: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2210 - acc: 0.9269 - val_loss: 0.2906 - val_acc: 0.9278\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9271\n",
      "Epoch 00244: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2231 - acc: 0.9272 - val_loss: 0.2765 - val_acc: 0.9292\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9239\n",
      "Epoch 00245: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2285 - acc: 0.9239 - val_loss: 0.2916 - val_acc: 0.9290\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9286\n",
      "Epoch 00246: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2207 - acc: 0.9286 - val_loss: 0.2922 - val_acc: 0.9266\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9270\n",
      "Epoch 00247: val_loss did not improve from 0.27326\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2209 - acc: 0.9270 - val_loss: 0.2921 - val_acc: 0.9299\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9273\n",
      "Epoch 00248: val_loss improved from 0.27326 to 0.27170, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/248-0.2717.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2230 - acc: 0.9273 - val_loss: 0.2717 - val_acc: 0.9280\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9284\n",
      "Epoch 00249: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2220 - acc: 0.9283 - val_loss: 0.2904 - val_acc: 0.9259\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9282\n",
      "Epoch 00250: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2190 - acc: 0.9282 - val_loss: 0.2726 - val_acc: 0.9269\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9299\n",
      "Epoch 00251: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2158 - acc: 0.9298 - val_loss: 0.2792 - val_acc: 0.9283\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9292\n",
      "Epoch 00252: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2179 - acc: 0.9292 - val_loss: 0.2875 - val_acc: 0.9283\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9313\n",
      "Epoch 00253: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2148 - acc: 0.9313 - val_loss: 0.2891 - val_acc: 0.9283\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9278\n",
      "Epoch 00254: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2171 - acc: 0.9278 - val_loss: 0.2726 - val_acc: 0.9304\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9280\n",
      "Epoch 00255: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2146 - acc: 0.9279 - val_loss: 0.2824 - val_acc: 0.9276\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9269\n",
      "Epoch 00256: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2194 - acc: 0.9269 - val_loss: 0.2860 - val_acc: 0.9292\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9286\n",
      "Epoch 00257: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2168 - acc: 0.9286 - val_loss: 0.2989 - val_acc: 0.9264\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9296\n",
      "Epoch 00258: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2160 - acc: 0.9296 - val_loss: 0.2833 - val_acc: 0.9292\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9296\n",
      "Epoch 00259: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2161 - acc: 0.9296 - val_loss: 0.2834 - val_acc: 0.9329\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9292\n",
      "Epoch 00260: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2131 - acc: 0.9292 - val_loss: 0.2732 - val_acc: 0.9313\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9292\n",
      "Epoch 00261: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2125 - acc: 0.9292 - val_loss: 0.2904 - val_acc: 0.9306\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9284\n",
      "Epoch 00262: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2142 - acc: 0.9284 - val_loss: 0.2725 - val_acc: 0.9290\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9321\n",
      "Epoch 00263: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2098 - acc: 0.9321 - val_loss: 0.2806 - val_acc: 0.9278\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9282\n",
      "Epoch 00264: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2213 - acc: 0.9282 - val_loss: 0.2805 - val_acc: 0.9329\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9322\n",
      "Epoch 00265: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2089 - acc: 0.9322 - val_loss: 0.2908 - val_acc: 0.9308\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9294\n",
      "Epoch 00266: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2112 - acc: 0.9294 - val_loss: 0.2905 - val_acc: 0.9322\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9311\n",
      "Epoch 00267: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2099 - acc: 0.9311 - val_loss: 0.3012 - val_acc: 0.9287\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9303\n",
      "Epoch 00268: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2145 - acc: 0.9303 - val_loss: 0.2804 - val_acc: 0.9269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9298\n",
      "Epoch 00269: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2118 - acc: 0.9298 - val_loss: 0.2866 - val_acc: 0.9283\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9312\n",
      "Epoch 00270: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2118 - acc: 0.9312 - val_loss: 0.2933 - val_acc: 0.9280\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9303\n",
      "Epoch 00271: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2080 - acc: 0.9303 - val_loss: 0.2797 - val_acc: 0.9297\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9304\n",
      "Epoch 00272: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2108 - acc: 0.9304 - val_loss: 0.2883 - val_acc: 0.9311\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9304\n",
      "Epoch 00273: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2107 - acc: 0.9304 - val_loss: 0.2799 - val_acc: 0.9273\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9326\n",
      "Epoch 00274: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2074 - acc: 0.9326 - val_loss: 0.2922 - val_acc: 0.9273\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9319\n",
      "Epoch 00275: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2074 - acc: 0.9319 - val_loss: 0.2960 - val_acc: 0.9290\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9328\n",
      "Epoch 00276: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2090 - acc: 0.9328 - val_loss: 0.2960 - val_acc: 0.9283\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9320\n",
      "Epoch 00277: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2068 - acc: 0.9320 - val_loss: 0.2789 - val_acc: 0.9271\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9327\n",
      "Epoch 00278: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2033 - acc: 0.9327 - val_loss: 0.2809 - val_acc: 0.9322\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9327\n",
      "Epoch 00279: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2046 - acc: 0.9327 - val_loss: 0.2866 - val_acc: 0.9299\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9329\n",
      "Epoch 00280: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2066 - acc: 0.9329 - val_loss: 0.2745 - val_acc: 0.9304\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9316\n",
      "Epoch 00281: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2062 - acc: 0.9316 - val_loss: 0.2963 - val_acc: 0.9313\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9329\n",
      "Epoch 00282: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2078 - acc: 0.9329 - val_loss: 0.2910 - val_acc: 0.9287\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 0.9321\n",
      "Epoch 00283: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2051 - acc: 0.9321 - val_loss: 0.2943 - val_acc: 0.9280\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9322\n",
      "Epoch 00284: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2038 - acc: 0.9322 - val_loss: 0.2823 - val_acc: 0.9278\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9299\n",
      "Epoch 00285: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2078 - acc: 0.9299 - val_loss: 0.2803 - val_acc: 0.9329\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9315\n",
      "Epoch 00286: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2062 - acc: 0.9315 - val_loss: 0.2899 - val_acc: 0.9304\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9327\n",
      "Epoch 00287: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2047 - acc: 0.9327 - val_loss: 0.2742 - val_acc: 0.9317\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9321\n",
      "Epoch 00288: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2006 - acc: 0.9321 - val_loss: 0.2782 - val_acc: 0.9338\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9313\n",
      "Epoch 00289: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2039 - acc: 0.9313 - val_loss: 0.2919 - val_acc: 0.9343\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9362\n",
      "Epoch 00290: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1967 - acc: 0.9362 - val_loss: 0.2817 - val_acc: 0.9306\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9325\n",
      "Epoch 00291: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2023 - acc: 0.9325 - val_loss: 0.2884 - val_acc: 0.9301\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9330\n",
      "Epoch 00292: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2002 - acc: 0.9330 - val_loss: 0.2832 - val_acc: 0.9320\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9329\n",
      "Epoch 00293: val_loss did not improve from 0.27170\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2029 - acc: 0.9329 - val_loss: 0.2970 - val_acc: 0.9308\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9336\n",
      "Epoch 00294: val_loss improved from 0.27170 to 0.26885, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/294-0.2689.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2023 - acc: 0.9336 - val_loss: 0.2689 - val_acc: 0.9297\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9340\n",
      "Epoch 00295: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.2004 - acc: 0.9340 - val_loss: 0.2841 - val_acc: 0.9341\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9350\n",
      "Epoch 00296: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2017 - acc: 0.9350 - val_loss: 0.2968 - val_acc: 0.9306\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9325\n",
      "Epoch 00297: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2033 - acc: 0.9325 - val_loss: 0.2790 - val_acc: 0.9301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9349\n",
      "Epoch 00298: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2001 - acc: 0.9349 - val_loss: 0.2890 - val_acc: 0.9271\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9357\n",
      "Epoch 00299: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1970 - acc: 0.9357 - val_loss: 0.2750 - val_acc: 0.9313\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9361\n",
      "Epoch 00300: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1940 - acc: 0.9360 - val_loss: 0.2699 - val_acc: 0.9301\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9358\n",
      "Epoch 00301: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1954 - acc: 0.9358 - val_loss: 0.2896 - val_acc: 0.9280\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9364\n",
      "Epoch 00302: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1928 - acc: 0.9364 - val_loss: 0.2734 - val_acc: 0.9329\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9339\n",
      "Epoch 00303: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2004 - acc: 0.9339 - val_loss: 0.2775 - val_acc: 0.9306\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9336\n",
      "Epoch 00304: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2002 - acc: 0.9336 - val_loss: 0.2879 - val_acc: 0.9334\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9340\n",
      "Epoch 00305: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2050 - acc: 0.9340 - val_loss: 0.2859 - val_acc: 0.9322\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9345\n",
      "Epoch 00306: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1943 - acc: 0.9345 - val_loss: 0.2854 - val_acc: 0.9343\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9341\n",
      "Epoch 00307: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1959 - acc: 0.9341 - val_loss: 0.2917 - val_acc: 0.9269\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9348\n",
      "Epoch 00308: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1970 - acc: 0.9348 - val_loss: 0.2802 - val_acc: 0.9334\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9357\n",
      "Epoch 00309: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1969 - acc: 0.9356 - val_loss: 0.2693 - val_acc: 0.9311\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9347\n",
      "Epoch 00310: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1935 - acc: 0.9347 - val_loss: 0.2773 - val_acc: 0.9324\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9336\n",
      "Epoch 00311: val_loss did not improve from 0.26885\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1993 - acc: 0.9336 - val_loss: 0.2811 - val_acc: 0.9315\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9347\n",
      "Epoch 00312: val_loss improved from 0.26885 to 0.26847, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_6_conv_checkpoint/312-0.2685.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.1972 - acc: 0.9347 - val_loss: 0.2685 - val_acc: 0.9297\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9364\n",
      "Epoch 00313: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1928 - acc: 0.9363 - val_loss: 0.2794 - val_acc: 0.9294\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9347\n",
      "Epoch 00314: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1938 - acc: 0.9347 - val_loss: 0.3033 - val_acc: 0.9297\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9347\n",
      "Epoch 00315: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1939 - acc: 0.9347 - val_loss: 0.2820 - val_acc: 0.9311\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9384\n",
      "Epoch 00316: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1877 - acc: 0.9384 - val_loss: 0.2736 - val_acc: 0.9324\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9356\n",
      "Epoch 00317: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1902 - acc: 0.9356 - val_loss: 0.2839 - val_acc: 0.9315\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9368\n",
      "Epoch 00318: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1920 - acc: 0.9368 - val_loss: 0.2855 - val_acc: 0.9283\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9376\n",
      "Epoch 00319: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1912 - acc: 0.9376 - val_loss: 0.2771 - val_acc: 0.9341\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9370\n",
      "Epoch 00320: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1896 - acc: 0.9370 - val_loss: 0.2801 - val_acc: 0.9317\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9375\n",
      "Epoch 00321: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1883 - acc: 0.9375 - val_loss: 0.2777 - val_acc: 0.9292\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9376\n",
      "Epoch 00322: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1872 - acc: 0.9375 - val_loss: 0.2988 - val_acc: 0.9287\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9358\n",
      "Epoch 00323: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1910 - acc: 0.9358 - val_loss: 0.3041 - val_acc: 0.9292\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9362\n",
      "Epoch 00324: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1906 - acc: 0.9361 - val_loss: 0.2742 - val_acc: 0.9336\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9393\n",
      "Epoch 00325: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1838 - acc: 0.9393 - val_loss: 0.3306 - val_acc: 0.9229\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9352\n",
      "Epoch 00326: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1916 - acc: 0.9352 - val_loss: 0.2901 - val_acc: 0.9301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9371\n",
      "Epoch 00327: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1909 - acc: 0.9371 - val_loss: 0.2780 - val_acc: 0.9324\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9395\n",
      "Epoch 00328: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1852 - acc: 0.9395 - val_loss: 0.2912 - val_acc: 0.9343\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9386\n",
      "Epoch 00329: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1861 - acc: 0.9386 - val_loss: 0.2890 - val_acc: 0.9324\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9378\n",
      "Epoch 00330: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.1893 - acc: 0.9378 - val_loss: 0.2796 - val_acc: 0.9355\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9397\n",
      "Epoch 00331: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1870 - acc: 0.9397 - val_loss: 0.3015 - val_acc: 0.9331\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9401\n",
      "Epoch 00332: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1838 - acc: 0.9401 - val_loss: 0.2946 - val_acc: 0.9315\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9378\n",
      "Epoch 00333: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1851 - acc: 0.9378 - val_loss: 0.2868 - val_acc: 0.9327\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9382\n",
      "Epoch 00334: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1847 - acc: 0.9382 - val_loss: 0.2826 - val_acc: 0.9313\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9387\n",
      "Epoch 00335: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1863 - acc: 0.9386 - val_loss: 0.2767 - val_acc: 0.9329\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9399\n",
      "Epoch 00336: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1834 - acc: 0.9399 - val_loss: 0.2933 - val_acc: 0.9345\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9392\n",
      "Epoch 00337: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1873 - acc: 0.9392 - val_loss: 0.2826 - val_acc: 0.9338\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9375\n",
      "Epoch 00338: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1846 - acc: 0.9375 - val_loss: 0.2895 - val_acc: 0.9331\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9364\n",
      "Epoch 00339: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1884 - acc: 0.9364 - val_loss: 0.2897 - val_acc: 0.9338\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9381\n",
      "Epoch 00340: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1873 - acc: 0.9381 - val_loss: 0.2792 - val_acc: 0.9313\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9385\n",
      "Epoch 00341: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1851 - acc: 0.9385 - val_loss: 0.2923 - val_acc: 0.9290\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9396\n",
      "Epoch 00342: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1808 - acc: 0.9397 - val_loss: 0.2906 - val_acc: 0.9287\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9383\n",
      "Epoch 00343: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1831 - acc: 0.9384 - val_loss: 0.2737 - val_acc: 0.9304\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9383\n",
      "Epoch 00344: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1811 - acc: 0.9384 - val_loss: 0.2797 - val_acc: 0.9308\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9416\n",
      "Epoch 00345: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1790 - acc: 0.9416 - val_loss: 0.2889 - val_acc: 0.9322\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9389\n",
      "Epoch 00346: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1822 - acc: 0.9389 - val_loss: 0.2810 - val_acc: 0.9320\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9392\n",
      "Epoch 00347: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1840 - acc: 0.9392 - val_loss: 0.2904 - val_acc: 0.9331\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9400\n",
      "Epoch 00348: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1815 - acc: 0.9400 - val_loss: 0.2862 - val_acc: 0.9324\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9398\n",
      "Epoch 00349: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1821 - acc: 0.9398 - val_loss: 0.2971 - val_acc: 0.9324\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9413\n",
      "Epoch 00350: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1780 - acc: 0.9413 - val_loss: 0.2761 - val_acc: 0.9341\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9407\n",
      "Epoch 00351: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1794 - acc: 0.9407 - val_loss: 0.2749 - val_acc: 0.9306\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9389\n",
      "Epoch 00352: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1869 - acc: 0.9389 - val_loss: 0.2797 - val_acc: 0.9350\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9387\n",
      "Epoch 00353: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1806 - acc: 0.9387 - val_loss: 0.3090 - val_acc: 0.9336\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9397\n",
      "Epoch 00354: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1809 - acc: 0.9397 - val_loss: 0.2856 - val_acc: 0.9338\n",
      "Epoch 355/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9415\n",
      "Epoch 00355: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1749 - acc: 0.9415 - val_loss: 0.3009 - val_acc: 0.9322\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9374\n",
      "Epoch 00356: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1833 - acc: 0.9374 - val_loss: 0.2769 - val_acc: 0.9336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9423\n",
      "Epoch 00357: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1743 - acc: 0.9423 - val_loss: 0.2895 - val_acc: 0.9285\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9390\n",
      "Epoch 00358: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.1836 - acc: 0.9390 - val_loss: 0.2829 - val_acc: 0.9345\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9420\n",
      "Epoch 00359: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1757 - acc: 0.9419 - val_loss: 0.3035 - val_acc: 0.9306\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9395\n",
      "Epoch 00360: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1817 - acc: 0.9395 - val_loss: 0.2772 - val_acc: 0.9343\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9402\n",
      "Epoch 00361: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1815 - acc: 0.9401 - val_loss: 0.2972 - val_acc: 0.9292\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9399\n",
      "Epoch 00362: val_loss did not improve from 0.26847\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.1787 - acc: 0.9399 - val_loss: 0.2880 - val_acc: 0.9362\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 389us/sample - loss: 0.3242 - acc: 0.9101\n",
      "Loss: 0.32421576377634936 Accuracy: 0.9100727\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6846 - acc: 0.1058\n",
      "Epoch 00001: val_loss improved from inf to 2.48453, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/001-2.4845.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 2.6846 - acc: 0.1058 - val_loss: 2.4845 - val_acc: 0.2103\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2768 - acc: 0.2481\n",
      "Epoch 00002: val_loss improved from 2.48453 to 1.81876, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/002-1.8188.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 2.2767 - acc: 0.2481 - val_loss: 1.8188 - val_acc: 0.4342\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8494 - acc: 0.3901\n",
      "Epoch 00003: val_loss improved from 1.81876 to 1.48761, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/003-1.4876.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 1.8493 - acc: 0.3901 - val_loss: 1.4876 - val_acc: 0.5369\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6352 - acc: 0.4537\n",
      "Epoch 00004: val_loss improved from 1.48761 to 1.32415, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/004-1.3242.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 1.6352 - acc: 0.4537 - val_loss: 1.3242 - val_acc: 0.5814\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5094 - acc: 0.4940\n",
      "Epoch 00005: val_loss improved from 1.32415 to 1.28374, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/005-1.2837.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 1.5094 - acc: 0.4940 - val_loss: 1.2837 - val_acc: 0.5870\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4218 - acc: 0.5260\n",
      "Epoch 00006: val_loss improved from 1.28374 to 1.15914, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/006-1.1591.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 1.4218 - acc: 0.5260 - val_loss: 1.1591 - val_acc: 0.6462\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3421 - acc: 0.5527\n",
      "Epoch 00007: val_loss improved from 1.15914 to 1.09947, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/007-1.0995.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 1.3421 - acc: 0.5527 - val_loss: 1.0995 - val_acc: 0.6494\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2811 - acc: 0.5789\n",
      "Epoch 00008: val_loss improved from 1.09947 to 1.04976, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/008-1.0498.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 1.2812 - acc: 0.5789 - val_loss: 1.0498 - val_acc: 0.6737\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2245 - acc: 0.6022\n",
      "Epoch 00009: val_loss improved from 1.04976 to 1.00923, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/009-1.0092.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 1.2245 - acc: 0.6021 - val_loss: 1.0092 - val_acc: 0.6830\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1722 - acc: 0.6231\n",
      "Epoch 00010: val_loss improved from 1.00923 to 0.93633, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/010-0.9363.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 1.1722 - acc: 0.6231 - val_loss: 0.9363 - val_acc: 0.7170\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1242 - acc: 0.6409\n",
      "Epoch 00011: val_loss improved from 0.93633 to 0.89112, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/011-0.8911.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 1.1244 - acc: 0.6408 - val_loss: 0.8911 - val_acc: 0.7375\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0739 - acc: 0.6566\n",
      "Epoch 00012: val_loss improved from 0.89112 to 0.86098, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/012-0.8610.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 1.0739 - acc: 0.6565 - val_loss: 0.8610 - val_acc: 0.7489\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0290 - acc: 0.6767\n",
      "Epoch 00013: val_loss improved from 0.86098 to 0.82977, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/013-0.8298.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 1.0288 - acc: 0.6767 - val_loss: 0.8298 - val_acc: 0.7475\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9863 - acc: 0.6914\n",
      "Epoch 00014: val_loss improved from 0.82977 to 0.76825, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/014-0.7682.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.9862 - acc: 0.6915 - val_loss: 0.7682 - val_acc: 0.7792\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9486 - acc: 0.7051\n",
      "Epoch 00015: val_loss improved from 0.76825 to 0.71756, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/015-0.7176.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.9486 - acc: 0.7051 - val_loss: 0.7176 - val_acc: 0.7959\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9136 - acc: 0.7174\n",
      "Epoch 00016: val_loss did not improve from 0.71756\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.9136 - acc: 0.7174 - val_loss: 0.7520 - val_acc: 0.7775\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8864 - acc: 0.7279\n",
      "Epoch 00017: val_loss improved from 0.71756 to 0.67837, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/017-0.6784.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.8864 - acc: 0.7279 - val_loss: 0.6784 - val_acc: 0.8053\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8615 - acc: 0.7334\n",
      "Epoch 00018: val_loss improved from 0.67837 to 0.62119, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/018-0.6212.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.8616 - acc: 0.7334 - val_loss: 0.6212 - val_acc: 0.8153\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8313 - acc: 0.7460\n",
      "Epoch 00019: val_loss improved from 0.62119 to 0.60673, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/019-0.6067.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.8312 - acc: 0.7461 - val_loss: 0.6067 - val_acc: 0.8199\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8053 - acc: 0.7517\n",
      "Epoch 00020: val_loss did not improve from 0.60673\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.8053 - acc: 0.7517 - val_loss: 0.6098 - val_acc: 0.8218\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7752 - acc: 0.7603\n",
      "Epoch 00021: val_loss did not improve from 0.60673\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.7751 - acc: 0.7603 - val_loss: 0.6074 - val_acc: 0.8241\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7630 - acc: 0.7651\n",
      "Epoch 00022: val_loss improved from 0.60673 to 0.58036, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/022-0.5804.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.7629 - acc: 0.7651 - val_loss: 0.5804 - val_acc: 0.8290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7371 - acc: 0.7738\n",
      "Epoch 00023: val_loss improved from 0.58036 to 0.52287, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/023-0.5229.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.7371 - acc: 0.7738 - val_loss: 0.5229 - val_acc: 0.8528\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7160 - acc: 0.7818\n",
      "Epoch 00024: val_loss improved from 0.52287 to 0.50120, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/024-0.5012.hdf5\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.7160 - acc: 0.7819 - val_loss: 0.5012 - val_acc: 0.8616\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6990 - acc: 0.7848\n",
      "Epoch 00025: val_loss improved from 0.50120 to 0.50091, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/025-0.5009.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.6990 - acc: 0.7848 - val_loss: 0.5009 - val_acc: 0.8602\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6856 - acc: 0.7909\n",
      "Epoch 00026: val_loss improved from 0.50091 to 0.47490, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/026-0.4749.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.6855 - acc: 0.7909 - val_loss: 0.4749 - val_acc: 0.8693\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6696 - acc: 0.7957\n",
      "Epoch 00027: val_loss improved from 0.47490 to 0.46693, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/027-0.4669.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.6695 - acc: 0.7957 - val_loss: 0.4669 - val_acc: 0.8768\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6623 - acc: 0.7971\n",
      "Epoch 00028: val_loss did not improve from 0.46693\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.6623 - acc: 0.7971 - val_loss: 0.4719 - val_acc: 0.8689\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6503 - acc: 0.8018\n",
      "Epoch 00029: val_loss improved from 0.46693 to 0.44564, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/029-0.4456.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.6502 - acc: 0.8018 - val_loss: 0.4456 - val_acc: 0.8812\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6312 - acc: 0.8062\n",
      "Epoch 00030: val_loss improved from 0.44564 to 0.43391, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/030-0.4339.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.6312 - acc: 0.8062 - val_loss: 0.4339 - val_acc: 0.8772\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6225 - acc: 0.8094\n",
      "Epoch 00031: val_loss did not improve from 0.43391\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.6225 - acc: 0.8094 - val_loss: 0.4351 - val_acc: 0.8786\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6158 - acc: 0.8111\n",
      "Epoch 00032: val_loss improved from 0.43391 to 0.42991, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/032-0.4299.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.6158 - acc: 0.8111 - val_loss: 0.4299 - val_acc: 0.8789\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.8167\n",
      "Epoch 00033: val_loss did not improve from 0.42991\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.6017 - acc: 0.8167 - val_loss: 0.4443 - val_acc: 0.8707\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5963 - acc: 0.8161\n",
      "Epoch 00034: val_loss improved from 0.42991 to 0.40270, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/034-0.4027.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.5963 - acc: 0.8161 - val_loss: 0.4027 - val_acc: 0.8952\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5825 - acc: 0.8222\n",
      "Epoch 00035: val_loss improved from 0.40270 to 0.39845, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/035-0.3984.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.5826 - acc: 0.8222 - val_loss: 0.3984 - val_acc: 0.8959\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.8255\n",
      "Epoch 00036: val_loss improved from 0.39845 to 0.38933, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/036-0.3893.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.5733 - acc: 0.8255 - val_loss: 0.3893 - val_acc: 0.8910\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.8268\n",
      "Epoch 00037: val_loss did not improve from 0.38933\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.5651 - acc: 0.8268 - val_loss: 0.4055 - val_acc: 0.8875\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.8305\n",
      "Epoch 00038: val_loss improved from 0.38933 to 0.36561, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/038-0.3656.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.5581 - acc: 0.8305 - val_loss: 0.3656 - val_acc: 0.9008\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.8314\n",
      "Epoch 00039: val_loss did not improve from 0.36561\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.5518 - acc: 0.8314 - val_loss: 0.3694 - val_acc: 0.9059\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.8338\n",
      "Epoch 00040: val_loss improved from 0.36561 to 0.36423, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/040-0.3642.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.5398 - acc: 0.8338 - val_loss: 0.3642 - val_acc: 0.9043\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.8338\n",
      "Epoch 00041: val_loss did not improve from 0.36423\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.5361 - acc: 0.8337 - val_loss: 0.3699 - val_acc: 0.8956\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.8401\n",
      "Epoch 00042: val_loss improved from 0.36423 to 0.34015, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/042-0.3401.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.5278 - acc: 0.8402 - val_loss: 0.3401 - val_acc: 0.9057\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5281 - acc: 0.8394\n",
      "Epoch 00043: val_loss improved from 0.34015 to 0.33357, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/043-0.3336.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.5281 - acc: 0.8394 - val_loss: 0.3336 - val_acc: 0.9124\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.8416\n",
      "Epoch 00044: val_loss did not improve from 0.33357\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.5160 - acc: 0.8416 - val_loss: 0.3595 - val_acc: 0.9019\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5055 - acc: 0.8451\n",
      "Epoch 00045: val_loss did not improve from 0.33357\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.5054 - acc: 0.8451 - val_loss: 0.3358 - val_acc: 0.9103\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8474\n",
      "Epoch 00046: val_loss improved from 0.33357 to 0.32162, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/046-0.3216.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.5020 - acc: 0.8474 - val_loss: 0.3216 - val_acc: 0.9185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4910 - acc: 0.8472\n",
      "Epoch 00047: val_loss improved from 0.32162 to 0.32151, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/047-0.3215.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.4909 - acc: 0.8472 - val_loss: 0.3215 - val_acc: 0.9106\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4872 - acc: 0.8509\n",
      "Epoch 00048: val_loss improved from 0.32151 to 0.30819, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/048-0.3082.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4872 - acc: 0.8509 - val_loss: 0.3082 - val_acc: 0.9143\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4865 - acc: 0.8519\n",
      "Epoch 00049: val_loss did not improve from 0.30819\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.4864 - acc: 0.8519 - val_loss: 0.3183 - val_acc: 0.9166\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4775 - acc: 0.8520\n",
      "Epoch 00050: val_loss improved from 0.30819 to 0.30663, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/050-0.3066.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.4775 - acc: 0.8520 - val_loss: 0.3066 - val_acc: 0.9178\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4755 - acc: 0.8553\n",
      "Epoch 00051: val_loss improved from 0.30663 to 0.29790, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/051-0.2979.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4754 - acc: 0.8553 - val_loss: 0.2979 - val_acc: 0.9217\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4688 - acc: 0.8577\n",
      "Epoch 00052: val_loss did not improve from 0.29790\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4688 - acc: 0.8577 - val_loss: 0.2982 - val_acc: 0.9220\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.8587\n",
      "Epoch 00053: val_loss did not improve from 0.29790\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4607 - acc: 0.8587 - val_loss: 0.3093 - val_acc: 0.9175\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4598 - acc: 0.8568\n",
      "Epoch 00054: val_loss did not improve from 0.29790\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4599 - acc: 0.8568 - val_loss: 0.3066 - val_acc: 0.9215\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8604\n",
      "Epoch 00055: val_loss did not improve from 0.29790\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.4543 - acc: 0.8604 - val_loss: 0.3129 - val_acc: 0.9196\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4510 - acc: 0.8604\n",
      "Epoch 00056: val_loss improved from 0.29790 to 0.29109, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/056-0.2911.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.4509 - acc: 0.8604 - val_loss: 0.2911 - val_acc: 0.9208\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.8634\n",
      "Epoch 00057: val_loss improved from 0.29109 to 0.28540, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/057-0.2854.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.4446 - acc: 0.8634 - val_loss: 0.2854 - val_acc: 0.9248\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4398 - acc: 0.8672\n",
      "Epoch 00058: val_loss improved from 0.28540 to 0.28535, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/058-0.2853.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4397 - acc: 0.8672 - val_loss: 0.2853 - val_acc: 0.9241\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4300 - acc: 0.8707\n",
      "Epoch 00059: val_loss improved from 0.28535 to 0.28239, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/059-0.2824.hdf5\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4300 - acc: 0.8707 - val_loss: 0.2824 - val_acc: 0.9227\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.8680\n",
      "Epoch 00060: val_loss improved from 0.28239 to 0.27916, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/060-0.2792.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4315 - acc: 0.8680 - val_loss: 0.2792 - val_acc: 0.9252\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4328 - acc: 0.8652\n",
      "Epoch 00061: val_loss improved from 0.27916 to 0.27143, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/061-0.2714.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.4328 - acc: 0.8653 - val_loss: 0.2714 - val_acc: 0.9292\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4328 - acc: 0.8674\n",
      "Epoch 00062: val_loss did not improve from 0.27143\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4331 - acc: 0.8674 - val_loss: 0.2843 - val_acc: 0.9236\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.8710\n",
      "Epoch 00063: val_loss improved from 0.27143 to 0.26575, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/063-0.2658.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.4171 - acc: 0.8710 - val_loss: 0.2658 - val_acc: 0.9283\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.8704\n",
      "Epoch 00064: val_loss did not improve from 0.26575\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4180 - acc: 0.8705 - val_loss: 0.2732 - val_acc: 0.9290\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4141 - acc: 0.8711\n",
      "Epoch 00065: val_loss improved from 0.26575 to 0.26199, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/065-0.2620.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4141 - acc: 0.8711 - val_loss: 0.2620 - val_acc: 0.9280\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4136 - acc: 0.8715\n",
      "Epoch 00066: val_loss improved from 0.26199 to 0.26008, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/066-0.2601.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4136 - acc: 0.8715 - val_loss: 0.2601 - val_acc: 0.9257\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8735\n",
      "Epoch 00067: val_loss did not improve from 0.26008\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4061 - acc: 0.8735 - val_loss: 0.2629 - val_acc: 0.9315\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8734\n",
      "Epoch 00068: val_loss did not improve from 0.26008\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4058 - acc: 0.8734 - val_loss: 0.2608 - val_acc: 0.9299\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8741\n",
      "Epoch 00069: val_loss improved from 0.26008 to 0.25981, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/069-0.2598.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4078 - acc: 0.8741 - val_loss: 0.2598 - val_acc: 0.9278\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8741\n",
      "Epoch 00070: val_loss improved from 0.25981 to 0.25049, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/070-0.2505.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4055 - acc: 0.8741 - val_loss: 0.2505 - val_acc: 0.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8773\n",
      "Epoch 00071: val_loss did not improve from 0.25049\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3985 - acc: 0.8774 - val_loss: 0.2586 - val_acc: 0.9336\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8773\n",
      "Epoch 00072: val_loss did not improve from 0.25049\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.3949 - acc: 0.8773 - val_loss: 0.2700 - val_acc: 0.9250\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8772\n",
      "Epoch 00073: val_loss did not improve from 0.25049\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.3948 - acc: 0.8772 - val_loss: 0.2598 - val_acc: 0.9304\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8779\n",
      "Epoch 00074: val_loss improved from 0.25049 to 0.24669, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/074-0.2467.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3902 - acc: 0.8779 - val_loss: 0.2467 - val_acc: 0.9327\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8804\n",
      "Epoch 00075: val_loss did not improve from 0.24669\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3845 - acc: 0.8804 - val_loss: 0.2550 - val_acc: 0.9324\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8796\n",
      "Epoch 00076: val_loss did not improve from 0.24669\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3892 - acc: 0.8796 - val_loss: 0.2602 - val_acc: 0.9334\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8805\n",
      "Epoch 00077: val_loss improved from 0.24669 to 0.24124, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/077-0.2412.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3870 - acc: 0.8805 - val_loss: 0.2412 - val_acc: 0.9362\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8829\n",
      "Epoch 00078: val_loss did not improve from 0.24124\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3711 - acc: 0.8829 - val_loss: 0.2534 - val_acc: 0.9317\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8821\n",
      "Epoch 00079: val_loss did not improve from 0.24124\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3777 - acc: 0.8821 - val_loss: 0.2470 - val_acc: 0.9329\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3688 - acc: 0.8857\n",
      "Epoch 00080: val_loss improved from 0.24124 to 0.23633, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/080-0.2363.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3689 - acc: 0.8857 - val_loss: 0.2363 - val_acc: 0.9373\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8844\n",
      "Epoch 00081: val_loss improved from 0.23633 to 0.23333, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/081-0.2333.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.3724 - acc: 0.8843 - val_loss: 0.2333 - val_acc: 0.9373\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8844\n",
      "Epoch 00082: val_loss did not improve from 0.23333\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3707 - acc: 0.8844 - val_loss: 0.2384 - val_acc: 0.9373\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3620 - acc: 0.8874\n",
      "Epoch 00083: val_loss did not improve from 0.23333\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3619 - acc: 0.8874 - val_loss: 0.2383 - val_acc: 0.9324\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8867\n",
      "Epoch 00084: val_loss did not improve from 0.23333\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3632 - acc: 0.8867 - val_loss: 0.2378 - val_acc: 0.9383\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8913\n",
      "Epoch 00085: val_loss did not improve from 0.23333\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3569 - acc: 0.8913 - val_loss: 0.2470 - val_acc: 0.9308\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3602 - acc: 0.8885\n",
      "Epoch 00086: val_loss improved from 0.23333 to 0.22635, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/086-0.2263.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3602 - acc: 0.8885 - val_loss: 0.2263 - val_acc: 0.9401\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8882\n",
      "Epoch 00087: val_loss did not improve from 0.22635\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3577 - acc: 0.8882 - val_loss: 0.2318 - val_acc: 0.9404\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8913\n",
      "Epoch 00088: val_loss did not improve from 0.22635\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3499 - acc: 0.8913 - val_loss: 0.2287 - val_acc: 0.9404\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3536 - acc: 0.8923\n",
      "Epoch 00089: val_loss did not improve from 0.22635\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3535 - acc: 0.8924 - val_loss: 0.2269 - val_acc: 0.9413\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8908\n",
      "Epoch 00090: val_loss improved from 0.22635 to 0.22084, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/090-0.2208.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.3484 - acc: 0.8908 - val_loss: 0.2208 - val_acc: 0.9385\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8923\n",
      "Epoch 00091: val_loss did not improve from 0.22084\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.3494 - acc: 0.8923 - val_loss: 0.2283 - val_acc: 0.9394\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8935\n",
      "Epoch 00092: val_loss did not improve from 0.22084\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.3369 - acc: 0.8935 - val_loss: 0.2270 - val_acc: 0.9401\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8914\n",
      "Epoch 00093: val_loss did not improve from 0.22084\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3410 - acc: 0.8914 - val_loss: 0.2290 - val_acc: 0.9411\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8938\n",
      "Epoch 00094: val_loss did not improve from 0.22084\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.3362 - acc: 0.8938 - val_loss: 0.2264 - val_acc: 0.9446\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8933\n",
      "Epoch 00095: val_loss improved from 0.22084 to 0.21765, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/095-0.2177.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3386 - acc: 0.8933 - val_loss: 0.2177 - val_acc: 0.9404\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.8943\n",
      "Epoch 00096: val_loss improved from 0.21765 to 0.20870, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/096-0.2087.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3365 - acc: 0.8943 - val_loss: 0.2087 - val_acc: 0.9427\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8965\n",
      "Epoch 00097: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3320 - acc: 0.8965 - val_loss: 0.2145 - val_acc: 0.9429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8944\n",
      "Epoch 00098: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3348 - acc: 0.8943 - val_loss: 0.2326 - val_acc: 0.9371\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.8952\n",
      "Epoch 00099: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3372 - acc: 0.8952 - val_loss: 0.2256 - val_acc: 0.9422\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8965\n",
      "Epoch 00100: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3241 - acc: 0.8965 - val_loss: 0.2169 - val_acc: 0.9420\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.8990\n",
      "Epoch 00101: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3244 - acc: 0.8990 - val_loss: 0.2254 - val_acc: 0.9378\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.8980\n",
      "Epoch 00102: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.3231 - acc: 0.8980 - val_loss: 0.2241 - val_acc: 0.9434\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8983\n",
      "Epoch 00103: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.3231 - acc: 0.8982 - val_loss: 0.2144 - val_acc: 0.9418\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.9005\n",
      "Epoch 00104: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3214 - acc: 0.9006 - val_loss: 0.2115 - val_acc: 0.9450\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.9010\n",
      "Epoch 00105: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.3146 - acc: 0.9009 - val_loss: 0.2129 - val_acc: 0.9432\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8996\n",
      "Epoch 00106: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3181 - acc: 0.8996 - val_loss: 0.2108 - val_acc: 0.9429\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.9008\n",
      "Epoch 00107: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3167 - acc: 0.9008 - val_loss: 0.2270 - val_acc: 0.9359\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9006\n",
      "Epoch 00108: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3129 - acc: 0.9006 - val_loss: 0.2163 - val_acc: 0.9448\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9017\n",
      "Epoch 00109: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3086 - acc: 0.9018 - val_loss: 0.2088 - val_acc: 0.9455\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9041\n",
      "Epoch 00110: val_loss did not improve from 0.20870\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3063 - acc: 0.9041 - val_loss: 0.2121 - val_acc: 0.9446\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9031\n",
      "Epoch 00111: val_loss improved from 0.20870 to 0.20690, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/111-0.2069.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.3071 - acc: 0.9031 - val_loss: 0.2069 - val_acc: 0.9455\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9026\n",
      "Epoch 00112: val_loss improved from 0.20690 to 0.20283, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/112-0.2028.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.3037 - acc: 0.9026 - val_loss: 0.2028 - val_acc: 0.9446\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9026\n",
      "Epoch 00113: val_loss improved from 0.20283 to 0.20172, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/113-0.2017.hdf5\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.3085 - acc: 0.9026 - val_loss: 0.2017 - val_acc: 0.9476\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9024\n",
      "Epoch 00114: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.3101 - acc: 0.9024 - val_loss: 0.2110 - val_acc: 0.9427\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9023\n",
      "Epoch 00115: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3079 - acc: 0.9023 - val_loss: 0.2352 - val_acc: 0.9378\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9065\n",
      "Epoch 00116: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2964 - acc: 0.9065 - val_loss: 0.2137 - val_acc: 0.9441\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9038\n",
      "Epoch 00117: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3007 - acc: 0.9038 - val_loss: 0.2044 - val_acc: 0.9462\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.9066\n",
      "Epoch 00118: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2994 - acc: 0.9066 - val_loss: 0.2066 - val_acc: 0.9460\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9049\n",
      "Epoch 00119: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3012 - acc: 0.9048 - val_loss: 0.2158 - val_acc: 0.9446\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9081\n",
      "Epoch 00120: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2910 - acc: 0.9081 - val_loss: 0.2166 - val_acc: 0.9415\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9082\n",
      "Epoch 00121: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2952 - acc: 0.9082 - val_loss: 0.2075 - val_acc: 0.9443\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9071\n",
      "Epoch 00122: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2938 - acc: 0.9071 - val_loss: 0.2212 - val_acc: 0.9404\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9100\n",
      "Epoch 00123: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2883 - acc: 0.9100 - val_loss: 0.2022 - val_acc: 0.9481\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9089\n",
      "Epoch 00124: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2878 - acc: 0.9089 - val_loss: 0.2047 - val_acc: 0.9462\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9058\n",
      "Epoch 00125: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2968 - acc: 0.9058 - val_loss: 0.2053 - val_acc: 0.9469\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9096\n",
      "Epoch 00126: val_loss did not improve from 0.20172\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2847 - acc: 0.9097 - val_loss: 0.2026 - val_acc: 0.9471\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9088\n",
      "Epoch 00127: val_loss improved from 0.20172 to 0.19472, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/127-0.1947.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.2914 - acc: 0.9088 - val_loss: 0.1947 - val_acc: 0.9476\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9102\n",
      "Epoch 00128: val_loss did not improve from 0.19472\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2866 - acc: 0.9102 - val_loss: 0.2072 - val_acc: 0.9429\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9094\n",
      "Epoch 00129: val_loss did not improve from 0.19472\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2875 - acc: 0.9094 - val_loss: 0.2007 - val_acc: 0.9469\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9099\n",
      "Epoch 00130: val_loss did not improve from 0.19472\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2815 - acc: 0.9099 - val_loss: 0.2198 - val_acc: 0.9385\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2838 - acc: 0.9101\n",
      "Epoch 00131: val_loss did not improve from 0.19472\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2838 - acc: 0.9101 - val_loss: 0.2061 - val_acc: 0.9453\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9099\n",
      "Epoch 00132: val_loss did not improve from 0.19472\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2815 - acc: 0.9099 - val_loss: 0.2037 - val_acc: 0.9453\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9102\n",
      "Epoch 00133: val_loss improved from 0.19472 to 0.19212, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/133-0.1921.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2823 - acc: 0.9102 - val_loss: 0.1921 - val_acc: 0.9490\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9104\n",
      "Epoch 00134: val_loss did not improve from 0.19212\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2816 - acc: 0.9104 - val_loss: 0.2006 - val_acc: 0.9499\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9141\n",
      "Epoch 00135: val_loss did not improve from 0.19212\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2735 - acc: 0.9141 - val_loss: 0.1964 - val_acc: 0.9513\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9144\n",
      "Epoch 00136: val_loss did not improve from 0.19212\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2702 - acc: 0.9144 - val_loss: 0.1952 - val_acc: 0.9492\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9130\n",
      "Epoch 00137: val_loss improved from 0.19212 to 0.19052, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/137-0.1905.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.2751 - acc: 0.9130 - val_loss: 0.1905 - val_acc: 0.9483\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9127\n",
      "Epoch 00138: val_loss did not improve from 0.19052\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2745 - acc: 0.9127 - val_loss: 0.1946 - val_acc: 0.9513\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9132\n",
      "Epoch 00139: val_loss did not improve from 0.19052\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2696 - acc: 0.9132 - val_loss: 0.1945 - val_acc: 0.9499\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9145\n",
      "Epoch 00140: val_loss did not improve from 0.19052\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2699 - acc: 0.9144 - val_loss: 0.1969 - val_acc: 0.9478\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9164\n",
      "Epoch 00141: val_loss did not improve from 0.19052\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2637 - acc: 0.9165 - val_loss: 0.2054 - val_acc: 0.9462\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9155\n",
      "Epoch 00142: val_loss did not improve from 0.19052\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2685 - acc: 0.9155 - val_loss: 0.1930 - val_acc: 0.9483\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9161\n",
      "Epoch 00143: val_loss did not improve from 0.19052\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2619 - acc: 0.9161 - val_loss: 0.2067 - val_acc: 0.9471\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9149\n",
      "Epoch 00144: val_loss did not improve from 0.19052\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2685 - acc: 0.9149 - val_loss: 0.2019 - val_acc: 0.9497\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9165\n",
      "Epoch 00145: val_loss did not improve from 0.19052\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2585 - acc: 0.9165 - val_loss: 0.1919 - val_acc: 0.9502\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9157\n",
      "Epoch 00146: val_loss did not improve from 0.19052\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2613 - acc: 0.9157 - val_loss: 0.2005 - val_acc: 0.9506\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9164\n",
      "Epoch 00147: val_loss improved from 0.19052 to 0.18790, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/147-0.1879.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.2640 - acc: 0.9164 - val_loss: 0.1879 - val_acc: 0.9504\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9176\n",
      "Epoch 00148: val_loss did not improve from 0.18790\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2574 - acc: 0.9176 - val_loss: 0.1986 - val_acc: 0.9483\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9169\n",
      "Epoch 00149: val_loss did not improve from 0.18790\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2582 - acc: 0.9169 - val_loss: 0.1879 - val_acc: 0.9464\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9183\n",
      "Epoch 00150: val_loss did not improve from 0.18790\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2585 - acc: 0.9184 - val_loss: 0.1904 - val_acc: 0.9502\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9187\n",
      "Epoch 00151: val_loss improved from 0.18790 to 0.18733, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/151-0.1873.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2541 - acc: 0.9188 - val_loss: 0.1873 - val_acc: 0.9499\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9178\n",
      "Epoch 00152: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2569 - acc: 0.9178 - val_loss: 0.1893 - val_acc: 0.9522\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9201\n",
      "Epoch 00153: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2536 - acc: 0.9201 - val_loss: 0.2044 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9189\n",
      "Epoch 00154: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2565 - acc: 0.9189 - val_loss: 0.1982 - val_acc: 0.9483\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9182\n",
      "Epoch 00155: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2557 - acc: 0.9182 - val_loss: 0.1974 - val_acc: 0.9492\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9170\n",
      "Epoch 00156: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2597 - acc: 0.9170 - val_loss: 0.1915 - val_acc: 0.9506\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9191\n",
      "Epoch 00157: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2477 - acc: 0.9191 - val_loss: 0.1971 - val_acc: 0.9492\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9186\n",
      "Epoch 00158: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2514 - acc: 0.9185 - val_loss: 0.1939 - val_acc: 0.9497\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9213\n",
      "Epoch 00159: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2463 - acc: 0.9213 - val_loss: 0.2085 - val_acc: 0.9450\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9227\n",
      "Epoch 00160: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2424 - acc: 0.9227 - val_loss: 0.1919 - val_acc: 0.9525\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9212\n",
      "Epoch 00161: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2452 - acc: 0.9212 - val_loss: 0.1898 - val_acc: 0.9495\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9224\n",
      "Epoch 00162: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2453 - acc: 0.9225 - val_loss: 0.2145 - val_acc: 0.9425\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9205\n",
      "Epoch 00163: val_loss improved from 0.18733 to 0.18698, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/163-0.1870.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.2465 - acc: 0.9205 - val_loss: 0.1870 - val_acc: 0.9522\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2382 - acc: 0.9249\n",
      "Epoch 00164: val_loss did not improve from 0.18698\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2381 - acc: 0.9249 - val_loss: 0.1886 - val_acc: 0.9522\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9229\n",
      "Epoch 00165: val_loss did not improve from 0.18698\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2394 - acc: 0.9229 - val_loss: 0.1934 - val_acc: 0.9509\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9241\n",
      "Epoch 00166: val_loss did not improve from 0.18698\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2385 - acc: 0.9241 - val_loss: 0.2052 - val_acc: 0.9488\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9226\n",
      "Epoch 00167: val_loss did not improve from 0.18698\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2400 - acc: 0.9226 - val_loss: 0.2054 - val_acc: 0.9483\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9229\n",
      "Epoch 00168: val_loss did not improve from 0.18698\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2383 - acc: 0.9229 - val_loss: 0.1925 - val_acc: 0.9502\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9243\n",
      "Epoch 00169: val_loss improved from 0.18698 to 0.18400, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/169-0.1840.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.2380 - acc: 0.9243 - val_loss: 0.1840 - val_acc: 0.9520\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9252\n",
      "Epoch 00170: val_loss did not improve from 0.18400\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2362 - acc: 0.9252 - val_loss: 0.1881 - val_acc: 0.9515\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9237\n",
      "Epoch 00171: val_loss improved from 0.18400 to 0.18326, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/171-0.1833.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2361 - acc: 0.9237 - val_loss: 0.1833 - val_acc: 0.9532\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9248\n",
      "Epoch 00172: val_loss did not improve from 0.18326\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2348 - acc: 0.9248 - val_loss: 0.1849 - val_acc: 0.9529\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9245\n",
      "Epoch 00173: val_loss improved from 0.18326 to 0.18032, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/173-0.1803.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2329 - acc: 0.9245 - val_loss: 0.1803 - val_acc: 0.9539\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9256\n",
      "Epoch 00174: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2327 - acc: 0.9256 - val_loss: 0.1869 - val_acc: 0.9509\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9253\n",
      "Epoch 00175: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2312 - acc: 0.9253 - val_loss: 0.1909 - val_acc: 0.9515\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9242\n",
      "Epoch 00176: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2359 - acc: 0.9242 - val_loss: 0.1852 - val_acc: 0.9529\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9259\n",
      "Epoch 00177: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2294 - acc: 0.9259 - val_loss: 0.1827 - val_acc: 0.9520\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9271\n",
      "Epoch 00178: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2295 - acc: 0.9272 - val_loss: 0.1879 - val_acc: 0.9511\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9279\n",
      "Epoch 00179: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2249 - acc: 0.9279 - val_loss: 0.1962 - val_acc: 0.9511\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9272\n",
      "Epoch 00180: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2272 - acc: 0.9272 - val_loss: 0.1862 - val_acc: 0.9536\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9271\n",
      "Epoch 00181: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2268 - acc: 0.9271 - val_loss: 0.1924 - val_acc: 0.9499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9259\n",
      "Epoch 00182: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2299 - acc: 0.9259 - val_loss: 0.1826 - val_acc: 0.9532\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9285\n",
      "Epoch 00183: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2255 - acc: 0.9285 - val_loss: 0.1940 - val_acc: 0.9520\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9271\n",
      "Epoch 00184: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2252 - acc: 0.9271 - val_loss: 0.1884 - val_acc: 0.9511\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9285\n",
      "Epoch 00185: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2228 - acc: 0.9285 - val_loss: 0.1827 - val_acc: 0.9546\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9283\n",
      "Epoch 00186: val_loss did not improve from 0.18032\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2192 - acc: 0.9283 - val_loss: 0.1823 - val_acc: 0.9509\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9273\n",
      "Epoch 00187: val_loss improved from 0.18032 to 0.17704, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/187-0.1770.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2217 - acc: 0.9273 - val_loss: 0.1770 - val_acc: 0.9529\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9295\n",
      "Epoch 00188: val_loss did not improve from 0.17704\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2210 - acc: 0.9295 - val_loss: 0.1880 - val_acc: 0.9515\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9270\n",
      "Epoch 00189: val_loss did not improve from 0.17704\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2292 - acc: 0.9270 - val_loss: 0.1856 - val_acc: 0.9509\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9291\n",
      "Epoch 00190: val_loss did not improve from 0.17704\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2194 - acc: 0.9291 - val_loss: 0.2038 - val_acc: 0.9455\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9292\n",
      "Epoch 00191: val_loss did not improve from 0.17704\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2157 - acc: 0.9292 - val_loss: 0.1903 - val_acc: 0.9506\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9301\n",
      "Epoch 00192: val_loss did not improve from 0.17704\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2185 - acc: 0.9301 - val_loss: 0.1806 - val_acc: 0.9534\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9319\n",
      "Epoch 00193: val_loss did not improve from 0.17704\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2116 - acc: 0.9319 - val_loss: 0.1984 - val_acc: 0.9497\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9315\n",
      "Epoch 00194: val_loss did not improve from 0.17704\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2146 - acc: 0.9315 - val_loss: 0.1888 - val_acc: 0.9522\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9306\n",
      "Epoch 00195: val_loss did not improve from 0.17704\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2126 - acc: 0.9306 - val_loss: 0.1839 - val_acc: 0.9513\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9314\n",
      "Epoch 00196: val_loss did not improve from 0.17704\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2169 - acc: 0.9313 - val_loss: 0.1857 - val_acc: 0.9525\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9298\n",
      "Epoch 00197: val_loss improved from 0.17704 to 0.17543, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/197-0.1754.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.2184 - acc: 0.9298 - val_loss: 0.1754 - val_acc: 0.9522\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9298\n",
      "Epoch 00198: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2148 - acc: 0.9298 - val_loss: 0.1961 - val_acc: 0.9485\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9314\n",
      "Epoch 00199: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2102 - acc: 0.9314 - val_loss: 0.1754 - val_acc: 0.9548\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9307\n",
      "Epoch 00200: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2106 - acc: 0.9307 - val_loss: 0.1850 - val_acc: 0.9518\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9296\n",
      "Epoch 00201: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2125 - acc: 0.9295 - val_loss: 0.1878 - val_acc: 0.9513\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9320\n",
      "Epoch 00202: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2111 - acc: 0.9319 - val_loss: 0.1768 - val_acc: 0.9553\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9324\n",
      "Epoch 00203: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2061 - acc: 0.9324 - val_loss: 0.1793 - val_acc: 0.9532\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9318\n",
      "Epoch 00204: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2074 - acc: 0.9318 - val_loss: 0.1885 - val_acc: 0.9497\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9296\n",
      "Epoch 00205: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2131 - acc: 0.9296 - val_loss: 0.1804 - val_acc: 0.9543\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9345\n",
      "Epoch 00206: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2021 - acc: 0.9345 - val_loss: 0.1872 - val_acc: 0.9527\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9309\n",
      "Epoch 00207: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2055 - acc: 0.9309 - val_loss: 0.1851 - val_acc: 0.9534\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9314\n",
      "Epoch 00208: val_loss did not improve from 0.17543\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2103 - acc: 0.9314 - val_loss: 0.1795 - val_acc: 0.9518\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9333\n",
      "Epoch 00209: val_loss improved from 0.17543 to 0.17481, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/209-0.1748.hdf5\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.2040 - acc: 0.9333 - val_loss: 0.1748 - val_acc: 0.9536\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9326\n",
      "Epoch 00210: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2033 - acc: 0.9326 - val_loss: 0.1774 - val_acc: 0.9534\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9337\n",
      "Epoch 00211: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2043 - acc: 0.9337 - val_loss: 0.1797 - val_acc: 0.9509\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9340\n",
      "Epoch 00212: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2006 - acc: 0.9340 - val_loss: 0.1805 - val_acc: 0.9515\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9341\n",
      "Epoch 00213: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2039 - acc: 0.9341 - val_loss: 0.1778 - val_acc: 0.9541\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9377\n",
      "Epoch 00214: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1954 - acc: 0.9378 - val_loss: 0.1835 - val_acc: 0.9527\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9343\n",
      "Epoch 00215: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2028 - acc: 0.9343 - val_loss: 0.1815 - val_acc: 0.9532\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9366\n",
      "Epoch 00216: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1974 - acc: 0.9366 - val_loss: 0.1904 - val_acc: 0.9536\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9356\n",
      "Epoch 00217: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1965 - acc: 0.9356 - val_loss: 0.1885 - val_acc: 0.9499\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9347\n",
      "Epoch 00218: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1970 - acc: 0.9347 - val_loss: 0.1875 - val_acc: 0.9541\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9372\n",
      "Epoch 00219: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1951 - acc: 0.9372 - val_loss: 0.1874 - val_acc: 0.9546\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9356\n",
      "Epoch 00220: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1995 - acc: 0.9356 - val_loss: 0.1844 - val_acc: 0.9543\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9354\n",
      "Epoch 00221: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1974 - acc: 0.9354 - val_loss: 0.1828 - val_acc: 0.9527\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9365\n",
      "Epoch 00222: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1970 - acc: 0.9365 - val_loss: 0.1836 - val_acc: 0.9506\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9355\n",
      "Epoch 00223: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1978 - acc: 0.9355 - val_loss: 0.1763 - val_acc: 0.9557\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9370\n",
      "Epoch 00224: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1944 - acc: 0.9370 - val_loss: 0.1867 - val_acc: 0.9529\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9354\n",
      "Epoch 00225: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1955 - acc: 0.9354 - val_loss: 0.1751 - val_acc: 0.9534\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9359\n",
      "Epoch 00226: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1940 - acc: 0.9359 - val_loss: 0.1968 - val_acc: 0.9518\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9366\n",
      "Epoch 00227: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1902 - acc: 0.9366 - val_loss: 0.1818 - val_acc: 0.9562\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9375\n",
      "Epoch 00228: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1912 - acc: 0.9375 - val_loss: 0.1803 - val_acc: 0.9536\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9368\n",
      "Epoch 00229: val_loss improved from 0.17481 to 0.17317, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/229-0.1732.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1927 - acc: 0.9369 - val_loss: 0.1732 - val_acc: 0.9525\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9381\n",
      "Epoch 00230: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1876 - acc: 0.9381 - val_loss: 0.1900 - val_acc: 0.9525\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9376\n",
      "Epoch 00231: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1865 - acc: 0.9375 - val_loss: 0.1966 - val_acc: 0.9515\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9376\n",
      "Epoch 00232: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1908 - acc: 0.9376 - val_loss: 0.1771 - val_acc: 0.9520\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9378\n",
      "Epoch 00233: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1882 - acc: 0.9378 - val_loss: 0.1887 - val_acc: 0.9513\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9370\n",
      "Epoch 00234: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1899 - acc: 0.9370 - val_loss: 0.1912 - val_acc: 0.9513\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9388\n",
      "Epoch 00235: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1885 - acc: 0.9388 - val_loss: 0.2004 - val_acc: 0.9511\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9389\n",
      "Epoch 00236: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1862 - acc: 0.9389 - val_loss: 0.1852 - val_acc: 0.9539\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9416\n",
      "Epoch 00237: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1821 - acc: 0.9416 - val_loss: 0.1878 - val_acc: 0.9534\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9385\n",
      "Epoch 00238: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1854 - acc: 0.9385 - val_loss: 0.1852 - val_acc: 0.9525\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9401\n",
      "Epoch 00239: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1807 - acc: 0.9401 - val_loss: 0.1837 - val_acc: 0.9576\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9402\n",
      "Epoch 00240: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1830 - acc: 0.9402 - val_loss: 0.1841 - val_acc: 0.9548\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9389\n",
      "Epoch 00241: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1827 - acc: 0.9389 - val_loss: 0.1785 - val_acc: 0.9520\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9397\n",
      "Epoch 00242: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1829 - acc: 0.9397 - val_loss: 0.1882 - val_acc: 0.9532\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9405\n",
      "Epoch 00243: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1824 - acc: 0.9404 - val_loss: 0.1886 - val_acc: 0.9541\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9405\n",
      "Epoch 00244: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1801 - acc: 0.9406 - val_loss: 0.1821 - val_acc: 0.9553\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9405\n",
      "Epoch 00245: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1819 - acc: 0.9405 - val_loss: 0.1836 - val_acc: 0.9550\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9360\n",
      "Epoch 00246: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1884 - acc: 0.9360 - val_loss: 0.1773 - val_acc: 0.9522\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9386\n",
      "Epoch 00247: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1835 - acc: 0.9386 - val_loss: 0.1845 - val_acc: 0.9532\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9409\n",
      "Epoch 00248: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1788 - acc: 0.9409 - val_loss: 0.1807 - val_acc: 0.9539\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9393\n",
      "Epoch 00249: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1848 - acc: 0.9394 - val_loss: 0.1835 - val_acc: 0.9529\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9401\n",
      "Epoch 00250: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1814 - acc: 0.9401 - val_loss: 0.1734 - val_acc: 0.9543\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9435\n",
      "Epoch 00251: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1771 - acc: 0.9435 - val_loss: 0.1919 - val_acc: 0.9539\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9414\n",
      "Epoch 00252: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1801 - acc: 0.9414 - val_loss: 0.1752 - val_acc: 0.9532\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9411\n",
      "Epoch 00253: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1804 - acc: 0.9411 - val_loss: 0.1783 - val_acc: 0.9543\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9417\n",
      "Epoch 00254: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1743 - acc: 0.9417 - val_loss: 0.1873 - val_acc: 0.9543\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9413\n",
      "Epoch 00255: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1768 - acc: 0.9413 - val_loss: 0.1756 - val_acc: 0.9527\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9420\n",
      "Epoch 00256: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1753 - acc: 0.9420 - val_loss: 0.1869 - val_acc: 0.9522\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9412\n",
      "Epoch 00257: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1789 - acc: 0.9411 - val_loss: 0.1894 - val_acc: 0.9515\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9419\n",
      "Epoch 00258: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1762 - acc: 0.9419 - val_loss: 0.1816 - val_acc: 0.9525\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9399\n",
      "Epoch 00259: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1806 - acc: 0.9398 - val_loss: 0.2028 - val_acc: 0.9518\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9422\n",
      "Epoch 00260: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1715 - acc: 0.9422 - val_loss: 0.1872 - val_acc: 0.9527\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9428\n",
      "Epoch 00261: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1731 - acc: 0.9428 - val_loss: 0.1756 - val_acc: 0.9555\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9458\n",
      "Epoch 00262: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1668 - acc: 0.9458 - val_loss: 0.1855 - val_acc: 0.9541\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9429\n",
      "Epoch 00263: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1714 - acc: 0.9429 - val_loss: 0.1945 - val_acc: 0.9532\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9421\n",
      "Epoch 00264: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1750 - acc: 0.9421 - val_loss: 0.1866 - val_acc: 0.9529\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9434\n",
      "Epoch 00265: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1678 - acc: 0.9434 - val_loss: 0.1957 - val_acc: 0.9495\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9434\n",
      "Epoch 00266: val_loss did not improve from 0.17317\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1707 - acc: 0.9434 - val_loss: 0.1852 - val_acc: 0.9534\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9445\n",
      "Epoch 00267: val_loss improved from 0.17317 to 0.17224, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_7_conv_checkpoint/267-0.1722.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1680 - acc: 0.9445 - val_loss: 0.1722 - val_acc: 0.9576\n",
      "Epoch 268/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9447\n",
      "Epoch 00268: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1674 - acc: 0.9447 - val_loss: 0.1807 - val_acc: 0.9546\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9448\n",
      "Epoch 00269: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1668 - acc: 0.9448 - val_loss: 0.1829 - val_acc: 0.9532\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9429\n",
      "Epoch 00270: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1685 - acc: 0.9429 - val_loss: 0.1780 - val_acc: 0.9529\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9416\n",
      "Epoch 00271: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1735 - acc: 0.9416 - val_loss: 0.1880 - val_acc: 0.9520\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9448\n",
      "Epoch 00272: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1683 - acc: 0.9448 - val_loss: 0.1784 - val_acc: 0.9536\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9476\n",
      "Epoch 00273: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1592 - acc: 0.9476 - val_loss: 0.1822 - val_acc: 0.9541\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9451\n",
      "Epoch 00274: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1669 - acc: 0.9451 - val_loss: 0.1860 - val_acc: 0.9515\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9453\n",
      "Epoch 00275: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1632 - acc: 0.9453 - val_loss: 0.1765 - val_acc: 0.9536\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9455\n",
      "Epoch 00276: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1651 - acc: 0.9455 - val_loss: 0.1796 - val_acc: 0.9543\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9441\n",
      "Epoch 00277: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1689 - acc: 0.9441 - val_loss: 0.1932 - val_acc: 0.9532\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9449\n",
      "Epoch 00278: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1628 - acc: 0.9450 - val_loss: 0.1993 - val_acc: 0.9536\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9444\n",
      "Epoch 00279: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1659 - acc: 0.9444 - val_loss: 0.1770 - val_acc: 0.9525\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9439\n",
      "Epoch 00280: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1673 - acc: 0.9439 - val_loss: 0.1960 - val_acc: 0.9513\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9459\n",
      "Epoch 00281: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1640 - acc: 0.9458 - val_loss: 0.1857 - val_acc: 0.9550\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9457\n",
      "Epoch 00282: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1626 - acc: 0.9457 - val_loss: 0.2075 - val_acc: 0.9497\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9475\n",
      "Epoch 00283: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1593 - acc: 0.9475 - val_loss: 0.2020 - val_acc: 0.9520\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9458\n",
      "Epoch 00284: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.1638 - acc: 0.9458 - val_loss: 0.1777 - val_acc: 0.9550\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9461\n",
      "Epoch 00285: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1618 - acc: 0.9461 - val_loss: 0.1957 - val_acc: 0.9536\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9451\n",
      "Epoch 00286: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1613 - acc: 0.9451 - val_loss: 0.1925 - val_acc: 0.9522\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9449\n",
      "Epoch 00287: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1621 - acc: 0.9449 - val_loss: 0.1871 - val_acc: 0.9546\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9479\n",
      "Epoch 00288: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1576 - acc: 0.9479 - val_loss: 0.1816 - val_acc: 0.9553\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9469\n",
      "Epoch 00289: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1584 - acc: 0.9469 - val_loss: 0.1820 - val_acc: 0.9546\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9481\n",
      "Epoch 00290: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1586 - acc: 0.9481 - val_loss: 0.1953 - val_acc: 0.9543\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9464\n",
      "Epoch 00291: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1597 - acc: 0.9463 - val_loss: 0.1828 - val_acc: 0.9511\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9484\n",
      "Epoch 00292: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1536 - acc: 0.9484 - val_loss: 0.1745 - val_acc: 0.9574\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9486\n",
      "Epoch 00293: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1552 - acc: 0.9486 - val_loss: 0.1960 - val_acc: 0.9513\n",
      "Epoch 294/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9474\n",
      "Epoch 00294: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1584 - acc: 0.9474 - val_loss: 0.1796 - val_acc: 0.9536\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9493\n",
      "Epoch 00295: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1512 - acc: 0.9494 - val_loss: 0.1971 - val_acc: 0.9529\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9472\n",
      "Epoch 00296: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1587 - acc: 0.9472 - val_loss: 0.1758 - val_acc: 0.9562\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9477\n",
      "Epoch 00297: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1526 - acc: 0.9477 - val_loss: 0.1836 - val_acc: 0.9574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1568 - acc: 0.9467\n",
      "Epoch 00298: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1568 - acc: 0.9467 - val_loss: 0.1790 - val_acc: 0.9569\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9504\n",
      "Epoch 00299: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1514 - acc: 0.9504 - val_loss: 0.1825 - val_acc: 0.9539\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9499\n",
      "Epoch 00300: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1503 - acc: 0.9499 - val_loss: 0.2011 - val_acc: 0.9525\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9488\n",
      "Epoch 00301: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1527 - acc: 0.9488 - val_loss: 0.1871 - val_acc: 0.9548\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9486\n",
      "Epoch 00302: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1562 - acc: 0.9486 - val_loss: 0.1845 - val_acc: 0.9539\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1568 - acc: 0.9478\n",
      "Epoch 00303: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1568 - acc: 0.9478 - val_loss: 0.1830 - val_acc: 0.9550\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9485\n",
      "Epoch 00304: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1545 - acc: 0.9485 - val_loss: 0.1840 - val_acc: 0.9560\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9503\n",
      "Epoch 00305: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1491 - acc: 0.9503 - val_loss: 0.2042 - val_acc: 0.9541\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9474\n",
      "Epoch 00306: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1603 - acc: 0.9475 - val_loss: 0.1858 - val_acc: 0.9534\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9492\n",
      "Epoch 00307: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1527 - acc: 0.9492 - val_loss: 0.1970 - val_acc: 0.9550\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9489\n",
      "Epoch 00308: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1523 - acc: 0.9489 - val_loss: 0.1758 - val_acc: 0.9553\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9480\n",
      "Epoch 00309: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1535 - acc: 0.9480 - val_loss: 0.1871 - val_acc: 0.9560\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9502\n",
      "Epoch 00310: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1516 - acc: 0.9502 - val_loss: 0.1855 - val_acc: 0.9560\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9504\n",
      "Epoch 00311: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1487 - acc: 0.9504 - val_loss: 0.1925 - val_acc: 0.9520\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9506\n",
      "Epoch 00312: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1454 - acc: 0.9506 - val_loss: 0.2018 - val_acc: 0.9532\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9475\n",
      "Epoch 00313: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1604 - acc: 0.9475 - val_loss: 0.1826 - val_acc: 0.9553\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9489\n",
      "Epoch 00314: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1495 - acc: 0.9489 - val_loss: 0.1897 - val_acc: 0.9543\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9500\n",
      "Epoch 00315: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1482 - acc: 0.9500 - val_loss: 0.2061 - val_acc: 0.9562\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9519\n",
      "Epoch 00316: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1441 - acc: 0.9519 - val_loss: 0.1910 - val_acc: 0.9557\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9523\n",
      "Epoch 00317: val_loss did not improve from 0.17224\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1444 - acc: 0.9523 - val_loss: 0.1902 - val_acc: 0.9546\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 423us/sample - loss: 0.1965 - acc: 0.9448\n",
      "Loss: 0.19645620453889992 Accuracy: 0.944756\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6600 - acc: 0.1160\n",
      "Epoch 00001: val_loss improved from inf to 2.22986, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/001-2.2299.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 2.6600 - acc: 0.1160 - val_loss: 2.2299 - val_acc: 0.3051\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1306 - acc: 0.2978\n",
      "Epoch 00002: val_loss improved from 2.22986 to 1.67703, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/002-1.6770.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 2.1305 - acc: 0.2978 - val_loss: 1.6770 - val_acc: 0.5139\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7895 - acc: 0.4079\n",
      "Epoch 00003: val_loss improved from 1.67703 to 1.34531, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/003-1.3453.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.7894 - acc: 0.4079 - val_loss: 1.3453 - val_acc: 0.5973\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5931 - acc: 0.4717\n",
      "Epoch 00004: val_loss improved from 1.34531 to 1.17742, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/004-1.1774.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.5930 - acc: 0.4718 - val_loss: 1.1774 - val_acc: 0.6427\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4469 - acc: 0.5182\n",
      "Epoch 00005: val_loss improved from 1.17742 to 1.04277, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/005-1.0428.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.4471 - acc: 0.5182 - val_loss: 1.0428 - val_acc: 0.6902\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3393 - acc: 0.5550\n",
      "Epoch 00006: val_loss improved from 1.04277 to 0.96248, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/006-0.9625.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.3393 - acc: 0.5550 - val_loss: 0.9625 - val_acc: 0.7195\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2407 - acc: 0.5912\n",
      "Epoch 00007: val_loss improved from 0.96248 to 0.86685, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/007-0.8669.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.2406 - acc: 0.5913 - val_loss: 0.8669 - val_acc: 0.7501\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1575 - acc: 0.6194\n",
      "Epoch 00008: val_loss improved from 0.86685 to 0.78068, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/008-0.7807.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.1574 - acc: 0.6194 - val_loss: 0.7807 - val_acc: 0.7671\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0977 - acc: 0.6421\n",
      "Epoch 00009: val_loss improved from 0.78068 to 0.74837, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/009-0.7484.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 1.0977 - acc: 0.6421 - val_loss: 0.7484 - val_acc: 0.7824\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0401 - acc: 0.6593\n",
      "Epoch 00010: val_loss improved from 0.74837 to 0.70731, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/010-0.7073.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.0400 - acc: 0.6594 - val_loss: 0.7073 - val_acc: 0.7796\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9874 - acc: 0.6796\n",
      "Epoch 00011: val_loss improved from 0.70731 to 0.65153, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/011-0.6515.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.9876 - acc: 0.6794 - val_loss: 0.6515 - val_acc: 0.8083\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9397 - acc: 0.6949\n",
      "Epoch 00012: val_loss improved from 0.65153 to 0.62586, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/012-0.6259.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.9399 - acc: 0.6949 - val_loss: 0.6259 - val_acc: 0.8204\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9096 - acc: 0.7083\n",
      "Epoch 00013: val_loss improved from 0.62586 to 0.57937, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/013-0.5794.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.9095 - acc: 0.7083 - val_loss: 0.5794 - val_acc: 0.8262\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8678 - acc: 0.7237\n",
      "Epoch 00014: val_loss improved from 0.57937 to 0.57201, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/014-0.5720.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.8677 - acc: 0.7237 - val_loss: 0.5720 - val_acc: 0.8337\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8459 - acc: 0.7281\n",
      "Epoch 00015: val_loss improved from 0.57201 to 0.53192, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/015-0.5319.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.8459 - acc: 0.7281 - val_loss: 0.5319 - val_acc: 0.8423\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8205 - acc: 0.7374\n",
      "Epoch 00016: val_loss improved from 0.53192 to 0.50104, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/016-0.5010.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.8205 - acc: 0.7374 - val_loss: 0.5010 - val_acc: 0.8463\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7831 - acc: 0.7486\n",
      "Epoch 00017: val_loss improved from 0.50104 to 0.49214, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/017-0.4921.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.7830 - acc: 0.7486 - val_loss: 0.4921 - val_acc: 0.8523\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7550 - acc: 0.7616\n",
      "Epoch 00018: val_loss improved from 0.49214 to 0.46869, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/018-0.4687.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.7550 - acc: 0.7616 - val_loss: 0.4687 - val_acc: 0.8588\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7413 - acc: 0.7638\n",
      "Epoch 00019: val_loss improved from 0.46869 to 0.44762, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/019-0.4476.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.7413 - acc: 0.7637 - val_loss: 0.4476 - val_acc: 0.8721\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7227 - acc: 0.7688\n",
      "Epoch 00020: val_loss did not improve from 0.44762\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.7227 - acc: 0.7688 - val_loss: 0.4805 - val_acc: 0.8602\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7023 - acc: 0.7780\n",
      "Epoch 00021: val_loss improved from 0.44762 to 0.44002, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/021-0.4400.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.7022 - acc: 0.7780 - val_loss: 0.4400 - val_acc: 0.8665\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.7801\n",
      "Epoch 00022: val_loss improved from 0.44002 to 0.43702, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/022-0.4370.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.6921 - acc: 0.7801 - val_loss: 0.4370 - val_acc: 0.8712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6667 - acc: 0.7871\n",
      "Epoch 00023: val_loss improved from 0.43702 to 0.38021, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/023-0.3802.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.6666 - acc: 0.7871 - val_loss: 0.3802 - val_acc: 0.8854\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6552 - acc: 0.7943\n",
      "Epoch 00024: val_loss improved from 0.38021 to 0.37371, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/024-0.3737.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.6551 - acc: 0.7943 - val_loss: 0.3737 - val_acc: 0.8901\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6361 - acc: 0.7977\n",
      "Epoch 00025: val_loss improved from 0.37371 to 0.35791, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/025-0.3579.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.6361 - acc: 0.7977 - val_loss: 0.3579 - val_acc: 0.8942\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6284 - acc: 0.8006\n",
      "Epoch 00026: val_loss improved from 0.35791 to 0.35384, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/026-0.3538.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.6283 - acc: 0.8006 - val_loss: 0.3538 - val_acc: 0.8938\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6133 - acc: 0.8062\n",
      "Epoch 00027: val_loss did not improve from 0.35384\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.6133 - acc: 0.8062 - val_loss: 0.3872 - val_acc: 0.8901\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5978 - acc: 0.8114\n",
      "Epoch 00028: val_loss did not improve from 0.35384\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.5978 - acc: 0.8114 - val_loss: 0.3556 - val_acc: 0.8910\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5947 - acc: 0.8124\n",
      "Epoch 00029: val_loss improved from 0.35384 to 0.33656, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/029-0.3366.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.5946 - acc: 0.8124 - val_loss: 0.3366 - val_acc: 0.9054\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5747 - acc: 0.8234\n",
      "Epoch 00030: val_loss improved from 0.33656 to 0.33212, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/030-0.3321.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.5751 - acc: 0.8233 - val_loss: 0.3321 - val_acc: 0.9096\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5731 - acc: 0.8219\n",
      "Epoch 00031: val_loss improved from 0.33212 to 0.31065, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/031-0.3107.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.5731 - acc: 0.8220 - val_loss: 0.3107 - val_acc: 0.9110\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.8285\n",
      "Epoch 00032: val_loss improved from 0.31065 to 0.30295, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/032-0.3030.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.5481 - acc: 0.8285 - val_loss: 0.3030 - val_acc: 0.9166\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.8299\n",
      "Epoch 00033: val_loss improved from 0.30295 to 0.30288, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/033-0.3029.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.5470 - acc: 0.8299 - val_loss: 0.3029 - val_acc: 0.9159\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.8316\n",
      "Epoch 00034: val_loss improved from 0.30288 to 0.28472, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/034-0.2847.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.5321 - acc: 0.8316 - val_loss: 0.2847 - val_acc: 0.9201\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.8351\n",
      "Epoch 00035: val_loss improved from 0.28472 to 0.28207, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/035-0.2821.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.5246 - acc: 0.8351 - val_loss: 0.2821 - val_acc: 0.9213\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.8382\n",
      "Epoch 00036: val_loss improved from 0.28207 to 0.27443, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/036-0.2744.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.5203 - acc: 0.8383 - val_loss: 0.2744 - val_acc: 0.9245\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4997 - acc: 0.8451\n",
      "Epoch 00037: val_loss improved from 0.27443 to 0.27419, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/037-0.2742.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.4997 - acc: 0.8452 - val_loss: 0.2742 - val_acc: 0.9264\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.8409\n",
      "Epoch 00038: val_loss improved from 0.27419 to 0.27075, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/038-0.2707.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.5003 - acc: 0.8409 - val_loss: 0.2707 - val_acc: 0.9276\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4915 - acc: 0.8457\n",
      "Epoch 00039: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.4915 - acc: 0.8457 - val_loss: 0.2861 - val_acc: 0.9187\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4847 - acc: 0.8491\n",
      "Epoch 00040: val_loss did not improve from 0.27075\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4847 - acc: 0.8491 - val_loss: 0.3185 - val_acc: 0.9099\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4741 - acc: 0.8505\n",
      "Epoch 00041: val_loss improved from 0.27075 to 0.27008, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/041-0.2701.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.4740 - acc: 0.8506 - val_loss: 0.2701 - val_acc: 0.9236\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4775 - acc: 0.8497\n",
      "Epoch 00042: val_loss improved from 0.27008 to 0.24664, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/042-0.2466.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.4775 - acc: 0.8497 - val_loss: 0.2466 - val_acc: 0.9331\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4683 - acc: 0.8556\n",
      "Epoch 00043: val_loss did not improve from 0.24664\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4683 - acc: 0.8556 - val_loss: 0.2516 - val_acc: 0.9315\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4533 - acc: 0.8576\n",
      "Epoch 00044: val_loss improved from 0.24664 to 0.24616, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/044-0.2462.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.4532 - acc: 0.8576 - val_loss: 0.2462 - val_acc: 0.9299\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4541 - acc: 0.8579\n",
      "Epoch 00045: val_loss improved from 0.24616 to 0.24047, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/045-0.2405.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.4539 - acc: 0.8579 - val_loss: 0.2405 - val_acc: 0.9317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4457 - acc: 0.8614\n",
      "Epoch 00046: val_loss did not improve from 0.24047\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4458 - acc: 0.8614 - val_loss: 0.2499 - val_acc: 0.9320\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8626\n",
      "Epoch 00047: val_loss improved from 0.24047 to 0.22496, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/047-0.2250.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.4419 - acc: 0.8626 - val_loss: 0.2250 - val_acc: 0.9392\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4351 - acc: 0.8655\n",
      "Epoch 00048: val_loss improved from 0.22496 to 0.22010, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/048-0.2201.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4350 - acc: 0.8655 - val_loss: 0.2201 - val_acc: 0.9397\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8655\n",
      "Epoch 00049: val_loss did not improve from 0.22010\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4299 - acc: 0.8655 - val_loss: 0.2255 - val_acc: 0.9357\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4255 - acc: 0.8671\n",
      "Epoch 00050: val_loss did not improve from 0.22010\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.4255 - acc: 0.8671 - val_loss: 0.2578 - val_acc: 0.9294\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8696\n",
      "Epoch 00051: val_loss did not improve from 0.22010\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4220 - acc: 0.8696 - val_loss: 0.2247 - val_acc: 0.9390\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8723\n",
      "Epoch 00052: val_loss improved from 0.22010 to 0.21856, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/052-0.2186.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.4112 - acc: 0.8723 - val_loss: 0.2186 - val_acc: 0.9373\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8725\n",
      "Epoch 00053: val_loss improved from 0.21856 to 0.21604, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/053-0.2160.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.4097 - acc: 0.8725 - val_loss: 0.2160 - val_acc: 0.9366\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8740\n",
      "Epoch 00054: val_loss improved from 0.21604 to 0.21454, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/054-0.2145.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.4004 - acc: 0.8740 - val_loss: 0.2145 - val_acc: 0.9406\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8759\n",
      "Epoch 00055: val_loss improved from 0.21454 to 0.20340, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/055-0.2034.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.3965 - acc: 0.8759 - val_loss: 0.2034 - val_acc: 0.9429\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.8787\n",
      "Epoch 00056: val_loss improved from 0.20340 to 0.20018, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/056-0.2002.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.3906 - acc: 0.8787 - val_loss: 0.2002 - val_acc: 0.9429\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8783\n",
      "Epoch 00057: val_loss did not improve from 0.20018\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.3899 - acc: 0.8783 - val_loss: 0.2034 - val_acc: 0.9427\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8818\n",
      "Epoch 00058: val_loss improved from 0.20018 to 0.19580, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/058-0.1958.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3820 - acc: 0.8819 - val_loss: 0.1958 - val_acc: 0.9455\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8839\n",
      "Epoch 00059: val_loss improved from 0.19580 to 0.19459, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/059-0.1946.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.3782 - acc: 0.8840 - val_loss: 0.1946 - val_acc: 0.9434\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8839\n",
      "Epoch 00060: val_loss improved from 0.19459 to 0.19270, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/060-0.1927.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.3726 - acc: 0.8839 - val_loss: 0.1927 - val_acc: 0.9443\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3698 - acc: 0.8841\n",
      "Epoch 00061: val_loss did not improve from 0.19270\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3698 - acc: 0.8841 - val_loss: 0.1943 - val_acc: 0.9448\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3659 - acc: 0.8849\n",
      "Epoch 00062: val_loss did not improve from 0.19270\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3658 - acc: 0.8849 - val_loss: 0.2166 - val_acc: 0.9371\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8856\n",
      "Epoch 00063: val_loss did not improve from 0.19270\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3658 - acc: 0.8856 - val_loss: 0.2091 - val_acc: 0.9411\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8876\n",
      "Epoch 00064: val_loss did not improve from 0.19270\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3594 - acc: 0.8876 - val_loss: 0.1967 - val_acc: 0.9408\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.8889\n",
      "Epoch 00065: val_loss improved from 0.19270 to 0.19032, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/065-0.1903.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3540 - acc: 0.8889 - val_loss: 0.1903 - val_acc: 0.9460\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8907\n",
      "Epoch 00066: val_loss improved from 0.19032 to 0.18460, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/066-0.1846.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.3503 - acc: 0.8907 - val_loss: 0.1846 - val_acc: 0.9460\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8918\n",
      "Epoch 00067: val_loss improved from 0.18460 to 0.18264, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/067-0.1826.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.3502 - acc: 0.8919 - val_loss: 0.1826 - val_acc: 0.9462\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8931\n",
      "Epoch 00068: val_loss improved from 0.18264 to 0.17648, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/068-0.1765.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.3438 - acc: 0.8931 - val_loss: 0.1765 - val_acc: 0.9483\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.8918\n",
      "Epoch 00069: val_loss did not improve from 0.17648\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.3390 - acc: 0.8918 - val_loss: 0.1817 - val_acc: 0.9471\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8941\n",
      "Epoch 00070: val_loss did not improve from 0.17648\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.3395 - acc: 0.8941 - val_loss: 0.1814 - val_acc: 0.9471\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8951\n",
      "Epoch 00071: val_loss improved from 0.17648 to 0.17448, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/071-0.1745.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.3362 - acc: 0.8951 - val_loss: 0.1745 - val_acc: 0.9492\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.8959\n",
      "Epoch 00072: val_loss did not improve from 0.17448\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.3286 - acc: 0.8960 - val_loss: 0.1788 - val_acc: 0.9488\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.8960\n",
      "Epoch 00073: val_loss improved from 0.17448 to 0.17346, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/073-0.1735.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3313 - acc: 0.8960 - val_loss: 0.1735 - val_acc: 0.9495\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8982\n",
      "Epoch 00074: val_loss did not improve from 0.17346\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3253 - acc: 0.8982 - val_loss: 0.1861 - val_acc: 0.9443\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8971\n",
      "Epoch 00075: val_loss did not improve from 0.17346\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3266 - acc: 0.8971 - val_loss: 0.1817 - val_acc: 0.9474\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8989\n",
      "Epoch 00076: val_loss did not improve from 0.17346\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3207 - acc: 0.8989 - val_loss: 0.1762 - val_acc: 0.9488\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.9026\n",
      "Epoch 00077: val_loss did not improve from 0.17346\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3132 - acc: 0.9025 - val_loss: 0.1743 - val_acc: 0.9485\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.9019\n",
      "Epoch 00078: val_loss improved from 0.17346 to 0.17016, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/078-0.1702.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.3083 - acc: 0.9019 - val_loss: 0.1702 - val_acc: 0.9499\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3077 - acc: 0.9045\n",
      "Epoch 00079: val_loss did not improve from 0.17016\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3077 - acc: 0.9045 - val_loss: 0.1819 - val_acc: 0.9457\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9039\n",
      "Epoch 00080: val_loss did not improve from 0.17016\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3057 - acc: 0.9039 - val_loss: 0.1861 - val_acc: 0.9453\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.9041\n",
      "Epoch 00081: val_loss improved from 0.17016 to 0.16648, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/081-0.1665.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.3013 - acc: 0.9041 - val_loss: 0.1665 - val_acc: 0.9511\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9055\n",
      "Epoch 00082: val_loss did not improve from 0.16648\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3033 - acc: 0.9054 - val_loss: 0.1780 - val_acc: 0.9481\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.9037\n",
      "Epoch 00083: val_loss did not improve from 0.16648\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3052 - acc: 0.9037 - val_loss: 0.1834 - val_acc: 0.9485\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9052\n",
      "Epoch 00084: val_loss did not improve from 0.16648\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2982 - acc: 0.9052 - val_loss: 0.1710 - val_acc: 0.9478\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9049\n",
      "Epoch 00085: val_loss did not improve from 0.16648\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2995 - acc: 0.9049 - val_loss: 0.1728 - val_acc: 0.9483\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9085\n",
      "Epoch 00086: val_loss did not improve from 0.16648\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2929 - acc: 0.9085 - val_loss: 0.1736 - val_acc: 0.9471\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9079\n",
      "Epoch 00087: val_loss improved from 0.16648 to 0.16472, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/087-0.1647.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2872 - acc: 0.9078 - val_loss: 0.1647 - val_acc: 0.9511\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9071\n",
      "Epoch 00088: val_loss improved from 0.16472 to 0.15626, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/088-0.1563.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2930 - acc: 0.9071 - val_loss: 0.1563 - val_acc: 0.9557\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9097\n",
      "Epoch 00089: val_loss did not improve from 0.15626\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2828 - acc: 0.9097 - val_loss: 0.1594 - val_acc: 0.9522\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9110\n",
      "Epoch 00090: val_loss did not improve from 0.15626\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2804 - acc: 0.9110 - val_loss: 0.1641 - val_acc: 0.9536\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9105\n",
      "Epoch 00091: val_loss did not improve from 0.15626\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2819 - acc: 0.9105 - val_loss: 0.1621 - val_acc: 0.9546\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9121\n",
      "Epoch 00092: val_loss did not improve from 0.15626\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2787 - acc: 0.9121 - val_loss: 0.1581 - val_acc: 0.9546\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9127\n",
      "Epoch 00093: val_loss did not improve from 0.15626\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2760 - acc: 0.9127 - val_loss: 0.1571 - val_acc: 0.9548\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9145\n",
      "Epoch 00094: val_loss did not improve from 0.15626\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2682 - acc: 0.9145 - val_loss: 0.1665 - val_acc: 0.9513\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.9129\n",
      "Epoch 00095: val_loss improved from 0.15626 to 0.15123, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/095-0.1512.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2770 - acc: 0.9129 - val_loss: 0.1512 - val_acc: 0.9548\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9146\n",
      "Epoch 00096: val_loss did not improve from 0.15123\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2695 - acc: 0.9146 - val_loss: 0.1651 - val_acc: 0.9543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9151\n",
      "Epoch 00097: val_loss did not improve from 0.15123\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2676 - acc: 0.9151 - val_loss: 0.1573 - val_acc: 0.9536\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9144\n",
      "Epoch 00098: val_loss did not improve from 0.15123\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2686 - acc: 0.9144 - val_loss: 0.1562 - val_acc: 0.9541\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9180\n",
      "Epoch 00099: val_loss did not improve from 0.15123\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2612 - acc: 0.9180 - val_loss: 0.1644 - val_acc: 0.9509\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9158\n",
      "Epoch 00100: val_loss did not improve from 0.15123\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2651 - acc: 0.9158 - val_loss: 0.1721 - val_acc: 0.9511\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9157\n",
      "Epoch 00101: val_loss improved from 0.15123 to 0.15010, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/101-0.1501.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2635 - acc: 0.9157 - val_loss: 0.1501 - val_acc: 0.9562\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9198\n",
      "Epoch 00102: val_loss did not improve from 0.15010\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2591 - acc: 0.9198 - val_loss: 0.1542 - val_acc: 0.9541\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9199\n",
      "Epoch 00103: val_loss did not improve from 0.15010\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2543 - acc: 0.9199 - val_loss: 0.1544 - val_acc: 0.9532\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9199\n",
      "Epoch 00104: val_loss did not improve from 0.15010\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2507 - acc: 0.9199 - val_loss: 0.1538 - val_acc: 0.9564\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9192\n",
      "Epoch 00105: val_loss did not improve from 0.15010\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2527 - acc: 0.9193 - val_loss: 0.1583 - val_acc: 0.9534\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9188\n",
      "Epoch 00106: val_loss did not improve from 0.15010\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2516 - acc: 0.9188 - val_loss: 0.1591 - val_acc: 0.9534\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2495 - acc: 0.9203\n",
      "Epoch 00107: val_loss did not improve from 0.15010\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2495 - acc: 0.9203 - val_loss: 0.1545 - val_acc: 0.9553\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9228\n",
      "Epoch 00108: val_loss improved from 0.15010 to 0.14894, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/108-0.1489.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2453 - acc: 0.9228 - val_loss: 0.1489 - val_acc: 0.9571\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9239\n",
      "Epoch 00109: val_loss improved from 0.14894 to 0.14581, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/109-0.1458.hdf5\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.2435 - acc: 0.9239 - val_loss: 0.1458 - val_acc: 0.9562\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9226\n",
      "Epoch 00110: val_loss did not improve from 0.14581\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2409 - acc: 0.9225 - val_loss: 0.1483 - val_acc: 0.9581\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9211\n",
      "Epoch 00111: val_loss did not improve from 0.14581\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2457 - acc: 0.9211 - val_loss: 0.1479 - val_acc: 0.9592\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9238\n",
      "Epoch 00112: val_loss did not improve from 0.14581\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2359 - acc: 0.9238 - val_loss: 0.1488 - val_acc: 0.9550\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9229\n",
      "Epoch 00113: val_loss did not improve from 0.14581\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2443 - acc: 0.9229 - val_loss: 0.1483 - val_acc: 0.9576\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9246\n",
      "Epoch 00114: val_loss improved from 0.14581 to 0.14337, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/114-0.1434.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2355 - acc: 0.9247 - val_loss: 0.1434 - val_acc: 0.9546\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9250\n",
      "Epoch 00115: val_loss improved from 0.14337 to 0.13973, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/115-0.1397.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2344 - acc: 0.9250 - val_loss: 0.1397 - val_acc: 0.9588\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9240\n",
      "Epoch 00116: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2358 - acc: 0.9240 - val_loss: 0.1468 - val_acc: 0.9567\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9260\n",
      "Epoch 00117: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2326 - acc: 0.9260 - val_loss: 0.1594 - val_acc: 0.9529\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9260\n",
      "Epoch 00118: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2331 - acc: 0.9260 - val_loss: 0.1442 - val_acc: 0.9590\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9279\n",
      "Epoch 00119: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2256 - acc: 0.9279 - val_loss: 0.1490 - val_acc: 0.9588\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9279\n",
      "Epoch 00120: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2263 - acc: 0.9279 - val_loss: 0.1448 - val_acc: 0.9571\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9269\n",
      "Epoch 00121: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2265 - acc: 0.9269 - val_loss: 0.1468 - val_acc: 0.9571\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9294\n",
      "Epoch 00122: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2234 - acc: 0.9294 - val_loss: 0.1460 - val_acc: 0.9571\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9261\n",
      "Epoch 00123: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2280 - acc: 0.9261 - val_loss: 0.1516 - val_acc: 0.9576\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9293\n",
      "Epoch 00124: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2195 - acc: 0.9293 - val_loss: 0.1450 - val_acc: 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9287\n",
      "Epoch 00125: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2213 - acc: 0.9288 - val_loss: 0.1420 - val_acc: 0.9576\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9287\n",
      "Epoch 00126: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2221 - acc: 0.9287 - val_loss: 0.1486 - val_acc: 0.9588\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9317\n",
      "Epoch 00127: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2142 - acc: 0.9317 - val_loss: 0.1507 - val_acc: 0.9562\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9310\n",
      "Epoch 00128: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2135 - acc: 0.9310 - val_loss: 0.1579 - val_acc: 0.9564\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9319\n",
      "Epoch 00129: val_loss did not improve from 0.13973\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2156 - acc: 0.9319 - val_loss: 0.1606 - val_acc: 0.9527\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9310\n",
      "Epoch 00130: val_loss improved from 0.13973 to 0.13758, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/130-0.1376.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2145 - acc: 0.9310 - val_loss: 0.1376 - val_acc: 0.9606\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9308\n",
      "Epoch 00131: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2131 - acc: 0.9307 - val_loss: 0.1419 - val_acc: 0.9588\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9321\n",
      "Epoch 00132: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2092 - acc: 0.9321 - val_loss: 0.1435 - val_acc: 0.9585\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9327\n",
      "Epoch 00133: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2090 - acc: 0.9326 - val_loss: 0.1408 - val_acc: 0.9602\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2160 - acc: 0.9308\n",
      "Epoch 00134: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2160 - acc: 0.9308 - val_loss: 0.1455 - val_acc: 0.9576\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9346\n",
      "Epoch 00135: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2050 - acc: 0.9346 - val_loss: 0.1554 - val_acc: 0.9555\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9332\n",
      "Epoch 00136: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2007 - acc: 0.9332 - val_loss: 0.1553 - val_acc: 0.9553\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9346\n",
      "Epoch 00137: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2044 - acc: 0.9346 - val_loss: 0.1521 - val_acc: 0.9585\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9351\n",
      "Epoch 00138: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1992 - acc: 0.9351 - val_loss: 0.1426 - val_acc: 0.9578\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9355\n",
      "Epoch 00139: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2012 - acc: 0.9355 - val_loss: 0.1463 - val_acc: 0.9560\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9354\n",
      "Epoch 00140: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2021 - acc: 0.9354 - val_loss: 0.1522 - val_acc: 0.9569\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9349\n",
      "Epoch 00141: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1992 - acc: 0.9348 - val_loss: 0.1626 - val_acc: 0.9578\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9345\n",
      "Epoch 00142: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1991 - acc: 0.9345 - val_loss: 0.1385 - val_acc: 0.9613\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9381\n",
      "Epoch 00143: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1914 - acc: 0.9381 - val_loss: 0.1498 - val_acc: 0.9571\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9367\n",
      "Epoch 00144: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1944 - acc: 0.9367 - val_loss: 0.1407 - val_acc: 0.9609\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9370\n",
      "Epoch 00145: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1910 - acc: 0.9370 - val_loss: 0.1453 - val_acc: 0.9574\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9383\n",
      "Epoch 00146: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1942 - acc: 0.9383 - val_loss: 0.1418 - val_acc: 0.9613\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9379\n",
      "Epoch 00147: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1913 - acc: 0.9379 - val_loss: 0.1547 - val_acc: 0.9539\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9401\n",
      "Epoch 00148: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1875 - acc: 0.9401 - val_loss: 0.1429 - val_acc: 0.9592\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9393\n",
      "Epoch 00149: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1893 - acc: 0.9393 - val_loss: 0.1506 - val_acc: 0.9576\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9379\n",
      "Epoch 00150: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1890 - acc: 0.9379 - val_loss: 0.1538 - val_acc: 0.9562\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9399\n",
      "Epoch 00151: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1856 - acc: 0.9399 - val_loss: 0.1481 - val_acc: 0.9602\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1903 - acc: 0.9372\n",
      "Epoch 00152: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1903 - acc: 0.9372 - val_loss: 0.1570 - val_acc: 0.9578\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9390\n",
      "Epoch 00153: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1861 - acc: 0.9391 - val_loss: 0.1455 - val_acc: 0.9569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9393\n",
      "Epoch 00154: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1850 - acc: 0.9394 - val_loss: 0.1569 - val_acc: 0.9553\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9416\n",
      "Epoch 00155: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1809 - acc: 0.9416 - val_loss: 0.1493 - val_acc: 0.9611\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9425\n",
      "Epoch 00156: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1814 - acc: 0.9425 - val_loss: 0.1479 - val_acc: 0.9592\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9398\n",
      "Epoch 00157: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1854 - acc: 0.9398 - val_loss: 0.1380 - val_acc: 0.9578\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9433\n",
      "Epoch 00158: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1771 - acc: 0.9433 - val_loss: 0.1560 - val_acc: 0.9560\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9423\n",
      "Epoch 00159: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1800 - acc: 0.9423 - val_loss: 0.1486 - val_acc: 0.9606\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9425\n",
      "Epoch 00160: val_loss did not improve from 0.13758\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1771 - acc: 0.9425 - val_loss: 0.1454 - val_acc: 0.9634\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9412\n",
      "Epoch 00161: val_loss improved from 0.13758 to 0.13541, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/161-0.1354.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1781 - acc: 0.9412 - val_loss: 0.1354 - val_acc: 0.9620\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9415\n",
      "Epoch 00162: val_loss did not improve from 0.13541\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1756 - acc: 0.9416 - val_loss: 0.1375 - val_acc: 0.9597\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9432\n",
      "Epoch 00163: val_loss did not improve from 0.13541\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1719 - acc: 0.9432 - val_loss: 0.1383 - val_acc: 0.9599\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9427\n",
      "Epoch 00164: val_loss did not improve from 0.13541\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1747 - acc: 0.9427 - val_loss: 0.1387 - val_acc: 0.9616\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9429\n",
      "Epoch 00165: val_loss did not improve from 0.13541\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1711 - acc: 0.9429 - val_loss: 0.1355 - val_acc: 0.9616\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9425\n",
      "Epoch 00166: val_loss did not improve from 0.13541\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1774 - acc: 0.9425 - val_loss: 0.1460 - val_acc: 0.9592\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9465\n",
      "Epoch 00167: val_loss did not improve from 0.13541\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1679 - acc: 0.9465 - val_loss: 0.1428 - val_acc: 0.9599\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9428\n",
      "Epoch 00168: val_loss improved from 0.13541 to 0.13151, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/168-0.1315.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1720 - acc: 0.9428 - val_loss: 0.1315 - val_acc: 0.9632\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9451\n",
      "Epoch 00169: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1687 - acc: 0.9451 - val_loss: 0.1390 - val_acc: 0.9611\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9473\n",
      "Epoch 00170: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1624 - acc: 0.9473 - val_loss: 0.1378 - val_acc: 0.9611\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9441\n",
      "Epoch 00171: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1693 - acc: 0.9441 - val_loss: 0.1359 - val_acc: 0.9606\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.9445\n",
      "Epoch 00172: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1681 - acc: 0.9445 - val_loss: 0.1521 - val_acc: 0.9604\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9463\n",
      "Epoch 00173: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1651 - acc: 0.9463 - val_loss: 0.1436 - val_acc: 0.9581\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9475\n",
      "Epoch 00174: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1650 - acc: 0.9475 - val_loss: 0.1478 - val_acc: 0.9599\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9496\n",
      "Epoch 00175: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1563 - acc: 0.9497 - val_loss: 0.1432 - val_acc: 0.9604\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9477\n",
      "Epoch 00176: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1603 - acc: 0.9477 - val_loss: 0.1406 - val_acc: 0.9604\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9461\n",
      "Epoch 00177: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1619 - acc: 0.9461 - val_loss: 0.1459 - val_acc: 0.9574\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9467\n",
      "Epoch 00178: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1626 - acc: 0.9467 - val_loss: 0.1509 - val_acc: 0.9588\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9469\n",
      "Epoch 00179: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1609 - acc: 0.9469 - val_loss: 0.1349 - val_acc: 0.9613\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9478\n",
      "Epoch 00180: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1623 - acc: 0.9478 - val_loss: 0.1402 - val_acc: 0.9592\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9476\n",
      "Epoch 00181: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1584 - acc: 0.9476 - val_loss: 0.1481 - val_acc: 0.9644\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9478\n",
      "Epoch 00182: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1586 - acc: 0.9478 - val_loss: 0.1367 - val_acc: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9480\n",
      "Epoch 00183: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1546 - acc: 0.9481 - val_loss: 0.1398 - val_acc: 0.9588\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9485\n",
      "Epoch 00184: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1552 - acc: 0.9485 - val_loss: 0.1407 - val_acc: 0.9620\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9483\n",
      "Epoch 00185: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1547 - acc: 0.9483 - val_loss: 0.1393 - val_acc: 0.9637\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9514\n",
      "Epoch 00186: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1489 - acc: 0.9514 - val_loss: 0.1472 - val_acc: 0.9597\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9488\n",
      "Epoch 00187: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1553 - acc: 0.9488 - val_loss: 0.1510 - val_acc: 0.9588\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9511\n",
      "Epoch 00188: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1488 - acc: 0.9511 - val_loss: 0.1391 - val_acc: 0.9604\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9493\n",
      "Epoch 00189: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1526 - acc: 0.9493 - val_loss: 0.1347 - val_acc: 0.9611\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9499\n",
      "Epoch 00190: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1504 - acc: 0.9499 - val_loss: 0.1428 - val_acc: 0.9606\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9495\n",
      "Epoch 00191: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1520 - acc: 0.9495 - val_loss: 0.1508 - val_acc: 0.9602\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9506\n",
      "Epoch 00192: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1477 - acc: 0.9506 - val_loss: 0.1427 - val_acc: 0.9597\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9509\n",
      "Epoch 00193: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1501 - acc: 0.9509 - val_loss: 0.1326 - val_acc: 0.9630\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9509\n",
      "Epoch 00194: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1488 - acc: 0.9509 - val_loss: 0.1417 - val_acc: 0.9611\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9513\n",
      "Epoch 00195: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1453 - acc: 0.9513 - val_loss: 0.1401 - val_acc: 0.9599\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9500\n",
      "Epoch 00196: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1440 - acc: 0.9500 - val_loss: 0.1387 - val_acc: 0.9616\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9520\n",
      "Epoch 00197: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1450 - acc: 0.9520 - val_loss: 0.1594 - val_acc: 0.9571\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9520\n",
      "Epoch 00198: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1462 - acc: 0.9520 - val_loss: 0.1395 - val_acc: 0.9634\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9533\n",
      "Epoch 00199: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1406 - acc: 0.9533 - val_loss: 0.1464 - val_acc: 0.9597\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9543\n",
      "Epoch 00200: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1369 - acc: 0.9543 - val_loss: 0.1384 - val_acc: 0.9618\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9522\n",
      "Epoch 00201: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1425 - acc: 0.9522 - val_loss: 0.1495 - val_acc: 0.9590\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9532\n",
      "Epoch 00202: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1426 - acc: 0.9531 - val_loss: 0.1484 - val_acc: 0.9613\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9538\n",
      "Epoch 00203: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1393 - acc: 0.9538 - val_loss: 0.1396 - val_acc: 0.9630\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9517\n",
      "Epoch 00204: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1410 - acc: 0.9517 - val_loss: 0.1398 - val_acc: 0.9611\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9529\n",
      "Epoch 00205: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1410 - acc: 0.9529 - val_loss: 0.1457 - val_acc: 0.9627\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9545\n",
      "Epoch 00206: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1379 - acc: 0.9544 - val_loss: 0.1617 - val_acc: 0.9569\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9533\n",
      "Epoch 00207: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1382 - acc: 0.9533 - val_loss: 0.1449 - val_acc: 0.9639\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9548\n",
      "Epoch 00208: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1351 - acc: 0.9548 - val_loss: 0.1471 - val_acc: 0.9616\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9557\n",
      "Epoch 00209: val_loss did not improve from 0.13151\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1339 - acc: 0.9557 - val_loss: 0.1475 - val_acc: 0.9611\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9550\n",
      "Epoch 00210: val_loss improved from 0.13151 to 0.13044, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_8_conv_checkpoint/210-0.1304.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1375 - acc: 0.9550 - val_loss: 0.1304 - val_acc: 0.9641\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9564\n",
      "Epoch 00211: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1348 - acc: 0.9564 - val_loss: 0.1543 - val_acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9529\n",
      "Epoch 00212: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1384 - acc: 0.9529 - val_loss: 0.1523 - val_acc: 0.9581\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9561\n",
      "Epoch 00213: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1296 - acc: 0.9561 - val_loss: 0.1408 - val_acc: 0.9637\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9540\n",
      "Epoch 00214: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1347 - acc: 0.9540 - val_loss: 0.1483 - val_acc: 0.9616\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9555\n",
      "Epoch 00215: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1347 - acc: 0.9555 - val_loss: 0.1376 - val_acc: 0.9625\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9559\n",
      "Epoch 00216: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1317 - acc: 0.9559 - val_loss: 0.1452 - val_acc: 0.9613\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9564\n",
      "Epoch 00217: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1300 - acc: 0.9564 - val_loss: 0.1581 - val_acc: 0.9592\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9568\n",
      "Epoch 00218: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1309 - acc: 0.9568 - val_loss: 0.1425 - val_acc: 0.9630\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9566\n",
      "Epoch 00219: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1275 - acc: 0.9566 - val_loss: 0.1458 - val_acc: 0.9630\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9565\n",
      "Epoch 00220: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1306 - acc: 0.9565 - val_loss: 0.1513 - val_acc: 0.9602\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9554\n",
      "Epoch 00221: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1301 - acc: 0.9554 - val_loss: 0.1547 - val_acc: 0.9567\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9570\n",
      "Epoch 00222: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1267 - acc: 0.9570 - val_loss: 0.1426 - val_acc: 0.9616\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9585\n",
      "Epoch 00223: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1235 - acc: 0.9585 - val_loss: 0.1443 - val_acc: 0.9620\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9592\n",
      "Epoch 00224: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1216 - acc: 0.9592 - val_loss: 0.1512 - val_acc: 0.9597\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9580\n",
      "Epoch 00225: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1251 - acc: 0.9580 - val_loss: 0.1749 - val_acc: 0.9585\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9575\n",
      "Epoch 00226: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1257 - acc: 0.9575 - val_loss: 0.1384 - val_acc: 0.9644\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9578\n",
      "Epoch 00227: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1247 - acc: 0.9578 - val_loss: 0.1566 - val_acc: 0.9606\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9577\n",
      "Epoch 00228: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1259 - acc: 0.9577 - val_loss: 0.1436 - val_acc: 0.9618\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9591\n",
      "Epoch 00229: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1215 - acc: 0.9591 - val_loss: 0.1641 - val_acc: 0.9583\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9587\n",
      "Epoch 00230: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1245 - acc: 0.9587 - val_loss: 0.1524 - val_acc: 0.9634\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9601\n",
      "Epoch 00231: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1170 - acc: 0.9601 - val_loss: 0.1410 - val_acc: 0.9618\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9595\n",
      "Epoch 00232: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1204 - acc: 0.9595 - val_loss: 0.1428 - val_acc: 0.9627\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9590\n",
      "Epoch 00233: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1229 - acc: 0.9590 - val_loss: 0.1345 - val_acc: 0.9632\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9586\n",
      "Epoch 00234: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1221 - acc: 0.9586 - val_loss: 0.1495 - val_acc: 0.9618\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9598\n",
      "Epoch 00235: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1201 - acc: 0.9598 - val_loss: 0.1529 - val_acc: 0.9625\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9585\n",
      "Epoch 00236: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1216 - acc: 0.9585 - val_loss: 0.1561 - val_acc: 0.9602\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9611\n",
      "Epoch 00237: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1166 - acc: 0.9611 - val_loss: 0.1542 - val_acc: 0.9620\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9588\n",
      "Epoch 00238: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1227 - acc: 0.9588 - val_loss: 0.1480 - val_acc: 0.9613\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9595\n",
      "Epoch 00239: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1185 - acc: 0.9595 - val_loss: 0.1665 - val_acc: 0.9585\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9604\n",
      "Epoch 00240: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1158 - acc: 0.9604 - val_loss: 0.1380 - val_acc: 0.9627\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9609\n",
      "Epoch 00241: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1159 - acc: 0.9609 - val_loss: 0.1451 - val_acc: 0.9632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9611\n",
      "Epoch 00242: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1161 - acc: 0.9611 - val_loss: 0.1482 - val_acc: 0.9632\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9618\n",
      "Epoch 00243: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1127 - acc: 0.9618 - val_loss: 0.1453 - val_acc: 0.9606\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9610\n",
      "Epoch 00244: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1155 - acc: 0.9610 - val_loss: 0.1385 - val_acc: 0.9632\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9623\n",
      "Epoch 00245: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1130 - acc: 0.9623 - val_loss: 0.1399 - val_acc: 0.9632\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9626\n",
      "Epoch 00246: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1091 - acc: 0.9626 - val_loss: 0.1508 - val_acc: 0.9606\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9610\n",
      "Epoch 00247: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1154 - acc: 0.9610 - val_loss: 0.1464 - val_acc: 0.9632\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9626\n",
      "Epoch 00248: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1109 - acc: 0.9626 - val_loss: 0.1451 - val_acc: 0.9658\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9619\n",
      "Epoch 00249: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.1082 - acc: 0.9619 - val_loss: 0.1450 - val_acc: 0.9634\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9638\n",
      "Epoch 00250: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1094 - acc: 0.9638 - val_loss: 0.1677 - val_acc: 0.9583\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9629\n",
      "Epoch 00251: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1111 - acc: 0.9629 - val_loss: 0.1552 - val_acc: 0.9625\n",
      "Epoch 252/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9626\n",
      "Epoch 00252: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.1130 - acc: 0.9626 - val_loss: 0.1474 - val_acc: 0.9609\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9640\n",
      "Epoch 00253: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1092 - acc: 0.9640 - val_loss: 0.1373 - val_acc: 0.9639\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9613\n",
      "Epoch 00254: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1142 - acc: 0.9613 - val_loss: 0.1459 - val_acc: 0.9618\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9630\n",
      "Epoch 00255: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.1086 - acc: 0.9630 - val_loss: 0.1397 - val_acc: 0.9634\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9632\n",
      "Epoch 00256: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.1051 - acc: 0.9632 - val_loss: 0.1605 - val_acc: 0.9602\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9630\n",
      "Epoch 00257: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1118 - acc: 0.9630 - val_loss: 0.1565 - val_acc: 0.9634\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9643\n",
      "Epoch 00258: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.1048 - acc: 0.9643 - val_loss: 0.1744 - val_acc: 0.9606\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9620\n",
      "Epoch 00259: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1130 - acc: 0.9620 - val_loss: 0.1547 - val_acc: 0.9641\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9614\n",
      "Epoch 00260: val_loss did not improve from 0.13044\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1091 - acc: 0.9614 - val_loss: 0.1524 - val_acc: 0.9646\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 379us/sample - loss: 0.1899 - acc: 0.9479\n",
      "Loss: 0.18992123000540403 Accuracy: 0.9478712\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5379 - acc: 0.1738\n",
      "Epoch 00001: val_loss improved from inf to 1.92672, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/001-1.9267.hdf5\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 2.5378 - acc: 0.1739 - val_loss: 1.9267 - val_acc: 0.4416\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8338 - acc: 0.4126\n",
      "Epoch 00002: val_loss improved from 1.92672 to 1.26491, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/002-1.2649.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.8339 - acc: 0.4126 - val_loss: 1.2649 - val_acc: 0.6194\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4509 - acc: 0.5301\n",
      "Epoch 00003: val_loss improved from 1.26491 to 1.01028, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/003-1.0103.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.4504 - acc: 0.5302 - val_loss: 1.0103 - val_acc: 0.6844\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2456 - acc: 0.5918\n",
      "Epoch 00004: val_loss improved from 1.01028 to 0.85865, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/004-0.8586.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.2455 - acc: 0.5918 - val_loss: 0.8586 - val_acc: 0.7331\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1193 - acc: 0.6320\n",
      "Epoch 00005: val_loss improved from 0.85865 to 0.73808, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/005-0.7381.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.1192 - acc: 0.6320 - val_loss: 0.7381 - val_acc: 0.7701\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0301 - acc: 0.6618\n",
      "Epoch 00006: val_loss improved from 0.73808 to 0.68767, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/006-0.6877.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.0301 - acc: 0.6618 - val_loss: 0.6877 - val_acc: 0.7883\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9464 - acc: 0.6925\n",
      "Epoch 00007: val_loss improved from 0.68767 to 0.61749, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/007-0.6175.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.9464 - acc: 0.6925 - val_loss: 0.6175 - val_acc: 0.8137\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8861 - acc: 0.7104\n",
      "Epoch 00008: val_loss improved from 0.61749 to 0.56932, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/008-0.5693.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.8861 - acc: 0.7104 - val_loss: 0.5693 - val_acc: 0.8297\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8273 - acc: 0.7300\n",
      "Epoch 00009: val_loss improved from 0.56932 to 0.52109, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/009-0.5211.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.8272 - acc: 0.7300 - val_loss: 0.5211 - val_acc: 0.8435\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7751 - acc: 0.7495\n",
      "Epoch 00010: val_loss improved from 0.52109 to 0.48857, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/010-0.4886.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.7750 - acc: 0.7495 - val_loss: 0.4886 - val_acc: 0.8549\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7254 - acc: 0.7667\n",
      "Epoch 00011: val_loss improved from 0.48857 to 0.44565, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/011-0.4457.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.7254 - acc: 0.7666 - val_loss: 0.4457 - val_acc: 0.8756\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6749 - acc: 0.7843\n",
      "Epoch 00012: val_loss improved from 0.44565 to 0.41127, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/012-0.4113.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.6748 - acc: 0.7843 - val_loss: 0.4113 - val_acc: 0.8779\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6441 - acc: 0.7954\n",
      "Epoch 00013: val_loss improved from 0.41127 to 0.37113, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/013-0.3711.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.6441 - acc: 0.7954 - val_loss: 0.3711 - val_acc: 0.8856\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6014 - acc: 0.8094\n",
      "Epoch 00014: val_loss improved from 0.37113 to 0.34522, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/014-0.3452.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.6014 - acc: 0.8094 - val_loss: 0.3452 - val_acc: 0.8982\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5766 - acc: 0.8186\n",
      "Epoch 00015: val_loss improved from 0.34522 to 0.34226, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/015-0.3423.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.5767 - acc: 0.8185 - val_loss: 0.3423 - val_acc: 0.9003\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.8285\n",
      "Epoch 00016: val_loss improved from 0.34226 to 0.31993, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/016-0.3199.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.5471 - acc: 0.8284 - val_loss: 0.3199 - val_acc: 0.9071\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.8363\n",
      "Epoch 00017: val_loss improved from 0.31993 to 0.31386, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/017-0.3139.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.5250 - acc: 0.8363 - val_loss: 0.3139 - val_acc: 0.9092\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8411\n",
      "Epoch 00018: val_loss did not improve from 0.31386\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.5037 - acc: 0.8411 - val_loss: 0.3189 - val_acc: 0.9043\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4865 - acc: 0.8467\n",
      "Epoch 00019: val_loss improved from 0.31386 to 0.31275, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/019-0.3128.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.4866 - acc: 0.8467 - val_loss: 0.3128 - val_acc: 0.9092\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4520 - acc: 0.8579\n",
      "Epoch 00020: val_loss improved from 0.31275 to 0.26345, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/020-0.2635.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.4520 - acc: 0.8579 - val_loss: 0.2635 - val_acc: 0.9180\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.8612\n",
      "Epoch 00021: val_loss did not improve from 0.26345\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4410 - acc: 0.8613 - val_loss: 0.2677 - val_acc: 0.9224\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4257 - acc: 0.8644\n",
      "Epoch 00022: val_loss improved from 0.26345 to 0.25310, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/022-0.2531.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.4257 - acc: 0.8644 - val_loss: 0.2531 - val_acc: 0.9243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8714\n",
      "Epoch 00023: val_loss did not improve from 0.25310\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.4119 - acc: 0.8713 - val_loss: 0.2584 - val_acc: 0.9203\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8741\n",
      "Epoch 00024: val_loss improved from 0.25310 to 0.24340, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/024-0.2434.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.4024 - acc: 0.8741 - val_loss: 0.2434 - val_acc: 0.9276\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3834 - acc: 0.8795\n",
      "Epoch 00025: val_loss improved from 0.24340 to 0.21994, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/025-0.2199.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.3834 - acc: 0.8794 - val_loss: 0.2199 - val_acc: 0.9329\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8818\n",
      "Epoch 00026: val_loss improved from 0.21994 to 0.21933, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/026-0.2193.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3733 - acc: 0.8818 - val_loss: 0.2193 - val_acc: 0.9338\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8840\n",
      "Epoch 00027: val_loss improved from 0.21933 to 0.21013, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/027-0.2101.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.3646 - acc: 0.8840 - val_loss: 0.2101 - val_acc: 0.9362\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8881\n",
      "Epoch 00028: val_loss did not improve from 0.21013\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.3536 - acc: 0.8880 - val_loss: 0.2324 - val_acc: 0.9285\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.8947\n",
      "Epoch 00029: val_loss improved from 0.21013 to 0.20375, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/029-0.2038.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.3380 - acc: 0.8947 - val_loss: 0.2038 - val_acc: 0.9385\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3336 - acc: 0.8954\n",
      "Epoch 00030: val_loss improved from 0.20375 to 0.19940, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/030-0.1994.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.3336 - acc: 0.8953 - val_loss: 0.1994 - val_acc: 0.9376\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.8987\n",
      "Epoch 00031: val_loss improved from 0.19940 to 0.19115, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/031-0.1912.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3224 - acc: 0.8987 - val_loss: 0.1912 - val_acc: 0.9446\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.9027\n",
      "Epoch 00032: val_loss improved from 0.19115 to 0.17702, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/032-0.1770.hdf5\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3118 - acc: 0.9027 - val_loss: 0.1770 - val_acc: 0.9467\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3030 - acc: 0.9046\n",
      "Epoch 00033: val_loss did not improve from 0.17702\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.3029 - acc: 0.9047 - val_loss: 0.1897 - val_acc: 0.9443\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9047\n",
      "Epoch 00034: val_loss did not improve from 0.17702\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.3042 - acc: 0.9047 - val_loss: 0.1928 - val_acc: 0.9401\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9070\n",
      "Epoch 00035: val_loss improved from 0.17702 to 0.17194, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/035-0.1719.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2956 - acc: 0.9069 - val_loss: 0.1719 - val_acc: 0.9488\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9099\n",
      "Epoch 00036: val_loss did not improve from 0.17194\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2836 - acc: 0.9099 - val_loss: 0.1813 - val_acc: 0.9443\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.9111\n",
      "Epoch 00037: val_loss did not improve from 0.17194\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2806 - acc: 0.9111 - val_loss: 0.1886 - val_acc: 0.9394\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9143\n",
      "Epoch 00038: val_loss improved from 0.17194 to 0.16810, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/038-0.1681.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2719 - acc: 0.9142 - val_loss: 0.1681 - val_acc: 0.9476\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9162\n",
      "Epoch 00039: val_loss did not improve from 0.16810\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2687 - acc: 0.9162 - val_loss: 0.1715 - val_acc: 0.9495\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.9160\n",
      "Epoch 00040: val_loss improved from 0.16810 to 0.16461, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/040-0.1646.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2667 - acc: 0.9160 - val_loss: 0.1646 - val_acc: 0.9469\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9194\n",
      "Epoch 00041: val_loss did not improve from 0.16461\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.2528 - acc: 0.9193 - val_loss: 0.1696 - val_acc: 0.9492\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9218\n",
      "Epoch 00042: val_loss improved from 0.16461 to 0.16384, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/042-0.1638.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2517 - acc: 0.9218 - val_loss: 0.1638 - val_acc: 0.9502\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.9217\n",
      "Epoch 00043: val_loss improved from 0.16384 to 0.15858, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/043-0.1586.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2471 - acc: 0.9217 - val_loss: 0.1586 - val_acc: 0.9509\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9238\n",
      "Epoch 00044: val_loss improved from 0.15858 to 0.15537, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/044-0.1554.hdf5\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2405 - acc: 0.9238 - val_loss: 0.1554 - val_acc: 0.9513\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9255\n",
      "Epoch 00045: val_loss did not improve from 0.15537\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2394 - acc: 0.9255 - val_loss: 0.1665 - val_acc: 0.9481\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9271\n",
      "Epoch 00046: val_loss did not improve from 0.15537\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2317 - acc: 0.9271 - val_loss: 0.1595 - val_acc: 0.9492\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9265\n",
      "Epoch 00047: val_loss improved from 0.15537 to 0.15404, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/047-0.1540.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2272 - acc: 0.9265 - val_loss: 0.1540 - val_acc: 0.9529\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9295\n",
      "Epoch 00048: val_loss improved from 0.15404 to 0.15028, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/048-0.1503.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2211 - acc: 0.9295 - val_loss: 0.1503 - val_acc: 0.9529\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9301\n",
      "Epoch 00049: val_loss did not improve from 0.15028\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.2188 - acc: 0.9301 - val_loss: 0.1524 - val_acc: 0.9499\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9330\n",
      "Epoch 00050: val_loss improved from 0.15028 to 0.14192, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/050-0.1419.hdf5\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.2132 - acc: 0.9330 - val_loss: 0.1419 - val_acc: 0.9548\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9318\n",
      "Epoch 00051: val_loss did not improve from 0.14192\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2135 - acc: 0.9318 - val_loss: 0.1617 - val_acc: 0.9481\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9353\n",
      "Epoch 00052: val_loss did not improve from 0.14192\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.2031 - acc: 0.9353 - val_loss: 0.1488 - val_acc: 0.9506\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9338\n",
      "Epoch 00053: val_loss did not improve from 0.14192\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.2062 - acc: 0.9338 - val_loss: 0.1471 - val_acc: 0.9564\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9350\n",
      "Epoch 00054: val_loss did not improve from 0.14192\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1983 - acc: 0.9350 - val_loss: 0.1524 - val_acc: 0.9532\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9370\n",
      "Epoch 00055: val_loss did not improve from 0.14192\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1986 - acc: 0.9370 - val_loss: 0.1579 - val_acc: 0.9499\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9371\n",
      "Epoch 00056: val_loss did not improve from 0.14192\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1976 - acc: 0.9371 - val_loss: 0.1471 - val_acc: 0.9529\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9371\n",
      "Epoch 00057: val_loss did not improve from 0.14192\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1923 - acc: 0.9371 - val_loss: 0.1435 - val_acc: 0.9550\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9398\n",
      "Epoch 00058: val_loss did not improve from 0.14192\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1873 - acc: 0.9398 - val_loss: 0.1437 - val_acc: 0.9536\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9404\n",
      "Epoch 00059: val_loss did not improve from 0.14192\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1826 - acc: 0.9404 - val_loss: 0.1496 - val_acc: 0.9557\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9399\n",
      "Epoch 00060: val_loss did not improve from 0.14192\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1824 - acc: 0.9399 - val_loss: 0.1555 - val_acc: 0.9525\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9426\n",
      "Epoch 00061: val_loss improved from 0.14192 to 0.13783, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/061-0.1378.hdf5\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1779 - acc: 0.9426 - val_loss: 0.1378 - val_acc: 0.9560\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9431\n",
      "Epoch 00062: val_loss did not improve from 0.13783\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1744 - acc: 0.9431 - val_loss: 0.1525 - val_acc: 0.9539\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9431\n",
      "Epoch 00063: val_loss did not improve from 0.13783\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1753 - acc: 0.9431 - val_loss: 0.1395 - val_acc: 0.9595\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9462\n",
      "Epoch 00064: val_loss did not improve from 0.13783\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1683 - acc: 0.9462 - val_loss: 0.1631 - val_acc: 0.9564\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9447\n",
      "Epoch 00065: val_loss did not improve from 0.13783\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1705 - acc: 0.9447 - val_loss: 0.1420 - val_acc: 0.9569\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9468\n",
      "Epoch 00066: val_loss did not improve from 0.13783\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1645 - acc: 0.9468 - val_loss: 0.1422 - val_acc: 0.9583\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9467\n",
      "Epoch 00067: val_loss did not improve from 0.13783\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1667 - acc: 0.9467 - val_loss: 0.1448 - val_acc: 0.9555\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9488\n",
      "Epoch 00068: val_loss did not improve from 0.13783\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1575 - acc: 0.9488 - val_loss: 0.1379 - val_acc: 0.9571\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9484\n",
      "Epoch 00069: val_loss did not improve from 0.13783\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1588 - acc: 0.9484 - val_loss: 0.1425 - val_acc: 0.9578\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9501\n",
      "Epoch 00070: val_loss did not improve from 0.13783\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1538 - acc: 0.9501 - val_loss: 0.1431 - val_acc: 0.9543\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9505\n",
      "Epoch 00071: val_loss did not improve from 0.13783\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1540 - acc: 0.9506 - val_loss: 0.1400 - val_acc: 0.9583\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9510\n",
      "Epoch 00072: val_loss improved from 0.13783 to 0.12996, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/072-0.1300.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1504 - acc: 0.9510 - val_loss: 0.1300 - val_acc: 0.9592\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9532\n",
      "Epoch 00073: val_loss did not improve from 0.12996\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1458 - acc: 0.9532 - val_loss: 0.1486 - val_acc: 0.9557\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9513\n",
      "Epoch 00074: val_loss did not improve from 0.12996\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1491 - acc: 0.9513 - val_loss: 0.1397 - val_acc: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9527\n",
      "Epoch 00075: val_loss did not improve from 0.12996\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.1440 - acc: 0.9527 - val_loss: 0.1441 - val_acc: 0.9574\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9527\n",
      "Epoch 00076: val_loss improved from 0.12996 to 0.12899, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/076-0.1290.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1463 - acc: 0.9526 - val_loss: 0.1290 - val_acc: 0.9583\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9545\n",
      "Epoch 00077: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1405 - acc: 0.9545 - val_loss: 0.1550 - val_acc: 0.9548\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9535\n",
      "Epoch 00078: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1391 - acc: 0.9535 - val_loss: 0.1552 - val_acc: 0.9574\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9539\n",
      "Epoch 00079: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1396 - acc: 0.9539 - val_loss: 0.1308 - val_acc: 0.9599\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9546\n",
      "Epoch 00080: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1349 - acc: 0.9546 - val_loss: 0.1587 - val_acc: 0.9557\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9574\n",
      "Epoch 00081: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1337 - acc: 0.9574 - val_loss: 0.1320 - val_acc: 0.9616\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9573\n",
      "Epoch 00082: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.1313 - acc: 0.9573 - val_loss: 0.1382 - val_acc: 0.9609\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9565\n",
      "Epoch 00083: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1320 - acc: 0.9565 - val_loss: 0.1361 - val_acc: 0.9613\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9573\n",
      "Epoch 00084: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1311 - acc: 0.9573 - val_loss: 0.1423 - val_acc: 0.9616\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9557\n",
      "Epoch 00085: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1336 - acc: 0.9557 - val_loss: 0.1338 - val_acc: 0.9597\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9581\n",
      "Epoch 00086: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1270 - acc: 0.9581 - val_loss: 0.1348 - val_acc: 0.9611\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9605\n",
      "Epoch 00087: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1211 - acc: 0.9604 - val_loss: 0.1392 - val_acc: 0.9592\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9585\n",
      "Epoch 00088: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1281 - acc: 0.9585 - val_loss: 0.1323 - val_acc: 0.9606\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9595\n",
      "Epoch 00089: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1200 - acc: 0.9596 - val_loss: 0.1421 - val_acc: 0.9560\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9597\n",
      "Epoch 00090: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1206 - acc: 0.9597 - val_loss: 0.1375 - val_acc: 0.9585\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9611\n",
      "Epoch 00091: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.1179 - acc: 0.9611 - val_loss: 0.1406 - val_acc: 0.9578\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9618\n",
      "Epoch 00092: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1153 - acc: 0.9618 - val_loss: 0.1461 - val_acc: 0.9604\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9613\n",
      "Epoch 00093: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.1153 - acc: 0.9613 - val_loss: 0.1348 - val_acc: 0.9616\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9624\n",
      "Epoch 00094: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1136 - acc: 0.9624 - val_loss: 0.1495 - val_acc: 0.9581\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9616\n",
      "Epoch 00095: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1107 - acc: 0.9616 - val_loss: 0.1411 - val_acc: 0.9592\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9626\n",
      "Epoch 00096: val_loss did not improve from 0.12899\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.1131 - acc: 0.9626 - val_loss: 0.1424 - val_acc: 0.9581\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9639\n",
      "Epoch 00097: val_loss improved from 0.12899 to 0.12147, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_9_conv_checkpoint/097-0.1215.hdf5\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1089 - acc: 0.9639 - val_loss: 0.1215 - val_acc: 0.9646\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9630\n",
      "Epoch 00098: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.1102 - acc: 0.9630 - val_loss: 0.1331 - val_acc: 0.9616\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9631\n",
      "Epoch 00099: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.1085 - acc: 0.9631 - val_loss: 0.1418 - val_acc: 0.9606\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9642\n",
      "Epoch 00100: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.1053 - acc: 0.9642 - val_loss: 0.1428 - val_acc: 0.9611\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9646\n",
      "Epoch 00101: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1049 - acc: 0.9646 - val_loss: 0.1397 - val_acc: 0.9604\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9640\n",
      "Epoch 00102: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.1038 - acc: 0.9641 - val_loss: 0.1341 - val_acc: 0.9646\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9664\n",
      "Epoch 00103: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1026 - acc: 0.9664 - val_loss: 0.1401 - val_acc: 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9642\n",
      "Epoch 00104: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1044 - acc: 0.9642 - val_loss: 0.1441 - val_acc: 0.9599\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9664\n",
      "Epoch 00105: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.0990 - acc: 0.9664 - val_loss: 0.1449 - val_acc: 0.9585\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9671\n",
      "Epoch 00106: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0976 - acc: 0.9671 - val_loss: 0.1545 - val_acc: 0.9602\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9671\n",
      "Epoch 00107: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1011 - acc: 0.9671 - val_loss: 0.1379 - val_acc: 0.9639\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9659\n",
      "Epoch 00108: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0966 - acc: 0.9659 - val_loss: 0.1428 - val_acc: 0.9623\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9679\n",
      "Epoch 00109: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0944 - acc: 0.9679 - val_loss: 0.1367 - val_acc: 0.9595\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9682\n",
      "Epoch 00110: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.0934 - acc: 0.9681 - val_loss: 0.1641 - val_acc: 0.9571\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9673\n",
      "Epoch 00111: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0966 - acc: 0.9673 - val_loss: 0.1803 - val_acc: 0.9564\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9688\n",
      "Epoch 00112: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.0934 - acc: 0.9688 - val_loss: 0.1365 - val_acc: 0.9653\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9682\n",
      "Epoch 00113: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0949 - acc: 0.9682 - val_loss: 0.1513 - val_acc: 0.9613\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9695\n",
      "Epoch 00114: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0913 - acc: 0.9695 - val_loss: 0.1394 - val_acc: 0.9620\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9686\n",
      "Epoch 00115: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.0922 - acc: 0.9686 - val_loss: 0.1400 - val_acc: 0.9632\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9697\n",
      "Epoch 00116: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.0861 - acc: 0.9697 - val_loss: 0.1407 - val_acc: 0.9634\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9714\n",
      "Epoch 00117: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0855 - acc: 0.9714 - val_loss: 0.1539 - val_acc: 0.9602\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9708\n",
      "Epoch 00118: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.0855 - acc: 0.9708 - val_loss: 0.1422 - val_acc: 0.9639\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9705\n",
      "Epoch 00119: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0890 - acc: 0.9705 - val_loss: 0.1576 - val_acc: 0.9576\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9723\n",
      "Epoch 00120: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.0832 - acc: 0.9723 - val_loss: 0.1399 - val_acc: 0.9630\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9708\n",
      "Epoch 00121: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.0845 - acc: 0.9708 - val_loss: 0.1537 - val_acc: 0.9616\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9702\n",
      "Epoch 00122: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0861 - acc: 0.9702 - val_loss: 0.1270 - val_acc: 0.9634\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9721\n",
      "Epoch 00123: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.0818 - acc: 0.9722 - val_loss: 0.1369 - val_acc: 0.9641\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9723\n",
      "Epoch 00124: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.0829 - acc: 0.9723 - val_loss: 0.1459 - val_acc: 0.9613\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9713\n",
      "Epoch 00125: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.0828 - acc: 0.9713 - val_loss: 0.1315 - val_acc: 0.9660\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9716\n",
      "Epoch 00126: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0823 - acc: 0.9716 - val_loss: 0.1653 - val_acc: 0.9609\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9717\n",
      "Epoch 00127: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.0812 - acc: 0.9717 - val_loss: 0.1515 - val_acc: 0.9597\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9727\n",
      "Epoch 00128: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0781 - acc: 0.9727 - val_loss: 0.1485 - val_acc: 0.9641\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9737\n",
      "Epoch 00129: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0770 - acc: 0.9737 - val_loss: 0.1563 - val_acc: 0.9611\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9745\n",
      "Epoch 00130: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.0755 - acc: 0.9744 - val_loss: 0.1689 - val_acc: 0.9599\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9718\n",
      "Epoch 00131: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0807 - acc: 0.9718 - val_loss: 0.1582 - val_acc: 0.9641\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9729\n",
      "Epoch 00132: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0788 - acc: 0.9729 - val_loss: 0.1430 - val_acc: 0.9627\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9758\n",
      "Epoch 00133: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0720 - acc: 0.9758 - val_loss: 0.1590 - val_acc: 0.9660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9742\n",
      "Epoch 00134: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0749 - acc: 0.9742 - val_loss: 0.1542 - val_acc: 0.9648\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9756\n",
      "Epoch 00135: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.0713 - acc: 0.9756 - val_loss: 0.1673 - val_acc: 0.9644\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9756\n",
      "Epoch 00136: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.0716 - acc: 0.9756 - val_loss: 0.1346 - val_acc: 0.9662\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9735\n",
      "Epoch 00137: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0760 - acc: 0.9735 - val_loss: 0.1474 - val_acc: 0.9634\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9751\n",
      "Epoch 00138: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.0723 - acc: 0.9751 - val_loss: 0.1543 - val_acc: 0.9648\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9755\n",
      "Epoch 00139: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.0712 - acc: 0.9755 - val_loss: 0.1535 - val_acc: 0.9648\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9770\n",
      "Epoch 00140: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.0669 - acc: 0.9770 - val_loss: 0.1636 - val_acc: 0.9641\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9763\n",
      "Epoch 00141: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.0710 - acc: 0.9763 - val_loss: 0.1543 - val_acc: 0.9606\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9751\n",
      "Epoch 00142: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.0719 - acc: 0.9751 - val_loss: 0.1465 - val_acc: 0.9665\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9766\n",
      "Epoch 00143: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.0686 - acc: 0.9766 - val_loss: 0.1461 - val_acc: 0.9676\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9771\n",
      "Epoch 00144: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0653 - acc: 0.9771 - val_loss: 0.1677 - val_acc: 0.9630\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9773\n",
      "Epoch 00145: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.0648 - acc: 0.9773 - val_loss: 0.1472 - val_acc: 0.9655\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9781\n",
      "Epoch 00146: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0653 - acc: 0.9781 - val_loss: 0.1534 - val_acc: 0.9658\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9767\n",
      "Epoch 00147: val_loss did not improve from 0.12147\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.0653 - acc: 0.9767 - val_loss: 0.1790 - val_acc: 0.9639\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 404us/sample - loss: 0.1870 - acc: 0.9485\n",
      "Loss: 0.18697071116367356 Accuracy: 0.9484943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_32_DO_075_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 94,768\n",
      "Trainable params: 94,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 371us/sample - loss: 0.1922 - acc: 0.9533\n",
      "Loss: 0.1922191024275336 Accuracy: 0.95327103\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_ch_32_DO_075_DO'\n",
    "\n",
    "i = 8\n",
    "model_name = base+'_{}_conv'.format(i)\n",
    "print()\n",
    "print(model_name, 'Model')\n",
    "model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "model = load_model(model_filename)\n",
    "model.summary()\n",
    "\n",
    "[loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "# del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[-0.3106074 ,  0.13599136,  0.39478263, -0.5487611 ,\n",
       "           0.04834766, -0.34348977, -0.09238157,  0.20059726,\n",
       "           0.09414005,  0.41970667, -0.13299997,  0.56170446,\n",
       "           0.4656292 , -0.27239114,  0.31932655,  0.00388216,\n",
       "           0.42863008,  0.47777995,  0.34016943, -0.3179274 ,\n",
       "           0.2721707 ,  0.3997097 ,  0.58544356,  0.24919471,\n",
       "          -0.09690873, -0.28347325,  0.39864233, -0.6718644 ,\n",
       "          -0.5072978 , -0.2002464 ,  0.07500112,  0.13127115]],\n",
       " \n",
       "        [[ 0.2190911 ,  0.5089676 , -0.19251813, -0.5095081 ,\n",
       "           0.08009236,  0.21228306,  0.02794367, -0.00592784,\n",
       "           0.07415169,  0.03490736,  0.10092944, -0.3860515 ,\n",
       "           0.04208849,  0.22005434,  0.38621005,  0.09656194,\n",
       "          -0.05904865,  0.10112352, -0.27672917, -0.11816657,\n",
       "           0.10031383, -0.17889047, -0.29038706, -0.08941289,\n",
       "          -0.4585213 , -0.10899372,  0.19605844, -0.05961814,\n",
       "           0.06720591,  0.09473217, -0.05715673,  0.19696848]],\n",
       " \n",
       "        [[ 0.16440551, -0.25483423,  0.05884143,  0.3463239 ,\n",
       "           0.15723197, -0.22416124,  0.38827634, -0.19919154,\n",
       "          -0.4462351 , -0.15690403, -0.29507875, -0.3082682 ,\n",
       "           0.08265136,  0.3069282 ,  0.04096038,  0.34408927,\n",
       "           0.19645405, -0.26930097, -0.12697613,  0.2729212 ,\n",
       "          -0.17599817, -0.22642446,  0.13355665, -0.00610351,\n",
       "          -0.13833827,  0.19035253, -0.16859734,  0.13638923,\n",
       "          -0.19008936, -0.08609984, -0.10979692, -0.03407854]],\n",
       " \n",
       "        [[ 0.17619774, -0.21627466,  0.16331384,  0.41811207,\n",
       "           0.16209003, -0.21029046, -0.18663624, -0.2265985 ,\n",
       "           0.325105  , -0.12305368,  0.06064206,  0.06900487,\n",
       "           0.12760675,  0.02712425, -0.3231702 , -0.3748782 ,\n",
       "          -0.3474851 , -0.04778234, -0.26251313,  0.22100714,\n",
       "          -0.2254076 ,  0.06385434, -0.40602222, -0.41673285,\n",
       "           0.44483307,  0.14448693, -0.07305451,  0.00993625,\n",
       "           0.45854953, -0.3101686 , -0.27480426,  0.21766369]],\n",
       " \n",
       "        [[-0.26138416, -0.05379539, -0.33912393,  0.3694954 ,\n",
       "          -0.08799253, -0.06562092,  0.47046632, -0.21715587,\n",
       "          -0.04725879, -0.19777714,  0.28197595,  0.06305604,\n",
       "          -0.31039253,  0.34877276, -0.25751653, -0.01074822,\n",
       "          -0.35509813, -0.06289337,  0.31699267,  0.08444732,\n",
       "          -0.40688735, -0.12238932, -0.29821953, -0.12922864,\n",
       "           0.09501145,  0.4581664 , -0.37801817,  0.5749784 ,\n",
       "           0.07278159, -0.13661958, -0.2535265 ,  0.13704777]]],\n",
       "       dtype=float32),\n",
       " array([-0.3106074 ,  0.2190911 ,  0.16440551,  0.17619774, -0.26138416],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(model.layers[0])\n",
    "x1w = model.layers[0].get_weights()[0]\n",
    "x1w, x1w[:,0,0]\n",
    "# x1w = x1w[0][0]\n",
    "# x1w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1w = model.layers[0].get_weights()[0]\n",
    "\n",
    "plt.figure(figsize=(10, 5), dpi=100)\n",
    "plt.subplots_adjust(wspace=1, hspace=0.5)\n",
    "\n",
    "y_min = np.around(x1w[:,0].min(), decimals=1)\n",
    "y_max = np.around(x1w[:,0].max(), decimals=1)\n",
    "\n",
    "for i in range(32):\n",
    "    plt.subplot(4,8,i+1)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks([])\n",
    "#     plt.title('CH '+str(i))\n",
    "    target_filter = x1w[:,0,i]\n",
    "    plt.bar(np.arange(len(target_filter)), target_filter, width=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1w = model.layers[2].get_weights()[0]\n",
    "\n",
    "plt.figure(figsize=(10, 5), dpi=100)\n",
    "plt.subplots_adjust(wspace=1, hspace=0.5)\n",
    "\n",
    "y_min = np.around(x1w[:,0].min(), decimals=1)\n",
    "y_max = np.around(x1w[:,0].max(), decimals=1)\n",
    "\n",
    "for i in range(32):\n",
    "    plt.subplot(4,8,i+1)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks([])\n",
    "#     plt.title('CH '+str(i))\n",
    "    target_filter = x1w[:,0,i]\n",
    "    plt.bar(np.arange(len(target_filter)), target_filter, width=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test_abs, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[369   0   5   1   3   0   1   4   0   1   0   0   1   0   0   0]\n",
      " [  1 347   0   0   4   1   1   0   0   7   2   0   0   1   0   0]\n",
      " [  8   0 369   2   0   0   0   0   1   0   0   0   2   2   0   0]\n",
      " [  1   0   2 356   0   1   1   2   6   0   0   0   0   0   0   8]\n",
      " [  2   2   1   1 359   2   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   6   0   3   4 386   0   2   1   3   0   1   2   0   0   0]\n",
      " [  0   0   0   3   1   0 366   1   2   0   1   0   0   0   0   0]\n",
      " [  3   0   0   0   1   0   2 367   0   0   3   0   0   0   0   0]\n",
      " [  1   0   1   2   1   0   0   0 367   0   2   0   1   0   0   1]\n",
      " [  0   6   0   0   0   3   0   0   0 364   3   1   0   0   0   0]\n",
      " [  1   0   2   0   0   0   0   0   6   0 166   5   2   1   0   0]\n",
      " [  0   1   1   1   0   0   2   0   0   2   7 137   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0   1   0 161   4   1   0]\n",
      " [  0   1   5   0   0   0   0   0   0   1   1   0   2 182   0   0]\n",
      " [  0   0   2   0   0   1   1   0   0   1   0   0   5   1 156   0]\n",
      " [  2   0   2  15   0   0   1   0   4   1   0   0   0   0   0 138]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(x_test_abs)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "y_real = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_real, y_pred))\n",
    "# print('Classification Report')\n",
    "# target_names = ['Cats', 'Dogs', 'Horse']\n",
    "# print(classification_report(y_test_onehot, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven',\n",
       "       'eight', 'nine', 'bed', 'bird', 'cat', 'dog', 'house', 'tree'],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_table.T[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
