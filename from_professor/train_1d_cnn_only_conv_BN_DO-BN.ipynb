{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_BN_only_conv(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=25, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=25, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model = build_1d_cnn_BN_only_conv(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3810 - acc: 0.2583\n",
      "Epoch 00001: val_loss improved from inf to 2.00441, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/001-2.0044.hdf5\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 2.3811 - acc: 0.2583 - val_loss: 2.0044 - val_acc: 0.3522\n",
      "Epoch 2/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.6398 - acc: 0.4739\n",
      "Epoch 00002: val_loss improved from 2.00441 to 1.86095, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/002-1.8609.hdf5\n",
      "36805/36805 [==============================] - 7s 198us/sample - loss: 1.6390 - acc: 0.4742 - val_loss: 1.8609 - val_acc: 0.4034\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3685 - acc: 0.5638\n",
      "Epoch 00003: val_loss improved from 1.86095 to 1.75336, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/003-1.7534.hdf5\n",
      "36805/36805 [==============================] - 7s 199us/sample - loss: 1.3685 - acc: 0.5638 - val_loss: 1.7534 - val_acc: 0.4440\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1932 - acc: 0.6253\n",
      "Epoch 00004: val_loss did not improve from 1.75336\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 1.1930 - acc: 0.6254 - val_loss: 1.7696 - val_acc: 0.4512\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0502 - acc: 0.6719\n",
      "Epoch 00005: val_loss improved from 1.75336 to 1.74330, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/005-1.7433.hdf5\n",
      "36805/36805 [==============================] - 7s 202us/sample - loss: 1.0502 - acc: 0.6719 - val_loss: 1.7433 - val_acc: 0.4642\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9543 - acc: 0.7058\n",
      "Epoch 00006: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 207us/sample - loss: 0.9538 - acc: 0.7059 - val_loss: 1.7782 - val_acc: 0.4610\n",
      "Epoch 7/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.8577 - acc: 0.7375\n",
      "Epoch 00007: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 7s 203us/sample - loss: 0.8586 - acc: 0.7373 - val_loss: 1.8031 - val_acc: 0.4684\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7841 - acc: 0.7634\n",
      "Epoch 00008: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 205us/sample - loss: 0.7844 - acc: 0.7632 - val_loss: 1.8632 - val_acc: 0.4652\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7233 - acc: 0.7829\n",
      "Epoch 00009: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 205us/sample - loss: 0.7234 - acc: 0.7829 - val_loss: 1.9623 - val_acc: 0.4403\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.8071\n",
      "Epoch 00010: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 205us/sample - loss: 0.6600 - acc: 0.8068 - val_loss: 1.9007 - val_acc: 0.4577\n",
      "Epoch 11/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.8222\n",
      "Epoch 00011: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 204us/sample - loss: 0.6119 - acc: 0.8220 - val_loss: 1.9441 - val_acc: 0.4617\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5679 - acc: 0.8412\n",
      "Epoch 00012: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 7s 202us/sample - loss: 0.5679 - acc: 0.8412 - val_loss: 2.0906 - val_acc: 0.4340\n",
      "Epoch 13/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5165 - acc: 0.8605\n",
      "Epoch 00013: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 0.5168 - acc: 0.8604 - val_loss: 2.0346 - val_acc: 0.4612\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8716\n",
      "Epoch 00014: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 7s 204us/sample - loss: 0.4809 - acc: 0.8715 - val_loss: 2.0934 - val_acc: 0.4561\n",
      "Epoch 15/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.8800\n",
      "Epoch 00015: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 205us/sample - loss: 0.4539 - acc: 0.8799 - val_loss: 2.1286 - val_acc: 0.4619\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8904\n",
      "Epoch 00016: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 0.4266 - acc: 0.8903 - val_loss: 2.1804 - val_acc: 0.4514\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.9070\n",
      "Epoch 00017: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 0.3842 - acc: 0.9070 - val_loss: 2.3505 - val_acc: 0.4363\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.9148\n",
      "Epoch 00018: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 7s 198us/sample - loss: 0.3602 - acc: 0.9143 - val_loss: 2.3500 - val_acc: 0.4391\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3386 - acc: 0.9191\n",
      "Epoch 00019: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 7s 197us/sample - loss: 0.3386 - acc: 0.9190 - val_loss: 2.2984 - val_acc: 0.4512\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.9280\n",
      "Epoch 00020: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 204us/sample - loss: 0.3160 - acc: 0.9279 - val_loss: 2.3504 - val_acc: 0.4570\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9327\n",
      "Epoch 00021: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.2983 - acc: 0.9327 - val_loss: 2.4080 - val_acc: 0.4547\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9377\n",
      "Epoch 00022: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.2821 - acc: 0.9375 - val_loss: 2.4392 - val_acc: 0.4526\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9459\n",
      "Epoch 00023: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.2591 - acc: 0.9459 - val_loss: 2.4672 - val_acc: 0.4559\n",
      "Epoch 24/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9521\n",
      "Epoch 00024: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.2411 - acc: 0.9520 - val_loss: 2.4629 - val_acc: 0.4605\n",
      "Epoch 25/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9553\n",
      "Epoch 00025: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.2297 - acc: 0.9553 - val_loss: 2.5833 - val_acc: 0.4479\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9579\n",
      "Epoch 00026: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.2203 - acc: 0.9579 - val_loss: 2.6370 - val_acc: 0.4470\n",
      "Epoch 27/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9641\n",
      "Epoch 00027: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.2019 - acc: 0.9640 - val_loss: 2.6467 - val_acc: 0.4486\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9639\n",
      "Epoch 00028: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.1969 - acc: 0.9639 - val_loss: 2.6891 - val_acc: 0.4440\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9683\n",
      "Epoch 00029: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.1828 - acc: 0.9683 - val_loss: 2.7005 - val_acc: 0.4477\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9732\n",
      "Epoch 00030: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.1716 - acc: 0.9732 - val_loss: 2.7663 - val_acc: 0.4475\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9761\n",
      "Epoch 00031: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.1625 - acc: 0.9761 - val_loss: 2.7977 - val_acc: 0.4533\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9746\n",
      "Epoch 00032: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.1616 - acc: 0.9745 - val_loss: 2.8492 - val_acc: 0.4526\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9736\n",
      "Epoch 00033: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.1608 - acc: 0.9736 - val_loss: 2.9018 - val_acc: 0.4400\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9813\n",
      "Epoch 00034: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.1367 - acc: 0.9813 - val_loss: 2.9822 - val_acc: 0.4382\n",
      "Epoch 35/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9822\n",
      "Epoch 00035: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.1323 - acc: 0.9822 - val_loss: 2.8762 - val_acc: 0.4540\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9827\n",
      "Epoch 00036: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.1258 - acc: 0.9827 - val_loss: 2.9582 - val_acc: 0.4528\n",
      "Epoch 37/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9827\n",
      "Epoch 00037: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.1227 - acc: 0.9827 - val_loss: 3.0319 - val_acc: 0.4407\n",
      "Epoch 38/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9847\n",
      "Epoch 00038: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.1153 - acc: 0.9846 - val_loss: 3.0553 - val_acc: 0.4340\n",
      "Epoch 39/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9835\n",
      "Epoch 00039: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.1152 - acc: 0.9836 - val_loss: 3.0715 - val_acc: 0.4503\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9893\n",
      "Epoch 00040: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0977 - acc: 0.9892 - val_loss: 3.1611 - val_acc: 0.4393\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9877\n",
      "Epoch 00041: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.1029 - acc: 0.9877 - val_loss: 3.2423 - val_acc: 0.4305\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9871\n",
      "Epoch 00042: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.1023 - acc: 0.9871 - val_loss: 3.1800 - val_acc: 0.4391\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9891\n",
      "Epoch 00043: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0934 - acc: 0.9891 - val_loss: 3.2613 - val_acc: 0.4430\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9862\n",
      "Epoch 00044: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.1006 - acc: 0.9863 - val_loss: 3.2189 - val_acc: 0.4414\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9900\n",
      "Epoch 00045: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0885 - acc: 0.9899 - val_loss: 3.2533 - val_acc: 0.4396\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9906\n",
      "Epoch 00046: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0835 - acc: 0.9905 - val_loss: 3.2941 - val_acc: 0.4391\n",
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9898\n",
      "Epoch 00047: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0847 - acc: 0.9898 - val_loss: 3.3165 - val_acc: 0.4365\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9937\n",
      "Epoch 00048: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0715 - acc: 0.9937 - val_loss: 3.4186 - val_acc: 0.4370\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9931\n",
      "Epoch 00049: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0706 - acc: 0.9931 - val_loss: 3.3855 - val_acc: 0.4463\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9901\n",
      "Epoch 00050: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0801 - acc: 0.9899 - val_loss: 3.4524 - val_acc: 0.4335\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9891\n",
      "Epoch 00051: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.0812 - acc: 0.9890 - val_loss: 3.5019 - val_acc: 0.4356\n",
      "Epoch 52/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9927\n",
      "Epoch 00052: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0684 - acc: 0.9927 - val_loss: 3.5044 - val_acc: 0.4349\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9936\n",
      "Epoch 00053: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0641 - acc: 0.9936 - val_loss: 3.4434 - val_acc: 0.4477\n",
      "Epoch 54/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9938\n",
      "Epoch 00054: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0630 - acc: 0.9938 - val_loss: 3.5209 - val_acc: 0.4349\n",
      "Epoch 55/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9939\n",
      "Epoch 00055: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0629 - acc: 0.9939 - val_loss: 3.5095 - val_acc: 0.4421\n",
      "Epoch 56/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9932\n",
      "Epoch 00056: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0623 - acc: 0.9933 - val_loss: 3.5274 - val_acc: 0.4403\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9939\n",
      "Epoch 00057: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0601 - acc: 0.9939 - val_loss: 3.6125 - val_acc: 0.4444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9940\n",
      "Epoch 00058: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0560 - acc: 0.9940 - val_loss: 3.5909 - val_acc: 0.4391\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9929\n",
      "Epoch 00059: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0602 - acc: 0.9930 - val_loss: 3.7059 - val_acc: 0.4279\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9945\n",
      "Epoch 00060: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0524 - acc: 0.9945 - val_loss: 3.6987 - val_acc: 0.4358\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9954\n",
      "Epoch 00061: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.0531 - acc: 0.9954 - val_loss: 3.6980 - val_acc: 0.4426\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9951\n",
      "Epoch 00062: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0498 - acc: 0.9950 - val_loss: 3.8046 - val_acc: 0.4153\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9923\n",
      "Epoch 00063: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0573 - acc: 0.9923 - val_loss: 3.7156 - val_acc: 0.4354\n",
      "Epoch 64/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9967\n",
      "Epoch 00064: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0462 - acc: 0.9965 - val_loss: 3.7836 - val_acc: 0.4270\n",
      "Epoch 65/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9939\n",
      "Epoch 00065: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0521 - acc: 0.9939 - val_loss: 3.8697 - val_acc: 0.4293\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9945\n",
      "Epoch 00066: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0501 - acc: 0.9945 - val_loss: 3.7861 - val_acc: 0.4393\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9948\n",
      "Epoch 00067: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0476 - acc: 0.9947 - val_loss: 3.8647 - val_acc: 0.4286\n",
      "Epoch 68/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9942\n",
      "Epoch 00068: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0507 - acc: 0.9942 - val_loss: 3.8078 - val_acc: 0.4300\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9961\n",
      "Epoch 00069: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0434 - acc: 0.9960 - val_loss: 3.8106 - val_acc: 0.4361\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9945\n",
      "Epoch 00070: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0456 - acc: 0.9945 - val_loss: 3.9774 - val_acc: 0.4253\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9952\n",
      "Epoch 00071: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0454 - acc: 0.9952 - val_loss: 3.8902 - val_acc: 0.4305\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9951\n",
      "Epoch 00072: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0446 - acc: 0.9951 - val_loss: 3.9132 - val_acc: 0.4330\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9953\n",
      "Epoch 00073: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0424 - acc: 0.9953 - val_loss: 3.9583 - val_acc: 0.4167\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9949\n",
      "Epoch 00074: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.0431 - acc: 0.9949 - val_loss: 3.9710 - val_acc: 0.4284\n",
      "Epoch 75/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9939\n",
      "Epoch 00075: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0468 - acc: 0.9938 - val_loss: 3.9653 - val_acc: 0.4309\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9951\n",
      "Epoch 00076: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0413 - acc: 0.9951 - val_loss: 3.9834 - val_acc: 0.4314\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9966\n",
      "Epoch 00077: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0376 - acc: 0.9966 - val_loss: 3.9822 - val_acc: 0.4365\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9965\n",
      "Epoch 00078: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0374 - acc: 0.9964 - val_loss: 4.0139 - val_acc: 0.4272\n",
      "Epoch 79/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9955\n",
      "Epoch 00079: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0413 - acc: 0.9955 - val_loss: 4.0330 - val_acc: 0.4337\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9953\n",
      "Epoch 00080: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0402 - acc: 0.9953 - val_loss: 4.0650 - val_acc: 0.4356\n",
      "Epoch 81/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9970\n",
      "Epoch 00081: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0346 - acc: 0.9970 - val_loss: 4.1968 - val_acc: 0.4188\n",
      "Epoch 82/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9966\n",
      "Epoch 00082: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0344 - acc: 0.9966 - val_loss: 4.1819 - val_acc: 0.4205\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9973\n",
      "Epoch 00083: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0335 - acc: 0.9973 - val_loss: 4.1491 - val_acc: 0.4249\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9948\n",
      "Epoch 00084: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0385 - acc: 0.9948 - val_loss: 4.1409 - val_acc: 0.4300\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9960\n",
      "Epoch 00085: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0353 - acc: 0.9960 - val_loss: 4.0864 - val_acc: 0.4323\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9977\n",
      "Epoch 00086: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0289 - acc: 0.9977 - val_loss: 4.1067 - val_acc: 0.4263\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9957\n",
      "Epoch 00087: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0366 - acc: 0.9957 - val_loss: 4.1535 - val_acc: 0.4305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9965\n",
      "Epoch 00088: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0323 - acc: 0.9965 - val_loss: 4.2212 - val_acc: 0.4232\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9945\n",
      "Epoch 00089: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0393 - acc: 0.9945 - val_loss: 4.2622 - val_acc: 0.4249\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9961\n",
      "Epoch 00090: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0344 - acc: 0.9961 - val_loss: 4.2600 - val_acc: 0.4328\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9964\n",
      "Epoch 00091: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0325 - acc: 0.9964 - val_loss: 4.2250 - val_acc: 0.4239\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9965\n",
      "Epoch 00092: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0312 - acc: 0.9964 - val_loss: 4.2324 - val_acc: 0.4246\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9964\n",
      "Epoch 00093: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0333 - acc: 0.9964 - val_loss: 4.2677 - val_acc: 0.4249\n",
      "Epoch 94/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9951\n",
      "Epoch 00094: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0355 - acc: 0.9951 - val_loss: 4.3352 - val_acc: 0.4205\n",
      "Epoch 95/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9964\n",
      "Epoch 00095: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0340 - acc: 0.9964 - val_loss: 4.2784 - val_acc: 0.4265\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9976\n",
      "Epoch 00096: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0256 - acc: 0.9976 - val_loss: 4.3100 - val_acc: 0.4205\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9948\n",
      "Epoch 00097: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0376 - acc: 0.9948 - val_loss: 4.2977 - val_acc: 0.4319\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9961\n",
      "Epoch 00098: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0312 - acc: 0.9961 - val_loss: 4.3322 - val_acc: 0.4293\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9949\n",
      "Epoch 00099: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0367 - acc: 0.9949 - val_loss: 4.3412 - val_acc: 0.4200\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9980\n",
      "Epoch 00100: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0249 - acc: 0.9980 - val_loss: 4.3381 - val_acc: 0.4309\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9968\n",
      "Epoch 00101: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0288 - acc: 0.9968 - val_loss: 4.4187 - val_acc: 0.4107\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9975\n",
      "Epoch 00102: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0255 - acc: 0.9974 - val_loss: 4.4709 - val_acc: 0.4111\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9926\n",
      "Epoch 00103: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0426 - acc: 0.9926 - val_loss: 4.4257 - val_acc: 0.4149\n",
      "Epoch 104/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9961\n",
      "Epoch 00104: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.0297 - acc: 0.9961 - val_loss: 4.4351 - val_acc: 0.4186\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9966\n",
      "Epoch 00105: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0290 - acc: 0.9966 - val_loss: 4.3743 - val_acc: 0.4258\n",
      "\n",
      "1D_CNN_BN_1_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 191us/sample - loss: 1.8429 - acc: 0.4289\n",
      "Loss: 1.8428832955325751 Accuracy: 0.42886811\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1350 - acc: 0.3176\n",
      "Epoch 00001: val_loss improved from inf to 1.79621, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/001-1.7962.hdf5\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 2.1341 - acc: 0.3178 - val_loss: 1.7962 - val_acc: 0.4067\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5566 - acc: 0.5060\n",
      "Epoch 00002: val_loss improved from 1.79621 to 1.46232, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/002-1.4623.hdf5\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 1.5562 - acc: 0.5062 - val_loss: 1.4623 - val_acc: 0.5423\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3504 - acc: 0.5758\n",
      "Epoch 00003: val_loss improved from 1.46232 to 1.41693, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/003-1.4169.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 1.3507 - acc: 0.5756 - val_loss: 1.4169 - val_acc: 0.5639\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2052 - acc: 0.6292\n",
      "Epoch 00004: val_loss improved from 1.41693 to 1.31060, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/004-1.3106.hdf5\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 1.2053 - acc: 0.6293 - val_loss: 1.3106 - val_acc: 0.5980\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1066 - acc: 0.6590\n",
      "Epoch 00005: val_loss did not improve from 1.31060\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 1.1066 - acc: 0.6590 - val_loss: 1.3107 - val_acc: 0.5926\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0206 - acc: 0.6885\n",
      "Epoch 00006: val_loss improved from 1.31060 to 1.29083, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/006-1.2908.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 1.0207 - acc: 0.6885 - val_loss: 1.2908 - val_acc: 0.5973\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9469 - acc: 0.7098\n",
      "Epoch 00007: val_loss improved from 1.29083 to 1.26857, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/007-1.2686.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.9471 - acc: 0.7100 - val_loss: 1.2686 - val_acc: 0.6061\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8790 - acc: 0.7327\n",
      "Epoch 00008: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.8790 - acc: 0.7326 - val_loss: 1.3056 - val_acc: 0.6066\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8207 - acc: 0.7587\n",
      "Epoch 00009: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.8208 - acc: 0.7586 - val_loss: 1.3215 - val_acc: 0.5924\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7727 - acc: 0.7682\n",
      "Epoch 00010: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.7724 - acc: 0.7682 - val_loss: 1.2896 - val_acc: 0.6198\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7215 - acc: 0.7855\n",
      "Epoch 00011: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.7212 - acc: 0.7857 - val_loss: 1.3149 - val_acc: 0.6129\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6746 - acc: 0.8024\n",
      "Epoch 00012: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.6747 - acc: 0.8024 - val_loss: 1.3599 - val_acc: 0.5996\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6378 - acc: 0.8151\n",
      "Epoch 00013: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.6380 - acc: 0.8151 - val_loss: 1.3545 - val_acc: 0.5991\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.8239\n",
      "Epoch 00014: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.6061 - acc: 0.8238 - val_loss: 1.3659 - val_acc: 0.5996\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.8372\n",
      "Epoch 00015: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.5702 - acc: 0.8372 - val_loss: 1.3470 - val_acc: 0.6138\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.8502\n",
      "Epoch 00016: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.5369 - acc: 0.8502 - val_loss: 1.4343 - val_acc: 0.5977\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8588\n",
      "Epoch 00017: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.5066 - acc: 0.8588 - val_loss: 1.3813 - val_acc: 0.6026\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8678\n",
      "Epoch 00018: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.4785 - acc: 0.8677 - val_loss: 1.3761 - val_acc: 0.6143\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8757\n",
      "Epoch 00019: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.4565 - acc: 0.8756 - val_loss: 1.4718 - val_acc: 0.5949\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.8870\n",
      "Epoch 00020: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4274 - acc: 0.8869 - val_loss: 1.4679 - val_acc: 0.5931\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8929\n",
      "Epoch 00021: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.4056 - acc: 0.8929 - val_loss: 1.4163 - val_acc: 0.6145\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3846 - acc: 0.9003\n",
      "Epoch 00022: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.3845 - acc: 0.9004 - val_loss: 1.5123 - val_acc: 0.5926\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.9053\n",
      "Epoch 00023: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.3667 - acc: 0.9052 - val_loss: 1.5105 - val_acc: 0.6019\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.9141\n",
      "Epoch 00024: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.3430 - acc: 0.9141 - val_loss: 1.5595 - val_acc: 0.5912\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.9181\n",
      "Epoch 00025: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.3293 - acc: 0.9181 - val_loss: 1.5455 - val_acc: 0.5954\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.9243\n",
      "Epoch 00026: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.3118 - acc: 0.9243 - val_loss: 1.5494 - val_acc: 0.5968\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9298\n",
      "Epoch 00027: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.2948 - acc: 0.9297 - val_loss: 1.5659 - val_acc: 0.6010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9345\n",
      "Epoch 00028: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.2791 - acc: 0.9344 - val_loss: 1.6914 - val_acc: 0.5749\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9354\n",
      "Epoch 00029: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.2748 - acc: 0.9354 - val_loss: 1.6060 - val_acc: 0.5947\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9430\n",
      "Epoch 00030: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.2535 - acc: 0.9430 - val_loss: 1.5833 - val_acc: 0.5980\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9490\n",
      "Epoch 00031: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.2389 - acc: 0.9491 - val_loss: 1.7322 - val_acc: 0.5768\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9496\n",
      "Epoch 00032: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.2314 - acc: 0.9495 - val_loss: 1.6307 - val_acc: 0.5952\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9533\n",
      "Epoch 00033: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.2209 - acc: 0.9534 - val_loss: 1.7373 - val_acc: 0.5702\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9573\n",
      "Epoch 00034: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.2074 - acc: 0.9573 - val_loss: 1.7112 - val_acc: 0.5842\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9607\n",
      "Epoch 00035: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.1983 - acc: 0.9607 - val_loss: 1.6949 - val_acc: 0.5949\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9645\n",
      "Epoch 00036: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.1853 - acc: 0.9645 - val_loss: 1.7366 - val_acc: 0.5926\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9657\n",
      "Epoch 00037: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1806 - acc: 0.9657 - val_loss: 1.7994 - val_acc: 0.5754\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9675\n",
      "Epoch 00038: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.1724 - acc: 0.9675 - val_loss: 1.7806 - val_acc: 0.5884\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9704\n",
      "Epoch 00039: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.1637 - acc: 0.9704 - val_loss: 1.8547 - val_acc: 0.5749\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9735\n",
      "Epoch 00040: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1537 - acc: 0.9735 - val_loss: 1.9483 - val_acc: 0.5747\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9744\n",
      "Epoch 00041: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1478 - acc: 0.9744 - val_loss: 1.8714 - val_acc: 0.5765\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9771\n",
      "Epoch 00042: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.1403 - acc: 0.9770 - val_loss: 1.9207 - val_acc: 0.5691\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9772\n",
      "Epoch 00043: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.1401 - acc: 0.9772 - val_loss: 1.8807 - val_acc: 0.5772\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9778\n",
      "Epoch 00044: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.1350 - acc: 0.9779 - val_loss: 1.9234 - val_acc: 0.5772\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9818\n",
      "Epoch 00045: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.1229 - acc: 0.9819 - val_loss: 1.9042 - val_acc: 0.5893\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9834\n",
      "Epoch 00046: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.1161 - acc: 0.9834 - val_loss: 1.9735 - val_acc: 0.5798\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9834\n",
      "Epoch 00047: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1135 - acc: 0.9833 - val_loss: 2.1074 - val_acc: 0.5607\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9818\n",
      "Epoch 00048: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.1165 - acc: 0.9819 - val_loss: 1.9288 - val_acc: 0.5840\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9864\n",
      "Epoch 00049: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.1018 - acc: 0.9863 - val_loss: 2.0568 - val_acc: 0.5721\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9864\n",
      "Epoch 00050: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.1009 - acc: 0.9864 - val_loss: 2.0320 - val_acc: 0.5788\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9854\n",
      "Epoch 00051: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1007 - acc: 0.9854 - val_loss: 2.0375 - val_acc: 0.5793\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9872\n",
      "Epoch 00052: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.0956 - acc: 0.9871 - val_loss: 2.1069 - val_acc: 0.5707\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9875\n",
      "Epoch 00053: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0929 - acc: 0.9874 - val_loss: 2.0321 - val_acc: 0.5833\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9902\n",
      "Epoch 00054: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.0857 - acc: 0.9902 - val_loss: 2.0959 - val_acc: 0.5791\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9905\n",
      "Epoch 00055: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0805 - acc: 0.9905 - val_loss: 2.0936 - val_acc: 0.5826\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9890\n",
      "Epoch 00056: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0851 - acc: 0.9890 - val_loss: 2.1139 - val_acc: 0.5714\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9921\n",
      "Epoch 00057: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.0748 - acc: 0.9921 - val_loss: 2.1185 - val_acc: 0.5821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9906\n",
      "Epoch 00058: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.0755 - acc: 0.9906 - val_loss: 2.1546 - val_acc: 0.5723\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9932\n",
      "Epoch 00059: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0688 - acc: 0.9932 - val_loss: 2.1704 - val_acc: 0.5761\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9925\n",
      "Epoch 00060: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.0693 - acc: 0.9925 - val_loss: 2.2797 - val_acc: 0.5663\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9920\n",
      "Epoch 00061: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0698 - acc: 0.9920 - val_loss: 2.1985 - val_acc: 0.5830\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9922\n",
      "Epoch 00062: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.0690 - acc: 0.9922 - val_loss: 2.3290 - val_acc: 0.5567\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9942\n",
      "Epoch 00063: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0597 - acc: 0.9942 - val_loss: 2.3741 - val_acc: 0.5609\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9935\n",
      "Epoch 00064: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0616 - acc: 0.9935 - val_loss: 2.4129 - val_acc: 0.5535\n",
      "Epoch 65/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9936\n",
      "Epoch 00065: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.0604 - acc: 0.9936 - val_loss: 2.2744 - val_acc: 0.5737\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9926\n",
      "Epoch 00066: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0624 - acc: 0.9926 - val_loss: 2.3136 - val_acc: 0.5705\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9936\n",
      "Epoch 00067: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0586 - acc: 0.9936 - val_loss: 2.2896 - val_acc: 0.5712\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9926\n",
      "Epoch 00068: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0618 - acc: 0.9926 - val_loss: 2.2968 - val_acc: 0.5740\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9956\n",
      "Epoch 00069: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.0490 - acc: 0.9956 - val_loss: 2.3332 - val_acc: 0.5805\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9950\n",
      "Epoch 00070: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0508 - acc: 0.9950 - val_loss: 2.3621 - val_acc: 0.5716\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9948\n",
      "Epoch 00071: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0506 - acc: 0.9948 - val_loss: 2.3363 - val_acc: 0.5756\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9953\n",
      "Epoch 00072: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0497 - acc: 0.9954 - val_loss: 2.4182 - val_acc: 0.5663\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9935\n",
      "Epoch 00073: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0563 - acc: 0.9934 - val_loss: 2.3859 - val_acc: 0.5777\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9966\n",
      "Epoch 00074: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0433 - acc: 0.9965 - val_loss: 2.4557 - val_acc: 0.5646\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9920\n",
      "Epoch 00075: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0579 - acc: 0.9920 - val_loss: 2.4103 - val_acc: 0.5705\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9967\n",
      "Epoch 00076: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0417 - acc: 0.9967 - val_loss: 2.4953 - val_acc: 0.5646\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9963\n",
      "Epoch 00077: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0414 - acc: 0.9962 - val_loss: 2.4624 - val_acc: 0.5712\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9955\n",
      "Epoch 00078: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.0440 - acc: 0.9955 - val_loss: 2.4798 - val_acc: 0.5777\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9955\n",
      "Epoch 00079: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0440 - acc: 0.9955 - val_loss: 2.7553 - val_acc: 0.5406\n",
      "Epoch 80/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9951\n",
      "Epoch 00080: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0434 - acc: 0.9951 - val_loss: 2.5263 - val_acc: 0.5621\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9980\n",
      "Epoch 00081: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0335 - acc: 0.9980 - val_loss: 2.4975 - val_acc: 0.5714\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9965\n",
      "Epoch 00082: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0391 - acc: 0.9965 - val_loss: 2.5135 - val_acc: 0.5672\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9959\n",
      "Epoch 00083: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.0392 - acc: 0.9959 - val_loss: 2.7452 - val_acc: 0.5502\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9958\n",
      "Epoch 00084: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0402 - acc: 0.9957 - val_loss: 2.5383 - val_acc: 0.5625\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9964\n",
      "Epoch 00085: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0387 - acc: 0.9964 - val_loss: 2.6869 - val_acc: 0.5553\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9971\n",
      "Epoch 00086: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0338 - acc: 0.9971 - val_loss: 2.5321 - val_acc: 0.5761\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9962\n",
      "Epoch 00087: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0358 - acc: 0.9962 - val_loss: 2.9158 - val_acc: 0.5285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9954\n",
      "Epoch 00088: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0389 - acc: 0.9953 - val_loss: 2.5815 - val_acc: 0.5632\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9952\n",
      "Epoch 00089: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0389 - acc: 0.9952 - val_loss: 2.7014 - val_acc: 0.5467\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9968\n",
      "Epoch 00090: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0334 - acc: 0.9968 - val_loss: 2.6241 - val_acc: 0.5577\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9974\n",
      "Epoch 00091: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0295 - acc: 0.9974 - val_loss: 2.6109 - val_acc: 0.5728\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9976\n",
      "Epoch 00092: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0291 - acc: 0.9976 - val_loss: 2.6223 - val_acc: 0.5714\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9961\n",
      "Epoch 00093: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.0351 - acc: 0.9961 - val_loss: 2.6340 - val_acc: 0.5749\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9978\n",
      "Epoch 00094: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0299 - acc: 0.9979 - val_loss: 2.6107 - val_acc: 0.5688\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9976\n",
      "Epoch 00095: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0291 - acc: 0.9975 - val_loss: 2.7903 - val_acc: 0.5614\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9933\n",
      "Epoch 00096: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0407 - acc: 0.9933 - val_loss: 2.7119 - val_acc: 0.5688\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9973\n",
      "Epoch 00097: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0291 - acc: 0.9973 - val_loss: 2.6763 - val_acc: 0.5695\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9970\n",
      "Epoch 00098: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0302 - acc: 0.9970 - val_loss: 2.8048 - val_acc: 0.5530\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9956\n",
      "Epoch 00099: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0350 - acc: 0.9956 - val_loss: 2.7792 - val_acc: 0.5656\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9959\n",
      "Epoch 00100: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0341 - acc: 0.9959 - val_loss: 2.8533 - val_acc: 0.5535\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9980\n",
      "Epoch 00101: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.0246 - acc: 0.9980 - val_loss: 2.7116 - val_acc: 0.5674\n",
      "Epoch 102/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9977\n",
      "Epoch 00102: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0262 - acc: 0.9977 - val_loss: 2.8080 - val_acc: 0.5558\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9966\n",
      "Epoch 00103: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0301 - acc: 0.9966 - val_loss: 3.0131 - val_acc: 0.5430\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9981\n",
      "Epoch 00104: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0244 - acc: 0.9982 - val_loss: 2.9776 - val_acc: 0.5360\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9977\n",
      "Epoch 00105: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0264 - acc: 0.9977 - val_loss: 2.7957 - val_acc: 0.5653\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9967\n",
      "Epoch 00106: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0279 - acc: 0.9967 - val_loss: 2.9051 - val_acc: 0.5437\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9974\n",
      "Epoch 00107: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0258 - acc: 0.9974 - val_loss: 2.8489 - val_acc: 0.5621\n",
      "\n",
      "1D_CNN_BN_2_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 219us/sample - loss: 1.3207 - acc: 0.5836\n",
      "Loss: 1.3206864595289418 Accuracy: 0.58359295\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9744 - acc: 0.3749\n",
      "Epoch 00001: val_loss improved from inf to 1.58715, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/001-1.5871.hdf5\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 1.9738 - acc: 0.3751 - val_loss: 1.5871 - val_acc: 0.4962\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3655 - acc: 0.5724\n",
      "Epoch 00002: val_loss improved from 1.58715 to 1.24479, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/002-1.2448.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 1.3649 - acc: 0.5726 - val_loss: 1.2448 - val_acc: 0.6056\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1688 - acc: 0.6452\n",
      "Epoch 00003: val_loss improved from 1.24479 to 1.11074, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/003-1.1107.hdf5\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 1.1692 - acc: 0.6451 - val_loss: 1.1107 - val_acc: 0.6622\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0459 - acc: 0.6823\n",
      "Epoch 00004: val_loss improved from 1.11074 to 1.00577, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/004-1.0058.hdf5\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 1.0464 - acc: 0.6821 - val_loss: 1.0058 - val_acc: 0.7002\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9496 - acc: 0.7149\n",
      "Epoch 00005: val_loss improved from 1.00577 to 0.92628, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/005-0.9263.hdf5\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.9499 - acc: 0.7148 - val_loss: 0.9263 - val_acc: 0.7230\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8700 - acc: 0.7428\n",
      "Epoch 00006: val_loss did not improve from 0.92628\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.8700 - acc: 0.7428 - val_loss: 1.0259 - val_acc: 0.6876\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8067 - acc: 0.7606\n",
      "Epoch 00007: val_loss improved from 0.92628 to 0.85093, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/007-0.8509.hdf5\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.8066 - acc: 0.7607 - val_loss: 0.8509 - val_acc: 0.7508\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7473 - acc: 0.7801\n",
      "Epoch 00008: val_loss improved from 0.85093 to 0.81987, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/008-0.8199.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.7475 - acc: 0.7801 - val_loss: 0.8199 - val_acc: 0.7671\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7001 - acc: 0.7962\n",
      "Epoch 00009: val_loss did not improve from 0.81987\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.7004 - acc: 0.7962 - val_loss: 0.8648 - val_acc: 0.7475\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6542 - acc: 0.8114\n",
      "Epoch 00010: val_loss improved from 0.81987 to 0.80093, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/010-0.8009.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.6545 - acc: 0.8112 - val_loss: 0.8009 - val_acc: 0.7696\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6143 - acc: 0.8249\n",
      "Epoch 00011: val_loss improved from 0.80093 to 0.79592, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/011-0.7959.hdf5\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.6142 - acc: 0.8248 - val_loss: 0.7959 - val_acc: 0.7596\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5760 - acc: 0.8356\n",
      "Epoch 00012: val_loss did not improve from 0.79592\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.5756 - acc: 0.8356 - val_loss: 0.8011 - val_acc: 0.7603\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.8445\n",
      "Epoch 00013: val_loss improved from 0.79592 to 0.72948, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/013-0.7295.hdf5\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.5460 - acc: 0.8444 - val_loss: 0.7295 - val_acc: 0.7880\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.8554\n",
      "Epoch 00014: val_loss did not improve from 0.72948\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.5132 - acc: 0.8555 - val_loss: 0.7412 - val_acc: 0.7876\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.8619\n",
      "Epoch 00015: val_loss did not improve from 0.72948\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.4894 - acc: 0.8619 - val_loss: 0.7335 - val_acc: 0.7887\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4624 - acc: 0.8709\n",
      "Epoch 00016: val_loss improved from 0.72948 to 0.69788, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/016-0.6979.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.4630 - acc: 0.8709 - val_loss: 0.6979 - val_acc: 0.7976\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8786\n",
      "Epoch 00017: val_loss improved from 0.69788 to 0.68028, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/017-0.6803.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.4359 - acc: 0.8787 - val_loss: 0.6803 - val_acc: 0.8057\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8852\n",
      "Epoch 00018: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.4135 - acc: 0.8852 - val_loss: 0.7813 - val_acc: 0.7766\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8904\n",
      "Epoch 00019: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.3967 - acc: 0.8904 - val_loss: 0.8332 - val_acc: 0.7543\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8981\n",
      "Epoch 00020: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.3742 - acc: 0.8981 - val_loss: 0.6815 - val_acc: 0.8036\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.9049\n",
      "Epoch 00021: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.3542 - acc: 0.9049 - val_loss: 0.7289 - val_acc: 0.7939\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.9109\n",
      "Epoch 00022: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.3370 - acc: 0.9109 - val_loss: 0.7830 - val_acc: 0.7717\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.9165\n",
      "Epoch 00023: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.3168 - acc: 0.9165 - val_loss: 0.7678 - val_acc: 0.7864\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9210\n",
      "Epoch 00024: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.3024 - acc: 0.9210 - val_loss: 0.7412 - val_acc: 0.7911\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9250\n",
      "Epoch 00025: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 337us/sample - loss: 0.2894 - acc: 0.9251 - val_loss: 0.6844 - val_acc: 0.8006\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9317\n",
      "Epoch 00026: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.2701 - acc: 0.9316 - val_loss: 0.7507 - val_acc: 0.7894\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9338\n",
      "Epoch 00027: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2603 - acc: 0.9337 - val_loss: 0.7253 - val_acc: 0.7964\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9395\n",
      "Epoch 00028: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.2475 - acc: 0.9394 - val_loss: 0.7571 - val_acc: 0.7894\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9418\n",
      "Epoch 00029: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.2379 - acc: 0.9416 - val_loss: 0.6999 - val_acc: 0.7962\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9481\n",
      "Epoch 00030: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.2191 - acc: 0.9481 - val_loss: 0.6982 - val_acc: 0.8137\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9517\n",
      "Epoch 00031: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2100 - acc: 0.9517 - val_loss: 0.6972 - val_acc: 0.8048\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9517\n",
      "Epoch 00032: val_loss improved from 0.68028 to 0.66477, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/032-0.6648.hdf5\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2048 - acc: 0.9517 - val_loss: 0.6648 - val_acc: 0.8169\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9559\n",
      "Epoch 00033: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.1889 - acc: 0.9559 - val_loss: 0.8468 - val_acc: 0.7570\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9603\n",
      "Epoch 00034: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.1812 - acc: 0.9604 - val_loss: 0.7747 - val_acc: 0.7878\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9625\n",
      "Epoch 00035: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1742 - acc: 0.9625 - val_loss: 0.7415 - val_acc: 0.7873\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9646\n",
      "Epoch 00036: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1628 - acc: 0.9646 - val_loss: 0.8604 - val_acc: 0.7680\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9662\n",
      "Epoch 00037: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1570 - acc: 0.9663 - val_loss: 0.7752 - val_acc: 0.7913\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9691\n",
      "Epoch 00038: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1478 - acc: 0.9690 - val_loss: 0.7509 - val_acc: 0.7978\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9694\n",
      "Epoch 00039: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1461 - acc: 0.9694 - val_loss: 0.7331 - val_acc: 0.8067\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9717\n",
      "Epoch 00040: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.1372 - acc: 0.9717 - val_loss: 0.8117 - val_acc: 0.7713\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9765\n",
      "Epoch 00041: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1263 - acc: 0.9765 - val_loss: 0.7304 - val_acc: 0.8109\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9760\n",
      "Epoch 00042: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1267 - acc: 0.9760 - val_loss: 0.7428 - val_acc: 0.7999\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9801\n",
      "Epoch 00043: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1146 - acc: 0.9801 - val_loss: 0.9331 - val_acc: 0.7587\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9802\n",
      "Epoch 00044: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.1120 - acc: 0.9802 - val_loss: 0.7338 - val_acc: 0.8048\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9815\n",
      "Epoch 00045: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1057 - acc: 0.9815 - val_loss: 0.8283 - val_acc: 0.7843\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9821\n",
      "Epoch 00046: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.1028 - acc: 0.9822 - val_loss: 0.8057 - val_acc: 0.7897\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9846\n",
      "Epoch 00047: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0963 - acc: 0.9845 - val_loss: 0.7914 - val_acc: 0.7934\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9831\n",
      "Epoch 00048: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0972 - acc: 0.9831 - val_loss: 0.7703 - val_acc: 0.7978\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9861\n",
      "Epoch 00049: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 340us/sample - loss: 0.0865 - acc: 0.9861 - val_loss: 0.9617 - val_acc: 0.7596\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9873\n",
      "Epoch 00050: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.0840 - acc: 0.9873 - val_loss: 0.8427 - val_acc: 0.7850\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9887\n",
      "Epoch 00051: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.0794 - acc: 0.9887 - val_loss: 0.7953 - val_acc: 0.7973\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9886\n",
      "Epoch 00052: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 337us/sample - loss: 0.0771 - acc: 0.9885 - val_loss: 0.8110 - val_acc: 0.7999\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9887\n",
      "Epoch 00053: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.0752 - acc: 0.9887 - val_loss: 0.8018 - val_acc: 0.8043\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9894\n",
      "Epoch 00054: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0748 - acc: 0.9894 - val_loss: 0.8765 - val_acc: 0.7801\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9904\n",
      "Epoch 00055: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0670 - acc: 0.9904 - val_loss: 0.8312 - val_acc: 0.7980\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9922\n",
      "Epoch 00056: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0631 - acc: 0.9921 - val_loss: 0.9565 - val_acc: 0.7710\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9888\n",
      "Epoch 00057: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0714 - acc: 0.9888 - val_loss: 0.9491 - val_acc: 0.7761\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9938\n",
      "Epoch 00058: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0567 - acc: 0.9938 - val_loss: 0.8767 - val_acc: 0.7973\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9913\n",
      "Epoch 00059: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0612 - acc: 0.9913 - val_loss: 0.8559 - val_acc: 0.7971\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9936\n",
      "Epoch 00060: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0538 - acc: 0.9936 - val_loss: 0.9201 - val_acc: 0.7745\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9935\n",
      "Epoch 00061: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0538 - acc: 0.9935 - val_loss: 0.8126 - val_acc: 0.8050\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9927\n",
      "Epoch 00062: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.0554 - acc: 0.9926 - val_loss: 0.8875 - val_acc: 0.7799\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9960\n",
      "Epoch 00063: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0460 - acc: 0.9959 - val_loss: 1.0305 - val_acc: 0.7561\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9908\n",
      "Epoch 00064: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0586 - acc: 0.9908 - val_loss: 0.8197 - val_acc: 0.8053\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9954\n",
      "Epoch 00065: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0446 - acc: 0.9954 - val_loss: 0.8641 - val_acc: 0.8046\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9947\n",
      "Epoch 00066: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0474 - acc: 0.9947 - val_loss: 0.8840 - val_acc: 0.7927\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9951\n",
      "Epoch 00067: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0436 - acc: 0.9951 - val_loss: 0.9204 - val_acc: 0.7894\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9958\n",
      "Epoch 00068: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 337us/sample - loss: 0.0422 - acc: 0.9958 - val_loss: 1.0181 - val_acc: 0.7729\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9943\n",
      "Epoch 00069: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0456 - acc: 0.9943 - val_loss: 0.9992 - val_acc: 0.7675\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9959\n",
      "Epoch 00070: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0406 - acc: 0.9959 - val_loss: 0.8884 - val_acc: 0.7952\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9953\n",
      "Epoch 00071: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0412 - acc: 0.9954 - val_loss: 0.9482 - val_acc: 0.7820\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9963\n",
      "Epoch 00072: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.0349 - acc: 0.9963 - val_loss: 0.9452 - val_acc: 0.7734\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9961\n",
      "Epoch 00073: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0373 - acc: 0.9961 - val_loss: 0.9465 - val_acc: 0.7899\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9945\n",
      "Epoch 00074: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0422 - acc: 0.9945 - val_loss: 0.8982 - val_acc: 0.7920\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9967\n",
      "Epoch 00075: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0353 - acc: 0.9966 - val_loss: 0.9884 - val_acc: 0.7773\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9924\n",
      "Epoch 00076: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0502 - acc: 0.9924 - val_loss: 0.8880 - val_acc: 0.8006\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9969\n",
      "Epoch 00077: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0329 - acc: 0.9969 - val_loss: 0.9128 - val_acc: 0.7897\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9954\n",
      "Epoch 00078: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0362 - acc: 0.9954 - val_loss: 0.8962 - val_acc: 0.7994\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9970\n",
      "Epoch 00079: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0324 - acc: 0.9970 - val_loss: 1.1286 - val_acc: 0.7661\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9973\n",
      "Epoch 00080: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0307 - acc: 0.9973 - val_loss: 0.9891 - val_acc: 0.7885\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9969\n",
      "Epoch 00081: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.0317 - acc: 0.9969 - val_loss: 0.9794 - val_acc: 0.7904\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9978\n",
      "Epoch 00082: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 340us/sample - loss: 0.0258 - acc: 0.9979 - val_loss: 1.0307 - val_acc: 0.7859\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9964\n",
      "Epoch 00083: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0304 - acc: 0.9964 - val_loss: 0.9535 - val_acc: 0.7918\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9973\n",
      "Epoch 00084: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0295 - acc: 0.9973 - val_loss: 0.8958 - val_acc: 0.8015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9967\n",
      "Epoch 00085: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0301 - acc: 0.9967 - val_loss: 1.1645 - val_acc: 0.7624\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9945\n",
      "Epoch 00086: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0383 - acc: 0.9945 - val_loss: 1.0364 - val_acc: 0.7803\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9978\n",
      "Epoch 00087: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0259 - acc: 0.9978 - val_loss: 0.9002 - val_acc: 0.7985\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9956\n",
      "Epoch 00088: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0324 - acc: 0.9956 - val_loss: 0.9141 - val_acc: 0.7994\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9984\n",
      "Epoch 00089: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0224 - acc: 0.9984 - val_loss: 0.9100 - val_acc: 0.8039\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9972\n",
      "Epoch 00090: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0256 - acc: 0.9972 - val_loss: 1.0264 - val_acc: 0.7876\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9954\n",
      "Epoch 00091: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0328 - acc: 0.9954 - val_loss: 0.8741 - val_acc: 0.8104\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9986\n",
      "Epoch 00092: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0204 - acc: 0.9986 - val_loss: 1.1427 - val_acc: 0.7601\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9965\n",
      "Epoch 00093: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0270 - acc: 0.9965 - val_loss: 0.9316 - val_acc: 0.8025\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9965\n",
      "Epoch 00094: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0271 - acc: 0.9965 - val_loss: 0.9275 - val_acc: 0.8048\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9976\n",
      "Epoch 00095: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0242 - acc: 0.9976 - val_loss: 0.9845 - val_acc: 0.7929\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9969\n",
      "Epoch 00096: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0269 - acc: 0.9969 - val_loss: 1.2078 - val_acc: 0.7582\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9974\n",
      "Epoch 00097: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0237 - acc: 0.9974 - val_loss: 0.8993 - val_acc: 0.8146\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9987\n",
      "Epoch 00098: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0183 - acc: 0.9987 - val_loss: 1.0594 - val_acc: 0.7848\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9945\n",
      "Epoch 00099: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.0319 - acc: 0.9945 - val_loss: 1.0541 - val_acc: 0.7822\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9993\n",
      "Epoch 00100: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.0160 - acc: 0.9993 - val_loss: 0.9642 - val_acc: 0.8034\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9970\n",
      "Epoch 00101: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0255 - acc: 0.9970 - val_loss: 0.9944 - val_acc: 0.7920\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9968\n",
      "Epoch 00102: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0256 - acc: 0.9968 - val_loss: 0.9446 - val_acc: 0.8015\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9991\n",
      "Epoch 00103: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0159 - acc: 0.9991 - val_loss: 1.0336 - val_acc: 0.7894\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9983\n",
      "Epoch 00104: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0191 - acc: 0.9983 - val_loss: 0.9957 - val_acc: 0.7980\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9979\n",
      "Epoch 00105: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0204 - acc: 0.9979 - val_loss: 1.4740 - val_acc: 0.7177\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9983\n",
      "Epoch 00106: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0197 - acc: 0.9983 - val_loss: 1.0934 - val_acc: 0.7754\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9965\n",
      "Epoch 00107: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0253 - acc: 0.9964 - val_loss: 1.0364 - val_acc: 0.7904\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9973\n",
      "Epoch 00108: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0219 - acc: 0.9973 - val_loss: 1.1627 - val_acc: 0.7796\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9969\n",
      "Epoch 00109: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0225 - acc: 0.9969 - val_loss: 0.9169 - val_acc: 0.8153\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9984\n",
      "Epoch 00110: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.0170 - acc: 0.9985 - val_loss: 1.0546 - val_acc: 0.7913\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9985\n",
      "Epoch 00111: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0167 - acc: 0.9985 - val_loss: 1.3170 - val_acc: 0.7582\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9982\n",
      "Epoch 00112: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0192 - acc: 0.9982 - val_loss: 0.9477 - val_acc: 0.8069\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9974\n",
      "Epoch 00113: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0198 - acc: 0.9974 - val_loss: 1.1765 - val_acc: 0.7766\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9965\n",
      "Epoch 00114: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0249 - acc: 0.9965 - val_loss: 0.9813 - val_acc: 0.7990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9981\n",
      "Epoch 00115: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0182 - acc: 0.9980 - val_loss: 1.0355 - val_acc: 0.7918\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9988\n",
      "Epoch 00116: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0153 - acc: 0.9988 - val_loss: 1.3192 - val_acc: 0.7487\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9947\n",
      "Epoch 00117: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0307 - acc: 0.9948 - val_loss: 0.9366 - val_acc: 0.8088\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9988\n",
      "Epoch 00118: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.0148 - acc: 0.9988 - val_loss: 0.9721 - val_acc: 0.8076\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9989\n",
      "Epoch 00119: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0146 - acc: 0.9989 - val_loss: 0.9807 - val_acc: 0.8097\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9974\n",
      "Epoch 00120: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0191 - acc: 0.9974 - val_loss: 1.2052 - val_acc: 0.7706\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9976\n",
      "Epoch 00121: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0193 - acc: 0.9976 - val_loss: 1.0935 - val_acc: 0.7855\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9976\n",
      "Epoch 00122: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0189 - acc: 0.9976 - val_loss: 1.1995 - val_acc: 0.7736\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9982\n",
      "Epoch 00123: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.0168 - acc: 0.9982 - val_loss: 1.0604 - val_acc: 0.7973\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9985\n",
      "Epoch 00124: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0160 - acc: 0.9984 - val_loss: 1.0581 - val_acc: 0.7899\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9955\n",
      "Epoch 00125: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0256 - acc: 0.9955 - val_loss: 0.9700 - val_acc: 0.8041\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9991\n",
      "Epoch 00126: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0125 - acc: 0.9991 - val_loss: 1.0052 - val_acc: 0.8062\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9985\n",
      "Epoch 00127: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0144 - acc: 0.9985 - val_loss: 1.0677 - val_acc: 0.7941\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9959\n",
      "Epoch 00128: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0250 - acc: 0.9959 - val_loss: 1.0036 - val_acc: 0.8057\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9986\n",
      "Epoch 00129: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0146 - acc: 0.9986 - val_loss: 1.0918 - val_acc: 0.7959\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9988\n",
      "Epoch 00130: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0135 - acc: 0.9988 - val_loss: 1.1004 - val_acc: 0.7929\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9993\n",
      "Epoch 00131: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0115 - acc: 0.9993 - val_loss: 1.1086 - val_acc: 0.7843\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9970\n",
      "Epoch 00132: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0210 - acc: 0.9970 - val_loss: 1.2484 - val_acc: 0.7603\n",
      "\n",
      "1D_CNN_BN_3_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4VEX3x7+zm05CKr2FXpJAgABBpAkioFLkRUAQlaa+WBB/vIKNIKAoYAFBRJogRaQICIoghNClJfQWSCCNNNLr7p7fH2dvdpNskk3IJgTm8zz32d17Z+6du8nO955zZs4IIoJEIpFIJCWhquwGSCQSiaRqIAVDIpFIJGYhBUMikUgkZiEFQyKRSCRmIQVDIpFIJGYhBUMikUgkZiEFQyKRSCRmIQVDIpFIJGYhBUMikUgkZmFV2Q0oTzw8PMjT07OymyGRSCRVhjNnzsQTUQ1zyj5SguHp6YnTp09XdjMkEomkyiCECDe3rHRJSSQSicQspGBIJBKJxCwsJhhCiAZCiINCiMtCiEtCiHdNlBFCiEVCiJtCiPNCiA5Gx14RQtzQb69Yqp0SiUQiMQ9LxjA0AN4norNCCCcAZ4QQ+4joslGZAQCa67cuAH4A0EUI4QZgJgA/AKSvu5OI7pe2Ebm5uYiIiEBWVtaD3s9jiZ2dHerXrw9ra+vKbopEIqlkLCYYRBQNIFr/PlUIcQVAPQDGgjEYwFriRTlOCCFchBB1APQCsI+IEgFACLEPQH8AG0vbjoiICDg5OcHT0xNCiAe6p8cNIkJCQgIiIiLQuHHjym6ORCKpZCokhiGE8ATQHsDJAofqAbhr9DlCv6+o/aUmKysL7u7uUizKgBAC7u7u0jqTSCQAKkAwhBCOALYCmEJEKRY4/yQhxGkhxOm4uLiiypT3ZR8b5HcnkUgULCoYQghrsFisJ6JtJopEAmhg9Lm+fl9R+wtBRMuJyI+I/GrUMGvuSSGys6Og0SSXqa5EIpE8LlhylJQAsBLAFSL6uohiOwGM1Y+W8geQrI997AXQTwjhKoRwBdBPv88i5OTEQKMpd+MHAJCUlISlS5eWqe7AgQORlJRkdvmAgAAsWLCgTNeSSCSSkrCkhdENwMsAnhJCBOu3gUKIN4QQb+jL7AFwC8BNAD8B+C8A6IPdswGc0m+fKQFwSyCECoDOIucuTjA0Gk2xdffs2QMXFxdLNEsikUhKjcUEg4iOEJEgorZE5Kvf9hDRMiJapi9DRDSZiJoSkQ8RnTaqv4qImum31ZZqJ6MCkWUEY/r06QgNDYWvry+mTZuGwMBAdO/eHYMGDUKbNm0AAEOGDEHHjh3h5eWF5cuX59X19PREfHw8wsLC0Lp1a0ycOBFeXl7o168fMjMzi71ucHAw/P390bZtWwwdOhT37/OI5EWLFqFNmzZo27YtRo4cCQA4dOgQfH194evri/bt2yM1NdUi34VEIqnaPFK5pErixo0pSEsLLrRfp0sHoIJKZV/qczo6+qJ582+LPD5v3jxcvHgRwcF83cDAQJw9exYXL17MG6q6atUquLm5ITMzE506dcKwYcPg7u5eoO03sHHjRvz000948cUXsXXrVowZM6bI644dOxaLFy9Gz5498emnn2LWrFn49ttvMW/ePNy+fRu2trZ57q4FCxZgyZIl6NatG9LS0mBnZ1fq70EikTz6yNQgAICKHQnUuXPnfPMaFi1ahHbt2sHf3x93797FjRs3CtVp3LgxfH19AQAdO3ZEWFhYkedPTk5GUlISevbsCQB45ZVXEBQUBABo27YtRo8ejV9++QVWVvy80K1bN0ydOhWLFi1CUlJS3n6JRCIx5rHqGYqyBDIyrgIQcHBoWSHtqFatWt77wMBA7N+/H8ePH4eDgwN69eplct6Dra1t3nu1Wl2iS6oodu/ejaCgIOzatQtz587FhQsXMH36dDz77LPYs2cPunXrhr1796JVq1ZlOr9EInl0kRYGAEvGMJycnIqNCSQnJ8PV1RUODg64evUqTpw48cDXdHZ2hqurKw4fPgwAWLduHXr27AmdToe7d++id+/e+PLLL5GcnIy0tDSEhobCx8cHH3zwATp16oSrV68+cBskEsmjx2NlYRSFECoQ5Vrk3O7u7ujWrRu8vb0xYMAAPPvss/mO9+/fH8uWLUPr1q3RsmVL+Pv7l8t1f/75Z7zxxhvIyMhAkyZNsHr1ami1WowZMwbJyckgIrzzzjtwcXHBJ598goMHD0KlUsHLywsDBgwolzZIJJJHC8FpnB4N/Pz8qOACSleuXEHr1q2LrZeZeQtabTocHX0s2bwqiznfoUQiqZoIIc4QkZ85ZaVLCgB/DZZxSUkkEsmjghQMKC4pKRgSiURSHFIwAEgLQyKRSEpGCgaU1CCERymeI5FIJOWNFAwAhq9BWhkSiURSFFIwoFgYkHEMiUQiKQYpGAAeNgvD0dGxVPslEomkIpCCAWlhSCQSiTlIwQBgSQtj+vTpWLJkSd5nZZGjtLQ09OnTBx06dICPjw927Nhh9jmJCNOmTYO3tzd8fHzw66+/AgCio6PRo0cP+Pr6wtvbG4cPH4ZWq8Wrr76aV/abb74p93uUSCSPB49XapApU4DgwunNrUgLe10GVCoHQKhLd05fX+DbotObjxgxAlOmTMHkyZMBAJs3b8bevXthZ2eH7du3o3r16oiPj4e/vz8GDRpk1hra27ZtQ3BwMEJCQhAfH49OnTqhR48e2LBhA5555hl89NFH0Gq1yMjIQHBwMCIjI3Hx4kUAKNUKfhKJRGKMxQRDCLEKwHMAYonI28TxaQBGG7WjNYAaRJQohAgDkApAC0Bj7rT1B6f8h9W2b98esbGxiIqKQlxcHFxdXdGgQQPk5ubiww8/RFBQEFQqFSIjI3Hv3j3Url27xHMeOXIEo0aNglqtRq1atdCzZ0+cOnUKnTp1wrhx45Cbm4shQ4bA19cXTZo0wa1bt/D222/j2WefRb9+/cr9HiUSyeOBJS2MNQC+B7DW1EEimg9gPgAIIZ4H8F6BZVh7E1F8ubaoCEtAp81AZsZl2Nk1g7V1+S+JOnz4cGzZsgUxMTEYMWIEAGD9+vWIi4vDmTNnYG1tDU9PT5NpzUtDjx49EBQUhN27d+PVV1/F1KlTMXbsWISEhGDv3r1YtmwZNm/ejFWrVpXHbUkkkscMSy7RGgTA3HW4RwHYaKm2lIziBtJa5OwjRozApk2bsGXLFgwfPhwApzWvWbMmrK2tcfDgQYSHh5t9vu7du+PXX3+FVqtFXFwcgoKC0LlzZ4SHh6NWrVqYOHEiJkyYgLNnzyI+Ph46nQ7Dhg3DnDlzcPbsWYvco0QiefSp9BiGEMIBQH8AbxntJgB/CyEIwI9EtNxk5XJrg2VHSXl5eSE1NRX16tVDnTp1AACjR4/G888/Dx8fH/j5+ZVqwaKhQ4fi+PHjaNeuHYQQ+Oqrr1C7dm38/PPPmD9/PqytreHo6Ii1a9ciMjISr732GnQ6vrcvvvjCIvcokUgefSya3lwI4QngD1MxDKMyIwCMIaLnjfbVI6JIIURNAPsAvK23WEzVnwRgEgA0bNiwY8EndXNSc+t0uUhPD4GtbUPY2NQ0694eJ2R6c4nk0aWqpTcfiQLuKCKK1L/GAtgOoHNRlYloORH5EZFfjRo1ytQAOQ9DIpFISqZSBUMI4QygJ4AdRvuqCSGclPcA+gG4aNmWPFwzvSUSieRhxJLDajcC6AXAQwgRAWAmAGsAIKJl+mJDAfxNROlGVWsB2K6fj2AFYAMR/WWpdurbCkBIC0MikUiKwWKCQUSjzCizBjz81njfLQDtLNOq4pBrYkgkEklxPAwxjIcCueqeRCKRFI8UjDykhSGRSCTFIQVDj6UsjKSkJCxdurRMdQcOHChzP0kkkocGKRh5WMbCKE4wNBpNsXX37NkDF5fyT1UikUgkZUEKhh5LWRjTp09HaGgofH19MW3aNAQGBqJ79+4YNGgQ2rRpAwAYMmQIOnbsCC8vLyxfbpjU7unpifj4eISFhaF169aYOHEivLy80K9fP2RmZha61q5du9ClSxe0b98effv2xb179wAAaWlpeO211+Dj44O2bdti69atAIC//voLHTp0QLt27dCnT59yv3eJRPJoUempQSqSIrKbAwB0ugYgIqjLN7s55s2bh4sXLyJYf+HAwECcPXsWFy9eROPGjQEAq1atgpubGzIzM9GpUycMGzYM7u7u+c5z48YNbNy4ET/99BNefPFFbN26FWPGjMlX5sknn8SJEycghMCKFSvw1VdfYeHChZg9ezacnZ1x4cIFAMD9+/cRFxeHiRMnIigoCI0bN0ZiorlpvyQSyePKYyUYxSNgifTmpujcuXOeWADAokWLsH37dgDA3bt3cePGjUKC0bhxY/j6+gIAOnbsiLCwsELnjYiIwIgRIxAdHY2cnJy8a+zfvx+bNm3KK+fq6opdu3ahR48eeWXc3NzK9R4lEsmjx2MlGMVZApmZMdBqU+Ho2Nbi7ahWrVre+8DAQOzfvx/Hjx+Hg4MDevXqZTLNua2tbd57tVpt0iX19ttvY+rUqRg0aBACAwMREBBgkfZLJJLHExnD0MP5pMo/huHk5ITU1NQijycnJ8PV1RUODg64evUqTpw4UeZrJScno169egCAn3/+OW//008/nW+Z2Pv378Pf3x9BQUG4ffs2AEiXlEQiKREpGHlYJujt7u6Obt26wdvbG9OmTSt0vH///tBoNGjdujWmT58Of3//Ml8rICAAw4cPR8eOHeHh4ZG3/+OPP8b9+/fh7e2Ndu3a4eDBg6hRowaWL1+OF154Ae3atctb2EkikUiKwqLpzSsaPz8/On36dL59JabmJgIiI5Frm40su/twdOxo1rrajxMyvblE8uhS1dKbVy5CAHFxUKXl6Hc8OgIqkUgk5YkUDABQqwEtC4XMJyWRSCSmkYIBAFZWEDrFspCCIZFIJKaQggHoLQwWCmlhSCQSiWmkYABsYWilhSGRSCTFYTHBEEKsEkLECiFMLq8qhOglhEgWQgTrt0+NjvUXQlwTQtwUQky3VBvzUKsBjRaAtDAkEomkKCxpYawB0L+EMoeJyFe/fQYAQgg1gCUABgBoA2CUEKKNBdsJWFmxS4qAh8HCcHR0rOwmSCQSSSEsJhhEFASgLNOHOwO4SUS3iCgHwCYAg8u1cQVRqyGIHhrBkEgkkoeRyo5hdBVChAgh/hRCeOn31QNw16hMhH6f5bDilFpCW/4uqenTp+dLyxEQEIAFCxYgLS0Nffr0QYcOHeDj44MdO3aUeK6i0qCbSlNeVEpziUQiKSuVmXzwLIBGRJQmhBgI4HcAzUt7EiHEJACTAKBhw4bFlp3y1xQEx5jIb67RAJmZ0J4DVFZ2EMLa7Ov71vbFt/2Lzmo4YsQITJkyBZMnTwYAbN68GXv37oWdnR22b9+O6tWrIz4+Hv7+/hg0aFCxs8xNpUHX6XQm05SbSmkukUgkD0KlCQYRpRi93yOEWCqE8AAQCaCBUdH6+n1FnWc5gOUApwYpU2P0nbSwwCTv9u3bIzY2FlFRUYiLi4OrqysaNGiA3NxcfPjhhwgKCoJKpUJkZCTu3buH2rVrF3kuU2nQ4+LiTKYpN5XSXCJ5pElLA2T8z6JUmmAIIWoDuEdEJIToDHaPJQBIAtBcCNEYLBQjAbxUHtcs0hJITweuXEFGXcDKoz5sbIrutMvC8OHDsWXLFsTExOQl+Vu/fj3i4uJw5swZWFtbw9PT02RacwVz06BLJI8lR44ATz0F3LoF1K9f2a15ZLHksNqNAI4DaCmEiBBCjBdCvCGEeENf5D8ALgohQgAsAjCSGA2AtwDsBXAFwGYiumSpdgIwxDB0lhlWO2LECGzatAlbtmzB8OHDAXAq8po1a8La2hoHDx5EeHh4secoKg16UWnKTaU0l0geWUJDgdxcoITfkeTBsJiFQUSjSjj+PYDvizi2B8AeS7TLJPp1WYUOsETyQS8vL6SmpqJevXqoU6cOAGD06NF4/vnn4ePjAz8/P7Rq1arYc/Tv3x/Lli1D69at0bJly7w06MZpynU6HWrWrIl9+/bh448/xuTJk+Ht7Q21Wo2ZM2fihRdeKPd7k0geCpQ1Z5KSKrcdjziP1Yp7RaIIhlZYbOKeEnxW8PDwwPHjx02WTUtLK7TP1tYWf/75p8nyAwYMwIABA/Ltc3R0zLeIkkTySKP8ZpKTK7cdjziVPaz24UAInouhFZDzMCSSKogiGNLCsChSMBSsrCwWw5BIJBZGcUlJC8OiPBaCYdaqgmo1hBaQFkZ+HqUVGSWPMNLCqBAeecGws7NDQkJCyR2ftDAKQURISEiAnZ1dZTdFIikeGcOoEB75oHf9+vURERGBuLi44gvGxYGyM5GrSYGNjaZiGlcFsLOzQ305rl3ysCNHSVUIj7xgWFtb582CLpbvvoNm82qE7PNFu3YnLd8wiURSfkgLo0J45F1SZuPqCnVqLnTa9MpuiUQiKS0yhlEhSMFQcHWF0BAoLbWyWyKRSEqLHCVVIUjBUNAn7dPFR8vAt0RS1ZAWRoUgBUNBn81VnZKLnJzYSm6MRCIpFdLCqBCkYCjoBcM6DcjOlgnMJJIqg07HGaetrICMDE5CKLEIUjAU9IJhlQpkZd2p5MZIJBKzycjgV31iz1JZGbm5wKlT5pc/eJBTqT+mSMFQUAQjBcjOloIhkVQZFHeUMl+oNHGMjRuBLl2A6Gjzyk+bBnzySena9wghBUNBH/S2SbeVFoZEUpVQAt6KYJTGwggPB4iAWDPjlrGxwGO8tswjP3HPbJycALUadpnVkSgtDImk6lBQMEpjYShCYW6d+Pi85RAeRyy54t4qIUSsEOJiEcdHCyHOCyEuCCGOCSHaGR0L0+8PFkKctlQbCzQIcHGBTYY9srJk0FsiqTIUdEmVxsK4d49fzRGM9HQgM/OxHrprSZfUGgD9izl+G0BPIvIBMBvA8gLHexORLxH5Wah9hXF1hU2atXRJSSRViYqyMJR8dCkpPDLrMcRigkFEQQASizl+jIgUZ+AJAJWf4c7NDdapKmg0CdDKFCESSdVAEYx69fjVUhaGIhg6neGajxkPS9B7PADj9UcJwN9CiDNCiEkV1gpXV1ilagEAWVl3K+yyEonkAVBcUnXrsmvZ0hYG8HBNEDxwAFi1ioP3FqbSBUMI0RssGB8Y7X6SiDoAGABgshCiRzH1JwkhTgshTpeYwrwk3Nygup8JQE7ek0iqDMrTvrMzD14xtzPPzQUS9U4QcwQjPt7w/mESjNWrgZkzWSwtTKUKhhCiLYAVAAYTUYKyn4gi9a+xALYD6FzUOYhoORH5EZFfjRo1HqxBzZtDdfceVDly8p5EUmVQBMPREXBxMd/CMH7ALK2F8TAFvi9fBtq0qZBLVZpgCCEaAtgG4GUium60v5oQwkl5D6AfAJMjrcodb28InQ4Od4ScvCeRVBVSUwFra8DGhq0Mc5/+lfgFUHVdUjodcOVKhQmGxeZhCCE2AugFwEMIEQFgJgBrACCiZQA+BeAOYKlgU0qjHxFVC8B2/T4rABuI6C9LtTMf3t4AAOe7rtLCkEgsBRFw8iTPsC4PN0paGruigNJZGEr8wtq6/C0MogpxESE8nIf6enlZ/lqwoGAQ0agSjk8AMMHE/lsA2hWuUQE0awbY2MAp3B4x0sKQSCzDyZNA165AUBDQvfuDny81ld1RAFsYkZHm1VMsjGbNzBeMWrW4XnEWxtq1wAcfALdvA3Z2vO/QIU4/1LateW0zl0uX+PVRd0k9lFhbA61aoVoYycl7EomlCA3l14iI8jlfWppBMMpiYbRoYX7Qu3lzfl9c+TNngJgYICSEPxMBI0dyHqry5vJlfm3duvzPbQIpGAXx9obdzTRkZ0eASFvZrZFIHj2iovg1IaH4cuZi7JIqbQzDzg5o0CC/AKSn81aQuDieHGhrW/w1lPtTsuDeucMColgD5cnly5ylV5881dJIwSiItzesI1OgSstFTs69kstLJJLSUd6CYeyScnHhztycOQmxsUDNmtzZJicbZm+/8gpbBAWJiwNq1ChZlJT7O63PanTyJL9GRpZ/sPzy5QqLXwBSMAqjD3xXCwMyM29UblskkormjTeAGTMsew0lxmApC0OrzW8h3LgBvPgiB4eNuXePYxIuLiwwyvDcS5eA8+fzl83J4c6+Ro2S3V7K/SkWxokThmOKC6k80OkqdEgtIAWjMIpg3AbS0kIquTESSQWzfz+wbZtlr2EJl5SxhQHk79C3bAF++42HnxqjWBgF60RFcaev0RjKKpP2SrIwiLi+tTVfLzWVBUNJW1KegnH3LgujFIxKpFEjULVqcAq3R1pacGW3RiKpWGJj+YlcWcWutBw8CEyZUnwZ5QnceOb0g1BwlBSQv0NXrIWYmPz1jC0MgAUjPZ2TC2q1+RdVUtrq4VG8hZGQwDPIn3qKxePECeDsWbZw7O3LN46hiI8UjEpEpYLw8oLTHSkYkipMeDgP6ywNmZnc+RIBF8s4V3bLFuC774oWHOUJHLCMS8qUhaGMVjIWAGXRpIIWhnGZcKORksocjJIsDOXeBg/m1xUrgOxs4IkneCRTeVoYUjAeEry9YR+ahfT0S9Dpciq7NRJJ6Zk0CXjppdLVMZ6YVtCHby7KUNWi5kIkJHA8QHn/oGg0QFZW0RZGZiZw7Rq/N7Yw7t/nugUtDKXDB3h0k0JBwSjKwlDu29cXaNjQ4N7z9+eOvTwF49Ilbr+7e/mdswSkYJjC2xtWCRmwSsxBRsbVym6NRFJ6bt/mTr806zYYL1P6oIJR1BwLpUOuV698BMM4jxRQ2MK4dMnwHRgLhtLOghaGOYKhjMQyhVK/bl2gUycWpXr1eDiulxfHHVJSSn+fBdFq2XKqQOsCkIJhGv1sTMcbkG4pSdUkOprdQndKkbFA6UTt7CwnGMoTeNu27P7KeUALXhEM41FSgKFDV9xRDg75BUOZ5V2UhWFrW1gwhADc3PgaGRkcqyiIUr9OHRYMgFOgAIbOvWDw3VyIgOBg4P33ee7I2bPlM1O+FEjBMEWnTiAh4HzVSgqGpOqRmmroSEvjAlE6++7dWTDKsr6CuRaGjw+/Jha5xpp5FGVhKNbL+fNAtWqAn1/RFkb16vxeEQx7e6BVq/yCER/PYqFWG65hysqIjGQrxMbGIBj+/vyqCEZZAt8nT7LItm8PLF4MdO7M8aKPPy79uR4AKRimqF4dwssLrteqScGQVD2MA7el6ZyUTrRvX/bxm5uTScF4fQlzBeNB3VLK4kmKYNjZsetn1y7+HBLC16pbt2gLw8qKLRQl6F2nDtCoUeGgt7J8gqmRWMb3V7cuv3/ySbYGxozhz40bc/vKEseYM4fbv2QJt/H334Fhw3j4bgUiBaMounaF46UspKWcA1XASlYSSblh7IcvrYVhb8+JAYHSu6WMO/+ixEZ5Aq9Thz8/6NDagi4pAHj9deDff9llExICtGsH1K5d2MJQqQwBY2WorNLhN2pU2CWlCIapkVgKxoJhYwMsWGC4V7WaLRdz/iZhYYb3Gg0nLxw2DPjvfys0yF0QKRhF4e8PdXI2bMKSkJ0tl2uVVCEUC6NOndILRs2ahoyqpRUMxUIRongLo25dQ6eniEx0NLB9e+mtmoIuKQB4+WUWvo8/5k69bVsWjLQ0Q/l793hOhVrNnwsKRsOGHJxWrAhzLYzISMMkPVN4eXFywjVreGlVU4MSDh9ma2THDv586hRbUn36mPWVWBKzBEMI8a4QorpgVgohzgoh+lm6cZWK3u9Y/bIMfEuqGIpg9O3LgmGuhawIhrMzP2GXVTBatCg+6F2vXmHB+PRT4IUXeDRR8+Y8msgcCrqkAO78R4wA/vyTPysWBmBwRd27x/dqXKegYAAGK8McC0Oj4fMqFoYp+vTh7+m11/j9li2Fy6xaxa+rV/PrP//wa+/eRZ+3gjDXwhhHRCng1e9cAbwMYJ7FWvUw0KoVyNkZ1S8DqalnKrs1Eon5REWxr/yJJ/iJ2tzOVxEMgJ/KyyoYHTpwx2lqBFRRFsaVK9yxf/EFcPOmIQZREqZcUgDnxFJQLAzA4Ja6cQNo0sRQxsWFv6e0tPyCER7OVkBCQskWxr17LM7FCcZrr/Fs8hs3+Jp79+Y/np7OImJjA+zZw9f95x+e1+HhUfx3UQGYKxjK0lEDAawjoktG+x5NVCqILl3gctUOyclHK7s1koeZAweAW7cquxUGlMCtksW0oFvqyJH8CfEUCgrG1as8S9lcjAUDyB9LATgorjyBOziwqCmCceMG0LEjLzzUsCF/p0VBBEyYAOzcadrCAHgUka8vL47k5JRfMHJygOvX82d5dXEx/A2VoDfAFkZ0NItGSRaG4k4rTjAAvvdmzYBevVgMjC3A7dtZtObP5+9r7Vrg2LGHwh0FmC8YZ4QQf4MFY69+ze0SZwQJIVYJIWKFECbzDOhdXIuEEDeFEOeFEB2Mjr0ihLih314xs53li78/7EOzkR5zDDqdfsz1sWNsUhvPipU8vuh0nAZizhzz60RElG3IqrkoT/GmhnFmZgJDh3IKb2OMU2UAnIRTqzXMkjbF3bv8tH38OH+OjeURR/oEnoXcUsoTuOLjd3dnwUhJ4brNm3P8o3dvIDCw6EmHoaHAypXA+PEGl1G1avnLCAFs3swbkF8wbtxg91FBwVCuV7cuj56ytubzr13L+/v25VfFmiloYRhPSjSHPn0Kp3D5+WeOX7z1Frdv1iwWuComGOMBTAfQiYgywGtzv2ZGvTUA+hdzfACA5vptEoAfAEAI4QZeA7wLgM4AZgohKmaFEGP8/SF0hGpXMpGWdo73/f47/2OEyEy2EhjcGOa6ffbt40lXb7zx4JPWikKxMNzdueMztjDWreORSdev86aQnMxPtMaCARSfUyooiDv7I0f4c2wsP4Ur7pyCglHwCdzDg9tyQ7+MgLKa3VNPsZBcuGD6uor1kZAALFvGAW4rE6tNN2/O8xaUa6lU+RcyKigYCnXrctkGDdjq+OEH7rAVAVarWTSKEoySLAwFRQSUGEVEBL9/+WW+/pgxfA0rqwqfoFcU5q7p3RVAMBEg6nVPAAAgAElEQVSlCyHGAOgA4LuSKhFRkBDCs5gigwGsJR63ekII4SKEqAOgF4B9RJQIAEKIfWDh2Whme8sH/QxN54tAcvJhVK/e2fDjeJhcEJLKQ5m1a+5yo3//zU+/y5dz3W3bCvmmiVhLiLjfsLbmKsqxjAz2ahjvS0xkr4ytLaCJikVS92ZIDwe0jftAeyYF2quATqOD9os/oK07EDZRt+Gy/iAc3muBrCwg81IirFAftrYN4JwN2LZoAVhZIf3cddxpz3PbatZkD1V4OHtjnP+OgivqwvXSbdgTkBGVjFCnbrgf1gg10Qo1riXAWt+nZmcDaWeTkIOWsBONYR0JZNl7I+OOQMafichAL9imt4N3ClC9d2/EogbOLgmF6j/t8haUU6vZ6Ln32y1Eu42Frlt32O76DbbO9rA9zt+TVsvGg5MTa5etLRtV2dlqqN07wCo0C2n37+G+6IrMe20gAvk7FoleUOEJ2CEL9in1YX8bsK/VDta/H0Z6rjVSp32I1BP8bFC9OuBVvQ4c7ichOoo9d5mZAA45wk71NFzu1EC1JDZYtNr8m/E+UCvUq+GPBn8HIvPFibjy6R7E0CDYNHwDdgeAup3HogFmI7NDL9y57oi4OL43jcZwn87OrE/GYSFLIsyZYyCEOA+gHYC2YKthBYAXiainGXU9AfxBRN4mjv0BYB4RHdF//gfAB2DBsCOiOfr9nwDIJKIFJs4xCWydoGHDhh3Dw8t5Le4OHZBCVxC+th98mv/Kf6GcHGD6dA7QSSodjYY70cxMzkOXmWnYsrL4wVmj4dfcXI4rJiXxw5uS0VoZnJOQwO79iAj+EdasyZ1xVBTXtbbmzcaGrx12Kg6h0fbwEAnoOqIhPDwEzp1jLdBquVNXqfhVpwOykzORS1aoXk0Hl7S7yHb0QILWBdnZfE61mttt7I1RqbjDdHDgB/LMTMDZmdC2rYBKxdkilIddGxtCTs6DhxednQG7tHjc05oXaLWxKT+DSTE8HmYEdHCyykSKplrJhc04Fz3gDAc3t7LPgRRCnCEiP3PKmmthaIiIhBCDAXxPRCuFEOPL1rzyhYiWA1gOAH5+fuXvGB44EE7zgpEWEQRK+hdC+VVIC+OByM3lJ2Nra+4czp/nwTE6He9PTeXOvOCWnMyxUhcXFonbt/PnzCstjo7s0TAOSdWpw27kkyf53O7uvM/OjgUoJYXbr9MBDa2j0AuBiKK6OHK4ARLvC/j68vIHtraG+9HpAJUuF3Y/LoW6S0ekduiFpN9uwRZX4Tb2OdjZGYTN3p7FQaXiehkZPPE6PR3wcMqC+7LPcaf1CwjR+EKn46S0LVrw8eSwJFRb8S3cRg+EQ+8uUP97HOrlS6F2d4WVtYA6NwvqZUuQ/ctmJO06gvTPFsLezR52l85At2QpsmcE4H61BoiJATJ3h6BJcjAaLXof6enszbGx4Xiwm4sOyS+8hqQsW9yv1gD33/oETssXoLmXLdxmvo24V/4PcTXaQPvKOBDxd+e4+1dY/7UL2T+tRY5GBbtNa1Dt9CE4dPaGQ8hxpK7egpAQ/pu2urgFfhdWw2rmR4j64DskwRn0f/+DcLBDrc8mo/bHE2E15Dlkn7+G7NhkZLfrjNxc9t6o1fw3io9ny8bBgdutXfANNKmZcMy9D9eGTrD/4lMQ6f8+hw5D99lsZNVpjMxvfuQHjs27kPvnPjgOHwDH/wyAkxP/vyQkABfeXYV7Wg+0mj4EbdoAjlZZwOTJyNJYIenLH5Gezu1QqfjVeFP2EQERm47g9sp/UM3VFq01F1Bv6yLkVndHRgY/pNy5w/8PjRoZwipWVrypVIaRwAUXE7QU5gpGqhBiBng4bXchhAocx3hQIgE0MPpcX78vEmxlGO8PLIfrlZ6BAyHmzkX1E0nIVW+HDcCjQKRgmEQZgRgTw1tWFneCKSn8j337Ni91fOFC/gXNTGFlxU/WLi68ubpyPDEri4XD0REYNIitg2rV+IdVcLOzM1gFyubgwOerXt0wbyszk2Opzs58PmHuQ/qTk4E7+lF0ey+C2ngVXTfwKLDs/4BPdvPwEdu/2D/+RYr5KR527AW+nw04/wv89Vfh40EXgBWfAa88CTwNYJw/0D+G8w8dPMhrVfzHCqjrCewYDTTryXMWlp0CsAp4azaguOA9jnDQdeib/KUZc/0mkLWWZy5fvQpMfxv4PgDoNAnoC8DrPJByBHhvHJfX6YCvPwK61QXG6Z+m79wEDq0F0jsD3nbAc8Bzz+nP/6sWGLkHmHkA8GnO/zzXYji+gZ3AxMVAQwAdW5r3vQHAoQvA7t1sMr76v/w9DHIB7AOa9wBG6HfVtwfuHAB+mAkUcPcMWfE7x4omDQT+7/84MJ6czGnlB5nfJLTyBFYGAPcBrF8PPF15s7jNwVzBGAHgJfB8jBghREMA88vh+jsBvCWE2AQOcCcTUbQQYi+Az40C3f0AWHih4SLo0gXk5gL3k0nQav8BWrbkIXvK6IvHjMREjvdfusS/F0UYlO3ePb1/tgjc3Fhv33+f/cy5udxxt23LX63i6nFy4g6/xI47JobH7E+YUIpevjD29oY4r9kQse/Jy4u/kIgICONAakEOH+Y2PvEEf+7UCfjmGw4sK8HZkjh4kF+PH+cvWlE8BeNZ3gBfb+hQ3pSgNMDxOQ8P4I8/WDAUM804nuLlxfd49aphqKzC2bP8OmoUMHMmcO4cmzhK0Lx+fY7XKBw4wIr82WeGfe7uLCTBwRzoNaZXL361tuZ5CRs3AgEB/KDWrJkhsF4aatc23GfBv5MS9DYOWPftW3TQ38WF//bz5rEYv/QSj9pS2m0u9evzcOKGDfm7fMgxSzD0IrEeQCchxHMA/iWitSXVE0JsBOu4hxAiAjzyyVp/zmUA9oCftW4CyIB+5BURJQohZgPQr6KOz5QAeIWjVgP9B8Ltz01Q6a4Bw8bwhJ/ERH6iUCbxPCKkpnJfcO0aWwMaDf+mQ0P5d20cIrKyYjO5dm3un9q35/fKvlq1+MlfrWZrQBl+X66sWgV89BE/eTZtWvr6UVEGtXrhBWDcODZlzCEujv8PXnklTzCK5cgRToSndE5KNtNTp/jLI+IvWlkGFDAMv1XE8OBB7kRTUviaShoP4/sBDIJhjPHMZrUaGDiQBUOj4Y7U1dWg2IChU714sbBgnDnDZYcOZcFQBoMYC0Z0NJ/bygr48UcWiGHDDOdQorRZWSwCxtSqxeft2pX9bZMnA19+yfc8aVLhezMHZWit8b0pmBKM4nB25lFfc+YAI0eydVBWjh3jv8cDPPBUFGYJhhDiRbBFEQiesLdYCDGNiEzMazdARMVKpn501OQijq0CsMqc9lkaMXAgbDZsAJAD6tYNQhGJ27d5clAVJT6e/fQREbwdOcKb4ipSXDgAPwB17Qq8+Sb3bT4+/JtWVXY2MiVJ27lzZROMY8fYhxYXx66FI0d48pQ5KCOk+vQBvv22eMHQaPhaY8ca9jVtyp30qVPcCW7ZwsEPwPAgkprKWU8DAw3BngkTeOnPo0cLC0Z0NHfkbm4lt3/YMHal7N6dfw6GQrNmfC5TGW/PnDEIrVrN1hOQXzB0OrYArax4OPqUKRzYUTC2ZpQhtcYEBOQvO2ECP82XdU6CIhgqFbfbGHd3vg9PT/PO5eLCARJ3d2DRorK1R8FYpB9yzHVJfQSegxELAEKIGgD2AyhWMB4Z+vcHCQFBhHRfdzgqYZdbt6qEYCiek717ObAcG8teBmNrWwgWgfffZ6u6dWsehl5hghAZyZ3BnDmmx9QXhWLynD0L/Oc/pb9uSAh3FOfP88i3H37gTrpgqglTKPMb2rZl9SxOMM6f5zGZTz5p2CcEWxn//sufly3j6OZbb7EQqtUsZBs38vwNZaW2cePYMjh2jBXcGGUOhjlPqwMHctnly/O7kxSsrbljLSgYRPx9jxzJnV2TJobJe8aCAfC8j3v3WDALWgbG40BNCUZBPvyQ7+vZZ0suawpFMJo14+CWMcoERHP9koogL1pkcPM9Bpj7y1QpYqEnAY9Tplt3d1CXjsi9ehoJbhfh6KIfTfwQBr6JeGRFSAi7kIKDOciszCtzdeW+zdOT3a7du/OIoJo1Kzy1fn62bGGXw0svFX5qLg5jwSgLISHcKdrZsUvqu+84aZ3ypK9w6RJ3cMZujStX2NdWvz5vxQmG8gRecAJWp07sB790if38s2ezpaOQk8N1585lN4qjIy8G9MQTbGEUxDi9dklYWbHffe5cvreeJkbJe3kZxEDh1i12x3bsyJ9btjRMvlMEw8eHg1Mffsifn3qqsCgYC4Y51mHt2vz3KSvK366oOJPiIjSHsWP5b17w/+QRx1zB+EsfiFYmzo0Axx8eG1RLl+P2sReRnvgnGnl+xD3vQyAYRPygGxjI26FDhiGiQvBvtGtXdvMPGFC2WGGFoHT8d++aLxiKOgIsGESl9wOHhADduvH7bt34aXHbtvwdQVYWd/T9+wMbNhj2X7nCppgQ3HncvGn6Gjk57EJq3tzw5K3QqRMHr999l89TMGWHjQ0LyJQprP7du7Oyd+vG7YyOZoto9Wrgvff4c+vW5t//hAksGPHxhS0MgJ+4N23i2MrXX7O1oFiAimC0aGEorzxtN2jArr7r11kMlWVKjVEEo0EDHnVgaRTLS1m86UGoUYMHCzxmmBv0niaEGAZA/8vCciIy09H7iNC+PWxcRiI6/HPk5ibCunHj/DlgKpCMDHYvbd/OIysVgWjQgEXB35/jDN7ehXOyPbQoglGaNaiVWWzNmnFnHRVlfh4fgAPWd+7wojQAu4CGDGEXUFaWwW2xezdPhDh2LH/9K1cM/vT69VmxTbFgAfv/du4sfKxzZ3795x8WpAYNCpeZMIFddfHxhhTXisht3MgjrSIiWDSSk/VDT82kUSO+7p9/mhYM5Wm8TRsW5IYN+UHJ1dVwTIkHVKuWP6eTlRXXU1JqFMTZmX2e5rijygMnJ/5bKt+5pNSY7VYioq1ENFW/PV5iocfNbSAAHe7f38d+2wqyMJTBM2vW8KAUDw/2nvzxB9CvHw8UunWL+9yff2a3tr9/FRILIL+FUdo6Q4fya2ndUkr6buM41NChHGtQ8vsAwC+/GK6nrKeQnMxxF+Vpvn59nkWlpNtWuH6dh5IOHw48/3zhNtSpYxC5ceNMt7NaNbYeAEMCvPbtWdDef5+vuWkT/2NkZZnvklJQYgumBMPPjwPVPXrw5JmbNzmWEhpqCGArgmGqfnGoVPw7MndIcXkwYEClrlhX5SGiIjcAqQBSTGypAFKKq1sZW8eOHcmS6HQaOnzYjS5fHkv0v/8R2dgQaTQWuda9e0Rr1hCNHElUs6YyH5Wofn2it94i+ucfopwci1y6cvDw4BscPdr8Olu2cJ2gICIhiGbNKt01v/2W60dHG/ZlZRFVr040fjx/TkggsrYm8vfnsjt28P4jR/jz77/z519+4c9XrxrOlZZG1K0bkbMzUVRU0e0YPpzvPyur6DK5uXxNY3r1IrK3N+zPyCBaupT/eUpDbi7R/PlEMTGmj6emEul0RdePjuZ779KldNclIoqN5XZLKg0Ap8nMPrZYlxQRmTFU5PFBCDXc3PojMfFPUONZnCYkKsq0G6EMZGbyqozr1rHLSavlAHW/fjy4pmtXdr9WgeHapSM93ZA8qDQuKcXC8PJiP7o5FsZLL3Hw8+uvOX5Rs2b+QLatLY/C2bqVn+qPHOHZhQsXclD45EmeXr57N7uwlCC2EpuIiOAn7pgYnrZ87hz/QU3Ni1BYvJgtFuMhpwWxsjK4oRRWruTvTvHJ29sXHjVlDlZW+QPtBSnJVK1Vi909tWqV/tqP0QijR4FSjF+UAIC7+0DExm5Aes1MOALsC1IEIySER7msWWO2Pygtjedi/f478NtvHL+sXx+YNo3jru3aPQRzHczh3j12VZTFH62IhJ1d6V1Sjo7sT+/QwfSooYJt3LSJFff11/nv1a5d4XKffMLxiCefZEFp04bVum1bFgyAlb1HD8PwSmPBiIri8vHxXC4v30UR1KpVts7WeMW4ykQIYMaMiotFSCqNqtAVPVS4uw+CSmWPWEf9JHTjwPfUqfxkunVriec5cYLX3XFz4wfWzZt5HtWBA9wPfvEFu3arhFgAPIqnZ8+iF70pDsVS6NyZO9zicosYc+cOB22FYMG4c6f4NKd//GHIGf7JJxyINiUYrVtzgLtmTY5BjBnD1+jShSfZXb/OQ9MGDzbUUeIQERE84Sw6moeslSQWjwozZpRtHoykSlFVuqOHBisrJ3h4DEW09V+g6tXZLaDRsJlw4AB3LEqQtABZWfyA27s3P4AeOQK88w6wfz/3c6tX87GHViS++46DhqY4eZI7yaIWvSkORTC6d+fvUgksm1NPGSesBE7PnSu6/I4dLDBTp7I5l5NjWjAAnqhy9ChbjIqbp0sXtqK++oo/DzLKMmdnx0HnwEAehfD66xwwlkgeJcwNdlSFzdJBb4X4+D/p4EFQ8tJ3Odj30Ucc3KxXj4PhQhBFROSVj4ri3S4uXLxRI6IFCziWaDbZ2USffVZ0YLIiGDiQb+Dy5fz7ExMNUfmvvir9eWfMILKyItq5k89x/Djvz80tPtjq5kb0xhv8PimJA8CvvGK6bFoakZ0d0TvvEMXFETk68rUuXDC/nVeucB21msjHp/BxX18+bm+fP5AukTzEoBRB74f1WfahxtW1L2xsauNO93AeCjl3Lj+Nfvwxj5knAjZsQGIiWxCenjwU/+mn2Zq4dYtHQ5Zq2OumTcCnn1ZullzF/VYw11JwML+q1ZzCorSEh3McSMnjo8Q0Xn2VJ5MoS3sak5bG8ygaNeLPzs78VP/LL6aHO+/bxybe4MFsCUyfznGDgjmFiqNFC76OVpvfHaWgxDHefjt/IF0ieUSQglEGVCor1Kz5EhISdiN34UwepdO0KYtH8+bQdfbHiu/S0aIFsGQJZxG4do37+j59yuByIuJRPQCfyILsurYLHx/4GOfvnYdWp8Wa4DXwWuqFRSe+MyT627YtfyVldNLIkZzGoqjVXLKyuC4RTkacRHSqPhV3eDh3/Ip76e5ddk3t2MGxgp49C4+eUtxYimAAPFJAreYUIwDPND5wgJPE7djBCeOUUU0ffsjnLE0+FJXKMOnLlGAogvK//5l/zkeEsKQw3M+8X+p6c4LmYNe1XRZoUWEyczMxettobL/y4NPIdKTDneQ7iE2PRVpOGu4m38WZqDNIzKycpNoVhrmmSFXYKsolRUSUknKODh4ERUQsIUpPZ7cMEYWFEfVpeYcAou5e8RR8KocnTJw6xZMnSiA9J53Wn19PA9cPpBn7Z/DOf/5hV4dKRdS3L60LWUcLjy2khIyEEs+XmZtJh8IOUXpOerHlsjXZ9M6edwgByNvcvnQjBIBsZttQ/fl1SSNA5OnJbQkLM1R+6SWeILJ7Nx/7+28iIkrNTqWVZ1fSk6uepCGbhpD222+IADr5x4955520cxKFtaqd50rKcXakN6f70O4/uCy9/z7PY2jYkGj//rxLav/YRTqA6OjR/Dfy5puks7aipCVf04VWbvR7S9DMgfb06n+sKOS1gfmKarSln0Oz7bs3adLE2pSZXfj73HhqNfkv6UA9V/ekQRsH0a5ru0hXnEvtAbmTdIcu3ruYb1/Y/bBS31dyVjL9evFXSs027SNNyEigq3FX6UzUGdp8cTPN2D+Dpu+bTvcz7xMR0d83/ya7OXbUe03vvDrZmmxacHQBjft9HA34ZQB9cuCTvPIK52POEwJAdnPs6GzU2XzHNFoNfXXkK9pwfkPe/UQkR9Bvl36jlKyUvHI6nc7s7/iDfR8QAkBWn1nR71d+z3fsfuZ96rS8E60+tzrf/qTMJJPn+vb4t/l+K8rW4ccOpfr+k7OSafq+6fTUz0+R1xIv6vJTF9p+ZTvpdDoKvB1Iz294nsZuH0snI06afc7SglK4pMxa07uq4OfnR6dPn66QaxERzpzpAEANP7/TIOL499SpAOl0WKibiomZ30HY2XEgXHnqvnABOa1bIFebCxu1DXJ1uUjLScOVuCtYG7IWv13+Dak5qXCxc0FSVhJ+ev4nTJi5gzOa9uyJ6JCj8Hw5HjnaHNhZ2WF8+/FY2G8hbK0MY/jTc9IRHBOMHdd2YNW5VUjITEANhxqY2nUqHG0csev6LsSkxeCZps/gyYZP4t/If7Hl8hZcS7iGd7u8iw+6fYAtl7cgMDwQo31GQ6vT4sUtL2LPL8CAyd8A770HzdcLgHffhVqoedGg5s2BDRuQWcMFK6b0xO42VjgUfghZmizUr14fESkRWBzaEv/95Rqe+KQuwqvrMLjlYKwOXo06CTm4Un0G7Gd9jpXP1cOETlFQQeCHXYRJu6KQEn4dp98fhfO6aFx4oikuNKuOi7EX0etGLnbPuwuhdwUREX4/vBwfb34Dl42G9wsC7DSAo70zgl4/gbpOdTF+53gEhQfh0KuH0MqjVV5ZrU6Ld/96F8tOLwMAONk6YenApRjlMwrn751HlxVdkKXJwvMtnsfWF7fCWs0WSrYmG00WNYFaqNHUrSlCE0NxN+UuutTrgpHeI9HMrRn86vqhtmNhV9XSU0txMvIkAnoGoLFr43zHEjMTcTLiJGLSYqAjHUb5jIKDtQMuxV5C7597Iy4jDv2b9cegFoOw/sJ6HL17FG91eguLBy7OO0dkSiRi0mJwP+s+Gjo3RDO3ZsjR5uD43ePYemUrfg75GWk5aZjYYSKWP7883//4wuMLMX3/dGjJMHLNSmUFHelQv3p9vNP5HXx04CNYq62RlpOGfyf8i071OmHBsQWYtm8a6jjWgYeDBy7EXoCzrTNm9ZqFd/3fBQBM3j0ZK8+thIeDB2zUNjgz6Qxc7V2RrcnGy9tfxm+XfwMAtHBvgTY12mDXtV3QkhYeDh743xP/Q1JWEtadX4eGzg1xZNyRQt+r8vTfxLUJzkafReefOmOE9wiEJobiXMw57By5E880ewYA8Pnhz/HRgY9go7bBsXHH0K52O4zbMQ7rzq9Dmxpt8EKrFzCt2zRUt60OIkLrJa1hZ2WHiR0mIj03HS52LohJi8HMwJn44dkf8IbfGyb7jOn7p2NV8CoMbD4Q7Wu3x1dHv0JMWgw61+uMOk51cCn2Em4k3sj7zdSqVgsZuRlIzUlFK49WaOHeAs3dmmNoq6F4osETEOUwKas0a3pLwXgAIiOX4MaNt1C37nlMmeKDP//k1OCrVwOedjFAUBCPHtLpeAz/+PE4N3MS+tr/ZtJ0dbRxxPA2wzG23Vh0a9ANz218DoG3DyLox1x0mRgAqNX44NAnWNBdhd9H/I4d13Zg5bmVeKZJP2xL6Iuw7t54+9ICBIYFQkc6qIUag1sNxpCWQ7D+wnrsDd0LAGjp3hL1qtfD4fDDyNXlQi3UeKLBE5jadSqGtBpSqF052hzU+8IDvc6n4rdPL2DH/z2HUZ3vIlPN1xh3WovvO3wM3ccfYfD7dfG32320dG+J/s36Y3ib4XiiwRMYuPYZHL6+Dx8cAT59ClgzeA1e8X0FB49vwFN/j8bnzi/g/97ehBYfO8M9S6CWcMQe51i08miF6wnXoSMerlszDfBJc4Cjkxt2VIvAxqHrMbLtS4hLj8PQX4fi6N2jaKWuhXGiAxo+NxqN3JvCp6YPIlMi0GNNT6hVajjaOCI0MRROtk5wt3fHiQkn4OHggWxNNsZsH4Mtl7fg5bYvo0H1BjgYdhAnI09i8YDF+O7kd0jJTsHbnd/GRwc+wijvUVg3dB3UKjVWnl2JCbsm4O8xf+Pppk8jV5uLNcFrMPfwXIQns/vMwdoB3/X/DuPbj8/7oW+7sg3DNvOiQrZqW7zn/x7e6vwW6lWvh93Xd+O1Ha8hLsOw4HhT16b4sPuHmPHPDKiFGhM7TMQPp39AXEYcmro2RTO3Ztgbuhd7x+xF3yZ98c6f72DJqSWF/s9ytbnI1mbDRm2Dkd4jodVpseHCBvw78V/41fVDanYqXtvxGrZe2YoXWr+AYa2HwcHaAQ2qN4B3TW8ExwRj9LbRCL0fCt/avtj24ja0/7E9+jXth2XPLUPTRU3hX98ff47+EwAQEhOCD/Z/gL2he7H7pd3o0agH6i6siyGthuC/nf6LHqt7oHWN1ujWoBsuxV1CUHgQ5j89H41dGmN20GxEpkbiNd/X0NuzN74+8TX239oPlVChiWsT3Ey8ifAp4WjozC5NHemw9NRSzPhnBtJy0vBs82cRnhyOhIwEXJ58GUSE3j/3RlhSGC68eQHuDu5o9G0jtKnRBrfv34aVygod6nTA1itb8ZrvawhLCsOh8EMY5zsOPw36CcfuHkO3Vd2wctBKjGtvSOeinPdC7AVcf+s6LsddxtcnvkZvz954vePrmHVoFr448kXePSZlJaF97fZY9twydK7Hrk6NToOfg3/G+gvrMajlILze8XXk6nKxLmQd9obuxe2k27iRcAPZ2mw0c2uG+U/PN/mbLQ2lEQyLuogA9AdwDbyi3nQTx78BEKzfrgNIMjqmNTq205zrVaRLiogoJyeRfvihO3l4JJODA9HixURaLR+7m3yXsjXZ+cpH9+lC9f9nTfW/rk/zDs+jOYfm0BeHv6DvT35PWy9vpbTstHzl49PjyXOmC9WdCrp5/STd37CKnGaARvzUL6/MijMrSAQI8vovyPoTkNssB/p4/4e08+pOupeWP0XExXsX6Vr8tbzPyVnJdPD2QUrMSCzxXqfM7kbWn4D+vRFIzjNtqd0boM92vk/jlj9HCAB1n9+Gnln3DIkAQat8QXTjRr76Yb/+SI4z2GzvPAGkjYrkAwcP0qCRIKfZDvR50OeEANDujtUp19WZpr3vQ8+se4YCDgbQ3pt7KSY1hujgQSInJ9IIUMe3bKjOgjoUlRJFnZZ3Irs5drT89HLK1eaavKlhbwwAACAASURBVIfzMefJ7Us3qjW/Fh0KO0TH7x4n29m25L/Cn97e8za1/r41IQD09bGv8+qk56RT37V9CQEg1SwVBd4OJCKieYfnEQJAE3dOJI1WQy0Wt6AOP3Yo5B7R6XQUlx5HR+8czTvP4I2DafuV7XTg1gFymOtA/iv86WbCTRqzbQwhACQCBPkt9yMEgNr90I7+ufUP3Uq8RftC91GzRc0IAaDaC2rT1ThOQ5KRk0Fnos6QVqeljJwMav19a6q7sC6N3T6WEAB684836fcrv1Pg7UBaeXYlTd49md7f+z7turYrz+WSlJlEtebXoq4rutKl2EvU6vtWpJqlovlH5xfp8knNTqVlp5bluUan75tOqlkqGr55OIkAQSExIfnKZ+VmkfdSb6q7sC59eeRLQgDo6B12Ka4/v57a/tCW3L50I4e5DoVcQwUJjg6myJRIuhx7mRAAWnZqGRERaXXavO+537p+NPPgTPL4yoMQANp2eVte/ZsJN8nxc0fqvaZ3nnspKCyIjt05RlafWRECQAuOLsgr/+6f75JqloquxF2h8TvGU7W51fK5xhRCYkJINUuV97/k9LlTPvfu67teJ51ORzmaHAqJCSnyf7U4UrNTac25NdTuh3Ym3WulBaVwSVlSLNQAQgE0AWADIARAm2LKvw1gldHntNJes6IFY+NGImvXMLJ7tS91WdI7r4PecH4DqWepqfaC2jQrcBaduHuCgsKCyH92I3L4EHT2hPl/4JDnO5P7DDXVW1iPJq0ZRggAnV3zRb4yGwY3IZtPBI1+uy7FOoBo6NByz3F1YfJwQgDIYa4DOc91otBaNkTPP0+0eDFt8AbZzrYlBIBW7p3H44ebNs2fP2n8ePrxSXtymG1HJ+uBE2UREa1eTVfdQVaz+EfaKaAexyYAorVrTTfm7FmimjXp5H+6kggQ5DrPldSz1LTj6o4S7yMmNSafL/3Xi7+SCBDkMNeBeq7uSb9e/LVQnYycDBq/Yzz9ePrHfPtn7J9BCAD1WN2DEADafHFzsdfW6rQ0/+h8sp9jn+fzbvB1A4pONQzBvZlwkz498Cm1+6EdTf1rKmXmZhZqy/cnv6cbCTcKnj6PM1Fn8jq9Tw98araPf/W51YQAkPVn1lTjqxp04NYBs+opRKVEkc1sG0IA6JXtr5gscyryFKlnqQkBoLY/tDXZNq1Oa/Y1dTodNfqmEQ3aOIiIiP668RchAPTlkS/zzp2ek07nos8VqrvizIq8+31y1ZN5+3dc3VHobxmbFktOnztR/1/6k+PnjjTu93FFtundP98lESDovb/eo7TsNNofup96relFk3dPLtW9lURyVjJ1/qkz2cy2ob9u/FXm8zwsgtEVwF6jzzMAzCim/DEATxt9fmgFIyeHY7FosZOsPnQnm1kguzk25PmtJ30W+BmJAEFPrnqS+v/Sv1BQbEtrEM2bZ96FcnOJqlWj81NGUc35NQkBoGfGIH+SvRMniADKXvwtz1lYuJD/rO++W/h8d+4QJScbPp86RdShA9HFi4XLFqRfP+o0xYEQANp6eSvRN/qgdL16RDVqUHDUOfr75t+GNlWrRuTlxXMedDqiOnWIhg+njOx0otq1iUaM4LIBAUQAvb3rv4QA0B8/vEd5czrCw4tuT0ICUWIiTdo5iYXq7MqS76EIolOjy/Skp9PpaPLuyYQAULNFzcwOdmbmZtKR8CO08NhCuhJ3pdTXNYdNFzbRijMrSlVHq9PS02ufpu6rutPd5Ltluu7EnRPJfo49hScV/bf7cP+HhADQ0n+XlukaBXnzjzep2txqlJWbRS/8+gLV+KpGIeveFDqdjgZtHMRW7fXdJZafFTgr73d8JPxIkeU0Wg1FpkSW6h7KSmJGIvku8yX3L91NWjzm8LAIxn8ArDD6/DKA74so2whANAC10T4NgNMATgAYYs41K0Iw4uOJuncnglMkqWZaUbulvrRpf0Na+48P1V5Qmzv1dc/kjUq6Hn+ddl3bRftD97MLoVMnos6d+WSXLhH9+2/Rk9POnOE/0caNdCXuCj299mk60752/oyuo0cTOTkRpRj9s7yn73S//dawLyeHO+rmzTlDaEoKWwGAITNrcbRoQefH9KNNFzbxZ62W6KmnuH6/foXLHzhAZGvLo6p++onLKVbFq68SubqyIL74IlGdOpSek05/XPuDdAcOUN7sRjPI0eTQ+ZjzZpW1BFqdluYdnkdBYUGV1oby5EFHdaXnpNOtxFvFlsnWZNP68+vN6tTNYefVnYQA0LqQdWT1mRVN+3ua2XWTs5Jp9/XdZt13anYq1fiqBrVY3MKio99KS2xaLB27c6zM9auiYHwAYHGBffX0r00AhAFoWkTdSXphOd2wYcMyf2nmkJ1N1KMH94OjFs8nBICuxl2lu3cX08GDoPPhv9Lik4spK7eYNNVffMFf+3PPGZ6kGzYkmjnTEABRWLSo8JP2008T+fnx++hoTr39zjv562k0RIMH8+xpxS20Z4/hen5+nDddpSLq2pXIwYFnSheFVsup3KcV+CHeucNpuefONV3vxAmiBg34mkIY0m7/+ivva9OGX0eONNS5eZP3jR1bdHskEiPSstPIZrZNXpzAOE5X3oTEhNDl2MslF6xCPCyCYbZLCsA5AE8Uc641AP5T0jUtaWGcvPsvDXx7LwFE69cT+Sz1oS4/cf5/jSaTjh6tQ+fO9Sr5RNevc+fp5ET06af81N2vH/8plizJX3bkSJ7fYMxbb3FdnY7ok0+43jUTP5Br1/jYnDn8+eWXObawZQuntgBYpE6d4vfff190myMjucxSEy6E9PTCQmdMXBzRkCFEo0YZ9iUmcvqMOnWIfvyRLQ2FnByi3r2J9u0r+pwSSQGeXvs0IQD55oJIzOP/27v3+KjKc9Hjv2fuGXIhF0K4hJAIIuiWSxHFdrfdar3t1ku39dLWSqtHPdVa99n9WP24ra11H7X21LbWXfV4b6u2um2LrZUqWo9bKxcVELlICBAChAC5JzPJXJ7zx1rBAQJMIMNMyPP9fObDrMu865lFZp5533et982VhOED6oDqlE7v4/vZ7zi3BiEp64qBoPu8DFh3oA7zvkemEsbL615W3/eDym1+vfKWD3TZtmXK99FfLPr4S3bz5p/q66+jLS1vHLzAxYudtq0+yaRTc8jP37M2MWGC02ST6v77dfdEPSNHOl/G+3P66U4ZHR1O2d9wO+p++1vVb37z4y/qWbOcsZH2V83umyzopZcO/t7StXGjk2yMGQQ/efsnyvfRZz54JtuhDDkDSRgZGxpEVePA9cACYDXwO1X9UETuEJGUYT65FHjWDbzPVGCpiCwHXgfuVtVVmYr1QP6y7i+c98z5JLYfRyA5kuXV3+DxZY/j9/i55ISPJ4EfM+ZqAoEKNm78wcELPemkPaeJFIGHH3YajK691vm3ocEZumLvSXP6xj664QZnStB///f9H+faa50yvvUtZ+ylyy5z1l98sTNmic+dDuWaa5xRZl98EZqanKE0UvUNCdI31tNgqKqCcHjwyjPD2lWzruL+c+7nomk2xHpGpZtZhsJjsGsYdc11mndnnoZunKlllbv0obee3X2d/AXP7vvLvr7+J24t481DO+DPfub8kv/ud1Wfftp5vmTJnvts2qS7+yLOOefA5fV1dIPq6NH7v9S2vd2ZlrSvXHA6aiZNUn3/fadZC2wqTWOOQgzWFK3D3Q0v30A85iH22Hyef7qEc+dezJ/rn2b+2vlcfuLl++w/duw11Nffw6ZNP2DkyEMYtfW665xf+vfc4/z6Dof3na9h/HhnKs5IxJkE6ED8frjySmc03Ysvdgbm609BgTNo4IoVTq2lrc15/PrXTq1k5kxnZNe8vIG/J2PM0SPdzDIUHoNZw/jjmj8611yf+uPdUy6oOpew3fvWvfu9br++/sf6+utoa+tb/W5Py+OPO53C/V2uqqr6qU+pnnVWemU1NKiefLJzCe9ALVzodNCD6imnDPz1xpichw0+eHi6Y91Me2AajfX5lD73PmtW+SkoSO+1iUQX77xTTX7+TKZPX3DoQWzZAoEAjBq177bOTqe2cCR+8d90E9x7rzN0+TPPZP54xpgjaiBjSVmTVD8efe9RZ8C4F/7GA/ennywAvN4RVFZ+h7q679LW9neKiuYeWhB9c0T3Z0AzLx2mO+90JiS64PAGODPGDH1Ww9iLqnLsz4+nbnU+X2hczB/+MPAy4vFOFi06hry8Y5g5861BGYLYGGMyYSA1DJtxby9v1r9Jbetqkov+J/fdd2hl+Hz51NTcTXv739m+/TeDG6AxxmSJJYy9PLj0Qby9IzkpfAnV1Qfff38qKq6goOAk6upuIh7vGLwAjTEmSyxhpGjqauL5Vc+TeO8KvnTB4d1UJuJh8uT76e3dxqZN/zFIERpjTPZYwkjxxLIniCVjsPQaLrzw8MsrLDyZiop5NDT8hO7udYdfoDHGZJEljBQv175MfscsTqiYyqRJg1NmdfVdeDwhamv/dXAKNMaYLLGE4UokEyzZspTO1XP54hcHr9xgsIKqqu/R3Pxndu16afAKNsaYI8wShmvtrrV0xjpgy5xBaY5KNX78DeTlHUtt7Y0kEpHBLdwYY44QSxiuxVsWA1Aem7PP8E2Hy+MJMHnyA0Qi61i37pscTfe+GGOGD0sYrsVbFuONFXJ8xbFk4j67kpIzqKq6jcbGJ9i27f8O/gGMMSbDLGG4Fm9ZjK/pJKomZO6UTJx4O8XFZ7Ju3bfo6Hg3Y8cxxphMsIQBRONRlm9fTs+GOVRWZu44Il6mTXuaQKCcVau+TCLRlbmDGWPMIMtowhCRs0VkrYjUisjN/WyfJyI7RGSZ+7gqZdsVIrLOfVyRyTiXNy4nnoxDwxwmTMjkkcDvL+W4454iElnH+vXfyezBjDFmEGUsYYiIF3gAOAeYBlwmItP62fW3qjrDfTzivrYEuB04GZgD3C4ixZmKta/Dmy0nZTxhABQX/xOVlf/G1q0PsnPni5k/oDHGDIJM1jDmALWqWqeqvcCzwPlpvvYs4BVVbVbVFuAV4OwMxcnirYsp9o6FjnFHJGEAVFffyYgR01m9+iu0ty86Mgc1xpjDkMmEMQ7YnLLc4K7b27+IyAoReV5E+noQ0n3toFi8ZTFjdA5ARvswUnk8QU488c/4/aNYvvwsOjreOzIHNsaYQ5TtTu8XgYmqeiJOLeLJgRYgIleLyFIRWbpjx44BB9Cb6KUgUEBR2ycpLYURIwZcxCELBscxY8Zr+HxFLF/+Obq7PzpyBzfGmAHKZMLYAqT+Xh/vrttNVXepao+7+AjwiXRfm1LGw6o6W1Vnj+pvOtODCHgDLL16KSVrvnPEmqNShUJVTJ++EBEPK1acS2/vwJOeMcYcCZlMGEuAySJSLSIB4FJgfuoOIjImZfE8YLX7fAFwpogUu53dZ7rrMqa+nqwkDIBweBInnPBHenoaWLnyAhKJaHYCMcaYA8hYwlDVOHA9zhf9auB3qvqhiNwhIue5u90gIh+KyHLgBmCe+9pm4Ic4SWcJcIe7LmOymTAAiopOZerUX9He/jZr1sxDNZm9YIwxph++TBauqi8BL+217nspz28BbtnPax8DHstkfH3a2pxHNhMGQHn5l4hG76au7mY2bKihpuZ/ZzcgY4xJkdGEMVRsdq/HOlJXSB1IZeVNRCLrqa+/i1CoirFjr8l2SMYYA1jCAJzmKMh+DQNARJg8+QF6ehr46KNrAWHs2KuzHZYxxmT9stqckEsJA8Dj8XP88S9QUnIuH310DQ0NP892SMYYYwkDnCYpnw8qKrIdyce83hAnnPACZWUXUFv7bVavnkc83pntsIwxw5glDJwaxvjx4PVmO5I9eTxBpk17jqqq29i+/SnefXcWXV1rsh2WMWaYsoRB9i+pPRCPx0d19R1Mn/4a8Xg7y5Z9hs7OldkOyxgzDFnCILcTRp/i4s8yc+YbiPhYtuyzdHQsy3ZIxphhZtgnjGQStm7N/YQBEA5PYcaMN/B6wyxb9mmam/+a7ZCMMcPIsE8YHg90dMAt/d4+mHvC4UnMnPk2oVA1K1acy9atNj+4MebIGPYJAyAQgPz8bEeRvlBoPDNnvklx8Rl89NHVrFlzJYlEd7bDMsYc5SxhDFE+XyH/8A9/YsKEW2lsfJx3351Dd/e6bIdljDmKWcIYwjweHzU1d3LiiQvo7W3kvffm0tb2drbDMsYcpSxhHAVKSj7HrFnv4PcXs2zZaTQ2PomqZjssY8xRxhLGUcLpDP87hYVzWLNmHitWnEUksj7bYRljjiKWMI4igUAZM2a8zqRJ99Pe/g6LFx/P+vU3E4+3ZTs0Y8xRwBLGUUbEy/jx1zNnzmrKyy9h8+Z7WLRoMs3NGZ2w0BgzDGQ0YYjI2SKyVkRqReTmfrb/LxFZJSIrRGShiFSlbEuIyDL3MX/v15oDCwbHMXXqk8yatYRAYAwrVpxLQ8Mvsh2WMWYIy1jCEBEv8ABwDjANuExEpu212/vAbFU9EXge+FHKtoiqznAf52EOSWHhbGbOfIvS0s9TW/stPvjgPFpaXrNOcWPMgGWyhjEHqFXVOlXtBZ4Fzk/dQVVfV9W+O87eAcZnMJ5hy+fL54QTXqC6+k7a2t5m+fLTWbp0Brt2vZzt0IwxQ0gmE8Y4YHPKcoO7bn+uBP6SshwSkaUi8o6IXJCJAIcTES9VVbcyd24DU6Y8TiLRxQcfnMPy5WfS0rLQahzGmIPKiU5vEfkqMBu4N2V1larOBr4M/FREjtnPa692E8vSHTt2HIFohzavN8SYMfOYM2cVxxxzH52d77N8+RksWTKNHTv+kO3wjDE5LJMJYwtQmbI83l23BxE5A7gVOE9Ve/rWq+oW99864G/AzP4OoqoPq+psVZ09atSowYv+KOfxBKisvJFTTtnMccc9iYifDz+8kA0bvodqMtvhGWNyUCYTxhJgsohUi0gAuBTY42onEZkJPISTLJpS1heLSNB9XgZ8EliVwViHLa83REXF15g1azEVFV9n06YfsmLF2TQ3L7DEYYzZgy9TBatqXESuBxYAXuAxVf1QRO4AlqrqfJwmqHzgOREBqHeviJoKPCQiSZykdreqWsLIIK83xJQpj1JQ8Ak2bLidFSvOJhispLj4dAoL51JW9kUCgbJsh2mMySI5mjo7Z8+erUuXLs12GENeMtnDzp1/ZPv2p2lvf4tYbCd+fzlTpjxMWdn5By/AGDNkiMi7bn/xQWWshmGGLo8nSHn5xZSXX4yq0tHxLh999D9YufICysr+hYqKyykuPhOvNy/boRpjjiBLGOaARITCwtnMmrWITZt+yJYtv2Dnzv/C4wlTUnIOo0ZdSFnZBXi9I7IdqjEmw6xJygxIMhmjtfUNdu58gZ07/0Bv7zb8/jIqK7/D2LHfxOcryHaIxpgBGEiTlCUMc8hUk7S1vcmmTXfR0rIAkSCFhadQXHwao0d/lby8mmyHaIw5CEsY5ohrb19EU9NztLb+jc7O9wCluPhzFBaejN9fRjg8jeLi03CGGDPG5Arr9DZHXGHhyRQWngxANNpAY+NjNDY+QUvLq4DzoyQYHE9FxdcZN+46AoHRWYzWGHMorIZhMko1QSzWQmvr32hsfJTm5gWIBBgz5uuMGHEiqnHC4eMoLj4D914cY8wRZDUMkzNEvAQCZZSXX0R5+UV0d69j8+YfsW3bo6jGdu9XWDiXqqrbyM+fQSAwGpGcGObMGJPCahgmK+LxDpLJbsDDzp2/Z+PGO+jtdYYaEwlQUHASJSVnUlg4l1ComlBoAh5PILtBG3MUsk5vM+QkElFaWxcSjW4iEqmjre0NOjrepa//QyRAUdE/UlJyJl5vIclkN3l5x1Jaeq7VRow5DNYkZYYcrzdEaek/77EuFttFV9dKotGNdHauoLl5AXV1391jn3B4KuPGXUc4fBzB4HhCoWPweOzP2phMsE+WyVl+fykjR34G+Iy75v/Q27sD1QQeT4jm5peor/8R69Zdv/s1Hk+I/PwZ5OVNIRAYvfvh95fj8eTh8fjJy5uE31+alfdkzFBmCcMMKYHAx3OejB79ZcrLLyMSWU9PTwM9PfV0di6jo+NdWltfo7d3O87swHvzUlx8BmVl55OfP51gcBy7dr3Ejh3PkZc3iZqauyyhGNMP68MwRy1VJR5vIxbbTm9vE8lklGSyh/b2t2hq+h3RaN0e++flHUs0WofPV8zYsdcSidQRiayjsPBkysouJBye6pYbIxbbRTIZoaBgNh6PH1Vl164X6enZQkXFPBuY0QwZ1ultzEGoKtHoRrq6PiQaraOo6NPk50+nq+sD1q69io6OJW6fSDUdHUtIJqP9lhMIjGH06K/R1vYm7e1vAxAK1VBTczceTx49PfX4fMWMGHE8oVDN7kEaY7EdRCLrCQQqCIUmDvgelEQiyubNPyKZjDJx4u14PMHDOyFm2LKEYcxhUFUSiXZ8viIAEokumptfobe3EVBEfPj9pajGaWx8iubmlwgEKpg48QeEQhOprb2B7u41+yndi8cTdC8pdvj95eTl1ZBMxhDxUlR0qnsjY4Ceni10d6+mvX0RPT2bKCr6NEVFp7J5831EImsByM//BFOn/hqAaHQDgUA54fBxhzyCcDLZS3PzAnp66hk9+qu7z4M5OuVMwhCRs4Gf4cy494iq3r3X9iDwFPAJYBdwiapudLfdAlwJJIAbVHXBwY5nCcNkQ2/vTrzefLzeEOB84ba0vIbfX0wwWEks1kx394dEo5uIx9tIJiOEQlWEQjX09DS4yaBhdyJpb39njxqNiJ/8/BkEg5W0tr5OPN5CKDSRY499iGQywpo184jHW/eJy+8fjd9fitdbQDLZTTIZZcSIExg58jQ8nhCdne8RidTh8QTxeEKoJlDtoa3tbeLxZgB8vlLGj7+RWGwHLS2vEgiUM27c9ZSWfoHe3ia6uz9k164/sWvXnwmFJlJd/R8UFc0lEtlAW9ubJJMRt5wSQqFqfL4CotF6YrEmAoEx5OVNIhgcj4iHZDLO1q3/SWPj41RW3kR5+aWIyO6r5VQVrzePgoLZaY1J1tOzhXi8za3ZhQb8/5pIdNPb20QoVLVHDVBV2b79KXbteonq6jsJhycPuOxU0Wg9Xm8Bfn/xPttisWY8nhBeb/iAZajqIY+UkBMJQ5z/0Y+AzwENOHN8X5Y61aqIfBM4UVWvFZFLgQtV9RIRmQY8A8wBxgKvAseqauJAx7SEYY4GiUSE9vZFiHgJBscSDI7f3eSUTMbp6lpJOHzs7i+RaHQTTU3PEgiMJRSqJhbbTlfXKnp6GojFdpFIdOD1hhHx0d6+hJ6eTQB4vYWEw8eSTMZIJiOI+PB4AoTDUykv/zKBwCg2bLiNlpZX8HjyKCr6NJHIWqLRjYDQd4+Mx5NHcfHptLcvIRbbTjA4gZ6e+rTfr89XQlHRp4hG6+jqWonfP5pYbDujRl2ExxOiqek5VHt27x8KVTN27DX4/WXuBQ9bSSTaSSS68HgCiPjp7Fye0kclBAIVqCZRjREOT6Go6FOEw1MQ8aOaoLd3Kz09W0gkulDtJRKppbNzGapxgsEqSkvPYcSI6QSD49m69Zc0N79EX22xpuYet0boIRqtp7PzPaLRDbuPDYKIB683303iZXi9eSQSERobn6C1dSFebyETJ36PceO+hWqc7u7VNDT8lO3bn8HnK2Ts2GuoqJiH3z8an69w971HnZ0r2br1QSKRWqZPf/mQ/t5yJWHMBb6vqme5y7cAqOpdKfsscPf5u4j4gEZgFHBz6r6p+x3omJYwjDm4SGQjqnHy8mrSuukxEqkjEBiL1+vUQnbufJGOjkUEg1Xk5U2iqOhUvN4w8XgnDQ0/paNjKcXF/0Rx8Rn4fM6v5lhsB9HoRuLxDkKhSvz+0fT2biESqaW9fTFtbW8CHmpq7qK09Ats3nwvGzfejscTYvTor1FWdh4ifnp7t7F168O0tb0BgIiPQGAMPt9IPJ4wqjGSySjh8LEUFX2aQKCcSKSWaHQzIj5EhM7OFXR0LNljaBpwalM+XwEifoLBcRQWziUQGENLy0JaWl4lmewCwOMJU1NzN2VlF7J27VW0tOzb+OH3lwEeIInzHZskkehANb7HfsFgJWPGXEV7+yKam19CxLd7H683n4qKK+npaWDnzt8DSfdVHny+IrzefHp6NiMSpLz8S0yZ8sgh9WXlSsK4CDhbVa9yly8HTlbV61P2Wenu0+AurwdOBr4PvKOqv3bXPwr8RVWfP9AxLWEYc/To7d2JxxPC58vfZ1sksh7wEAxWHtKNmolEhFisCafRQggExhyw2Uo1SW/vNqLRjYRCEwkGx7nrlZaWV4jFmoEEfn85BQWz+r0sWzVJPN66+wo71TgjRpy4O/7m5gW0tCzE5yshGBxDael5u5upIpE6WlvfIB5vIR5vIRZrIR5vJT9/BhUV8wgEygZ8DvoMqzu9ReRq4GqACRMmZDkaY8xgOdCXYF7eMYdVttebh9dblfb+Ih6CwXG7E8XH64WSkjPTLsPvL8HvL+l3e0nJWZSUnNXvtry8mpyYkCyTg/BsASpTlse76/rdx22SKsLp/E7ntQCo6sOqOltVZ48aNaq/XYwxxgyCTCaMJcBkEakWkQBwKTB/r33mA1e4zy8CXlOnjWw+cKmIBEWkGpgMLM5grMYYYw4iY01SqhoXkeuBBTiX1T6mqh+KyB3AUlWdDzwK/EpEaoFmnKSCu9/vgFVAHLjuYFdIGWOMySy7cc8YY4axgXR620QCxhhj0mIJwxhjTFosYRhjjEmLJQxjjDFpOao6vUVkB7DpEF9eBuwcxHCOlKEaNwzd2Idq3DB0Yx+qcUPux16lqmndxHZUJYzDISJL071SIJcM1bhh6MY+VOOGoRv7UI0bhnbse7MmKWOMMWmxhGGMMSYtljA+9nC2tuzdYQAABeVJREFUAzhEQzVuGLqxD9W4YejGPlTjhqEd+x6sD8MYY0xarIZhjDEmLcM+YYjI2SKyVkRqReTmbMdzICJSKSKvi8gqEflQRL7tri8RkVdEZJ37776TA+cAEfGKyPsi8id3uVpEFrnn/rfuqMY5R0RGisjzIrJGRFaLyNyhcM5F5F/dv5OVIvKMiIRy9ZyLyGMi0uROqta3rt9zLI6fu+9hhYjMyrG473X/VlaIyO9FZGTKtlvcuNeKSP+TX+SwYZ0w3HnHHwDOAaYBl7nzieeqOPBvqjoNOAW4zo33ZmChqk4GFrrLuejbwOqU5XuA+1R1EtACXJmVqA7uZ8DLqnocMB3nPeT0OReRccANwGxVPQFnxOhLyd1z/gRw9l7r9neOz8GZ8mAyzuRpvzxCMfbnCfaN+xXgBFU9EfgIuAXA/axeChzvvuY/3e+gIWNYJwxgDlCrqnWq2gs8C5yf5Zj2S1W3qep77vMOnC+ucTgxP+nu9iRwQXYi3D8RGQ/8M/CIuyzAaUDftLu5GncR8GmcofhR1V5VbWUInHOc6Qvy3MnJwsA2cvScq+r/w5niINX+zvH5wFPqeAcYKSJjjkyke+ovblX9q348efc7OBPAgRP3s6rao6obgFqc76AhY7gnjHHA5pTlBnddzhORicBMYBEwWlW3uZsagdFZCutAfgrcxMcz2ZcCrSkfrFw999XADuBxtzntEREZQY6fc1XdAvwYqMdJFG3AuwyNc95nf+d4KH1uvwH8xX0+lOLu13BPGEOSiOQD/wXcqKrtqdvcGQtz6tI3Efk80KSq72Y7lkPgA2YBv1TVmUAXezU/5eg5L8b5RVsNjAVGsG/TyZCRi+f4YETkVpxm5N9kO5bBMtwTRtpzh+cKEfHjJIvfqOoL7urtfVVy99+mbMW3H58EzhORjTjNfqfh9AuMdJtLIHfPfQPQoKqL3OXncRJIrp/zM4ANqrpDVWPACzj/D0PhnPfZ3znO+c+tiMwDPg98RT++dyHn4z6Y4Z4w0pl3PGe47f6PAqtV9Scpm1LnRr8C+OORju1AVPUWVR2vqhNxzvFrqvoV4HWcudwhB+MGUNVGYLOITHFXnY4zdXBOn3OcpqhTRCTs/t30xZ3z5zzF/s7xfOBr7tVSpwBtKU1XWSciZ+M0v56nqt0pm+YDl4pIUESqcTrtF2cjxkOmqsP6AZyLcyXDeuDWbMdzkFg/hVMtXwEscx/n4vQHLATWAa8CJdmO9QDv4bPAn9znNTgfmFrgOSCY7fj2E/MMYKl73v8AFA+Fcw78AFgDrAR+BQRz9ZwDz+D0tcRwanVX7u8cA4JzdeN64AOcK8FyKe5anL6Kvs/ogyn73+rGvRY4J9vnfaAPu9PbGGNMWoZ7k5Qxxpg0WcIwxhiTFksYxhhj0mIJwxhjTFosYRhjjEmLJQxjcoCIfLZvFF9jcpUlDGOMMWmxhGHMAIjIV0VksYgsE5GH3Dk+OkXkPnfuiYUiMsrdd4aIvJMyL0LffA6TRORVEVkuIu+JyDFu8fkp8278xr1D25icYQnDmDSJyFTgEuCTqjoDSABfwRnYb6mqHg+8AdzuvuQp4LvqzIvwQcr63wAPqOp04FScO4XBGX34Rpy5WWpwxn4yJmf4Dr6LMcZ1OvAJYIn74z8PZ0C8JPBbd59fAy+482iMVNU33PVPAs+JSAEwTlV/D6CqUQC3vMWq2uAuLwMmAv+d+bdlTHosYRiTPgGeVNVb9lgpctte+x3qeDs9Kc8T2OfT5BhrkjImfQuBi0SkHHbPOV2F8znqGwH2y8B/q2ob0CIi/+iuvxx4Q52ZEhtE5AK3jKCIhI/ouzDmENkvGGPSpKqrROTfgb+KiAdnhNLrcCZVmuNua8Lp5wBnSO4H3YRQB3zdXX858JCI3OGW8aUj+DaMOWQ2Wq0xh0lEOlU1P9txGJNp1iRljDEmLVbDMMYYkxarYRhjjEmLJQxjjDFpsYRhjDEmLZYwjDHGpMUShjHGmLRYwjDGGJOW/w/dYbY9U8tuzgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 258us/sample - loss: 0.7525 - acc: 0.7890\n",
      "Loss: 0.7524969367594734 Accuracy: 0.7889927\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9704 - acc: 0.3779\n",
      "Epoch 00001: val_loss improved from inf to 1.66222, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/001-1.6622.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 1.9704 - acc: 0.3779 - val_loss: 1.6622 - val_acc: 0.4677\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2920 - acc: 0.6000\n",
      "Epoch 00002: val_loss improved from 1.66222 to 1.18882, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/002-1.1888.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 1.2921 - acc: 0.6000 - val_loss: 1.1888 - val_acc: 0.6161\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0185 - acc: 0.6920\n",
      "Epoch 00003: val_loss improved from 1.18882 to 0.89441, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/003-0.8944.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 1.0177 - acc: 0.6924 - val_loss: 0.8944 - val_acc: 0.7333\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8372 - acc: 0.7514\n",
      "Epoch 00004: val_loss improved from 0.89441 to 0.78422, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/004-0.7842.hdf5\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.8375 - acc: 0.7514 - val_loss: 0.7842 - val_acc: 0.7622\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7102 - acc: 0.7940\n",
      "Epoch 00005: val_loss improved from 0.78422 to 0.73763, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/005-0.7376.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.7097 - acc: 0.7942 - val_loss: 0.7376 - val_acc: 0.7808\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6149 - acc: 0.8237\n",
      "Epoch 00006: val_loss improved from 0.73763 to 0.63869, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/006-0.6387.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.6146 - acc: 0.8238 - val_loss: 0.6387 - val_acc: 0.8164\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5419 - acc: 0.8447\n",
      "Epoch 00007: val_loss improved from 0.63869 to 0.57049, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/007-0.5705.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.5416 - acc: 0.8447 - val_loss: 0.5705 - val_acc: 0.8318\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4849 - acc: 0.8612\n",
      "Epoch 00008: val_loss improved from 0.57049 to 0.55116, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/008-0.5512.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.4851 - acc: 0.8612 - val_loss: 0.5512 - val_acc: 0.8397\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8743\n",
      "Epoch 00009: val_loss improved from 0.55116 to 0.47891, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/009-0.4789.hdf5\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.4415 - acc: 0.8743 - val_loss: 0.4789 - val_acc: 0.8630\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4067 - acc: 0.8860\n",
      "Epoch 00010: val_loss improved from 0.47891 to 0.47874, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/010-0.4787.hdf5\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.4066 - acc: 0.8860 - val_loss: 0.4787 - val_acc: 0.8651\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8968\n",
      "Epoch 00011: val_loss improved from 0.47874 to 0.41844, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/011-0.4184.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.3710 - acc: 0.8969 - val_loss: 0.4184 - val_acc: 0.8807\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.9047\n",
      "Epoch 00012: val_loss did not improve from 0.41844\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.3427 - acc: 0.9047 - val_loss: 0.4547 - val_acc: 0.8728\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.9090\n",
      "Epoch 00013: val_loss did not improve from 0.41844\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.3248 - acc: 0.9091 - val_loss: 0.4430 - val_acc: 0.8691\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9168\n",
      "Epoch 00014: val_loss improved from 0.41844 to 0.38272, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/014-0.3827.hdf5\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.2998 - acc: 0.9168 - val_loss: 0.3827 - val_acc: 0.8959\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9226\n",
      "Epoch 00015: val_loss did not improve from 0.38272\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.2797 - acc: 0.9227 - val_loss: 0.4275 - val_acc: 0.8807\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9259\n",
      "Epoch 00016: val_loss improved from 0.38272 to 0.36469, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/016-0.3647.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.2635 - acc: 0.9260 - val_loss: 0.3647 - val_acc: 0.8915\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9327\n",
      "Epoch 00017: val_loss improved from 0.36469 to 0.33632, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/017-0.3363.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.2458 - acc: 0.9327 - val_loss: 0.3363 - val_acc: 0.9066\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9357\n",
      "Epoch 00018: val_loss did not improve from 0.33632\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.2340 - acc: 0.9357 - val_loss: 0.3778 - val_acc: 0.8912\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9397\n",
      "Epoch 00019: val_loss did not improve from 0.33632\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.2194 - acc: 0.9395 - val_loss: 0.4364 - val_acc: 0.8779\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9410\n",
      "Epoch 00020: val_loss did not improve from 0.33632\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.2146 - acc: 0.9410 - val_loss: 0.3655 - val_acc: 0.8968\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9486\n",
      "Epoch 00021: val_loss did not improve from 0.33632\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1951 - acc: 0.9486 - val_loss: 0.3462 - val_acc: 0.9029\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9492\n",
      "Epoch 00022: val_loss did not improve from 0.33632\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1902 - acc: 0.9492 - val_loss: 0.3439 - val_acc: 0.9052\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9523\n",
      "Epoch 00023: val_loss improved from 0.33632 to 0.33480, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/023-0.3348.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.1782 - acc: 0.9522 - val_loss: 0.3348 - val_acc: 0.9001\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9549\n",
      "Epoch 00024: val_loss did not improve from 0.33480\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1722 - acc: 0.9550 - val_loss: 0.3944 - val_acc: 0.8887\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9579\n",
      "Epoch 00025: val_loss did not improve from 0.33480\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1599 - acc: 0.9579 - val_loss: 0.3477 - val_acc: 0.8996\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9618\n",
      "Epoch 00026: val_loss improved from 0.33480 to 0.31781, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/026-0.3178.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.1502 - acc: 0.9617 - val_loss: 0.3178 - val_acc: 0.9082\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9616\n",
      "Epoch 00027: val_loss did not improve from 0.31781\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1464 - acc: 0.9616 - val_loss: 0.3947 - val_acc: 0.8842\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9653\n",
      "Epoch 00028: val_loss improved from 0.31781 to 0.31460, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/028-0.3146.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.1362 - acc: 0.9653 - val_loss: 0.3146 - val_acc: 0.9124\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9684\n",
      "Epoch 00029: val_loss did not improve from 0.31460\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1299 - acc: 0.9683 - val_loss: 0.3226 - val_acc: 0.9113\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9707\n",
      "Epoch 00030: val_loss improved from 0.31460 to 0.30026, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/030-0.3003.hdf5\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.1225 - acc: 0.9707 - val_loss: 0.3003 - val_acc: 0.9175\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9702\n",
      "Epoch 00031: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.1231 - acc: 0.9702 - val_loss: 0.3797 - val_acc: 0.8908\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9736\n",
      "Epoch 00032: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1115 - acc: 0.9736 - val_loss: 0.3376 - val_acc: 0.9033\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9757\n",
      "Epoch 00033: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1049 - acc: 0.9757 - val_loss: 0.4060 - val_acc: 0.8901\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9756\n",
      "Epoch 00034: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.1032 - acc: 0.9757 - val_loss: 0.3388 - val_acc: 0.9103\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9777\n",
      "Epoch 00035: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0956 - acc: 0.9777 - val_loss: 0.3897 - val_acc: 0.8968\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9795\n",
      "Epoch 00036: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0923 - acc: 0.9794 - val_loss: 0.3909 - val_acc: 0.8921\n",
      "Epoch 37/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9791\n",
      "Epoch 00037: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0911 - acc: 0.9791 - val_loss: 0.3289 - val_acc: 0.9094\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9826\n",
      "Epoch 00038: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0813 - acc: 0.9827 - val_loss: 0.3228 - val_acc: 0.9133\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9831\n",
      "Epoch 00039: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0770 - acc: 0.9832 - val_loss: 0.3279 - val_acc: 0.9089\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9836\n",
      "Epoch 00040: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0763 - acc: 0.9836 - val_loss: 0.3016 - val_acc: 0.9168\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9830\n",
      "Epoch 00041: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0757 - acc: 0.9829 - val_loss: 0.3249 - val_acc: 0.9094\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9865\n",
      "Epoch 00042: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0670 - acc: 0.9865 - val_loss: 0.3424 - val_acc: 0.9061\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9883\n",
      "Epoch 00043: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0625 - acc: 0.9883 - val_loss: 0.4314 - val_acc: 0.8873\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9875\n",
      "Epoch 00044: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 13s 367us/sample - loss: 0.0641 - acc: 0.9875 - val_loss: 0.3904 - val_acc: 0.8966\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9872\n",
      "Epoch 00045: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0625 - acc: 0.9871 - val_loss: 0.4127 - val_acc: 0.8942\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9874\n",
      "Epoch 00046: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0630 - acc: 0.9874 - val_loss: 0.4276 - val_acc: 0.8880\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9897\n",
      "Epoch 00047: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0555 - acc: 0.9898 - val_loss: 0.3268 - val_acc: 0.9136\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9905\n",
      "Epoch 00048: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0528 - acc: 0.9905 - val_loss: 0.3800 - val_acc: 0.8977\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9854\n",
      "Epoch 00049: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0680 - acc: 0.9854 - val_loss: 0.3076 - val_acc: 0.9182\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9944\n",
      "Epoch 00050: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0419 - acc: 0.9944 - val_loss: 0.3494 - val_acc: 0.9029\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9907\n",
      "Epoch 00051: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 369us/sample - loss: 0.0510 - acc: 0.9907 - val_loss: 0.3110 - val_acc: 0.9154\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9927\n",
      "Epoch 00052: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0419 - acc: 0.9927 - val_loss: 0.3772 - val_acc: 0.9005\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9927\n",
      "Epoch 00053: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0416 - acc: 0.9927 - val_loss: 0.4013 - val_acc: 0.9022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9899\n",
      "Epoch 00054: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0508 - acc: 0.9899 - val_loss: 0.3319 - val_acc: 0.9113\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9942\n",
      "Epoch 00055: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0368 - acc: 0.9942 - val_loss: 0.3332 - val_acc: 0.9185\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9936\n",
      "Epoch 00056: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0403 - acc: 0.9936 - val_loss: 0.3815 - val_acc: 0.9036\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9961\n",
      "Epoch 00057: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0322 - acc: 0.9960 - val_loss: 0.4263 - val_acc: 0.8933\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9933\n",
      "Epoch 00058: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0397 - acc: 0.9933 - val_loss: 0.3392 - val_acc: 0.9126\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9942\n",
      "Epoch 00059: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0366 - acc: 0.9942 - val_loss: 0.3440 - val_acc: 0.9103\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9959\n",
      "Epoch 00060: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0302 - acc: 0.9959 - val_loss: 0.3248 - val_acc: 0.9147\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9968\n",
      "Epoch 00061: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0279 - acc: 0.9967 - val_loss: 0.4463 - val_acc: 0.8926\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9944\n",
      "Epoch 00062: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0347 - acc: 0.9944 - val_loss: 0.3600 - val_acc: 0.9012\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9960\n",
      "Epoch 00063: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0289 - acc: 0.9959 - val_loss: 0.6083 - val_acc: 0.8595\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9900\n",
      "Epoch 00064: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0436 - acc: 0.9900 - val_loss: 0.3837 - val_acc: 0.9052\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9975\n",
      "Epoch 00065: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0224 - acc: 0.9975 - val_loss: 0.3960 - val_acc: 0.9010\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9974\n",
      "Epoch 00066: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0229 - acc: 0.9973 - val_loss: 0.3495 - val_acc: 0.9092\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9952\n",
      "Epoch 00067: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0299 - acc: 0.9952 - val_loss: 0.4071 - val_acc: 0.9040\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9956\n",
      "Epoch 00068: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0270 - acc: 0.9957 - val_loss: 0.3214 - val_acc: 0.9206\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9963\n",
      "Epoch 00069: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0235 - acc: 0.9963 - val_loss: 0.4092 - val_acc: 0.9057\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9959\n",
      "Epoch 00070: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0260 - acc: 0.9959 - val_loss: 0.3889 - val_acc: 0.9059\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9967\n",
      "Epoch 00071: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0229 - acc: 0.9967 - val_loss: 0.3683 - val_acc: 0.9110\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9949\n",
      "Epoch 00072: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0276 - acc: 0.9949 - val_loss: 0.4122 - val_acc: 0.9043\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9961\n",
      "Epoch 00073: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0244 - acc: 0.9961 - val_loss: 0.3973 - val_acc: 0.9026\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9987\n",
      "Epoch 00074: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0155 - acc: 0.9987 - val_loss: 0.4097 - val_acc: 0.9064\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9933\n",
      "Epoch 00075: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0328 - acc: 0.9933 - val_loss: 0.3244 - val_acc: 0.9194\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9985\n",
      "Epoch 00076: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0154 - acc: 0.9985 - val_loss: 0.3583 - val_acc: 0.9119\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9958\n",
      "Epoch 00077: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0255 - acc: 0.9958 - val_loss: 0.5033 - val_acc: 0.8754\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9977\n",
      "Epoch 00078: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0185 - acc: 0.9977 - val_loss: 0.3190 - val_acc: 0.9243\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9986\n",
      "Epoch 00079: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0144 - acc: 0.9986 - val_loss: 0.3344 - val_acc: 0.9199\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9973\n",
      "Epoch 00080: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0190 - acc: 0.9973 - val_loss: 0.3561 - val_acc: 0.9110\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9975\n",
      "Epoch 00081: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0185 - acc: 0.9974 - val_loss: 0.6020 - val_acc: 0.8600\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9944\n",
      "Epoch 00082: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0270 - acc: 0.9944 - val_loss: 0.4358 - val_acc: 0.8975\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9979\n",
      "Epoch 00083: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0160 - acc: 0.9979 - val_loss: 0.3500 - val_acc: 0.9145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9990\n",
      "Epoch 00084: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0116 - acc: 0.9990 - val_loss: 0.3881 - val_acc: 0.9047\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9938\n",
      "Epoch 00085: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0271 - acc: 0.9938 - val_loss: 0.3182 - val_acc: 0.9201\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9992\n",
      "Epoch 00086: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0110 - acc: 0.9992 - val_loss: 0.3221 - val_acc: 0.9196\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9973\n",
      "Epoch 00087: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0168 - acc: 0.9973 - val_loss: 0.3507 - val_acc: 0.9166\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9977\n",
      "Epoch 00088: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0162 - acc: 0.9977 - val_loss: 0.3556 - val_acc: 0.9171\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9972\n",
      "Epoch 00089: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0180 - acc: 0.9972 - val_loss: 0.3564 - val_acc: 0.9122\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9977\n",
      "Epoch 00090: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0171 - acc: 0.9977 - val_loss: 0.3529 - val_acc: 0.9157\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9951\n",
      "Epoch 00091: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0236 - acc: 0.9951 - val_loss: 0.3661 - val_acc: 0.9110\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9994\n",
      "Epoch 00092: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0094 - acc: 0.9994 - val_loss: 0.3301 - val_acc: 0.9215\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9982\n",
      "Epoch 00093: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0137 - acc: 0.9982 - val_loss: 0.3893 - val_acc: 0.9064\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9987\n",
      "Epoch 00094: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0120 - acc: 0.9986 - val_loss: 0.3894 - val_acc: 0.9085\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9941\n",
      "Epoch 00095: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0285 - acc: 0.9942 - val_loss: 0.3142 - val_acc: 0.9227\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9997\n",
      "Epoch 00096: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0072 - acc: 0.9997 - val_loss: 0.3269 - val_acc: 0.9241\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9986\n",
      "Epoch 00097: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0109 - acc: 0.9986 - val_loss: 0.4857 - val_acc: 0.8928\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9958\n",
      "Epoch 00098: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0209 - acc: 0.9958 - val_loss: 0.3435 - val_acc: 0.9217\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9989\n",
      "Epoch 00099: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0096 - acc: 0.9989 - val_loss: 0.4267 - val_acc: 0.9061\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9945\n",
      "Epoch 00100: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0248 - acc: 0.9945 - val_loss: 0.3477 - val_acc: 0.9168\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9992\n",
      "Epoch 00101: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0087 - acc: 0.9992 - val_loss: 0.3418 - val_acc: 0.9178\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9962\n",
      "Epoch 00102: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 369us/sample - loss: 0.0198 - acc: 0.9961 - val_loss: 0.3486 - val_acc: 0.9210\n",
      "Epoch 103/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9972\n",
      "Epoch 00103: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0151 - acc: 0.9972 - val_loss: 0.3531 - val_acc: 0.9201\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9956\n",
      "Epoch 00104: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0227 - acc: 0.9955 - val_loss: 0.4104 - val_acc: 0.9040\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9960\n",
      "Epoch 00105: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0200 - acc: 0.9960 - val_loss: 0.3381 - val_acc: 0.9189\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9996\n",
      "Epoch 00106: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0067 - acc: 0.9996 - val_loss: 0.3216 - val_acc: 0.9229\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9996\n",
      "Epoch 00107: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0059 - acc: 0.9996 - val_loss: 0.3302 - val_acc: 0.9220\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9973\n",
      "Epoch 00108: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0129 - acc: 0.9973 - val_loss: 0.5717 - val_acc: 0.8763\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9977\n",
      "Epoch 00109: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 369us/sample - loss: 0.0137 - acc: 0.9976 - val_loss: 0.7455 - val_acc: 0.8484\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9985\n",
      "Epoch 00110: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0103 - acc: 0.9985 - val_loss: 0.3444 - val_acc: 0.9224\n",
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9979\n",
      "Epoch 00111: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0124 - acc: 0.9979 - val_loss: 0.5252 - val_acc: 0.8866\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9971\n",
      "Epoch 00112: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0165 - acc: 0.9971 - val_loss: 0.4405 - val_acc: 0.8956\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9986\n",
      "Epoch 00113: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0087 - acc: 0.9986 - val_loss: 0.3690 - val_acc: 0.9173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9992\n",
      "Epoch 00114: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0064 - acc: 0.9992 - val_loss: 0.3512 - val_acc: 0.9243\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9996\n",
      "Epoch 00115: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0053 - acc: 0.9996 - val_loss: 0.3536 - val_acc: 0.9229\n",
      "Epoch 116/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9961\n",
      "Epoch 00116: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0171 - acc: 0.9961 - val_loss: 0.3295 - val_acc: 0.9269\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9991\n",
      "Epoch 00117: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0070 - acc: 0.9991 - val_loss: 0.7607 - val_acc: 0.8435\n",
      "Epoch 118/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9969\n",
      "Epoch 00118: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0150 - acc: 0.9969 - val_loss: 0.3301 - val_acc: 0.9206\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9993\n",
      "Epoch 00119: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0067 - acc: 0.9993 - val_loss: 0.3458 - val_acc: 0.9231\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9996\n",
      "Epoch 00120: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0052 - acc: 0.9996 - val_loss: 0.3438 - val_acc: 0.9238\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9964\n",
      "Epoch 00121: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0165 - acc: 0.9964 - val_loss: 0.7777 - val_acc: 0.8553\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9980\n",
      "Epoch 00122: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0110 - acc: 0.9980 - val_loss: 0.3668 - val_acc: 0.9145\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9975\n",
      "Epoch 00123: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0134 - acc: 0.9974 - val_loss: 0.3998 - val_acc: 0.9159\n",
      "Epoch 124/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9976\n",
      "Epoch 00124: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0126 - acc: 0.9976 - val_loss: 0.3414 - val_acc: 0.9220\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9984\n",
      "Epoch 00125: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0105 - acc: 0.9985 - val_loss: 0.3431 - val_acc: 0.9234\n",
      "Epoch 126/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9996\n",
      "Epoch 00126: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0049 - acc: 0.9996 - val_loss: 0.3614 - val_acc: 0.9234\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9973\n",
      "Epoch 00127: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0135 - acc: 0.9973 - val_loss: 0.3454 - val_acc: 0.9238\n",
      "Epoch 128/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9997\n",
      "Epoch 00128: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0043 - acc: 0.9997 - val_loss: 0.3375 - val_acc: 0.9257\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9988\n",
      "Epoch 00129: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0080 - acc: 0.9988 - val_loss: 0.6902 - val_acc: 0.8614\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9963\n",
      "Epoch 00130: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0156 - acc: 0.9963 - val_loss: 0.3410 - val_acc: 0.9210\n",
      "\n",
      "1D_CNN_BN_4_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 289us/sample - loss: 0.4007 - acc: 0.8804\n",
      "Loss: 0.4007288916583868 Accuracy: 0.88037384\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6549 - acc: 0.4812\n",
      "Epoch 00001: val_loss improved from inf to 1.26308, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/001-1.2631.hdf5\n",
      "36805/36805 [==============================] - 22s 587us/sample - loss: 1.6540 - acc: 0.4815 - val_loss: 1.2631 - val_acc: 0.5956\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9233 - acc: 0.7261\n",
      "Epoch 00002: val_loss improved from 1.26308 to 0.75987, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/002-0.7599.hdf5\n",
      "36805/36805 [==============================] - 15s 407us/sample - loss: 0.9232 - acc: 0.7261 - val_loss: 0.7599 - val_acc: 0.7680\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6496 - acc: 0.8097\n",
      "Epoch 00003: val_loss improved from 0.75987 to 0.55025, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/003-0.5502.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.6498 - acc: 0.8096 - val_loss: 0.5502 - val_acc: 0.8328\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.8511\n",
      "Epoch 00004: val_loss improved from 0.55025 to 0.45722, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/004-0.4572.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.5071 - acc: 0.8510 - val_loss: 0.4572 - val_acc: 0.8607\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4154 - acc: 0.8797\n",
      "Epoch 00005: val_loss improved from 0.45722 to 0.40577, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/005-0.4058.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.4153 - acc: 0.8798 - val_loss: 0.4058 - val_acc: 0.8749\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8980\n",
      "Epoch 00006: val_loss improved from 0.40577 to 0.39029, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/006-0.3903.hdf5\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.3524 - acc: 0.8979 - val_loss: 0.3903 - val_acc: 0.8765\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.9099\n",
      "Epoch 00007: val_loss improved from 0.39029 to 0.33065, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/007-0.3307.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.3090 - acc: 0.9099 - val_loss: 0.3307 - val_acc: 0.8963\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9222\n",
      "Epoch 00008: val_loss did not improve from 0.33065\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.2685 - acc: 0.9223 - val_loss: 0.3310 - val_acc: 0.8980\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9290\n",
      "Epoch 00009: val_loss improved from 0.33065 to 0.27651, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/009-0.2765.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.2460 - acc: 0.9289 - val_loss: 0.2765 - val_acc: 0.9168\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9374\n",
      "Epoch 00010: val_loss did not improve from 0.27651\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.2219 - acc: 0.9374 - val_loss: 0.2823 - val_acc: 0.9094\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9440\n",
      "Epoch 00011: val_loss did not improve from 0.27651\n",
      "36805/36805 [==============================] - 15s 407us/sample - loss: 0.1977 - acc: 0.9440 - val_loss: 0.3614 - val_acc: 0.8891\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9480\n",
      "Epoch 00012: val_loss improved from 0.27651 to 0.24893, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/012-0.2489.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.1818 - acc: 0.9480 - val_loss: 0.2489 - val_acc: 0.9180\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9524\n",
      "Epoch 00013: val_loss improved from 0.24893 to 0.24780, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/013-0.2478.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.1699 - acc: 0.9523 - val_loss: 0.2478 - val_acc: 0.9217\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9551\n",
      "Epoch 00014: val_loss did not improve from 0.24780\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.1583 - acc: 0.9551 - val_loss: 0.2487 - val_acc: 0.9238\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9617\n",
      "Epoch 00015: val_loss did not improve from 0.24780\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.1387 - acc: 0.9617 - val_loss: 0.2548 - val_acc: 0.9231\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9670\n",
      "Epoch 00016: val_loss improved from 0.24780 to 0.23983, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/016-0.2398.hdf5\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.1246 - acc: 0.9670 - val_loss: 0.2398 - val_acc: 0.9271\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9682\n",
      "Epoch 00017: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.1181 - acc: 0.9681 - val_loss: 0.2862 - val_acc: 0.9113\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9701\n",
      "Epoch 00018: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.1117 - acc: 0.9700 - val_loss: 0.2666 - val_acc: 0.9199\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9712\n",
      "Epoch 00019: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.1081 - acc: 0.9712 - val_loss: 0.2541 - val_acc: 0.9248\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9765\n",
      "Epoch 00020: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0924 - acc: 0.9764 - val_loss: 0.2418 - val_acc: 0.9290\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9758\n",
      "Epoch 00021: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0932 - acc: 0.9758 - val_loss: 0.2402 - val_acc: 0.9280\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9816\n",
      "Epoch 00022: val_loss improved from 0.23983 to 0.23914, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/022-0.2391.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.0765 - acc: 0.9816 - val_loss: 0.2391 - val_acc: 0.9250\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9824\n",
      "Epoch 00023: val_loss did not improve from 0.23914\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0724 - acc: 0.9824 - val_loss: 0.2468 - val_acc: 0.9250\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9842\n",
      "Epoch 00024: val_loss did not improve from 0.23914\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0673 - acc: 0.9842 - val_loss: 0.2698 - val_acc: 0.9206\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9849\n",
      "Epoch 00025: val_loss did not improve from 0.23914\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0639 - acc: 0.9849 - val_loss: 0.2704 - val_acc: 0.9238\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9862\n",
      "Epoch 00026: val_loss did not improve from 0.23914\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0596 - acc: 0.9861 - val_loss: 0.2822 - val_acc: 0.9189\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9864\n",
      "Epoch 00027: val_loss did not improve from 0.23914\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0583 - acc: 0.9864 - val_loss: 0.2520 - val_acc: 0.9266\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9900\n",
      "Epoch 00028: val_loss improved from 0.23914 to 0.23319, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/028-0.2332.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.0481 - acc: 0.9901 - val_loss: 0.2332 - val_acc: 0.9334\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9918\n",
      "Epoch 00029: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0423 - acc: 0.9917 - val_loss: 0.2842 - val_acc: 0.9173\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9886\n",
      "Epoch 00030: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0504 - acc: 0.9886 - val_loss: 0.2481 - val_acc: 0.9269\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9927\n",
      "Epoch 00031: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0386 - acc: 0.9927 - val_loss: 0.2831 - val_acc: 0.9180\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9906\n",
      "Epoch 00032: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0445 - acc: 0.9906 - val_loss: 0.2365 - val_acc: 0.9331\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9941\n",
      "Epoch 00033: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0337 - acc: 0.9940 - val_loss: 0.2600 - val_acc: 0.9271\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9916\n",
      "Epoch 00034: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0408 - acc: 0.9916 - val_loss: 0.2989 - val_acc: 0.9231\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9865\n",
      "Epoch 00035: val_loss improved from 0.23319 to 0.22666, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/035-0.2267.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.0563 - acc: 0.9865 - val_loss: 0.2267 - val_acc: 0.9369\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9957\n",
      "Epoch 00036: val_loss improved from 0.22666 to 0.21759, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/036-0.2176.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.0280 - acc: 0.9956 - val_loss: 0.2176 - val_acc: 0.9390\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9916\n",
      "Epoch 00037: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0405 - acc: 0.9916 - val_loss: 0.2308 - val_acc: 0.9359\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9966\n",
      "Epoch 00038: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0232 - acc: 0.9965 - val_loss: 0.2276 - val_acc: 0.9352\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9929\n",
      "Epoch 00039: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.0342 - acc: 0.9929 - val_loss: 0.2268 - val_acc: 0.9343\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9965\n",
      "Epoch 00040: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0229 - acc: 0.9965 - val_loss: 0.2540 - val_acc: 0.9287\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9967\n",
      "Epoch 00041: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0212 - acc: 0.9967 - val_loss: 0.2726 - val_acc: 0.9255\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9938\n",
      "Epoch 00042: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0286 - acc: 0.9938 - val_loss: 0.2773 - val_acc: 0.9194\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9964\n",
      "Epoch 00043: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0206 - acc: 0.9964 - val_loss: 0.2493 - val_acc: 0.9357\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9952\n",
      "Epoch 00044: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0238 - acc: 0.9952 - val_loss: 0.2810 - val_acc: 0.9285\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9909\n",
      "Epoch 00045: val_loss improved from 0.21759 to 0.21400, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/045-0.2140.hdf5\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0373 - acc: 0.9909 - val_loss: 0.2140 - val_acc: 0.9390\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9975\n",
      "Epoch 00046: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0166 - acc: 0.9975 - val_loss: 0.2470 - val_acc: 0.9364\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9982\n",
      "Epoch 00047: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0143 - acc: 0.9982 - val_loss: 0.2803 - val_acc: 0.9283\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9972\n",
      "Epoch 00048: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0167 - acc: 0.9972 - val_loss: 0.3721 - val_acc: 0.9073\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9938\n",
      "Epoch 00049: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0255 - acc: 0.9938 - val_loss: 0.2573 - val_acc: 0.9331\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9984\n",
      "Epoch 00050: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0130 - acc: 0.9984 - val_loss: 0.2452 - val_acc: 0.9329\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9939\n",
      "Epoch 00051: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0274 - acc: 0.9939 - val_loss: 0.2256 - val_acc: 0.9385\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9988\n",
      "Epoch 00052: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0105 - acc: 0.9988 - val_loss: 0.2269 - val_acc: 0.9411\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9969\n",
      "Epoch 00053: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0177 - acc: 0.9969 - val_loss: 0.3754 - val_acc: 0.9103\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9970\n",
      "Epoch 00054: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0171 - acc: 0.9970 - val_loss: 0.2628 - val_acc: 0.9341\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9979\n",
      "Epoch 00055: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0135 - acc: 0.9979 - val_loss: 0.2284 - val_acc: 0.9406\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9954\n",
      "Epoch 00056: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0214 - acc: 0.9954 - val_loss: 0.2570 - val_acc: 0.9324\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9979\n",
      "Epoch 00057: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0122 - acc: 0.9979 - val_loss: 0.2877 - val_acc: 0.9304\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9928\n",
      "Epoch 00058: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0298 - acc: 0.9928 - val_loss: 0.2360 - val_acc: 0.9378\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9979\n",
      "Epoch 00059: val_loss improved from 0.21400 to 0.21339, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/059-0.2134.hdf5\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0133 - acc: 0.9979 - val_loss: 0.2134 - val_acc: 0.9434\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9993\n",
      "Epoch 00060: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0074 - acc: 0.9993 - val_loss: 0.2337 - val_acc: 0.9441\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9964\n",
      "Epoch 00061: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0173 - acc: 0.9964 - val_loss: 0.2237 - val_acc: 0.9411\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9987\n",
      "Epoch 00062: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0088 - acc: 0.9987 - val_loss: 0.2897 - val_acc: 0.9285\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9963\n",
      "Epoch 00063: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0170 - acc: 0.9962 - val_loss: 0.2867 - val_acc: 0.9290\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9980\n",
      "Epoch 00064: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0119 - acc: 0.9980 - val_loss: 0.2679 - val_acc: 0.9373\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9989\n",
      "Epoch 00065: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0075 - acc: 0.9989 - val_loss: 0.2206 - val_acc: 0.9425\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9974\n",
      "Epoch 00066: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0129 - acc: 0.9974 - val_loss: 0.3316 - val_acc: 0.9252\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9984\n",
      "Epoch 00067: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0096 - acc: 0.9985 - val_loss: 0.2341 - val_acc: 0.9418\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9954\n",
      "Epoch 00068: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0178 - acc: 0.9954 - val_loss: 0.2860 - val_acc: 0.9299\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9984\n",
      "Epoch 00069: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0105 - acc: 0.9985 - val_loss: 0.2418 - val_acc: 0.9385\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9990\n",
      "Epoch 00070: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0074 - acc: 0.9990 - val_loss: 0.2695 - val_acc: 0.9343\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9970\n",
      "Epoch 00071: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0145 - acc: 0.9970 - val_loss: 0.2384 - val_acc: 0.9413\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9995\n",
      "Epoch 00072: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0052 - acc: 0.9995 - val_loss: 0.2289 - val_acc: 0.9434\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9977\n",
      "Epoch 00073: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0113 - acc: 0.9977 - val_loss: 0.2802 - val_acc: 0.9317\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9953\n",
      "Epoch 00074: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0199 - acc: 0.9953 - val_loss: 0.2526 - val_acc: 0.9359\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9990\n",
      "Epoch 00075: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0067 - acc: 0.9990 - val_loss: 0.2255 - val_acc: 0.9455\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9990\n",
      "Epoch 00076: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0067 - acc: 0.9990 - val_loss: 0.2883 - val_acc: 0.9357\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9979\n",
      "Epoch 00077: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0103 - acc: 0.9979 - val_loss: 0.2572 - val_acc: 0.9327\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9995\n",
      "Epoch 00078: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0048 - acc: 0.9995 - val_loss: 0.2526 - val_acc: 0.9397\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9969\n",
      "Epoch 00079: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0139 - acc: 0.9969 - val_loss: 0.2298 - val_acc: 0.9404\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9987\n",
      "Epoch 00080: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0078 - acc: 0.9987 - val_loss: 0.2332 - val_acc: 0.9387\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9957\n",
      "Epoch 00081: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0190 - acc: 0.9958 - val_loss: 0.2457 - val_acc: 0.9380\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9997\n",
      "Epoch 00082: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0039 - acc: 0.9997 - val_loss: 0.2309 - val_acc: 0.9443\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9989\n",
      "Epoch 00083: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0059 - acc: 0.9989 - val_loss: 0.3538 - val_acc: 0.9143\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9995\n",
      "Epoch 00084: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0045 - acc: 0.9995 - val_loss: 0.2478 - val_acc: 0.9413\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9958\n",
      "Epoch 00085: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0169 - acc: 0.9958 - val_loss: 0.3002 - val_acc: 0.9313\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9992\n",
      "Epoch 00086: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0055 - acc: 0.9992 - val_loss: 0.2411 - val_acc: 0.9415\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9968\n",
      "Epoch 00087: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0144 - acc: 0.9968 - val_loss: 0.2285 - val_acc: 0.9418\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9988\n",
      "Epoch 00088: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0068 - acc: 0.9988 - val_loss: 0.2391 - val_acc: 0.9427\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9993\n",
      "Epoch 00089: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.2624 - val_acc: 0.9380\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9996\n",
      "Epoch 00090: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0038 - acc: 0.9996 - val_loss: 0.2600 - val_acc: 0.9411\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9994\n",
      "Epoch 00091: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0048 - acc: 0.9993 - val_loss: 0.3319 - val_acc: 0.9245\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9936\n",
      "Epoch 00092: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0237 - acc: 0.9936 - val_loss: 0.2347 - val_acc: 0.9425\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9991\n",
      "Epoch 00093: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0058 - acc: 0.9991 - val_loss: 0.2427 - val_acc: 0.9432\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9996\n",
      "Epoch 00094: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0039 - acc: 0.9996 - val_loss: 0.2428 - val_acc: 0.9425\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 00095: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0027 - acc: 0.9997 - val_loss: 0.2691 - val_acc: 0.9376\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9981\n",
      "Epoch 00096: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0098 - acc: 0.9981 - val_loss: 0.3094 - val_acc: 0.9306\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9985\n",
      "Epoch 00097: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0080 - acc: 0.9985 - val_loss: 0.4262 - val_acc: 0.9150\n",
      "Epoch 98/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9989\n",
      "Epoch 00098: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0063 - acc: 0.9989 - val_loss: 0.2960 - val_acc: 0.9320\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9982\n",
      "Epoch 00099: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0073 - acc: 0.9982 - val_loss: 0.3279 - val_acc: 0.9322\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9985\n",
      "Epoch 00100: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0080 - acc: 0.9985 - val_loss: 0.2944 - val_acc: 0.9378\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9984\n",
      "Epoch 00101: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0080 - acc: 0.9984 - val_loss: 0.2528 - val_acc: 0.9422\n",
      "Epoch 102/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9977\n",
      "Epoch 00102: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0113 - acc: 0.9977 - val_loss: 0.2578 - val_acc: 0.9380\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9949\n",
      "Epoch 00103: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0191 - acc: 0.9949 - val_loss: 0.2496 - val_acc: 0.9432\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9979\n",
      "Epoch 00104: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0105 - acc: 0.9979 - val_loss: 0.2481 - val_acc: 0.9390\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9992\n",
      "Epoch 00105: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0048 - acc: 0.9992 - val_loss: 0.2348 - val_acc: 0.9446\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9975\n",
      "Epoch 00106: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0105 - acc: 0.9975 - val_loss: 0.2422 - val_acc: 0.9474\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 00107: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0034 - acc: 0.9996 - val_loss: 0.2526 - val_acc: 0.9453\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9997\n",
      "Epoch 00108: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0027 - acc: 0.9997 - val_loss: 0.2413 - val_acc: 0.9446\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9966\n",
      "Epoch 00109: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0132 - acc: 0.9966 - val_loss: 0.2364 - val_acc: 0.9436\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9994\n",
      "Epoch 00110: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.2573 - val_acc: 0.9406\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9972\n",
      "Epoch 00111: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0128 - acc: 0.9972 - val_loss: 0.2379 - val_acc: 0.9448\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 00112: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0024 - acc: 0.9998 - val_loss: 0.2271 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9996\n",
      "Epoch 00113: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0029 - acc: 0.9996 - val_loss: 0.2798 - val_acc: 0.9387\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9982\n",
      "Epoch 00114: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0075 - acc: 0.9982 - val_loss: 0.2333 - val_acc: 0.9429\n",
      "Epoch 115/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9993\n",
      "Epoch 00115: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0040 - acc: 0.9993 - val_loss: 0.2275 - val_acc: 0.9441\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9989\n",
      "Epoch 00116: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0055 - acc: 0.9989 - val_loss: 0.2910 - val_acc: 0.9322\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9966\n",
      "Epoch 00117: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0138 - acc: 0.9966 - val_loss: 0.2563 - val_acc: 0.9411\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9997\n",
      "Epoch 00118: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0027 - acc: 0.9996 - val_loss: 0.2868 - val_acc: 0.9399\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9959\n",
      "Epoch 00119: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0154 - acc: 0.9958 - val_loss: 0.2286 - val_acc: 0.9446\n",
      "Epoch 120/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9968\n",
      "Epoch 00120: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0128 - acc: 0.9968 - val_loss: 0.2403 - val_acc: 0.9436\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9996\n",
      "Epoch 00121: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0029 - acc: 0.9996 - val_loss: 0.2618 - val_acc: 0.9436\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9984\n",
      "Epoch 00122: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0083 - acc: 0.9983 - val_loss: 0.2474 - val_acc: 0.9448\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9972\n",
      "Epoch 00123: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0113 - acc: 0.9972 - val_loss: 0.2412 - val_acc: 0.9478\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00124: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0031 - acc: 0.9996 - val_loss: 0.2283 - val_acc: 0.9490\n",
      "Epoch 125/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00125: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.2509 - val_acc: 0.9422\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9986\n",
      "Epoch 00126: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0065 - acc: 0.9986 - val_loss: 0.2427 - val_acc: 0.9434\n",
      "Epoch 127/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 00127: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0028 - acc: 0.9997 - val_loss: 0.2435 - val_acc: 0.9464\n",
      "Epoch 128/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 00128: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0047 - acc: 0.9989 - val_loss: 0.3639 - val_acc: 0.9164\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9971\n",
      "Epoch 00129: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0103 - acc: 0.9971 - val_loss: 0.2438 - val_acc: 0.9457\n",
      "Epoch 130/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9988\n",
      "Epoch 00130: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0062 - acc: 0.9988 - val_loss: 0.2382 - val_acc: 0.9499\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9992\n",
      "Epoch 00131: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0044 - acc: 0.9992 - val_loss: 0.2996 - val_acc: 0.9320\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9994\n",
      "Epoch 00132: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0037 - acc: 0.9994 - val_loss: 0.2525 - val_acc: 0.9448\n",
      "Epoch 133/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 00133: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0025 - acc: 0.9996 - val_loss: 0.2658 - val_acc: 0.9401\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9972\n",
      "Epoch 00134: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0104 - acc: 0.9972 - val_loss: 0.3416 - val_acc: 0.9292\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9969\n",
      "Epoch 00135: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0138 - acc: 0.9969 - val_loss: 0.2381 - val_acc: 0.9443\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9996\n",
      "Epoch 00136: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0027 - acc: 0.9996 - val_loss: 0.2546 - val_acc: 0.9422\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 00137: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0017 - acc: 0.9999 - val_loss: 0.2533 - val_acc: 0.9408\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00138: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.2688 - val_acc: 0.9383\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 00139: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0022 - acc: 0.9997 - val_loss: 0.2932 - val_acc: 0.9380\n",
      "Epoch 140/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9974\n",
      "Epoch 00140: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0105 - acc: 0.9974 - val_loss: 0.2431 - val_acc: 0.9443\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9984\n",
      "Epoch 00141: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0071 - acc: 0.9984 - val_loss: 0.2921 - val_acc: 0.9336\n",
      "Epoch 142/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9997\n",
      "Epoch 00142: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0023 - acc: 0.9997 - val_loss: 0.2428 - val_acc: 0.9460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9993\n",
      "Epoch 00143: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0044 - acc: 0.9992 - val_loss: 0.2442 - val_acc: 0.9422\n",
      "Epoch 144/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9973\n",
      "Epoch 00144: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0103 - acc: 0.9973 - val_loss: 0.2374 - val_acc: 0.9427\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 00145: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0017 - acc: 0.9999 - val_loss: 0.2553 - val_acc: 0.9376\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00146: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.2517 - val_acc: 0.9415\n",
      "Epoch 147/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9964\n",
      "Epoch 00147: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0142 - acc: 0.9964 - val_loss: 0.2424 - val_acc: 0.9443\n",
      "Epoch 148/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9997\n",
      "Epoch 00148: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0025 - acc: 0.9997 - val_loss: 0.2505 - val_acc: 0.9448\n",
      "Epoch 149/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 00149: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0068 - acc: 0.9984 - val_loss: 0.2954 - val_acc: 0.9362\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9983\n",
      "Epoch 00150: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0080 - acc: 0.9983 - val_loss: 0.2442 - val_acc: 0.9455\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00151: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.2492 - val_acc: 0.9434\n",
      "Epoch 152/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9990\n",
      "Epoch 00152: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0053 - acc: 0.9990 - val_loss: 0.2459 - val_acc: 0.9434\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00153: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0038 - acc: 0.9993 - val_loss: 0.2421 - val_acc: 0.9474\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9965\n",
      "Epoch 00154: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0143 - acc: 0.9965 - val_loss: 0.2340 - val_acc: 0.9469\n",
      "Epoch 155/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00155: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.2309 - val_acc: 0.9490\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n",
      "Epoch 00156: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0041 - acc: 0.9990 - val_loss: 0.2417 - val_acc: 0.9485\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 00157: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.2450 - val_acc: 0.9492\n",
      "Epoch 158/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 00158: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0067 - acc: 0.9984 - val_loss: 0.2788 - val_acc: 0.9390\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00159: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.2686 - val_acc: 0.9455\n",
      "\n",
      "1D_CNN_BN_5_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 298us/sample - loss: 0.2892 - acc: 0.9288\n",
      "Loss: 0.2892399005359764 Accuracy: 0.9287643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_BN_{}_only_conv'.format(i)\n",
    "    model = build_1d_cnn_BN_only_conv(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_BN_1_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 215us/sample - loss: 1.8429 - acc: 0.4289\n",
      "Loss: 1.8428832955325751 Accuracy: 0.42886811\n",
      "\n",
      "1D_CNN_BN_2_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 274us/sample - loss: 1.3207 - acc: 0.5836\n",
      "Loss: 1.3206864595289418 Accuracy: 0.58359295\n",
      "\n",
      "1D_CNN_BN_3_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 266us/sample - loss: 0.7525 - acc: 0.7890\n",
      "Loss: 0.7524969367594734 Accuracy: 0.7889927\n",
      "\n",
      "1D_CNN_BN_4_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 317us/sample - loss: 0.4007 - acc: 0.8804\n",
      "Loss: 0.4007288916583868 Accuracy: 0.88037384\n",
      "\n",
      "1D_CNN_BN_5_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 328us/sample - loss: 0.2892 - acc: 0.9288\n",
      "Loss: 0.2892399005359764 Accuracy: 0.9287643\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_BN_{}_only_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_BN_DO_only_conv(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=25, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=25, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_31 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_40 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model = build_1d_cnn_BN_only_conv(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 2.7927 - acc: 0.2032\n",
      "Epoch 00001: val_loss improved from inf to 2.08284, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/001-2.0828.hdf5\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 2.7896 - acc: 0.2038 - val_loss: 2.0828 - val_acc: 0.3329\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.0085 - acc: 0.3725\n",
      "Epoch 00002: val_loss improved from 2.08284 to 1.78809, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/002-1.7881.hdf5\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 2.0094 - acc: 0.3722 - val_loss: 1.7881 - val_acc: 0.4396\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7271 - acc: 0.4502\n",
      "Epoch 00003: val_loss improved from 1.78809 to 1.69556, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/003-1.6956.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.7270 - acc: 0.4502 - val_loss: 1.6956 - val_acc: 0.4631\n",
      "Epoch 4/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.5461 - acc: 0.5022\n",
      "Epoch 00004: val_loss improved from 1.69556 to 1.68302, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/004-1.6830.hdf5\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.5468 - acc: 0.5023 - val_loss: 1.6830 - val_acc: 0.4740\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4236 - acc: 0.5401\n",
      "Epoch 00005: val_loss improved from 1.68302 to 1.63619, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/005-1.6362.hdf5\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 1.4235 - acc: 0.5401 - val_loss: 1.6362 - val_acc: 0.4955\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3344 - acc: 0.5699\n",
      "Epoch 00006: val_loss improved from 1.63619 to 1.60176, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/006-1.6018.hdf5\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 1.3343 - acc: 0.5699 - val_loss: 1.6018 - val_acc: 0.5092\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2511 - acc: 0.5961\n",
      "Epoch 00007: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.2512 - acc: 0.5961 - val_loss: 1.6087 - val_acc: 0.4992\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1844 - acc: 0.6170\n",
      "Epoch 00008: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 1.1845 - acc: 0.6170 - val_loss: 1.6135 - val_acc: 0.5043\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1377 - acc: 0.6310\n",
      "Epoch 00009: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 1.1380 - acc: 0.6311 - val_loss: 1.6092 - val_acc: 0.5050\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0887 - acc: 0.6461\n",
      "Epoch 00010: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 1.0888 - acc: 0.6462 - val_loss: 1.6637 - val_acc: 0.5003\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0481 - acc: 0.6579\n",
      "Epoch 00011: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 1.0487 - acc: 0.6577 - val_loss: 1.6473 - val_acc: 0.5059\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0165 - acc: 0.6660\n",
      "Epoch 00012: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 1.0169 - acc: 0.6660 - val_loss: 1.6610 - val_acc: 0.5024\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9799 - acc: 0.6762\n",
      "Epoch 00013: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.9802 - acc: 0.6762 - val_loss: 1.6839 - val_acc: 0.4997\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9575 - acc: 0.6905\n",
      "Epoch 00014: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.9574 - acc: 0.6903 - val_loss: 1.6746 - val_acc: 0.5104\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9357 - acc: 0.6939\n",
      "Epoch 00015: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.9356 - acc: 0.6939 - val_loss: 1.6802 - val_acc: 0.5125\n",
      "Epoch 16/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.9084 - acc: 0.7042\n",
      "Epoch 00016: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.9088 - acc: 0.7038 - val_loss: 1.6723 - val_acc: 0.5164\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8788 - acc: 0.7121\n",
      "Epoch 00017: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.8798 - acc: 0.7118 - val_loss: 1.7551 - val_acc: 0.4936\n",
      "Epoch 18/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8678 - acc: 0.7136\n",
      "Epoch 00018: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8682 - acc: 0.7134 - val_loss: 1.7312 - val_acc: 0.5071\n",
      "Epoch 19/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8544 - acc: 0.7203\n",
      "Epoch 00019: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8538 - acc: 0.7204 - val_loss: 1.7296 - val_acc: 0.5111\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8318 - acc: 0.7251\n",
      "Epoch 00020: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.8319 - acc: 0.7253 - val_loss: 1.7276 - val_acc: 0.5094\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8220 - acc: 0.7274\n",
      "Epoch 00021: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.8217 - acc: 0.7274 - val_loss: 1.7378 - val_acc: 0.5125\n",
      "Epoch 22/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8104 - acc: 0.7336\n",
      "Epoch 00022: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.8103 - acc: 0.7335 - val_loss: 1.8083 - val_acc: 0.4950\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8005 - acc: 0.7348\n",
      "Epoch 00023: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.8011 - acc: 0.7346 - val_loss: 1.7948 - val_acc: 0.5108\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7836 - acc: 0.7411\n",
      "Epoch 00024: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.7839 - acc: 0.7413 - val_loss: 1.7679 - val_acc: 0.5183\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7805 - acc: 0.7411\n",
      "Epoch 00025: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.7806 - acc: 0.7410 - val_loss: 1.7936 - val_acc: 0.5085\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7713 - acc: 0.7454\n",
      "Epoch 00026: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7713 - acc: 0.7454 - val_loss: 1.7770 - val_acc: 0.5146\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7605 - acc: 0.7478\n",
      "Epoch 00027: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.7610 - acc: 0.7473 - val_loss: 1.7921 - val_acc: 0.5064\n",
      "Epoch 28/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7434 - acc: 0.7504\n",
      "Epoch 00028: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.7431 - acc: 0.7506 - val_loss: 1.8066 - val_acc: 0.5078\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7382 - acc: 0.7547\n",
      "Epoch 00029: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.7381 - acc: 0.7547 - val_loss: 1.8372 - val_acc: 0.5160\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7280 - acc: 0.7594\n",
      "Epoch 00030: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.7279 - acc: 0.7594 - val_loss: 1.8473 - val_acc: 0.5092\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7129 - acc: 0.7610\n",
      "Epoch 00031: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.7129 - acc: 0.7610 - val_loss: 1.8646 - val_acc: 0.5101\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7131 - acc: 0.7627\n",
      "Epoch 00032: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.7130 - acc: 0.7627 - val_loss: 1.8484 - val_acc: 0.5062\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7137 - acc: 0.7616\n",
      "Epoch 00033: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.7139 - acc: 0.7615 - val_loss: 1.8677 - val_acc: 0.5178\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6968 - acc: 0.7659\n",
      "Epoch 00034: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6968 - acc: 0.7659 - val_loss: 1.8430 - val_acc: 0.5206\n",
      "Epoch 35/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.7687\n",
      "Epoch 00035: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6925 - acc: 0.7689 - val_loss: 1.8607 - val_acc: 0.5190\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6905 - acc: 0.7712\n",
      "Epoch 00036: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6900 - acc: 0.7714 - val_loss: 1.8819 - val_acc: 0.5069\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.7728\n",
      "Epoch 00037: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.6803 - acc: 0.7728 - val_loss: 1.8786 - val_acc: 0.5113\n",
      "Epoch 38/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.7721\n",
      "Epoch 00038: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.6824 - acc: 0.7722 - val_loss: 1.8767 - val_acc: 0.5262\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6726 - acc: 0.7743\n",
      "Epoch 00039: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6726 - acc: 0.7744 - val_loss: 1.8997 - val_acc: 0.5071\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6681 - acc: 0.7780\n",
      "Epoch 00040: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6681 - acc: 0.7779 - val_loss: 1.9124 - val_acc: 0.5178\n",
      "Epoch 41/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6720 - acc: 0.7769\n",
      "Epoch 00041: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6728 - acc: 0.7768 - val_loss: 1.8655 - val_acc: 0.5276\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6546 - acc: 0.7804\n",
      "Epoch 00042: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.6548 - acc: 0.7804 - val_loss: 1.8886 - val_acc: 0.5201\n",
      "Epoch 43/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6555 - acc: 0.7795\n",
      "Epoch 00043: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.6562 - acc: 0.7794 - val_loss: 1.9243 - val_acc: 0.5134\n",
      "Epoch 44/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.7841\n",
      "Epoch 00044: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.6469 - acc: 0.7841 - val_loss: 1.9406 - val_acc: 0.5122\n",
      "Epoch 45/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.6475 - acc: 0.7825\n",
      "Epoch 00045: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6474 - acc: 0.7827 - val_loss: 1.9426 - val_acc: 0.5178\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6478 - acc: 0.7817\n",
      "Epoch 00046: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6473 - acc: 0.7819 - val_loss: 1.9557 - val_acc: 0.5167\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.7842\n",
      "Epoch 00047: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6430 - acc: 0.7841 - val_loss: 1.9344 - val_acc: 0.5208\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6366 - acc: 0.7867\n",
      "Epoch 00048: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6363 - acc: 0.7868 - val_loss: 1.9484 - val_acc: 0.5141\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6230 - acc: 0.7908\n",
      "Epoch 00049: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6233 - acc: 0.7908 - val_loss: 1.9352 - val_acc: 0.5255\n",
      "Epoch 50/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6233 - acc: 0.7902\n",
      "Epoch 00050: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6231 - acc: 0.7903 - val_loss: 1.9765 - val_acc: 0.5222\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.7918\n",
      "Epoch 00051: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.6222 - acc: 0.7919 - val_loss: 1.9919 - val_acc: 0.5222\n",
      "Epoch 52/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6158 - acc: 0.7939\n",
      "Epoch 00052: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.6158 - acc: 0.7938 - val_loss: 1.9463 - val_acc: 0.5220\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6187 - acc: 0.7917\n",
      "Epoch 00053: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6194 - acc: 0.7916 - val_loss: 1.9587 - val_acc: 0.5269\n",
      "Epoch 54/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6210 - acc: 0.7913\n",
      "Epoch 00054: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6219 - acc: 0.7910 - val_loss: 1.9969 - val_acc: 0.5213\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6074 - acc: 0.7957\n",
      "Epoch 00055: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6074 - acc: 0.7957 - val_loss: 1.9853 - val_acc: 0.5304\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6140 - acc: 0.7951\n",
      "Epoch 00056: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.6136 - acc: 0.7953 - val_loss: 1.9805 - val_acc: 0.5211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6099 - acc: 0.7932\n",
      "Epoch 00057: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.6092 - acc: 0.7934 - val_loss: 1.9583 - val_acc: 0.5225\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6055 - acc: 0.7985\n",
      "Epoch 00058: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.6054 - acc: 0.7986 - val_loss: 2.0763 - val_acc: 0.5029\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6034 - acc: 0.7976\n",
      "Epoch 00059: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.6033 - acc: 0.7976 - val_loss: 1.9606 - val_acc: 0.5232\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.7974\n",
      "Epoch 00060: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5958 - acc: 0.7975 - val_loss: 2.0049 - val_acc: 0.5218\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5912 - acc: 0.8018\n",
      "Epoch 00061: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5920 - acc: 0.8017 - val_loss: 1.9928 - val_acc: 0.5250\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5940 - acc: 0.8014\n",
      "Epoch 00062: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5938 - acc: 0.8015 - val_loss: 2.0197 - val_acc: 0.5229\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.7976\n",
      "Epoch 00063: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5964 - acc: 0.7974 - val_loss: 2.0389 - val_acc: 0.5155\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5983 - acc: 0.7987\n",
      "Epoch 00064: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5981 - acc: 0.7986 - val_loss: 2.0142 - val_acc: 0.5234\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5893 - acc: 0.8013\n",
      "Epoch 00065: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5893 - acc: 0.8013 - val_loss: 1.9952 - val_acc: 0.5316\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5856 - acc: 0.8029\n",
      "Epoch 00066: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5856 - acc: 0.8028 - val_loss: 1.9908 - val_acc: 0.5292\n",
      "Epoch 67/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5831 - acc: 0.8048\n",
      "Epoch 00067: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5833 - acc: 0.8047 - val_loss: 2.0416 - val_acc: 0.5195\n",
      "Epoch 68/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5746 - acc: 0.8074\n",
      "Epoch 00068: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5749 - acc: 0.8074 - val_loss: 2.0510 - val_acc: 0.5185\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5808 - acc: 0.8056\n",
      "Epoch 00069: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5808 - acc: 0.8056 - val_loss: 2.0263 - val_acc: 0.5215\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.8060\n",
      "Epoch 00070: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5739 - acc: 0.8060 - val_loss: 2.0717 - val_acc: 0.5246\n",
      "Epoch 71/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5779 - acc: 0.8078\n",
      "Epoch 00071: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5782 - acc: 0.8076 - val_loss: 2.0679 - val_acc: 0.5197\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5720 - acc: 0.8090\n",
      "Epoch 00072: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5719 - acc: 0.8090 - val_loss: 2.0530 - val_acc: 0.5243\n",
      "Epoch 73/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5729 - acc: 0.8082\n",
      "Epoch 00073: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5725 - acc: 0.8083 - val_loss: 2.0309 - val_acc: 0.5281\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5673 - acc: 0.8120\n",
      "Epoch 00074: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5672 - acc: 0.8120 - val_loss: 2.0363 - val_acc: 0.5262\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.8084\n",
      "Epoch 00075: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5687 - acc: 0.8084 - val_loss: 2.0191 - val_acc: 0.5283\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5580 - acc: 0.8148\n",
      "Epoch 00076: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5582 - acc: 0.8147 - val_loss: 2.0458 - val_acc: 0.5299\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.8100\n",
      "Epoch 00077: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5677 - acc: 0.8097 - val_loss: 2.0284 - val_acc: 0.5229\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.8132\n",
      "Epoch 00078: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5603 - acc: 0.8132 - val_loss: 2.0280 - val_acc: 0.5292\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5660 - acc: 0.8106\n",
      "Epoch 00079: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5661 - acc: 0.8104 - val_loss: 2.1736 - val_acc: 0.5066\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5588 - acc: 0.8114\n",
      "Epoch 00080: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5591 - acc: 0.8114 - val_loss: 2.0574 - val_acc: 0.5283\n",
      "Epoch 81/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.8119\n",
      "Epoch 00081: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5592 - acc: 0.8118 - val_loss: 2.0376 - val_acc: 0.5269\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5615 - acc: 0.8112\n",
      "Epoch 00082: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5615 - acc: 0.8112 - val_loss: 2.0923 - val_acc: 0.5260\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5563 - acc: 0.8138\n",
      "Epoch 00083: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5563 - acc: 0.8138 - val_loss: 2.0767 - val_acc: 0.5320\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.8115\n",
      "Epoch 00084: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5585 - acc: 0.8114 - val_loss: 2.0869 - val_acc: 0.5320\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.8136\n",
      "Epoch 00085: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5550 - acc: 0.8137 - val_loss: 2.0740 - val_acc: 0.5330\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.8122\n",
      "Epoch 00086: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5558 - acc: 0.8123 - val_loss: 2.0451 - val_acc: 0.5337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.8154\n",
      "Epoch 00087: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5490 - acc: 0.8155 - val_loss: 2.1286 - val_acc: 0.5181\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.8160\n",
      "Epoch 00088: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5508 - acc: 0.8158 - val_loss: 2.1133 - val_acc: 0.5199\n",
      "Epoch 89/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.8187\n",
      "Epoch 00089: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5409 - acc: 0.8187 - val_loss: 2.0984 - val_acc: 0.5218\n",
      "Epoch 90/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5443 - acc: 0.8173\n",
      "Epoch 00090: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5449 - acc: 0.8171 - val_loss: 2.0874 - val_acc: 0.5267\n",
      "Epoch 91/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.8188\n",
      "Epoch 00091: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5366 - acc: 0.8185 - val_loss: 2.1254 - val_acc: 0.5313\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.8213\n",
      "Epoch 00092: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5402 - acc: 0.8210 - val_loss: 2.0521 - val_acc: 0.5271\n",
      "Epoch 93/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8204\n",
      "Epoch 00093: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5381 - acc: 0.8201 - val_loss: 2.1195 - val_acc: 0.5330\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.8141\n",
      "Epoch 00094: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5501 - acc: 0.8140 - val_loss: 2.1450 - val_acc: 0.5215\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.8174\n",
      "Epoch 00095: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5436 - acc: 0.8174 - val_loss: 2.1019 - val_acc: 0.5292\n",
      "Epoch 96/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.8173\n",
      "Epoch 00096: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5401 - acc: 0.8176 - val_loss: 2.1043 - val_acc: 0.5299\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.8197\n",
      "Epoch 00097: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.5403 - acc: 0.8197 - val_loss: 2.0673 - val_acc: 0.5339\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.8216\n",
      "Epoch 00098: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5268 - acc: 0.8216 - val_loss: 2.0922 - val_acc: 0.5360\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.8190\n",
      "Epoch 00099: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5372 - acc: 0.8190 - val_loss: 2.1095 - val_acc: 0.5316\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.8223\n",
      "Epoch 00100: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.5272 - acc: 0.8223 - val_loss: 2.1570 - val_acc: 0.5262\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.8219\n",
      "Epoch 00101: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5294 - acc: 0.8218 - val_loss: 2.1011 - val_acc: 0.5353\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.8206\n",
      "Epoch 00102: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.5334 - acc: 0.8206 - val_loss: 2.1530 - val_acc: 0.5188\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.8248\n",
      "Epoch 00103: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5239 - acc: 0.8248 - val_loss: 2.2135 - val_acc: 0.5104\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.8196\n",
      "Epoch 00104: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5340 - acc: 0.8196 - val_loss: 2.1224 - val_acc: 0.5269\n",
      "Epoch 105/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.8223\n",
      "Epoch 00105: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.5311 - acc: 0.8226 - val_loss: 2.1140 - val_acc: 0.5302\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8237\n",
      "Epoch 00106: val_loss did not improve from 1.60176\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.5322 - acc: 0.8237 - val_loss: 2.1210 - val_acc: 0.5267\n",
      "\n",
      "1D_CNN_BN_DO_1_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 284us/sample - loss: 1.8043 - acc: 0.4490\n",
      "Loss: 1.8042701047281362 Accuracy: 0.4490135\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.7814 - acc: 0.2001\n",
      "Epoch 00001: val_loss improved from inf to 1.90158, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/001-1.9016.hdf5\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 2.7803 - acc: 0.2003 - val_loss: 1.9016 - val_acc: 0.3862\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9343 - acc: 0.3847\n",
      "Epoch 00002: val_loss improved from 1.90158 to 1.52449, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/002-1.5245.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 1.9342 - acc: 0.3847 - val_loss: 1.5245 - val_acc: 0.5141\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6377 - acc: 0.4786\n",
      "Epoch 00003: val_loss improved from 1.52449 to 1.37190, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/003-1.3719.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 1.6379 - acc: 0.4785 - val_loss: 1.3719 - val_acc: 0.5623\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4440 - acc: 0.5425\n",
      "Epoch 00004: val_loss improved from 1.37190 to 1.27708, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/004-1.2771.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 1.4441 - acc: 0.5426 - val_loss: 1.2771 - val_acc: 0.6147\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3319 - acc: 0.5753\n",
      "Epoch 00005: val_loss improved from 1.27708 to 1.18747, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/005-1.1875.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 1.3319 - acc: 0.5753 - val_loss: 1.1875 - val_acc: 0.6359\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2503 - acc: 0.6082\n",
      "Epoch 00006: val_loss improved from 1.18747 to 1.14039, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/006-1.1404.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 1.2505 - acc: 0.6082 - val_loss: 1.1404 - val_acc: 0.6550\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1765 - acc: 0.6299\n",
      "Epoch 00007: val_loss did not improve from 1.14039\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 1.1768 - acc: 0.6299 - val_loss: 1.1506 - val_acc: 0.6567\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1330 - acc: 0.6448\n",
      "Epoch 00008: val_loss improved from 1.14039 to 1.13208, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/008-1.1321.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 1.1328 - acc: 0.6449 - val_loss: 1.1321 - val_acc: 0.6674\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0855 - acc: 0.6627\n",
      "Epoch 00009: val_loss improved from 1.13208 to 1.08831, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/009-1.0883.hdf5\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 1.0856 - acc: 0.6627 - val_loss: 1.0883 - val_acc: 0.6709\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0509 - acc: 0.6711\n",
      "Epoch 00010: val_loss did not improve from 1.08831\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 1.0514 - acc: 0.6709 - val_loss: 1.0885 - val_acc: 0.6674\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0191 - acc: 0.6810\n",
      "Epoch 00011: val_loss improved from 1.08831 to 1.05621, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/011-1.0562.hdf5\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 1.0195 - acc: 0.6810 - val_loss: 1.0562 - val_acc: 0.6893\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9896 - acc: 0.6927\n",
      "Epoch 00012: val_loss improved from 1.05621 to 1.03487, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/012-1.0349.hdf5\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.9895 - acc: 0.6927 - val_loss: 1.0349 - val_acc: 0.6981\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9557 - acc: 0.7021\n",
      "Epoch 00013: val_loss improved from 1.03487 to 1.03072, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/013-1.0307.hdf5\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.9563 - acc: 0.7019 - val_loss: 1.0307 - val_acc: 0.6883\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9339 - acc: 0.7098\n",
      "Epoch 00014: val_loss did not improve from 1.03072\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.9339 - acc: 0.7098 - val_loss: 1.0889 - val_acc: 0.6709\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9109 - acc: 0.7167\n",
      "Epoch 00015: val_loss improved from 1.03072 to 1.03067, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/015-1.0307.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.9122 - acc: 0.7165 - val_loss: 1.0307 - val_acc: 0.7018\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8960 - acc: 0.7196\n",
      "Epoch 00016: val_loss did not improve from 1.03067\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.8959 - acc: 0.7197 - val_loss: 1.0799 - val_acc: 0.6662\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8754 - acc: 0.7282\n",
      "Epoch 00017: val_loss did not improve from 1.03067\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.8757 - acc: 0.7280 - val_loss: 1.0374 - val_acc: 0.6879\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8526 - acc: 0.7371\n",
      "Epoch 00018: val_loss improved from 1.03067 to 0.98998, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/018-0.9900.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.8526 - acc: 0.7371 - val_loss: 0.9900 - val_acc: 0.7060\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8376 - acc: 0.7390\n",
      "Epoch 00019: val_loss did not improve from 0.98998\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.8377 - acc: 0.7391 - val_loss: 0.9965 - val_acc: 0.7002\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8223 - acc: 0.7421\n",
      "Epoch 00020: val_loss improved from 0.98998 to 0.97001, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/020-0.9700.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.8221 - acc: 0.7422 - val_loss: 0.9700 - val_acc: 0.7163\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8049 - acc: 0.7485\n",
      "Epoch 00021: val_loss did not improve from 0.97001\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.8048 - acc: 0.7485 - val_loss: 0.9761 - val_acc: 0.7179\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7937 - acc: 0.7481\n",
      "Epoch 00022: val_loss improved from 0.97001 to 0.96161, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/022-0.9616.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.7937 - acc: 0.7482 - val_loss: 0.9616 - val_acc: 0.7191\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7859 - acc: 0.7530\n",
      "Epoch 00023: val_loss did not improve from 0.96161\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.7859 - acc: 0.7530 - val_loss: 0.9776 - val_acc: 0.7142\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7653 - acc: 0.7591\n",
      "Epoch 00024: val_loss did not improve from 0.96161\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.7653 - acc: 0.7592 - val_loss: 0.9657 - val_acc: 0.7193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7550 - acc: 0.7614\n",
      "Epoch 00025: val_loss did not improve from 0.96161\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.7551 - acc: 0.7614 - val_loss: 0.9723 - val_acc: 0.7109\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7477 - acc: 0.7647\n",
      "Epoch 00026: val_loss did not improve from 0.96161\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.7472 - acc: 0.7648 - val_loss: 1.0369 - val_acc: 0.6949\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7318 - acc: 0.7669\n",
      "Epoch 00027: val_loss improved from 0.96161 to 0.95603, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/027-0.9560.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.7320 - acc: 0.7667 - val_loss: 0.9560 - val_acc: 0.7235\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7220 - acc: 0.7688\n",
      "Epoch 00028: val_loss did not improve from 0.95603\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.7219 - acc: 0.7688 - val_loss: 0.9649 - val_acc: 0.7237\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7096 - acc: 0.7749\n",
      "Epoch 00029: val_loss did not improve from 0.95603\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.7095 - acc: 0.7748 - val_loss: 0.9645 - val_acc: 0.7261\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7081 - acc: 0.7715\n",
      "Epoch 00030: val_loss improved from 0.95603 to 0.94118, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/030-0.9412.hdf5\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.7081 - acc: 0.7715 - val_loss: 0.9412 - val_acc: 0.7338\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6848 - acc: 0.7821\n",
      "Epoch 00031: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.6849 - acc: 0.7821 - val_loss: 0.9820 - val_acc: 0.7160\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6844 - acc: 0.7825\n",
      "Epoch 00032: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.6844 - acc: 0.7825 - val_loss: 0.9761 - val_acc: 0.7149\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6743 - acc: 0.7844\n",
      "Epoch 00033: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.6743 - acc: 0.7844 - val_loss: 0.9633 - val_acc: 0.7149\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6678 - acc: 0.7856\n",
      "Epoch 00034: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.6678 - acc: 0.7857 - val_loss: 0.9432 - val_acc: 0.7282\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6637 - acc: 0.7888\n",
      "Epoch 00035: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.6636 - acc: 0.7889 - val_loss: 0.9551 - val_acc: 0.7265\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6479 - acc: 0.7899\n",
      "Epoch 00036: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.6477 - acc: 0.7899 - val_loss: 0.9610 - val_acc: 0.7263\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.7944\n",
      "Epoch 00037: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.6422 - acc: 0.7945 - val_loss: 0.9722 - val_acc: 0.7181\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6356 - acc: 0.7948\n",
      "Epoch 00038: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.6357 - acc: 0.7947 - val_loss: 0.9635 - val_acc: 0.7179\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6255 - acc: 0.7977\n",
      "Epoch 00039: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.6256 - acc: 0.7977 - val_loss: 0.9553 - val_acc: 0.7282\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6230 - acc: 0.8000\n",
      "Epoch 00040: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.6230 - acc: 0.8000 - val_loss: 0.9831 - val_acc: 0.7167\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6084 - acc: 0.8037\n",
      "Epoch 00041: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.6092 - acc: 0.8036 - val_loss: 1.0015 - val_acc: 0.7119\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6040 - acc: 0.8079\n",
      "Epoch 00042: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.6040 - acc: 0.8079 - val_loss: 0.9610 - val_acc: 0.7282\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6062 - acc: 0.8025\n",
      "Epoch 00043: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.6064 - acc: 0.8025 - val_loss: 1.0327 - val_acc: 0.6993\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5946 - acc: 0.8081\n",
      "Epoch 00044: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.5944 - acc: 0.8082 - val_loss: 0.9444 - val_acc: 0.7349\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8101\n",
      "Epoch 00045: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.5851 - acc: 0.8098 - val_loss: 0.9614 - val_acc: 0.7282\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5806 - acc: 0.8126\n",
      "Epoch 00046: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.5810 - acc: 0.8124 - val_loss: 0.9645 - val_acc: 0.7270\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5754 - acc: 0.8153\n",
      "Epoch 00047: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5753 - acc: 0.8153 - val_loss: 0.9577 - val_acc: 0.7279\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8145\n",
      "Epoch 00048: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.5736 - acc: 0.8143 - val_loss: 0.9655 - val_acc: 0.7254\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5697 - acc: 0.8139\n",
      "Epoch 00049: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5700 - acc: 0.8138 - val_loss: 0.9543 - val_acc: 0.7244\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5638 - acc: 0.8167\n",
      "Epoch 00050: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.5639 - acc: 0.8166 - val_loss: 0.9524 - val_acc: 0.7314\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5522 - acc: 0.8188\n",
      "Epoch 00051: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5527 - acc: 0.8186 - val_loss: 1.0078 - val_acc: 0.7126\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.8214\n",
      "Epoch 00052: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.5472 - acc: 0.8214 - val_loss: 0.9736 - val_acc: 0.7226\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.8209\n",
      "Epoch 00053: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5478 - acc: 0.8207 - val_loss: 1.0764 - val_acc: 0.6890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.8249\n",
      "Epoch 00054: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.5381 - acc: 0.8248 - val_loss: 0.9554 - val_acc: 0.7317\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.8264\n",
      "Epoch 00055: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5286 - acc: 0.8263 - val_loss: 1.0714 - val_acc: 0.6923\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.8293\n",
      "Epoch 00056: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.5282 - acc: 0.8292 - val_loss: 0.9776 - val_acc: 0.7314\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.8268\n",
      "Epoch 00057: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5307 - acc: 0.8266 - val_loss: 0.9881 - val_acc: 0.7198\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.8302\n",
      "Epoch 00058: val_loss did not improve from 0.94118\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5195 - acc: 0.8302 - val_loss: 0.9526 - val_acc: 0.7349\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8306\n",
      "Epoch 00059: val_loss improved from 0.94118 to 0.93558, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/059-0.9356.hdf5\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.5188 - acc: 0.8305 - val_loss: 0.9356 - val_acc: 0.7354\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.8310\n",
      "Epoch 00060: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5150 - acc: 0.8311 - val_loss: 0.9570 - val_acc: 0.7312\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.8353\n",
      "Epoch 00061: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.5036 - acc: 0.8353 - val_loss: 0.9825 - val_acc: 0.7181\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8356\n",
      "Epoch 00062: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.5019 - acc: 0.8356 - val_loss: 0.9583 - val_acc: 0.7300\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8365\n",
      "Epoch 00063: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.4986 - acc: 0.8364 - val_loss: 0.9654 - val_acc: 0.7284\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4893 - acc: 0.8412\n",
      "Epoch 00064: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4891 - acc: 0.8412 - val_loss: 0.9747 - val_acc: 0.7277\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4886 - acc: 0.8396\n",
      "Epoch 00065: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4887 - acc: 0.8395 - val_loss: 1.0115 - val_acc: 0.7172\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8407\n",
      "Epoch 00066: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.4871 - acc: 0.8406 - val_loss: 1.0500 - val_acc: 0.7081\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8397\n",
      "Epoch 00067: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4884 - acc: 0.8397 - val_loss: 0.9529 - val_acc: 0.7312\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4803 - acc: 0.8421\n",
      "Epoch 00068: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4802 - acc: 0.8421 - val_loss: 0.9452 - val_acc: 0.7358\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4718 - acc: 0.8440\n",
      "Epoch 00069: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4717 - acc: 0.8441 - val_loss: 0.9930 - val_acc: 0.7198\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.8439\n",
      "Epoch 00070: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4767 - acc: 0.8440 - val_loss: 0.9858 - val_acc: 0.7247\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.8454\n",
      "Epoch 00071: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4647 - acc: 0.8454 - val_loss: 0.9816 - val_acc: 0.7279\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4669 - acc: 0.8473\n",
      "Epoch 00072: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4669 - acc: 0.8473 - val_loss: 1.0065 - val_acc: 0.7233\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.8482\n",
      "Epoch 00073: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.4596 - acc: 0.8482 - val_loss: 0.9452 - val_acc: 0.7407\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.8522\n",
      "Epoch 00074: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4550 - acc: 0.8520 - val_loss: 1.0261 - val_acc: 0.7181\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8490\n",
      "Epoch 00075: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4585 - acc: 0.8489 - val_loss: 0.9773 - val_acc: 0.7244\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8507\n",
      "Epoch 00076: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4544 - acc: 0.8507 - val_loss: 1.0295 - val_acc: 0.7130\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4476 - acc: 0.8525\n",
      "Epoch 00077: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.4477 - acc: 0.8524 - val_loss: 0.9946 - val_acc: 0.7223\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.8530\n",
      "Epoch 00078: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4461 - acc: 0.8531 - val_loss: 0.9511 - val_acc: 0.7393\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.8562\n",
      "Epoch 00079: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4404 - acc: 0.8562 - val_loss: 0.9849 - val_acc: 0.7328\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4448 - acc: 0.8534\n",
      "Epoch 00080: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4451 - acc: 0.8533 - val_loss: 0.9625 - val_acc: 0.7328\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4387 - acc: 0.8568\n",
      "Epoch 00081: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.4387 - acc: 0.8568 - val_loss: 0.9644 - val_acc: 0.7382\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.8605\n",
      "Epoch 00082: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4279 - acc: 0.8604 - val_loss: 1.0134 - val_acc: 0.7158\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8581\n",
      "Epoch 00083: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4286 - acc: 0.8580 - val_loss: 0.9536 - val_acc: 0.7372\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.8577\n",
      "Epoch 00084: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4315 - acc: 0.8575 - val_loss: 0.9442 - val_acc: 0.7410\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4303 - acc: 0.8581\n",
      "Epoch 00085: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4303 - acc: 0.8580 - val_loss: 1.0011 - val_acc: 0.7326\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4223 - acc: 0.8611\n",
      "Epoch 00086: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4223 - acc: 0.8612 - val_loss: 0.9800 - val_acc: 0.7300\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8625\n",
      "Epoch 00087: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4143 - acc: 0.8626 - val_loss: 1.0105 - val_acc: 0.7237\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8618\n",
      "Epoch 00088: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4199 - acc: 0.8617 - val_loss: 0.9907 - val_acc: 0.7221\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8658\n",
      "Epoch 00089: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.4123 - acc: 0.8658 - val_loss: 1.0166 - val_acc: 0.7209\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8657\n",
      "Epoch 00090: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4120 - acc: 0.8657 - val_loss: 1.1225 - val_acc: 0.6895\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8657\n",
      "Epoch 00091: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4094 - acc: 0.8657 - val_loss: 1.0057 - val_acc: 0.7247\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8653\n",
      "Epoch 00092: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.4049 - acc: 0.8654 - val_loss: 1.0387 - val_acc: 0.7149\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8692\n",
      "Epoch 00093: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4032 - acc: 0.8691 - val_loss: 1.0506 - val_acc: 0.7174\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8698\n",
      "Epoch 00094: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.4002 - acc: 0.8698 - val_loss: 0.9941 - val_acc: 0.7307\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8684\n",
      "Epoch 00095: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.4008 - acc: 0.8684 - val_loss: 1.0841 - val_acc: 0.6986\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3944 - acc: 0.8706\n",
      "Epoch 00096: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3945 - acc: 0.8706 - val_loss: 0.9818 - val_acc: 0.7324\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8695\n",
      "Epoch 00097: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3981 - acc: 0.8696 - val_loss: 0.9601 - val_acc: 0.7389\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8696\n",
      "Epoch 00098: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3933 - acc: 0.8696 - val_loss: 0.9498 - val_acc: 0.7384\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8733\n",
      "Epoch 00099: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3892 - acc: 0.8732 - val_loss: 0.9926 - val_acc: 0.7282\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8724\n",
      "Epoch 00100: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3878 - acc: 0.8724 - val_loss: 0.9841 - val_acc: 0.7317\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8726\n",
      "Epoch 00101: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3808 - acc: 0.8726 - val_loss: 1.0271 - val_acc: 0.7247\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3835 - acc: 0.8734\n",
      "Epoch 00102: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3835 - acc: 0.8732 - val_loss: 1.0209 - val_acc: 0.7247\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.8765\n",
      "Epoch 00103: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3779 - acc: 0.8764 - val_loss: 0.9997 - val_acc: 0.7324\n",
      "Epoch 104/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8770\n",
      "Epoch 00104: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3767 - acc: 0.8768 - val_loss: 1.0508 - val_acc: 0.7202\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8762\n",
      "Epoch 00105: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3743 - acc: 0.8762 - val_loss: 1.0053 - val_acc: 0.7333\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3698 - acc: 0.8792\n",
      "Epoch 00106: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3696 - acc: 0.8793 - val_loss: 0.9776 - val_acc: 0.7345\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8777\n",
      "Epoch 00107: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3738 - acc: 0.8776 - val_loss: 1.0110 - val_acc: 0.7338\n",
      "Epoch 108/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8784\n",
      "Epoch 00108: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3699 - acc: 0.8782 - val_loss: 1.0159 - val_acc: 0.7275\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8786\n",
      "Epoch 00109: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3643 - acc: 0.8787 - val_loss: 0.9955 - val_acc: 0.7340\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8784\n",
      "Epoch 00110: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3668 - acc: 0.8785 - val_loss: 0.9869 - val_acc: 0.7365\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8825\n",
      "Epoch 00111: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3578 - acc: 0.8824 - val_loss: 0.9714 - val_acc: 0.7398\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8827\n",
      "Epoch 00112: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.3622 - acc: 0.8827 - val_loss: 1.0117 - val_acc: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3525 - acc: 0.8828\n",
      "Epoch 00113: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3519 - acc: 0.8830 - val_loss: 1.0918 - val_acc: 0.7137\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3597 - acc: 0.8825\n",
      "Epoch 00114: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3598 - acc: 0.8825 - val_loss: 0.9892 - val_acc: 0.7424\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8832\n",
      "Epoch 00115: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3543 - acc: 0.8834 - val_loss: 1.0353 - val_acc: 0.7205\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8836\n",
      "Epoch 00116: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3547 - acc: 0.8836 - val_loss: 1.0145 - val_acc: 0.7356\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8856\n",
      "Epoch 00117: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3502 - acc: 0.8856 - val_loss: 0.9799 - val_acc: 0.7370\n",
      "Epoch 118/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8833\n",
      "Epoch 00118: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3487 - acc: 0.8833 - val_loss: 1.0020 - val_acc: 0.7331\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8832\n",
      "Epoch 00119: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3463 - acc: 0.8831 - val_loss: 0.9727 - val_acc: 0.7407\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8882\n",
      "Epoch 00120: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3406 - acc: 0.8881 - val_loss: 1.0344 - val_acc: 0.7286\n",
      "Epoch 121/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8898\n",
      "Epoch 00121: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3404 - acc: 0.8897 - val_loss: 1.0088 - val_acc: 0.7326\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8879\n",
      "Epoch 00122: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3445 - acc: 0.8879 - val_loss: 0.9785 - val_acc: 0.7424\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8887\n",
      "Epoch 00123: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3405 - acc: 0.8886 - val_loss: 0.9968 - val_acc: 0.7356\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8887\n",
      "Epoch 00124: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3370 - acc: 0.8887 - val_loss: 1.0683 - val_acc: 0.7158\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8902\n",
      "Epoch 00125: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3343 - acc: 0.8902 - val_loss: 1.0123 - val_acc: 0.7321\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8923\n",
      "Epoch 00126: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3298 - acc: 0.8923 - val_loss: 0.9660 - val_acc: 0.7454\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8935\n",
      "Epoch 00127: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3300 - acc: 0.8934 - val_loss: 0.9985 - val_acc: 0.7349\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8898\n",
      "Epoch 00128: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3337 - acc: 0.8896 - val_loss: 1.0194 - val_acc: 0.7358\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8933\n",
      "Epoch 00129: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3309 - acc: 0.8932 - val_loss: 1.0203 - val_acc: 0.7358\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8910\n",
      "Epoch 00130: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.3311 - acc: 0.8912 - val_loss: 0.9784 - val_acc: 0.7384\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8939\n",
      "Epoch 00131: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3230 - acc: 0.8938 - val_loss: 1.0232 - val_acc: 0.7300\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8954\n",
      "Epoch 00132: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3199 - acc: 0.8955 - val_loss: 0.9911 - val_acc: 0.7403\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.8944\n",
      "Epoch 00133: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3207 - acc: 0.8943 - val_loss: 1.0839 - val_acc: 0.7195\n",
      "Epoch 134/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8957\n",
      "Epoch 00134: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3213 - acc: 0.8957 - val_loss: 1.0768 - val_acc: 0.7228\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8970\n",
      "Epoch 00135: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3163 - acc: 0.8969 - val_loss: 1.1603 - val_acc: 0.7042\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.8973\n",
      "Epoch 00136: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3133 - acc: 0.8973 - val_loss: 0.9846 - val_acc: 0.7361\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.8957\n",
      "Epoch 00137: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3164 - acc: 0.8957 - val_loss: 0.9945 - val_acc: 0.7461\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.8957\n",
      "Epoch 00138: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3135 - acc: 0.8957 - val_loss: 1.0148 - val_acc: 0.7354\n",
      "Epoch 139/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3166 - acc: 0.8976\n",
      "Epoch 00139: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3166 - acc: 0.8976 - val_loss: 0.9915 - val_acc: 0.7398\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8985\n",
      "Epoch 00140: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.3107 - acc: 0.8985 - val_loss: 0.9923 - val_acc: 0.7424\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8948\n",
      "Epoch 00141: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.3177 - acc: 0.8948 - val_loss: 1.0029 - val_acc: 0.7428\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.8995\n",
      "Epoch 00142: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.3100 - acc: 0.8994 - val_loss: 1.0056 - val_acc: 0.7375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3039 - acc: 0.9002\n",
      "Epoch 00143: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3039 - acc: 0.9001 - val_loss: 0.9829 - val_acc: 0.7482\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.8990\n",
      "Epoch 00144: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.3035 - acc: 0.8990 - val_loss: 1.0427 - val_acc: 0.7331\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8970\n",
      "Epoch 00145: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.3110 - acc: 0.8971 - val_loss: 0.9736 - val_acc: 0.7503\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.9004\n",
      "Epoch 00146: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3009 - acc: 0.9004 - val_loss: 1.0045 - val_acc: 0.7386\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9009\n",
      "Epoch 00147: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3027 - acc: 0.9008 - val_loss: 1.0243 - val_acc: 0.7379\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3075 - acc: 0.8999\n",
      "Epoch 00148: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.3075 - acc: 0.8999 - val_loss: 0.9929 - val_acc: 0.7489\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9027\n",
      "Epoch 00149: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.2977 - acc: 0.9027 - val_loss: 1.0005 - val_acc: 0.7377\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9007\n",
      "Epoch 00150: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.2992 - acc: 0.9007 - val_loss: 1.0058 - val_acc: 0.7417\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.9052\n",
      "Epoch 00151: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.2947 - acc: 0.9053 - val_loss: 1.0222 - val_acc: 0.7438\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2959 - acc: 0.9036\n",
      "Epoch 00152: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.2959 - acc: 0.9036 - val_loss: 1.0311 - val_acc: 0.7370\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9013\n",
      "Epoch 00153: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.3010 - acc: 0.9012 - val_loss: 1.0420 - val_acc: 0.7291\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9034\n",
      "Epoch 00154: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.2925 - acc: 0.9034 - val_loss: 1.0092 - val_acc: 0.7461\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9032\n",
      "Epoch 00155: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.2945 - acc: 0.9033 - val_loss: 1.1165 - val_acc: 0.7133\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2948 - acc: 0.9034\n",
      "Epoch 00156: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.2949 - acc: 0.9034 - val_loss: 1.0731 - val_acc: 0.7184\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9053\n",
      "Epoch 00157: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.2917 - acc: 0.9053 - val_loss: 1.0128 - val_acc: 0.7466\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9058\n",
      "Epoch 00158: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.2880 - acc: 0.9058 - val_loss: 1.0358 - val_acc: 0.7375\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9048\n",
      "Epoch 00159: val_loss did not improve from 0.93558\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.2886 - acc: 0.9047 - val_loss: 0.9812 - val_acc: 0.7498\n",
      "\n",
      "1D_CNN_BN_DO_2_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 316us/sample - loss: 1.0973 - acc: 0.6885\n",
      "Loss: 1.097267709664714 Accuracy: 0.6884735\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.6178 - acc: 0.2159\n",
      "Epoch 00001: val_loss improved from inf to 1.95258, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/001-1.9526.hdf5\n",
      "36805/36805 [==============================] - 22s 611us/sample - loss: 2.6169 - acc: 0.2162 - val_loss: 1.9526 - val_acc: 0.3201\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8290 - acc: 0.4174\n",
      "Epoch 00002: val_loss improved from 1.95258 to 1.38704, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/002-1.3870.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 1.8289 - acc: 0.4174 - val_loss: 1.3870 - val_acc: 0.5623\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5293 - acc: 0.5113\n",
      "Epoch 00003: val_loss improved from 1.38704 to 1.22565, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/003-1.2256.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 1.5290 - acc: 0.5114 - val_loss: 1.2256 - val_acc: 0.6177\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3344 - acc: 0.5779\n",
      "Epoch 00004: val_loss improved from 1.22565 to 1.09019, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/004-1.0902.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 1.3342 - acc: 0.5780 - val_loss: 1.0902 - val_acc: 0.6678\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2044 - acc: 0.6225\n",
      "Epoch 00005: val_loss improved from 1.09019 to 1.04910, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/005-1.0491.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 1.2047 - acc: 0.6224 - val_loss: 1.0491 - val_acc: 0.6739\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1158 - acc: 0.6515\n",
      "Epoch 00006: val_loss improved from 1.04910 to 0.93213, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/006-0.9321.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 1.1152 - acc: 0.6517 - val_loss: 0.9321 - val_acc: 0.7209\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0415 - acc: 0.6779\n",
      "Epoch 00007: val_loss improved from 0.93213 to 0.91204, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/007-0.9120.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 1.0409 - acc: 0.6779 - val_loss: 0.9120 - val_acc: 0.7293\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9822 - acc: 0.6986\n",
      "Epoch 00008: val_loss improved from 0.91204 to 0.87234, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/008-0.8723.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.9826 - acc: 0.6985 - val_loss: 0.8723 - val_acc: 0.7391\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9322 - acc: 0.7148\n",
      "Epoch 00009: val_loss did not improve from 0.87234\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.9332 - acc: 0.7146 - val_loss: 0.9922 - val_acc: 0.6976\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8878 - acc: 0.7297\n",
      "Epoch 00010: val_loss improved from 0.87234 to 0.80592, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/010-0.8059.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.8883 - acc: 0.7297 - val_loss: 0.8059 - val_acc: 0.7605\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8460 - acc: 0.7450\n",
      "Epoch 00011: val_loss improved from 0.80592 to 0.77635, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/011-0.7764.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.8458 - acc: 0.7451 - val_loss: 0.7764 - val_acc: 0.7717\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8101 - acc: 0.7544\n",
      "Epoch 00012: val_loss did not improve from 0.77635\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.8104 - acc: 0.7546 - val_loss: 0.8117 - val_acc: 0.7536\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7773 - acc: 0.7668\n",
      "Epoch 00013: val_loss did not improve from 0.77635\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.7777 - acc: 0.7666 - val_loss: 0.8059 - val_acc: 0.7678\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7460 - acc: 0.7745\n",
      "Epoch 00014: val_loss improved from 0.77635 to 0.69212, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/014-0.6921.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.7464 - acc: 0.7743 - val_loss: 0.6921 - val_acc: 0.8032\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7205 - acc: 0.7829\n",
      "Epoch 00015: val_loss did not improve from 0.69212\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.7202 - acc: 0.7830 - val_loss: 0.7953 - val_acc: 0.7561\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6978 - acc: 0.7901\n",
      "Epoch 00016: val_loss improved from 0.69212 to 0.66358, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/016-0.6636.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.6976 - acc: 0.7903 - val_loss: 0.6636 - val_acc: 0.8183\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6724 - acc: 0.7994\n",
      "Epoch 00017: val_loss did not improve from 0.66358\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.6724 - acc: 0.7994 - val_loss: 0.7812 - val_acc: 0.7689\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6485 - acc: 0.8056\n",
      "Epoch 00018: val_loss did not improve from 0.66358\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.6485 - acc: 0.8055 - val_loss: 0.7487 - val_acc: 0.7862\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6325 - acc: 0.8109\n",
      "Epoch 00019: val_loss did not improve from 0.66358\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.6325 - acc: 0.8109 - val_loss: 0.7366 - val_acc: 0.7906\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6200 - acc: 0.8137\n",
      "Epoch 00020: val_loss did not improve from 0.66358\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.6203 - acc: 0.8137 - val_loss: 0.6668 - val_acc: 0.8104\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.8209\n",
      "Epoch 00021: val_loss did not improve from 0.66358\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.6019 - acc: 0.8210 - val_loss: 0.7371 - val_acc: 0.7808\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5770 - acc: 0.8292\n",
      "Epoch 00022: val_loss improved from 0.66358 to 0.65024, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/022-0.6502.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.5769 - acc: 0.8292 - val_loss: 0.6502 - val_acc: 0.8143\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5685 - acc: 0.8306\n",
      "Epoch 00023: val_loss improved from 0.65024 to 0.59473, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/023-0.5947.hdf5\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.5685 - acc: 0.8306 - val_loss: 0.5947 - val_acc: 0.8337\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.8343\n",
      "Epoch 00024: val_loss did not improve from 0.59473\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.5538 - acc: 0.8343 - val_loss: 0.6027 - val_acc: 0.8314\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.8383\n",
      "Epoch 00025: val_loss improved from 0.59473 to 0.58547, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/025-0.5855.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.5429 - acc: 0.8381 - val_loss: 0.5855 - val_acc: 0.8383\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.8420\n",
      "Epoch 00026: val_loss did not improve from 0.58547\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.5295 - acc: 0.8418 - val_loss: 0.6228 - val_acc: 0.8267\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.8447\n",
      "Epoch 00027: val_loss did not improve from 0.58547\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.5193 - acc: 0.8446 - val_loss: 0.6004 - val_acc: 0.8323\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8485\n",
      "Epoch 00028: val_loss improved from 0.58547 to 0.57381, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/028-0.5738.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.5117 - acc: 0.8486 - val_loss: 0.5738 - val_acc: 0.8439\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5011 - acc: 0.8483\n",
      "Epoch 00029: val_loss did not improve from 0.57381\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.5011 - acc: 0.8483 - val_loss: 0.5909 - val_acc: 0.8288\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4886 - acc: 0.8539\n",
      "Epoch 00030: val_loss did not improve from 0.57381\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4887 - acc: 0.8539 - val_loss: 0.6099 - val_acc: 0.8209\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8540\n",
      "Epoch 00031: val_loss improved from 0.57381 to 0.56241, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/031-0.5624.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4871 - acc: 0.8540 - val_loss: 0.5624 - val_acc: 0.8458\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4764 - acc: 0.8571\n",
      "Epoch 00032: val_loss did not improve from 0.56241\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4765 - acc: 0.8570 - val_loss: 0.5636 - val_acc: 0.8383\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4711 - acc: 0.8579\n",
      "Epoch 00033: val_loss did not improve from 0.56241\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4709 - acc: 0.8579 - val_loss: 0.5984 - val_acc: 0.8297\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4554 - acc: 0.8616\n",
      "Epoch 00034: val_loss improved from 0.56241 to 0.55570, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/034-0.5557.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4557 - acc: 0.8615 - val_loss: 0.5557 - val_acc: 0.8449\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8636\n",
      "Epoch 00035: val_loss did not improve from 0.55570\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4542 - acc: 0.8637 - val_loss: 0.5794 - val_acc: 0.8393\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8658\n",
      "Epoch 00036: val_loss did not improve from 0.55570\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.4466 - acc: 0.8657 - val_loss: 0.5775 - val_acc: 0.8355\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4428 - acc: 0.8639\n",
      "Epoch 00037: val_loss did not improve from 0.55570\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.4425 - acc: 0.8640 - val_loss: 0.5621 - val_acc: 0.8439\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8680\n",
      "Epoch 00038: val_loss did not improve from 0.55570\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.4348 - acc: 0.8678 - val_loss: 0.5661 - val_acc: 0.8435\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8681\n",
      "Epoch 00039: val_loss improved from 0.55570 to 0.51766, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/039-0.5177.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4363 - acc: 0.8682 - val_loss: 0.5177 - val_acc: 0.8586\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8741\n",
      "Epoch 00040: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.4188 - acc: 0.8740 - val_loss: 0.5384 - val_acc: 0.8477\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8741\n",
      "Epoch 00041: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.4144 - acc: 0.8741 - val_loss: 0.5943 - val_acc: 0.8355\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8755\n",
      "Epoch 00042: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.4112 - acc: 0.8756 - val_loss: 0.5445 - val_acc: 0.8472\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8783\n",
      "Epoch 00043: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3988 - acc: 0.8782 - val_loss: 0.5627 - val_acc: 0.8425\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8779\n",
      "Epoch 00044: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3959 - acc: 0.8779 - val_loss: 0.5826 - val_acc: 0.8402\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8807\n",
      "Epoch 00045: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.3922 - acc: 0.8807 - val_loss: 0.5530 - val_acc: 0.8456\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8822\n",
      "Epoch 00046: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3870 - acc: 0.8821 - val_loss: 0.5345 - val_acc: 0.8537\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8825\n",
      "Epoch 00047: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3846 - acc: 0.8825 - val_loss: 0.5459 - val_acc: 0.8425\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8832\n",
      "Epoch 00048: val_loss did not improve from 0.51766\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3782 - acc: 0.8831 - val_loss: 0.6523 - val_acc: 0.8109\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.8872\n",
      "Epoch 00049: val_loss improved from 0.51766 to 0.50354, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/049-0.5035.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.3708 - acc: 0.8871 - val_loss: 0.5035 - val_acc: 0.8628\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8862\n",
      "Epoch 00050: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3711 - acc: 0.8864 - val_loss: 0.5421 - val_acc: 0.8514\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8857\n",
      "Epoch 00051: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3748 - acc: 0.8857 - val_loss: 0.5346 - val_acc: 0.8567\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8885\n",
      "Epoch 00052: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3630 - acc: 0.8886 - val_loss: 0.5992 - val_acc: 0.8390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3638 - acc: 0.8889\n",
      "Epoch 00053: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3638 - acc: 0.8889 - val_loss: 0.5341 - val_acc: 0.8493\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8928\n",
      "Epoch 00054: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3522 - acc: 0.8927 - val_loss: 0.5087 - val_acc: 0.8609\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8940\n",
      "Epoch 00055: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3477 - acc: 0.8940 - val_loss: 0.5459 - val_acc: 0.8528\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8918\n",
      "Epoch 00056: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3528 - acc: 0.8919 - val_loss: 0.5917 - val_acc: 0.8304\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8919\n",
      "Epoch 00057: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3465 - acc: 0.8919 - val_loss: 0.5415 - val_acc: 0.8519\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.8941\n",
      "Epoch 00058: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3424 - acc: 0.8941 - val_loss: 0.6046 - val_acc: 0.8346\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8963\n",
      "Epoch 00059: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3387 - acc: 0.8962 - val_loss: 0.5277 - val_acc: 0.8505\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8995\n",
      "Epoch 00060: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3344 - acc: 0.8995 - val_loss: 0.5442 - val_acc: 0.8409\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3304 - acc: 0.8996\n",
      "Epoch 00061: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3307 - acc: 0.8995 - val_loss: 0.5094 - val_acc: 0.8637\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.8982\n",
      "Epoch 00062: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3240 - acc: 0.8982 - val_loss: 0.5426 - val_acc: 0.8472\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8996\n",
      "Epoch 00063: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3247 - acc: 0.8995 - val_loss: 0.5367 - val_acc: 0.8558\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.9002\n",
      "Epoch 00064: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3236 - acc: 0.9001 - val_loss: 0.5143 - val_acc: 0.8528\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.9019\n",
      "Epoch 00065: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.3175 - acc: 0.9019 - val_loss: 0.5253 - val_acc: 0.8556\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.9009\n",
      "Epoch 00066: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.3204 - acc: 0.9008 - val_loss: 0.5426 - val_acc: 0.8523\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9033\n",
      "Epoch 00067: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.3075 - acc: 0.9032 - val_loss: 0.5273 - val_acc: 0.8586\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9064\n",
      "Epoch 00068: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3026 - acc: 0.9064 - val_loss: 0.5381 - val_acc: 0.8491\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9070\n",
      "Epoch 00069: val_loss did not improve from 0.50354\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.3009 - acc: 0.9071 - val_loss: 0.6499 - val_acc: 0.8230\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9034\n",
      "Epoch 00070: val_loss improved from 0.50354 to 0.49490, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/070-0.4949.hdf5\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.3088 - acc: 0.9035 - val_loss: 0.4949 - val_acc: 0.8595\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9064\n",
      "Epoch 00071: val_loss improved from 0.49490 to 0.49024, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/071-0.4902.hdf5\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.3001 - acc: 0.9063 - val_loss: 0.4902 - val_acc: 0.8595\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.9067\n",
      "Epoch 00072: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.3003 - acc: 0.9068 - val_loss: 0.5081 - val_acc: 0.8630\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9072\n",
      "Epoch 00073: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.2975 - acc: 0.9072 - val_loss: 0.6161 - val_acc: 0.8279\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9094\n",
      "Epoch 00074: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.2932 - acc: 0.9093 - val_loss: 0.6169 - val_acc: 0.8337\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9082\n",
      "Epoch 00075: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2939 - acc: 0.9081 - val_loss: 0.5145 - val_acc: 0.8616\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9098\n",
      "Epoch 00076: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2870 - acc: 0.9099 - val_loss: 0.5127 - val_acc: 0.8609\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9124\n",
      "Epoch 00077: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2848 - acc: 0.9124 - val_loss: 0.5567 - val_acc: 0.8484\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9116\n",
      "Epoch 00078: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2799 - acc: 0.9116 - val_loss: 0.5203 - val_acc: 0.8560\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9115\n",
      "Epoch 00079: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2846 - acc: 0.9114 - val_loss: 0.5017 - val_acc: 0.8656\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9143\n",
      "Epoch 00080: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2736 - acc: 0.9143 - val_loss: 0.5596 - val_acc: 0.8512\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.9124\n",
      "Epoch 00081: val_loss did not improve from 0.49024\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2806 - acc: 0.9123 - val_loss: 0.4978 - val_acc: 0.8684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9146\n",
      "Epoch 00082: val_loss improved from 0.49024 to 0.47455, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/082-0.4746.hdf5\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.2693 - acc: 0.9147 - val_loss: 0.4746 - val_acc: 0.8749\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9173\n",
      "Epoch 00083: val_loss improved from 0.47455 to 0.47264, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/083-0.4726.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2683 - acc: 0.9173 - val_loss: 0.4726 - val_acc: 0.8649\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9153\n",
      "Epoch 00084: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2704 - acc: 0.9153 - val_loss: 0.5026 - val_acc: 0.8679\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9184\n",
      "Epoch 00085: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2642 - acc: 0.9185 - val_loss: 0.4937 - val_acc: 0.8635\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9185\n",
      "Epoch 00086: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2609 - acc: 0.9184 - val_loss: 0.5457 - val_acc: 0.8519\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9183\n",
      "Epoch 00087: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2603 - acc: 0.9184 - val_loss: 0.5191 - val_acc: 0.8668\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2594 - acc: 0.9192\n",
      "Epoch 00088: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2593 - acc: 0.9192 - val_loss: 0.5175 - val_acc: 0.8649\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.9184\n",
      "Epoch 00089: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2579 - acc: 0.9183 - val_loss: 0.4999 - val_acc: 0.8670\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9193\n",
      "Epoch 00090: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2567 - acc: 0.9191 - val_loss: 0.5062 - val_acc: 0.8689\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9205\n",
      "Epoch 00091: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2563 - acc: 0.9205 - val_loss: 0.4863 - val_acc: 0.8742\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9192\n",
      "Epoch 00092: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2563 - acc: 0.9193 - val_loss: 0.5221 - val_acc: 0.8591\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9224\n",
      "Epoch 00093: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2470 - acc: 0.9225 - val_loss: 0.5137 - val_acc: 0.8630\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9243\n",
      "Epoch 00094: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2423 - acc: 0.9244 - val_loss: 0.4911 - val_acc: 0.8693\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9241\n",
      "Epoch 00095: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2446 - acc: 0.9241 - val_loss: 0.5119 - val_acc: 0.8640\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9229\n",
      "Epoch 00096: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2436 - acc: 0.9230 - val_loss: 0.5332 - val_acc: 0.8563\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9253\n",
      "Epoch 00097: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2430 - acc: 0.9252 - val_loss: 0.4854 - val_acc: 0.8700\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9241\n",
      "Epoch 00098: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2373 - acc: 0.9241 - val_loss: 0.5267 - val_acc: 0.8661\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9238\n",
      "Epoch 00099: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.2355 - acc: 0.9238 - val_loss: 0.5566 - val_acc: 0.8577\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9248\n",
      "Epoch 00100: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2374 - acc: 0.9247 - val_loss: 0.5087 - val_acc: 0.8693\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9248\n",
      "Epoch 00101: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2353 - acc: 0.9247 - val_loss: 0.4738 - val_acc: 0.8749\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9265\n",
      "Epoch 00102: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2306 - acc: 0.9265 - val_loss: 0.5399 - val_acc: 0.8584\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9272\n",
      "Epoch 00103: val_loss did not improve from 0.47264\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2306 - acc: 0.9273 - val_loss: 0.5138 - val_acc: 0.8700\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9253\n",
      "Epoch 00104: val_loss improved from 0.47264 to 0.46607, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/104-0.4661.hdf5\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.2312 - acc: 0.9251 - val_loss: 0.4661 - val_acc: 0.8812\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9268\n",
      "Epoch 00105: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2321 - acc: 0.9268 - val_loss: 0.4775 - val_acc: 0.8793\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9270\n",
      "Epoch 00106: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2276 - acc: 0.9271 - val_loss: 0.5061 - val_acc: 0.8679\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9276\n",
      "Epoch 00107: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2274 - acc: 0.9277 - val_loss: 0.4996 - val_acc: 0.8726\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9302\n",
      "Epoch 00108: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2248 - acc: 0.9302 - val_loss: 0.4944 - val_acc: 0.8651\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9281\n",
      "Epoch 00109: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.2261 - acc: 0.9280 - val_loss: 0.4694 - val_acc: 0.8786\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9293\n",
      "Epoch 00110: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2222 - acc: 0.9292 - val_loss: 0.5016 - val_acc: 0.8663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9316\n",
      "Epoch 00111: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2145 - acc: 0.9316 - val_loss: 0.4810 - val_acc: 0.8782\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9304\n",
      "Epoch 00112: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2192 - acc: 0.9304 - val_loss: 0.4851 - val_acc: 0.8714\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9300\n",
      "Epoch 00113: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2182 - acc: 0.9300 - val_loss: 0.4701 - val_acc: 0.8793\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9312\n",
      "Epoch 00114: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2146 - acc: 0.9313 - val_loss: 0.5690 - val_acc: 0.8519\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9316\n",
      "Epoch 00115: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.2156 - acc: 0.9316 - val_loss: 0.4665 - val_acc: 0.8782\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9319\n",
      "Epoch 00116: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2148 - acc: 0.9319 - val_loss: 0.5176 - val_acc: 0.8612\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9318\n",
      "Epoch 00117: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2122 - acc: 0.9319 - val_loss: 0.4752 - val_acc: 0.8737\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9323\n",
      "Epoch 00118: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2125 - acc: 0.9322 - val_loss: 0.5875 - val_acc: 0.8574\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9342\n",
      "Epoch 00119: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2088 - acc: 0.9342 - val_loss: 0.4959 - val_acc: 0.8761\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9361\n",
      "Epoch 00120: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2038 - acc: 0.9362 - val_loss: 0.4784 - val_acc: 0.8786\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9338\n",
      "Epoch 00121: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.2113 - acc: 0.9338 - val_loss: 0.4717 - val_acc: 0.8800\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9333\n",
      "Epoch 00122: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2089 - acc: 0.9332 - val_loss: 0.5370 - val_acc: 0.8684\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9361\n",
      "Epoch 00123: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.2040 - acc: 0.9361 - val_loss: 0.5305 - val_acc: 0.8656\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9359\n",
      "Epoch 00124: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2040 - acc: 0.9359 - val_loss: 0.5076 - val_acc: 0.8677\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9376\n",
      "Epoch 00125: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.2028 - acc: 0.9376 - val_loss: 0.5499 - val_acc: 0.8591\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9375\n",
      "Epoch 00126: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1976 - acc: 0.9375 - val_loss: 0.5465 - val_acc: 0.8588\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9397\n",
      "Epoch 00127: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1935 - acc: 0.9396 - val_loss: 0.5156 - val_acc: 0.8721\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9376\n",
      "Epoch 00128: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1984 - acc: 0.9376 - val_loss: 0.5051 - val_acc: 0.8710\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9364\n",
      "Epoch 00129: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.2006 - acc: 0.9363 - val_loss: 0.5132 - val_acc: 0.8717\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9372\n",
      "Epoch 00130: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1982 - acc: 0.9372 - val_loss: 0.4813 - val_acc: 0.8763\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9397\n",
      "Epoch 00131: val_loss did not improve from 0.46607\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1912 - acc: 0.9398 - val_loss: 0.4793 - val_acc: 0.8740\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9388\n",
      "Epoch 00132: val_loss improved from 0.46607 to 0.45023, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/132-0.4502.hdf5\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1931 - acc: 0.9388 - val_loss: 0.4502 - val_acc: 0.8873\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9416\n",
      "Epoch 00133: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1865 - acc: 0.9416 - val_loss: 0.4950 - val_acc: 0.8758\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9395\n",
      "Epoch 00134: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1931 - acc: 0.9395 - val_loss: 0.5113 - val_acc: 0.8710\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9389\n",
      "Epoch 00135: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1863 - acc: 0.9389 - val_loss: 0.5148 - val_acc: 0.8721\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9415\n",
      "Epoch 00136: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1872 - acc: 0.9415 - val_loss: 0.5370 - val_acc: 0.8614\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9399\n",
      "Epoch 00137: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1901 - acc: 0.9398 - val_loss: 0.5514 - val_acc: 0.8614\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9383\n",
      "Epoch 00138: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1919 - acc: 0.9384 - val_loss: 0.5736 - val_acc: 0.8521\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9428\n",
      "Epoch 00139: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1840 - acc: 0.9428 - val_loss: 0.5280 - val_acc: 0.8626\n",
      "Epoch 140/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9422\n",
      "Epoch 00140: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1807 - acc: 0.9421 - val_loss: 0.4960 - val_acc: 0.8812\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9414\n",
      "Epoch 00141: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1841 - acc: 0.9414 - val_loss: 0.5034 - val_acc: 0.8724\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9421\n",
      "Epoch 00142: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1857 - acc: 0.9420 - val_loss: 0.5083 - val_acc: 0.8717\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9404\n",
      "Epoch 00143: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1839 - acc: 0.9403 - val_loss: 0.4835 - val_acc: 0.8782\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9433\n",
      "Epoch 00144: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1777 - acc: 0.9432 - val_loss: 0.4777 - val_acc: 0.8838\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9423\n",
      "Epoch 00145: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1807 - acc: 0.9423 - val_loss: 0.6053 - val_acc: 0.8521\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9435\n",
      "Epoch 00146: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1782 - acc: 0.9436 - val_loss: 0.4805 - val_acc: 0.8779\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9437\n",
      "Epoch 00147: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1751 - acc: 0.9436 - val_loss: 0.4696 - val_acc: 0.8800\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9431\n",
      "Epoch 00148: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1807 - acc: 0.9430 - val_loss: 0.4718 - val_acc: 0.8840\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9441\n",
      "Epoch 00149: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1746 - acc: 0.9440 - val_loss: 0.5119 - val_acc: 0.8765\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9448\n",
      "Epoch 00150: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1777 - acc: 0.9447 - val_loss: 0.5317 - val_acc: 0.8633\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9443\n",
      "Epoch 00151: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1730 - acc: 0.9443 - val_loss: 0.5112 - val_acc: 0.8733\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9434\n",
      "Epoch 00152: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1780 - acc: 0.9435 - val_loss: 0.4558 - val_acc: 0.8845\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9458\n",
      "Epoch 00153: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1704 - acc: 0.9458 - val_loss: 0.4571 - val_acc: 0.8852\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9450\n",
      "Epoch 00154: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1721 - acc: 0.9450 - val_loss: 0.4636 - val_acc: 0.8889\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9450\n",
      "Epoch 00155: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1713 - acc: 0.9450 - val_loss: 0.5024 - val_acc: 0.8754\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9474\n",
      "Epoch 00156: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1680 - acc: 0.9474 - val_loss: 0.5200 - val_acc: 0.8717\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9453\n",
      "Epoch 00157: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1724 - acc: 0.9454 - val_loss: 0.5041 - val_acc: 0.8749\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9477\n",
      "Epoch 00158: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1675 - acc: 0.9477 - val_loss: 0.4965 - val_acc: 0.8737\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9482\n",
      "Epoch 00159: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1646 - acc: 0.9482 - val_loss: 0.4895 - val_acc: 0.8751\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9485\n",
      "Epoch 00160: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1633 - acc: 0.9486 - val_loss: 0.4768 - val_acc: 0.8835\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9484\n",
      "Epoch 00161: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1650 - acc: 0.9484 - val_loss: 0.5077 - val_acc: 0.8689\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9474\n",
      "Epoch 00162: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1648 - acc: 0.9473 - val_loss: 0.4784 - val_acc: 0.8782\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9481\n",
      "Epoch 00163: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1663 - acc: 0.9481 - val_loss: 0.5312 - val_acc: 0.8696\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9462\n",
      "Epoch 00164: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1648 - acc: 0.9462 - val_loss: 0.5379 - val_acc: 0.8730\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9486\n",
      "Epoch 00165: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1629 - acc: 0.9485 - val_loss: 0.5329 - val_acc: 0.8698\n",
      "Epoch 166/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9492\n",
      "Epoch 00166: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1628 - acc: 0.9491 - val_loss: 0.5538 - val_acc: 0.8686\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9471\n",
      "Epoch 00167: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1671 - acc: 0.9471 - val_loss: 0.4735 - val_acc: 0.8845\n",
      "Epoch 168/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9506\n",
      "Epoch 00168: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1587 - acc: 0.9506 - val_loss: 0.4682 - val_acc: 0.8845\n",
      "Epoch 169/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9513\n",
      "Epoch 00169: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1551 - acc: 0.9513 - val_loss: 0.5339 - val_acc: 0.8691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9511\n",
      "Epoch 00170: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1569 - acc: 0.9511 - val_loss: 0.5189 - val_acc: 0.8775\n",
      "Epoch 171/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9488\n",
      "Epoch 00171: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1613 - acc: 0.9488 - val_loss: 0.5417 - val_acc: 0.8649\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9491\n",
      "Epoch 00172: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1585 - acc: 0.9491 - val_loss: 0.5058 - val_acc: 0.8728\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9504\n",
      "Epoch 00173: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.1556 - acc: 0.9505 - val_loss: 0.4546 - val_acc: 0.8873\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9504\n",
      "Epoch 00174: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1603 - acc: 0.9504 - val_loss: 0.5623 - val_acc: 0.8661\n",
      "Epoch 175/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9497\n",
      "Epoch 00175: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1565 - acc: 0.9496 - val_loss: 0.4872 - val_acc: 0.8754\n",
      "Epoch 176/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9488\n",
      "Epoch 00176: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1593 - acc: 0.9488 - val_loss: 0.4988 - val_acc: 0.8765\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9509\n",
      "Epoch 00177: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1529 - acc: 0.9508 - val_loss: 0.5138 - val_acc: 0.8717\n",
      "Epoch 178/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9503\n",
      "Epoch 00178: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1538 - acc: 0.9503 - val_loss: 0.5960 - val_acc: 0.8586\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9470\n",
      "Epoch 00179: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1754 - acc: 0.9470 - val_loss: 0.4906 - val_acc: 0.8770\n",
      "Epoch 180/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9514\n",
      "Epoch 00180: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1488 - acc: 0.9514 - val_loss: 0.4916 - val_acc: 0.8807\n",
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9515\n",
      "Epoch 00181: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1505 - acc: 0.9514 - val_loss: 0.4904 - val_acc: 0.8812\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9533\n",
      "Epoch 00182: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1485 - acc: 0.9533 - val_loss: 0.4785 - val_acc: 0.8856\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9533\n",
      "Epoch 00183: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1482 - acc: 0.9531 - val_loss: 0.4647 - val_acc: 0.8866\n",
      "Epoch 184/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9499\n",
      "Epoch 00184: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1550 - acc: 0.9499 - val_loss: 0.5723 - val_acc: 0.8616\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9539\n",
      "Epoch 00185: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1509 - acc: 0.9539 - val_loss: 0.5047 - val_acc: 0.8784\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9527\n",
      "Epoch 00186: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1486 - acc: 0.9527 - val_loss: 0.5157 - val_acc: 0.8803\n",
      "Epoch 187/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9533\n",
      "Epoch 00187: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1432 - acc: 0.9533 - val_loss: 0.5148 - val_acc: 0.8698\n",
      "Epoch 188/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9525\n",
      "Epoch 00188: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1464 - acc: 0.9525 - val_loss: 0.5345 - val_acc: 0.8714\n",
      "Epoch 189/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9529\n",
      "Epoch 00189: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1473 - acc: 0.9529 - val_loss: 0.4927 - val_acc: 0.8803\n",
      "Epoch 190/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9552\n",
      "Epoch 00190: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1446 - acc: 0.9551 - val_loss: 0.5414 - val_acc: 0.8717\n",
      "Epoch 191/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9536\n",
      "Epoch 00191: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1454 - acc: 0.9536 - val_loss: 0.4868 - val_acc: 0.8807\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9544\n",
      "Epoch 00192: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1462 - acc: 0.9544 - val_loss: 0.5315 - val_acc: 0.8707\n",
      "Epoch 193/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9556\n",
      "Epoch 00193: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1412 - acc: 0.9556 - val_loss: 0.5268 - val_acc: 0.8700\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9541\n",
      "Epoch 00194: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1427 - acc: 0.9542 - val_loss: 0.5141 - val_acc: 0.8793\n",
      "Epoch 195/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9550\n",
      "Epoch 00195: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1454 - acc: 0.9550 - val_loss: 0.4946 - val_acc: 0.8775\n",
      "Epoch 196/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9551\n",
      "Epoch 00196: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1411 - acc: 0.9551 - val_loss: 0.4964 - val_acc: 0.8819\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9521\n",
      "Epoch 00197: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1503 - acc: 0.9520 - val_loss: 0.4875 - val_acc: 0.8828\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9550\n",
      "Epoch 00198: val_loss did not improve from 0.45023\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1407 - acc: 0.9549 - val_loss: 0.5374 - val_acc: 0.8696\n",
      "Epoch 199/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9541\n",
      "Epoch 00199: val_loss improved from 0.45023 to 0.44975, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/199-0.4498.hdf5\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1429 - acc: 0.9541 - val_loss: 0.4498 - val_acc: 0.8852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9558\n",
      "Epoch 00200: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1391 - acc: 0.9558 - val_loss: 0.4927 - val_acc: 0.8819\n",
      "Epoch 201/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9547\n",
      "Epoch 00201: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1394 - acc: 0.9547 - val_loss: 0.5022 - val_acc: 0.8775\n",
      "Epoch 202/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9552\n",
      "Epoch 00202: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1421 - acc: 0.9553 - val_loss: 0.5126 - val_acc: 0.8763\n",
      "Epoch 203/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9554\n",
      "Epoch 00203: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1389 - acc: 0.9555 - val_loss: 0.5100 - val_acc: 0.8733\n",
      "Epoch 204/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9558\n",
      "Epoch 00204: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1367 - acc: 0.9557 - val_loss: 0.4922 - val_acc: 0.8805\n",
      "Epoch 205/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9579\n",
      "Epoch 00205: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1341 - acc: 0.9579 - val_loss: 0.5133 - val_acc: 0.8786\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9570\n",
      "Epoch 00206: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1328 - acc: 0.9570 - val_loss: 0.5714 - val_acc: 0.8661\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9558\n",
      "Epoch 00207: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1400 - acc: 0.9558 - val_loss: 0.4636 - val_acc: 0.8880\n",
      "Epoch 208/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9575\n",
      "Epoch 00208: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1347 - acc: 0.9575 - val_loss: 0.5012 - val_acc: 0.8693\n",
      "Epoch 209/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9589\n",
      "Epoch 00209: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1317 - acc: 0.9589 - val_loss: 0.4788 - val_acc: 0.8840\n",
      "Epoch 210/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9588\n",
      "Epoch 00210: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1332 - acc: 0.9588 - val_loss: 0.4861 - val_acc: 0.8824\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9557\n",
      "Epoch 00211: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1398 - acc: 0.9557 - val_loss: 0.4646 - val_acc: 0.8891\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9584\n",
      "Epoch 00212: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1356 - acc: 0.9583 - val_loss: 0.5116 - val_acc: 0.8756\n",
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9561\n",
      "Epoch 00213: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1354 - acc: 0.9561 - val_loss: 0.5104 - val_acc: 0.8814\n",
      "Epoch 214/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9572\n",
      "Epoch 00214: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1327 - acc: 0.9572 - val_loss: 0.4988 - val_acc: 0.8819\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9582\n",
      "Epoch 00215: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1346 - acc: 0.9581 - val_loss: 0.4866 - val_acc: 0.8817\n",
      "Epoch 216/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9593\n",
      "Epoch 00216: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1328 - acc: 0.9593 - val_loss: 0.4922 - val_acc: 0.8866\n",
      "Epoch 217/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9575\n",
      "Epoch 00217: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1360 - acc: 0.9575 - val_loss: 0.4592 - val_acc: 0.8866\n",
      "Epoch 218/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9596\n",
      "Epoch 00218: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1281 - acc: 0.9596 - val_loss: 0.6224 - val_acc: 0.8505\n",
      "Epoch 219/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9585\n",
      "Epoch 00219: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1296 - acc: 0.9585 - val_loss: 0.4904 - val_acc: 0.8859\n",
      "Epoch 220/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9598\n",
      "Epoch 00220: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1296 - acc: 0.9599 - val_loss: 0.4893 - val_acc: 0.8833\n",
      "Epoch 221/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9585\n",
      "Epoch 00221: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1305 - acc: 0.9586 - val_loss: 0.4813 - val_acc: 0.8838\n",
      "Epoch 222/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9594\n",
      "Epoch 00222: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1278 - acc: 0.9594 - val_loss: 0.4649 - val_acc: 0.8933\n",
      "Epoch 223/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9615\n",
      "Epoch 00223: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1269 - acc: 0.9614 - val_loss: 0.4807 - val_acc: 0.8873\n",
      "Epoch 224/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9606\n",
      "Epoch 00224: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1274 - acc: 0.9606 - val_loss: 0.5029 - val_acc: 0.8819\n",
      "Epoch 225/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9574\n",
      "Epoch 00225: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1304 - acc: 0.9574 - val_loss: 0.4945 - val_acc: 0.8831\n",
      "Epoch 226/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9596\n",
      "Epoch 00226: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1282 - acc: 0.9596 - val_loss: 0.5227 - val_acc: 0.8742\n",
      "Epoch 227/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9592\n",
      "Epoch 00227: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1270 - acc: 0.9592 - val_loss: 0.5781 - val_acc: 0.8668\n",
      "Epoch 228/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9597\n",
      "Epoch 00228: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1296 - acc: 0.9598 - val_loss: 0.4732 - val_acc: 0.8880\n",
      "Epoch 229/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9596\n",
      "Epoch 00229: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1272 - acc: 0.9596 - val_loss: 0.5242 - val_acc: 0.8793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9599\n",
      "Epoch 00230: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1245 - acc: 0.9599 - val_loss: 0.4805 - val_acc: 0.8842\n",
      "Epoch 231/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9602\n",
      "Epoch 00231: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1278 - acc: 0.9602 - val_loss: 0.4896 - val_acc: 0.8833\n",
      "Epoch 232/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9609\n",
      "Epoch 00232: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1253 - acc: 0.9610 - val_loss: 0.5524 - val_acc: 0.8705\n",
      "Epoch 233/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9589\n",
      "Epoch 00233: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1305 - acc: 0.9589 - val_loss: 0.4581 - val_acc: 0.8905\n",
      "Epoch 234/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9594\n",
      "Epoch 00234: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1237 - acc: 0.9595 - val_loss: 0.4920 - val_acc: 0.8845\n",
      "Epoch 235/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9617\n",
      "Epoch 00235: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1225 - acc: 0.9617 - val_loss: 0.4882 - val_acc: 0.8810\n",
      "Epoch 236/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9606\n",
      "Epoch 00236: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1237 - acc: 0.9606 - val_loss: 0.5227 - val_acc: 0.8812\n",
      "Epoch 237/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9610\n",
      "Epoch 00237: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1237 - acc: 0.9610 - val_loss: 0.5272 - val_acc: 0.8754\n",
      "Epoch 238/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9614\n",
      "Epoch 00238: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1244 - acc: 0.9615 - val_loss: 0.4613 - val_acc: 0.8924\n",
      "Epoch 239/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9628\n",
      "Epoch 00239: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1180 - acc: 0.9629 - val_loss: 0.5356 - val_acc: 0.8779\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9605\n",
      "Epoch 00240: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1223 - acc: 0.9605 - val_loss: 0.5313 - val_acc: 0.8724\n",
      "Epoch 241/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9627\n",
      "Epoch 00241: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1202 - acc: 0.9628 - val_loss: 0.5155 - val_acc: 0.8761\n",
      "Epoch 242/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9609\n",
      "Epoch 00242: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1223 - acc: 0.9608 - val_loss: 0.4732 - val_acc: 0.8889\n",
      "Epoch 243/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9626\n",
      "Epoch 00243: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1203 - acc: 0.9627 - val_loss: 0.5402 - val_acc: 0.8724\n",
      "Epoch 244/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9631\n",
      "Epoch 00244: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1209 - acc: 0.9632 - val_loss: 0.4952 - val_acc: 0.8856\n",
      "Epoch 245/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9628\n",
      "Epoch 00245: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1169 - acc: 0.9628 - val_loss: 0.5326 - val_acc: 0.8761\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9622\n",
      "Epoch 00246: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1167 - acc: 0.9622 - val_loss: 0.5107 - val_acc: 0.8789\n",
      "Epoch 247/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9621\n",
      "Epoch 00247: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1200 - acc: 0.9620 - val_loss: 0.4777 - val_acc: 0.8835\n",
      "Epoch 248/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9633\n",
      "Epoch 00248: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1178 - acc: 0.9633 - val_loss: 0.4686 - val_acc: 0.8917\n",
      "Epoch 249/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9649\n",
      "Epoch 00249: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1128 - acc: 0.9649 - val_loss: 0.4692 - val_acc: 0.8901\n",
      "Epoch 250/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9616\n",
      "Epoch 00250: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1215 - acc: 0.9616 - val_loss: 0.4791 - val_acc: 0.8831\n",
      "Epoch 251/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9642\n",
      "Epoch 00251: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1165 - acc: 0.9643 - val_loss: 0.4624 - val_acc: 0.8903\n",
      "Epoch 252/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9628\n",
      "Epoch 00252: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1160 - acc: 0.9629 - val_loss: 0.4770 - val_acc: 0.8903\n",
      "Epoch 253/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9631\n",
      "Epoch 00253: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1143 - acc: 0.9631 - val_loss: 0.4815 - val_acc: 0.8877\n",
      "Epoch 254/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9620\n",
      "Epoch 00254: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1186 - acc: 0.9620 - val_loss: 0.4939 - val_acc: 0.8852\n",
      "Epoch 255/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9622\n",
      "Epoch 00255: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1187 - acc: 0.9622 - val_loss: 0.4782 - val_acc: 0.8847\n",
      "Epoch 256/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9634\n",
      "Epoch 00256: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1154 - acc: 0.9634 - val_loss: 0.4779 - val_acc: 0.8863\n",
      "Epoch 257/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9642\n",
      "Epoch 00257: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1136 - acc: 0.9642 - val_loss: 0.4886 - val_acc: 0.8828\n",
      "Epoch 258/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9639\n",
      "Epoch 00258: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1161 - acc: 0.9639 - val_loss: 0.5168 - val_acc: 0.8842\n",
      "Epoch 259/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9637\n",
      "Epoch 00259: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1144 - acc: 0.9636 - val_loss: 0.4823 - val_acc: 0.8854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9630\n",
      "Epoch 00260: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1190 - acc: 0.9630 - val_loss: 0.5334 - val_acc: 0.8812\n",
      "Epoch 261/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9647\n",
      "Epoch 00261: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1122 - acc: 0.9647 - val_loss: 0.5126 - val_acc: 0.8784\n",
      "Epoch 262/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9625\n",
      "Epoch 00262: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1171 - acc: 0.9625 - val_loss: 0.5021 - val_acc: 0.8835\n",
      "Epoch 263/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9646\n",
      "Epoch 00263: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1126 - acc: 0.9646 - val_loss: 0.5173 - val_acc: 0.8821\n",
      "Epoch 264/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9639\n",
      "Epoch 00264: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.1123 - acc: 0.9639 - val_loss: 0.5168 - val_acc: 0.8782\n",
      "Epoch 265/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9662\n",
      "Epoch 00265: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1103 - acc: 0.9662 - val_loss: 0.5515 - val_acc: 0.8765\n",
      "Epoch 266/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9643\n",
      "Epoch 00266: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1122 - acc: 0.9644 - val_loss: 0.4997 - val_acc: 0.8800\n",
      "Epoch 267/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9642\n",
      "Epoch 00267: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1110 - acc: 0.9642 - val_loss: 0.4923 - val_acc: 0.8845\n",
      "Epoch 268/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9647\n",
      "Epoch 00268: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1100 - acc: 0.9647 - val_loss: 0.5187 - val_acc: 0.8826\n",
      "Epoch 269/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9671\n",
      "Epoch 00269: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1074 - acc: 0.9672 - val_loss: 0.5206 - val_acc: 0.8847\n",
      "Epoch 270/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9629\n",
      "Epoch 00270: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1143 - acc: 0.9628 - val_loss: 0.4835 - val_acc: 0.8912\n",
      "Epoch 271/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9656\n",
      "Epoch 00271: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1093 - acc: 0.9655 - val_loss: 0.5638 - val_acc: 0.8658\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9620\n",
      "Epoch 00272: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1177 - acc: 0.9620 - val_loss: 0.5317 - val_acc: 0.8712\n",
      "Epoch 273/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9655\n",
      "Epoch 00273: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1138 - acc: 0.9654 - val_loss: 0.5146 - val_acc: 0.8826\n",
      "Epoch 274/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9666\n",
      "Epoch 00274: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1049 - acc: 0.9666 - val_loss: 0.4989 - val_acc: 0.8828\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9665\n",
      "Epoch 00275: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1058 - acc: 0.9665 - val_loss: 0.4726 - val_acc: 0.8915\n",
      "Epoch 276/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9659\n",
      "Epoch 00276: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1082 - acc: 0.9659 - val_loss: 0.4776 - val_acc: 0.8880\n",
      "Epoch 277/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9661\n",
      "Epoch 00277: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1083 - acc: 0.9661 - val_loss: 0.5031 - val_acc: 0.8842\n",
      "Epoch 278/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9667\n",
      "Epoch 00278: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1079 - acc: 0.9667 - val_loss: 0.4934 - val_acc: 0.8896\n",
      "Epoch 279/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9645\n",
      "Epoch 00279: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1112 - acc: 0.9645 - val_loss: 0.5003 - val_acc: 0.8866\n",
      "Epoch 280/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9656\n",
      "Epoch 00280: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1087 - acc: 0.9656 - val_loss: 0.5038 - val_acc: 0.8863\n",
      "Epoch 281/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9657\n",
      "Epoch 00281: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1065 - acc: 0.9656 - val_loss: 0.4991 - val_acc: 0.8859\n",
      "Epoch 282/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9650\n",
      "Epoch 00282: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1076 - acc: 0.9650 - val_loss: 0.4948 - val_acc: 0.8896\n",
      "Epoch 283/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9672\n",
      "Epoch 00283: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1030 - acc: 0.9672 - val_loss: 0.5248 - val_acc: 0.8796\n",
      "Epoch 284/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9675\n",
      "Epoch 00284: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1051 - acc: 0.9675 - val_loss: 0.6143 - val_acc: 0.8612\n",
      "Epoch 285/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9660\n",
      "Epoch 00285: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1067 - acc: 0.9660 - val_loss: 0.5444 - val_acc: 0.8793\n",
      "Epoch 286/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9662\n",
      "Epoch 00286: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1079 - acc: 0.9661 - val_loss: 0.5080 - val_acc: 0.8849\n",
      "Epoch 287/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9674\n",
      "Epoch 00287: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1035 - acc: 0.9675 - val_loss: 0.5380 - val_acc: 0.8772\n",
      "Epoch 288/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9678\n",
      "Epoch 00288: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1061 - acc: 0.9678 - val_loss: 0.5070 - val_acc: 0.8828\n",
      "Epoch 289/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9675\n",
      "Epoch 00289: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1069 - acc: 0.9675 - val_loss: 0.5441 - val_acc: 0.8714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9674\n",
      "Epoch 00290: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1071 - acc: 0.9674 - val_loss: 0.4617 - val_acc: 0.8931\n",
      "Epoch 291/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9666\n",
      "Epoch 00291: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1026 - acc: 0.9665 - val_loss: 0.5005 - val_acc: 0.8933\n",
      "Epoch 292/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9660\n",
      "Epoch 00292: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.1046 - acc: 0.9660 - val_loss: 0.5394 - val_acc: 0.8796\n",
      "Epoch 293/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9679\n",
      "Epoch 00293: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1023 - acc: 0.9679 - val_loss: 0.5237 - val_acc: 0.8875\n",
      "Epoch 294/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9677\n",
      "Epoch 00294: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1014 - acc: 0.9677 - val_loss: 0.4805 - val_acc: 0.8919\n",
      "Epoch 295/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9647\n",
      "Epoch 00295: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1104 - acc: 0.9648 - val_loss: 0.4804 - val_acc: 0.8894\n",
      "Epoch 296/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9684\n",
      "Epoch 00296: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1025 - acc: 0.9683 - val_loss: 0.5238 - val_acc: 0.8887\n",
      "Epoch 297/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9679\n",
      "Epoch 00297: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1013 - acc: 0.9679 - val_loss: 0.4759 - val_acc: 0.8940\n",
      "Epoch 298/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9692\n",
      "Epoch 00298: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1021 - acc: 0.9692 - val_loss: 0.4989 - val_acc: 0.8919\n",
      "Epoch 299/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9679\n",
      "Epoch 00299: val_loss did not improve from 0.44975\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1018 - acc: 0.9678 - val_loss: 0.5029 - val_acc: 0.8910\n",
      "\n",
      "1D_CNN_BN_DO_3_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 364us/sample - loss: 0.5562 - acc: 0.8538\n",
      "Loss: 0.5561911058079293 Accuracy: 0.8537902\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5863 - acc: 0.2200\n",
      "Epoch 00001: val_loss improved from inf to 1.79158, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/001-1.7916.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 2.5862 - acc: 0.2200 - val_loss: 1.7916 - val_acc: 0.4018\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.6182 - acc: 0.4804\n",
      "Epoch 00002: val_loss improved from 1.79158 to 1.13488, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/002-1.1349.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 1.6169 - acc: 0.4807 - val_loss: 1.1349 - val_acc: 0.6560\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2519 - acc: 0.6040\n",
      "Epoch 00003: val_loss improved from 1.13488 to 0.91621, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/003-0.9162.hdf5\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 1.2517 - acc: 0.6041 - val_loss: 0.9162 - val_acc: 0.7251\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0557 - acc: 0.6717\n",
      "Epoch 00004: val_loss improved from 0.91621 to 0.78597, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/004-0.7860.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 1.0556 - acc: 0.6716 - val_loss: 0.7860 - val_acc: 0.7794\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9043 - acc: 0.7221\n",
      "Epoch 00005: val_loss improved from 0.78597 to 0.68841, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/005-0.6884.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.9045 - acc: 0.7221 - val_loss: 0.6884 - val_acc: 0.8069\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7981 - acc: 0.7575\n",
      "Epoch 00006: val_loss improved from 0.68841 to 0.62102, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/006-0.6210.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.7980 - acc: 0.7576 - val_loss: 0.6210 - val_acc: 0.8272\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7256 - acc: 0.7816\n",
      "Epoch 00007: val_loss improved from 0.62102 to 0.59469, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/007-0.5947.hdf5\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.7256 - acc: 0.7816 - val_loss: 0.5947 - val_acc: 0.8304\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6545 - acc: 0.8012\n",
      "Epoch 00008: val_loss improved from 0.59469 to 0.57503, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/008-0.5750.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.6545 - acc: 0.8012 - val_loss: 0.5750 - val_acc: 0.8388\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.8176\n",
      "Epoch 00009: val_loss improved from 0.57503 to 0.51476, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/009-0.5148.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.6052 - acc: 0.8177 - val_loss: 0.5148 - val_acc: 0.8505\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5643 - acc: 0.8303\n",
      "Epoch 00010: val_loss improved from 0.51476 to 0.46321, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/010-0.4632.hdf5\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.5643 - acc: 0.8303 - val_loss: 0.4632 - val_acc: 0.8758\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.8429\n",
      "Epoch 00011: val_loss improved from 0.46321 to 0.46015, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/011-0.4601.hdf5\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.5260 - acc: 0.8428 - val_loss: 0.4601 - val_acc: 0.8670\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8518\n",
      "Epoch 00012: val_loss improved from 0.46015 to 0.44779, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/012-0.4478.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.4913 - acc: 0.8519 - val_loss: 0.4478 - val_acc: 0.8761\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4696 - acc: 0.8584\n",
      "Epoch 00013: val_loss improved from 0.44779 to 0.39837, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/013-0.3984.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.4692 - acc: 0.8585 - val_loss: 0.3984 - val_acc: 0.8861\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4452 - acc: 0.8660\n",
      "Epoch 00014: val_loss improved from 0.39837 to 0.38309, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/014-0.3831.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.4454 - acc: 0.8659 - val_loss: 0.3831 - val_acc: 0.8898\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8739\n",
      "Epoch 00015: val_loss did not improve from 0.38309\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.4164 - acc: 0.8739 - val_loss: 0.3940 - val_acc: 0.8896\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.8798\n",
      "Epoch 00016: val_loss did not improve from 0.38309\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.4013 - acc: 0.8798 - val_loss: 0.4072 - val_acc: 0.8873\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8832\n",
      "Epoch 00017: val_loss did not improve from 0.38309\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.3867 - acc: 0.8831 - val_loss: 0.4218 - val_acc: 0.8747\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8886\n",
      "Epoch 00018: val_loss improved from 0.38309 to 0.33867, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/018-0.3387.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.3666 - acc: 0.8884 - val_loss: 0.3387 - val_acc: 0.9012\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8919\n",
      "Epoch 00019: val_loss improved from 0.33867 to 0.31896, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/019-0.3190.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.3581 - acc: 0.8918 - val_loss: 0.3190 - val_acc: 0.9173\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8943\n",
      "Epoch 00020: val_loss improved from 0.31896 to 0.30342, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/020-0.3034.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.3483 - acc: 0.8943 - val_loss: 0.3034 - val_acc: 0.9192\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8985\n",
      "Epoch 00021: val_loss did not improve from 0.30342\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.3353 - acc: 0.8985 - val_loss: 0.3980 - val_acc: 0.8838\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9008\n",
      "Epoch 00022: val_loss improved from 0.30342 to 0.29567, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/022-0.2957.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.3207 - acc: 0.9008 - val_loss: 0.2957 - val_acc: 0.9208\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9054\n",
      "Epoch 00023: val_loss did not improve from 0.29567\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.3094 - acc: 0.9053 - val_loss: 0.3553 - val_acc: 0.8956\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9080\n",
      "Epoch 00024: val_loss improved from 0.29567 to 0.25957, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/024-0.2596.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.3039 - acc: 0.9080 - val_loss: 0.2596 - val_acc: 0.9320\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9101\n",
      "Epoch 00025: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.2945 - acc: 0.9101 - val_loss: 0.2924 - val_acc: 0.9175\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9146\n",
      "Epoch 00026: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.2876 - acc: 0.9147 - val_loss: 0.2906 - val_acc: 0.9122\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9137\n",
      "Epoch 00027: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.2801 - acc: 0.9137 - val_loss: 0.2939 - val_acc: 0.9150\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9170\n",
      "Epoch 00028: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.2757 - acc: 0.9169 - val_loss: 0.3345 - val_acc: 0.8996\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9156\n",
      "Epoch 00029: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.2713 - acc: 0.9155 - val_loss: 0.2992 - val_acc: 0.9217\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2575 - acc: 0.9212\n",
      "Epoch 00030: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.2576 - acc: 0.9211 - val_loss: 0.2675 - val_acc: 0.9224\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9204\n",
      "Epoch 00031: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.2562 - acc: 0.9205 - val_loss: 0.2668 - val_acc: 0.9238\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9234\n",
      "Epoch 00032: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.2491 - acc: 0.9234 - val_loss: 0.2906 - val_acc: 0.9145\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9252\n",
      "Epoch 00033: val_loss did not improve from 0.25957\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.2429 - acc: 0.9250 - val_loss: 0.2688 - val_acc: 0.9171\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9263\n",
      "Epoch 00034: val_loss improved from 0.25957 to 0.25347, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/034-0.2535.hdf5\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.2393 - acc: 0.9263 - val_loss: 0.2535 - val_acc: 0.9278\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9274\n",
      "Epoch 00035: val_loss did not improve from 0.25347\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.2338 - acc: 0.9274 - val_loss: 0.2644 - val_acc: 0.9266\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9284\n",
      "Epoch 00036: val_loss did not improve from 0.25347\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.2284 - acc: 0.9284 - val_loss: 0.3014 - val_acc: 0.9182\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9301\n",
      "Epoch 00037: val_loss did not improve from 0.25347\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.2250 - acc: 0.9301 - val_loss: 0.2855 - val_acc: 0.9178\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9325\n",
      "Epoch 00038: val_loss did not improve from 0.25347\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.2194 - acc: 0.9325 - val_loss: 0.2663 - val_acc: 0.9248\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9326\n",
      "Epoch 00039: val_loss improved from 0.25347 to 0.24531, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/039-0.2453.hdf5\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.2156 - acc: 0.9325 - val_loss: 0.2453 - val_acc: 0.9292\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9341\n",
      "Epoch 00040: val_loss did not improve from 0.24531\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.2118 - acc: 0.9341 - val_loss: 0.2496 - val_acc: 0.9334\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9362\n",
      "Epoch 00041: val_loss did not improve from 0.24531\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.2054 - acc: 0.9363 - val_loss: 0.2685 - val_acc: 0.9248\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9370\n",
      "Epoch 00042: val_loss improved from 0.24531 to 0.22884, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/042-0.2288.hdf5\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.2026 - acc: 0.9370 - val_loss: 0.2288 - val_acc: 0.9357\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9369\n",
      "Epoch 00043: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.2044 - acc: 0.9369 - val_loss: 0.2564 - val_acc: 0.9269\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9393\n",
      "Epoch 00044: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1977 - acc: 0.9392 - val_loss: 0.2469 - val_acc: 0.9317\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9378\n",
      "Epoch 00045: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1975 - acc: 0.9378 - val_loss: 0.2465 - val_acc: 0.9306\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9407\n",
      "Epoch 00046: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1885 - acc: 0.9409 - val_loss: 0.2413 - val_acc: 0.9315\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9419\n",
      "Epoch 00047: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1847 - acc: 0.9419 - val_loss: 0.2432 - val_acc: 0.9297\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9410\n",
      "Epoch 00048: val_loss did not improve from 0.22884\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1881 - acc: 0.9409 - val_loss: 0.2898 - val_acc: 0.9126\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9435\n",
      "Epoch 00049: val_loss improved from 0.22884 to 0.22737, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/049-0.2274.hdf5\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1805 - acc: 0.9435 - val_loss: 0.2274 - val_acc: 0.9355\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9431\n",
      "Epoch 00050: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1803 - acc: 0.9431 - val_loss: 0.2333 - val_acc: 0.9352\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9441\n",
      "Epoch 00051: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1748 - acc: 0.9441 - val_loss: 0.2578 - val_acc: 0.9269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9458\n",
      "Epoch 00052: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1721 - acc: 0.9458 - val_loss: 0.2914 - val_acc: 0.9185\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9456\n",
      "Epoch 00053: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1699 - acc: 0.9456 - val_loss: 0.2567 - val_acc: 0.9276\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9477\n",
      "Epoch 00054: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.1687 - acc: 0.9477 - val_loss: 0.3040 - val_acc: 0.9087\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9473\n",
      "Epoch 00055: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1656 - acc: 0.9472 - val_loss: 0.2520 - val_acc: 0.9336\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9485\n",
      "Epoch 00056: val_loss did not improve from 0.22737\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1635 - acc: 0.9485 - val_loss: 0.2368 - val_acc: 0.9366\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9490\n",
      "Epoch 00057: val_loss improved from 0.22737 to 0.22066, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/057-0.2207.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1614 - acc: 0.9490 - val_loss: 0.2207 - val_acc: 0.9345\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9497\n",
      "Epoch 00058: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1594 - acc: 0.9498 - val_loss: 0.2301 - val_acc: 0.9355\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9483\n",
      "Epoch 00059: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1617 - acc: 0.9484 - val_loss: 0.2373 - val_acc: 0.9266\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9513\n",
      "Epoch 00060: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1543 - acc: 0.9513 - val_loss: 0.2278 - val_acc: 0.9364\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9495\n",
      "Epoch 00061: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1587 - acc: 0.9495 - val_loss: 0.2391 - val_acc: 0.9311\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9510\n",
      "Epoch 00062: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.1541 - acc: 0.9508 - val_loss: 0.2210 - val_acc: 0.9383\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9508\n",
      "Epoch 00063: val_loss did not improve from 0.22066\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1540 - acc: 0.9508 - val_loss: 0.2267 - val_acc: 0.9376\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9529\n",
      "Epoch 00064: val_loss improved from 0.22066 to 0.21734, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/064-0.2173.hdf5\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1474 - acc: 0.9529 - val_loss: 0.2173 - val_acc: 0.9415\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9554\n",
      "Epoch 00065: val_loss did not improve from 0.21734\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1407 - acc: 0.9554 - val_loss: 0.2465 - val_acc: 0.9299\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9537\n",
      "Epoch 00066: val_loss improved from 0.21734 to 0.21076, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/066-0.2108.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1447 - acc: 0.9537 - val_loss: 0.2108 - val_acc: 0.9385\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9539\n",
      "Epoch 00067: val_loss did not improve from 0.21076\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1468 - acc: 0.9539 - val_loss: 0.2600 - val_acc: 0.9285\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9549\n",
      "Epoch 00068: val_loss improved from 0.21076 to 0.20823, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/068-0.2082.hdf5\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1418 - acc: 0.9549 - val_loss: 0.2082 - val_acc: 0.9420\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9548\n",
      "Epoch 00069: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1409 - acc: 0.9549 - val_loss: 0.2138 - val_acc: 0.9413\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9574\n",
      "Epoch 00070: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1382 - acc: 0.9574 - val_loss: 0.2271 - val_acc: 0.9376\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9577\n",
      "Epoch 00071: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1323 - acc: 0.9577 - val_loss: 0.2271 - val_acc: 0.9317\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9587\n",
      "Epoch 00072: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1300 - acc: 0.9587 - val_loss: 0.2315 - val_acc: 0.9364\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9580\n",
      "Epoch 00073: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1323 - acc: 0.9580 - val_loss: 0.2651 - val_acc: 0.9283\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9564\n",
      "Epoch 00074: val_loss did not improve from 0.20823\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1356 - acc: 0.9564 - val_loss: 0.2625 - val_acc: 0.9266\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9584\n",
      "Epoch 00075: val_loss improved from 0.20823 to 0.19534, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/075-0.1953.hdf5\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1282 - acc: 0.9583 - val_loss: 0.1953 - val_acc: 0.9481\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9575\n",
      "Epoch 00076: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.1277 - acc: 0.9576 - val_loss: 0.2370 - val_acc: 0.9324\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9582\n",
      "Epoch 00077: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1283 - acc: 0.9582 - val_loss: 0.2389 - val_acc: 0.9329\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9597\n",
      "Epoch 00078: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1275 - acc: 0.9596 - val_loss: 0.2417 - val_acc: 0.9287\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9606\n",
      "Epoch 00079: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.1231 - acc: 0.9606 - val_loss: 0.2260 - val_acc: 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9612\n",
      "Epoch 00080: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1209 - acc: 0.9612 - val_loss: 0.2759 - val_acc: 0.9236\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9593\n",
      "Epoch 00081: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.1260 - acc: 0.9594 - val_loss: 0.2080 - val_acc: 0.9385\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9626\n",
      "Epoch 00082: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.1152 - acc: 0.9626 - val_loss: 0.2743 - val_acc: 0.9278\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9614\n",
      "Epoch 00083: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.1191 - acc: 0.9614 - val_loss: 0.2389 - val_acc: 0.9329\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9625\n",
      "Epoch 00084: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1190 - acc: 0.9625 - val_loss: 0.2359 - val_acc: 0.9392\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9629\n",
      "Epoch 00085: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.1138 - acc: 0.9630 - val_loss: 0.2048 - val_acc: 0.9411\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9641\n",
      "Epoch 00086: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1135 - acc: 0.9641 - val_loss: 0.2205 - val_acc: 0.9378\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9641\n",
      "Epoch 00087: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1127 - acc: 0.9642 - val_loss: 0.2300 - val_acc: 0.9373\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9648\n",
      "Epoch 00088: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1096 - acc: 0.9648 - val_loss: 0.2554 - val_acc: 0.9327\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9634\n",
      "Epoch 00089: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1121 - acc: 0.9634 - val_loss: 0.2199 - val_acc: 0.9411\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9636\n",
      "Epoch 00090: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 390us/sample - loss: 0.1121 - acc: 0.9636 - val_loss: 0.2439 - val_acc: 0.9294\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9653\n",
      "Epoch 00091: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1098 - acc: 0.9653 - val_loss: 0.2354 - val_acc: 0.9371\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9653\n",
      "Epoch 00092: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1062 - acc: 0.9654 - val_loss: 0.3351 - val_acc: 0.9140\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9662\n",
      "Epoch 00093: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1044 - acc: 0.9661 - val_loss: 0.2232 - val_acc: 0.9390\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9667\n",
      "Epoch 00094: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.1049 - acc: 0.9667 - val_loss: 0.2085 - val_acc: 0.9434\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9669\n",
      "Epoch 00095: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1036 - acc: 0.9670 - val_loss: 0.2330 - val_acc: 0.9343\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9665\n",
      "Epoch 00096: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 386us/sample - loss: 0.1061 - acc: 0.9665 - val_loss: 0.2231 - val_acc: 0.9429\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9673\n",
      "Epoch 00097: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.1006 - acc: 0.9673 - val_loss: 0.2236 - val_acc: 0.9408\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9671\n",
      "Epoch 00098: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.0991 - acc: 0.9671 - val_loss: 0.2363 - val_acc: 0.9378\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9666\n",
      "Epoch 00099: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.1013 - acc: 0.9666 - val_loss: 0.2674 - val_acc: 0.9222\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9676\n",
      "Epoch 00100: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.0991 - acc: 0.9677 - val_loss: 0.2373 - val_acc: 0.9369\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9688\n",
      "Epoch 00101: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.0988 - acc: 0.9688 - val_loss: 0.2295 - val_acc: 0.9399\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9661\n",
      "Epoch 00102: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.1011 - acc: 0.9661 - val_loss: 0.2334 - val_acc: 0.9355\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9698\n",
      "Epoch 00103: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.0940 - acc: 0.9697 - val_loss: 0.2361 - val_acc: 0.9364\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9689\n",
      "Epoch 00104: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0984 - acc: 0.9689 - val_loss: 0.2167 - val_acc: 0.9404\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9658\n",
      "Epoch 00105: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.1019 - acc: 0.9658 - val_loss: 0.2202 - val_acc: 0.9392\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9694\n",
      "Epoch 00106: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0940 - acc: 0.9694 - val_loss: 0.2279 - val_acc: 0.9317\n",
      "Epoch 107/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9702\n",
      "Epoch 00107: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0918 - acc: 0.9701 - val_loss: 0.2817 - val_acc: 0.9245\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9707\n",
      "Epoch 00108: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0925 - acc: 0.9706 - val_loss: 0.2283 - val_acc: 0.9387\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9688\n",
      "Epoch 00109: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0939 - acc: 0.9688 - val_loss: 0.2738 - val_acc: 0.9227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9690\n",
      "Epoch 00110: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0924 - acc: 0.9691 - val_loss: 0.2368 - val_acc: 0.9362\n",
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9720\n",
      "Epoch 00111: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0889 - acc: 0.9720 - val_loss: 0.2305 - val_acc: 0.9439\n",
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9704\n",
      "Epoch 00112: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0906 - acc: 0.9703 - val_loss: 0.2018 - val_acc: 0.9448\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9707\n",
      "Epoch 00113: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0883 - acc: 0.9706 - val_loss: 0.2361 - val_acc: 0.9371\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9719\n",
      "Epoch 00114: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0886 - acc: 0.9719 - val_loss: 0.2334 - val_acc: 0.9366\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9731\n",
      "Epoch 00115: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.0855 - acc: 0.9731 - val_loss: 0.2062 - val_acc: 0.9446\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9716\n",
      "Epoch 00116: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0884 - acc: 0.9716 - val_loss: 0.2879 - val_acc: 0.9252\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9726\n",
      "Epoch 00117: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0851 - acc: 0.9726 - val_loss: 0.2173 - val_acc: 0.9408\n",
      "Epoch 118/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9729\n",
      "Epoch 00118: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0845 - acc: 0.9729 - val_loss: 0.2283 - val_acc: 0.9397\n",
      "Epoch 119/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9734\n",
      "Epoch 00119: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0835 - acc: 0.9733 - val_loss: 0.2237 - val_acc: 0.9392\n",
      "Epoch 120/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9750\n",
      "Epoch 00120: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0804 - acc: 0.9749 - val_loss: 0.3242 - val_acc: 0.9201\n",
      "Epoch 121/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9715\n",
      "Epoch 00121: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 385us/sample - loss: 0.0869 - acc: 0.9715 - val_loss: 0.2725 - val_acc: 0.9257\n",
      "Epoch 122/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9741\n",
      "Epoch 00122: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0825 - acc: 0.9739 - val_loss: 0.2279 - val_acc: 0.9434\n",
      "Epoch 123/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9713\n",
      "Epoch 00123: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.0870 - acc: 0.9714 - val_loss: 0.2119 - val_acc: 0.9453\n",
      "Epoch 124/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9743\n",
      "Epoch 00124: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0776 - acc: 0.9742 - val_loss: 0.2132 - val_acc: 0.9446\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9753\n",
      "Epoch 00125: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0785 - acc: 0.9753 - val_loss: 0.2479 - val_acc: 0.9322\n",
      "Epoch 126/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9741\n",
      "Epoch 00126: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0805 - acc: 0.9741 - val_loss: 0.2724 - val_acc: 0.9348\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9758\n",
      "Epoch 00127: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0792 - acc: 0.9758 - val_loss: 0.2326 - val_acc: 0.9399\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9750\n",
      "Epoch 00128: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0769 - acc: 0.9749 - val_loss: 0.2245 - val_acc: 0.9413\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9757\n",
      "Epoch 00129: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0763 - acc: 0.9757 - val_loss: 0.2396 - val_acc: 0.9385\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9753\n",
      "Epoch 00130: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0769 - acc: 0.9753 - val_loss: 0.2176 - val_acc: 0.9436\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9757\n",
      "Epoch 00131: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0760 - acc: 0.9757 - val_loss: 0.2245 - val_acc: 0.9427\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9770\n",
      "Epoch 00132: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0715 - acc: 0.9770 - val_loss: 0.2276 - val_acc: 0.9434\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9741\n",
      "Epoch 00133: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0773 - acc: 0.9741 - val_loss: 0.2445 - val_acc: 0.9385\n",
      "Epoch 134/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9767\n",
      "Epoch 00134: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0731 - acc: 0.9767 - val_loss: 0.2386 - val_acc: 0.9380\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9756\n",
      "Epoch 00135: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0754 - acc: 0.9756 - val_loss: 0.2258 - val_acc: 0.9399\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9769\n",
      "Epoch 00136: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0707 - acc: 0.9769 - val_loss: 0.2170 - val_acc: 0.9427\n",
      "Epoch 137/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9761\n",
      "Epoch 00137: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0743 - acc: 0.9761 - val_loss: 0.2484 - val_acc: 0.9376\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9765\n",
      "Epoch 00138: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0706 - acc: 0.9765 - val_loss: 0.2476 - val_acc: 0.9390\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9748\n",
      "Epoch 00139: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0759 - acc: 0.9748 - val_loss: 0.2262 - val_acc: 0.9439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9787\n",
      "Epoch 00140: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0678 - acc: 0.9786 - val_loss: 0.2229 - val_acc: 0.9394\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9773\n",
      "Epoch 00141: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0729 - acc: 0.9773 - val_loss: 0.2063 - val_acc: 0.9462\n",
      "Epoch 142/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9788\n",
      "Epoch 00142: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0664 - acc: 0.9788 - val_loss: 0.4502 - val_acc: 0.8910\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9775\n",
      "Epoch 00143: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0713 - acc: 0.9774 - val_loss: 0.2536 - val_acc: 0.9397\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9759\n",
      "Epoch 00144: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.0784 - acc: 0.9758 - val_loss: 0.2116 - val_acc: 0.9443\n",
      "Epoch 145/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9777\n",
      "Epoch 00145: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0669 - acc: 0.9777 - val_loss: 0.2195 - val_acc: 0.9436\n",
      "Epoch 146/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9792\n",
      "Epoch 00146: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0647 - acc: 0.9791 - val_loss: 0.2549 - val_acc: 0.9343\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9775\n",
      "Epoch 00147: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0678 - acc: 0.9775 - val_loss: 0.2496 - val_acc: 0.9373\n",
      "Epoch 148/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9785\n",
      "Epoch 00148: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0649 - acc: 0.9784 - val_loss: 0.2467 - val_acc: 0.9345\n",
      "Epoch 149/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9791\n",
      "Epoch 00149: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0666 - acc: 0.9792 - val_loss: 0.2234 - val_acc: 0.9406\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9785\n",
      "Epoch 00150: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0644 - acc: 0.9785 - val_loss: 0.2296 - val_acc: 0.9397\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9774\n",
      "Epoch 00151: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0706 - acc: 0.9774 - val_loss: 0.2576 - val_acc: 0.9394\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9801\n",
      "Epoch 00152: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0617 - acc: 0.9801 - val_loss: 0.2563 - val_acc: 0.9366\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9793\n",
      "Epoch 00153: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0633 - acc: 0.9793 - val_loss: 0.2392 - val_acc: 0.9408\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9787\n",
      "Epoch 00154: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0661 - acc: 0.9787 - val_loss: 0.2262 - val_acc: 0.9436\n",
      "Epoch 155/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9822\n",
      "Epoch 00155: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0580 - acc: 0.9823 - val_loss: 0.2940 - val_acc: 0.9250\n",
      "Epoch 156/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9800\n",
      "Epoch 00156: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0607 - acc: 0.9800 - val_loss: 0.2480 - val_acc: 0.9422\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9786\n",
      "Epoch 00157: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0664 - acc: 0.9785 - val_loss: 0.3569 - val_acc: 0.9201\n",
      "Epoch 158/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9788\n",
      "Epoch 00158: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0666 - acc: 0.9789 - val_loss: 0.5028 - val_acc: 0.8749\n",
      "Epoch 159/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9809\n",
      "Epoch 00159: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0595 - acc: 0.9809 - val_loss: 0.2311 - val_acc: 0.9404\n",
      "Epoch 160/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9814\n",
      "Epoch 00160: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0602 - acc: 0.9814 - val_loss: 0.2297 - val_acc: 0.9397\n",
      "Epoch 161/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9801\n",
      "Epoch 00161: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0599 - acc: 0.9802 - val_loss: 0.2218 - val_acc: 0.9411\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9809\n",
      "Epoch 00162: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0598 - acc: 0.9809 - val_loss: 0.2581 - val_acc: 0.9366\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9800\n",
      "Epoch 00163: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0611 - acc: 0.9800 - val_loss: 0.2868 - val_acc: 0.9276\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9794\n",
      "Epoch 00164: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0631 - acc: 0.9794 - val_loss: 0.3102 - val_acc: 0.9224\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9812\n",
      "Epoch 00165: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0573 - acc: 0.9811 - val_loss: 0.2406 - val_acc: 0.9415\n",
      "Epoch 166/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9800\n",
      "Epoch 00166: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0601 - acc: 0.9800 - val_loss: 0.2319 - val_acc: 0.9401\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9816\n",
      "Epoch 00167: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0593 - acc: 0.9816 - val_loss: 0.2696 - val_acc: 0.9338\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9804\n",
      "Epoch 00168: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0610 - acc: 0.9803 - val_loss: 0.2772 - val_acc: 0.9334\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9806\n",
      "Epoch 00169: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0601 - acc: 0.9806 - val_loss: 0.2357 - val_acc: 0.9439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9816\n",
      "Epoch 00170: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0579 - acc: 0.9816 - val_loss: 0.2316 - val_acc: 0.9383\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9808\n",
      "Epoch 00171: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0568 - acc: 0.9808 - val_loss: 0.2190 - val_acc: 0.9439\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9820\n",
      "Epoch 00172: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0567 - acc: 0.9819 - val_loss: 0.2512 - val_acc: 0.9441\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9815\n",
      "Epoch 00173: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0556 - acc: 0.9815 - val_loss: 0.2284 - val_acc: 0.9411\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9821\n",
      "Epoch 00174: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0558 - acc: 0.9821 - val_loss: 0.3552 - val_acc: 0.9133\n",
      "Epoch 175/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9821\n",
      "Epoch 00175: val_loss did not improve from 0.19534\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0561 - acc: 0.9821 - val_loss: 0.2391 - val_acc: 0.9406\n",
      "\n",
      "1D_CNN_BN_DO_4_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 390us/sample - loss: 0.2656 - acc: 0.9238\n",
      "Loss: 0.26559395780196937 Accuracy: 0.92377985\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3234 - acc: 0.2957\n",
      "Epoch 00001: val_loss improved from inf to 1.55178, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/001-1.5518.hdf5\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 2.3233 - acc: 0.2957 - val_loss: 1.5518 - val_acc: 0.4992\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4253 - acc: 0.5412\n",
      "Epoch 00002: val_loss improved from 1.55178 to 0.99858, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/002-0.9986.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.4256 - acc: 0.5412 - val_loss: 0.9986 - val_acc: 0.6781\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0699 - acc: 0.6581\n",
      "Epoch 00003: val_loss improved from 0.99858 to 0.75564, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/003-0.7556.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.0694 - acc: 0.6582 - val_loss: 0.7556 - val_acc: 0.7617\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8372 - acc: 0.7361\n",
      "Epoch 00004: val_loss improved from 0.75564 to 0.55334, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/004-0.5533.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.8372 - acc: 0.7361 - val_loss: 0.5533 - val_acc: 0.8318\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6855 - acc: 0.7836\n",
      "Epoch 00005: val_loss improved from 0.55334 to 0.49779, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/005-0.4978.hdf5\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.6859 - acc: 0.7835 - val_loss: 0.4978 - val_acc: 0.8544\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5797 - acc: 0.8190\n",
      "Epoch 00006: val_loss improved from 0.49779 to 0.41374, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/006-0.4137.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.5797 - acc: 0.8190 - val_loss: 0.4137 - val_acc: 0.8751\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.8421\n",
      "Epoch 00007: val_loss improved from 0.41374 to 0.36017, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/007-0.3602.hdf5\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.5063 - acc: 0.8422 - val_loss: 0.3602 - val_acc: 0.8910\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4536 - acc: 0.8586\n",
      "Epoch 00008: val_loss improved from 0.36017 to 0.35755, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/008-0.3575.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.4536 - acc: 0.8586 - val_loss: 0.3575 - val_acc: 0.8952\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8728\n",
      "Epoch 00009: val_loss improved from 0.35755 to 0.31313, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/009-0.3131.hdf5\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.4101 - acc: 0.8727 - val_loss: 0.3131 - val_acc: 0.9138\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8815\n",
      "Epoch 00010: val_loss did not improve from 0.31313\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.3807 - acc: 0.8815 - val_loss: 0.3200 - val_acc: 0.9038\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8906\n",
      "Epoch 00011: val_loss improved from 0.31313 to 0.29475, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/011-0.2947.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.3528 - acc: 0.8905 - val_loss: 0.2947 - val_acc: 0.9175\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8977\n",
      "Epoch 00012: val_loss improved from 0.29475 to 0.27143, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/012-0.2714.hdf5\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.3288 - acc: 0.8977 - val_loss: 0.2714 - val_acc: 0.9173\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9032\n",
      "Epoch 00013: val_loss improved from 0.27143 to 0.26826, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/013-0.2683.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.3108 - acc: 0.9032 - val_loss: 0.2683 - val_acc: 0.9196\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9088\n",
      "Epoch 00014: val_loss improved from 0.26826 to 0.25094, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/014-0.2509.hdf5\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.2911 - acc: 0.9088 - val_loss: 0.2509 - val_acc: 0.9241\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9138\n",
      "Epoch 00015: val_loss did not improve from 0.25094\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.2740 - acc: 0.9138 - val_loss: 0.2839 - val_acc: 0.9136\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9194\n",
      "Epoch 00016: val_loss did not improve from 0.25094\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.2587 - acc: 0.9194 - val_loss: 0.3009 - val_acc: 0.9136\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9223\n",
      "Epoch 00017: val_loss improved from 0.25094 to 0.24071, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/017-0.2407.hdf5\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.2467 - acc: 0.9222 - val_loss: 0.2407 - val_acc: 0.9273\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9262\n",
      "Epoch 00018: val_loss did not improve from 0.24071\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.2341 - acc: 0.9262 - val_loss: 0.2423 - val_acc: 0.9283\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9297\n",
      "Epoch 00019: val_loss improved from 0.24071 to 0.22239, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/019-0.2224.hdf5\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.2233 - acc: 0.9296 - val_loss: 0.2224 - val_acc: 0.9334\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9337\n",
      "Epoch 00020: val_loss did not improve from 0.22239\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.2113 - acc: 0.9337 - val_loss: 0.2253 - val_acc: 0.9345\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9364\n",
      "Epoch 00021: val_loss did not improve from 0.22239\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.2037 - acc: 0.9364 - val_loss: 0.2605 - val_acc: 0.9285\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9378\n",
      "Epoch 00022: val_loss improved from 0.22239 to 0.22137, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/022-0.2214.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1988 - acc: 0.9377 - val_loss: 0.2214 - val_acc: 0.9322\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9439\n",
      "Epoch 00023: val_loss improved from 0.22137 to 0.21105, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/023-0.2110.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1821 - acc: 0.9439 - val_loss: 0.2110 - val_acc: 0.9357\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9420\n",
      "Epoch 00024: val_loss improved from 0.21105 to 0.21099, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/024-0.2110.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.1843 - acc: 0.9420 - val_loss: 0.2110 - val_acc: 0.9373\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9451\n",
      "Epoch 00025: val_loss improved from 0.21099 to 0.19841, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/025-0.1984.hdf5\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.1766 - acc: 0.9451 - val_loss: 0.1984 - val_acc: 0.9429\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9458\n",
      "Epoch 00026: val_loss did not improve from 0.19841\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.1698 - acc: 0.9458 - val_loss: 0.2030 - val_acc: 0.9399\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9489\n",
      "Epoch 00027: val_loss improved from 0.19841 to 0.18234, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/027-0.1823.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.1612 - acc: 0.9489 - val_loss: 0.1823 - val_acc: 0.9450\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9494\n",
      "Epoch 00028: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1614 - acc: 0.9495 - val_loss: 0.1872 - val_acc: 0.9474\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9518\n",
      "Epoch 00029: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.1521 - acc: 0.9519 - val_loss: 0.2107 - val_acc: 0.9359\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9535\n",
      "Epoch 00030: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1458 - acc: 0.9534 - val_loss: 0.1971 - val_acc: 0.9427\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9548\n",
      "Epoch 00031: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1466 - acc: 0.9547 - val_loss: 0.1987 - val_acc: 0.9443\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9573\n",
      "Epoch 00032: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1357 - acc: 0.9573 - val_loss: 0.2117 - val_acc: 0.9383\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9553\n",
      "Epoch 00033: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.1396 - acc: 0.9553 - val_loss: 0.1958 - val_acc: 0.9441\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9592\n",
      "Epoch 00034: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1290 - acc: 0.9591 - val_loss: 0.1871 - val_acc: 0.9455\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9602\n",
      "Epoch 00035: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.1293 - acc: 0.9602 - val_loss: 0.1945 - val_acc: 0.9453\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9620\n",
      "Epoch 00036: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1190 - acc: 0.9621 - val_loss: 0.1911 - val_acc: 0.9415\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9619\n",
      "Epoch 00037: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.1205 - acc: 0.9619 - val_loss: 0.2773 - val_acc: 0.9189\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9630\n",
      "Epoch 00038: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.1127 - acc: 0.9630 - val_loss: 0.1934 - val_acc: 0.9474\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9628\n",
      "Epoch 00039: val_loss did not improve from 0.18234\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.1142 - acc: 0.9628 - val_loss: 0.1890 - val_acc: 0.9429\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9650\n",
      "Epoch 00040: val_loss improved from 0.18234 to 0.18037, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/040-0.1804.hdf5\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.1082 - acc: 0.9650 - val_loss: 0.1804 - val_acc: 0.9471\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9669\n",
      "Epoch 00041: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.1045 - acc: 0.9669 - val_loss: 0.2207 - val_acc: 0.9446\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9672\n",
      "Epoch 00042: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.1028 - acc: 0.9672 - val_loss: 0.1979 - val_acc: 0.9427\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9668\n",
      "Epoch 00043: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.1064 - acc: 0.9668 - val_loss: 0.1809 - val_acc: 0.9492\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9697\n",
      "Epoch 00044: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0975 - acc: 0.9696 - val_loss: 0.1825 - val_acc: 0.9499\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9683\n",
      "Epoch 00045: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0960 - acc: 0.9683 - val_loss: 0.2059 - val_acc: 0.9481\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9702\n",
      "Epoch 00046: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0936 - acc: 0.9702 - val_loss: 0.2059 - val_acc: 0.9420\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9706\n",
      "Epoch 00047: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0916 - acc: 0.9706 - val_loss: 0.1934 - val_acc: 0.9476\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9713\n",
      "Epoch 00048: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.0896 - acc: 0.9713 - val_loss: 0.1847 - val_acc: 0.9464\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9706\n",
      "Epoch 00049: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0919 - acc: 0.9706 - val_loss: 0.2219 - val_acc: 0.9406\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9722\n",
      "Epoch 00050: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0864 - acc: 0.9722 - val_loss: 0.1869 - val_acc: 0.9471\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9740\n",
      "Epoch 00051: val_loss did not improve from 0.18037\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0824 - acc: 0.9740 - val_loss: 0.2145 - val_acc: 0.9425\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9758\n",
      "Epoch 00052: val_loss improved from 0.18037 to 0.17716, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/052-0.1772.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0765 - acc: 0.9758 - val_loss: 0.1772 - val_acc: 0.9485\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9749\n",
      "Epoch 00053: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0798 - acc: 0.9749 - val_loss: 0.1947 - val_acc: 0.9506\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9751\n",
      "Epoch 00054: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0775 - acc: 0.9751 - val_loss: 0.2322 - val_acc: 0.9345\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9747\n",
      "Epoch 00055: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0792 - acc: 0.9747 - val_loss: 0.1989 - val_acc: 0.9464\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9775\n",
      "Epoch 00056: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0721 - acc: 0.9775 - val_loss: 0.2690 - val_acc: 0.9297\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9748\n",
      "Epoch 00057: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0786 - acc: 0.9748 - val_loss: 0.1966 - val_acc: 0.9471\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9777\n",
      "Epoch 00058: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0711 - acc: 0.9777 - val_loss: 0.2035 - val_acc: 0.9478\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9788\n",
      "Epoch 00059: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0665 - acc: 0.9788 - val_loss: 0.2020 - val_acc: 0.9476\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9766\n",
      "Epoch 00060: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0731 - acc: 0.9766 - val_loss: 0.2264 - val_acc: 0.9425\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9797\n",
      "Epoch 00061: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0650 - acc: 0.9798 - val_loss: 0.1945 - val_acc: 0.9474\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9793\n",
      "Epoch 00062: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0662 - acc: 0.9794 - val_loss: 0.1863 - val_acc: 0.9478\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9789\n",
      "Epoch 00063: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0663 - acc: 0.9789 - val_loss: 0.2022 - val_acc: 0.9425\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9809\n",
      "Epoch 00064: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0608 - acc: 0.9809 - val_loss: 0.2578 - val_acc: 0.9383\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9808\n",
      "Epoch 00065: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0612 - acc: 0.9808 - val_loss: 0.2425 - val_acc: 0.9369\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9812\n",
      "Epoch 00066: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0592 - acc: 0.9812 - val_loss: 0.1826 - val_acc: 0.9485\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9777\n",
      "Epoch 00067: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0698 - acc: 0.9777 - val_loss: 0.1963 - val_acc: 0.9488\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9816\n",
      "Epoch 00068: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0590 - acc: 0.9816 - val_loss: 0.2073 - val_acc: 0.9504\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9814\n",
      "Epoch 00069: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0583 - acc: 0.9814 - val_loss: 0.2067 - val_acc: 0.9478\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9828\n",
      "Epoch 00070: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0548 - acc: 0.9828 - val_loss: 0.2569 - val_acc: 0.9359\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9787\n",
      "Epoch 00071: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.0668 - acc: 0.9787 - val_loss: 0.1957 - val_acc: 0.9509\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9831\n",
      "Epoch 00072: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0532 - acc: 0.9830 - val_loss: 0.2168 - val_acc: 0.9471\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9843\n",
      "Epoch 00073: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0499 - acc: 0.9842 - val_loss: 0.1991 - val_acc: 0.9506\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9808\n",
      "Epoch 00074: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0629 - acc: 0.9808 - val_loss: 0.2115 - val_acc: 0.9485\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9819\n",
      "Epoch 00075: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0563 - acc: 0.9819 - val_loss: 0.2176 - val_acc: 0.9455\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9858\n",
      "Epoch 00076: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0469 - acc: 0.9858 - val_loss: 0.2142 - val_acc: 0.9441\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9854\n",
      "Epoch 00077: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0477 - acc: 0.9854 - val_loss: 0.2744 - val_acc: 0.9352\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9805\n",
      "Epoch 00078: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0609 - acc: 0.9805 - val_loss: 0.2422 - val_acc: 0.9376\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9852\n",
      "Epoch 00079: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0480 - acc: 0.9852 - val_loss: 0.2236 - val_acc: 0.9457\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9830\n",
      "Epoch 00080: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0550 - acc: 0.9830 - val_loss: 0.1964 - val_acc: 0.9481\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9866\n",
      "Epoch 00081: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0433 - acc: 0.9866 - val_loss: 0.2081 - val_acc: 0.9439\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9853\n",
      "Epoch 00082: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0500 - acc: 0.9853 - val_loss: 0.2175 - val_acc: 0.9457\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9848\n",
      "Epoch 00083: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0514 - acc: 0.9848 - val_loss: 0.1992 - val_acc: 0.9502\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9869\n",
      "Epoch 00084: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0431 - acc: 0.9867 - val_loss: 0.2306 - val_acc: 0.9425\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9834\n",
      "Epoch 00085: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0526 - acc: 0.9834 - val_loss: 0.2332 - val_acc: 0.9418\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9860\n",
      "Epoch 00086: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0459 - acc: 0.9860 - val_loss: 0.1997 - val_acc: 0.9522\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9869\n",
      "Epoch 00087: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0421 - acc: 0.9869 - val_loss: 0.3272 - val_acc: 0.9206\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9846\n",
      "Epoch 00088: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0493 - acc: 0.9846 - val_loss: 0.1875 - val_acc: 0.9567\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9878\n",
      "Epoch 00089: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0400 - acc: 0.9877 - val_loss: 0.2205 - val_acc: 0.9495\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9847\n",
      "Epoch 00090: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0482 - acc: 0.9847 - val_loss: 0.2416 - val_acc: 0.9460\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9885\n",
      "Epoch 00091: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0386 - acc: 0.9885 - val_loss: 0.1970 - val_acc: 0.9532\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9871\n",
      "Epoch 00092: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0420 - acc: 0.9871 - val_loss: 0.2126 - val_acc: 0.9497\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9878\n",
      "Epoch 00093: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0389 - acc: 0.9878 - val_loss: 0.2442 - val_acc: 0.9455\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9878\n",
      "Epoch 00094: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0385 - acc: 0.9878 - val_loss: 0.2150 - val_acc: 0.9464\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9886\n",
      "Epoch 00095: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0378 - acc: 0.9887 - val_loss: 0.2282 - val_acc: 0.9485\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9876\n",
      "Epoch 00096: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0404 - acc: 0.9876 - val_loss: 0.2201 - val_acc: 0.9464\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9889\n",
      "Epoch 00097: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0352 - acc: 0.9889 - val_loss: 0.2642 - val_acc: 0.9315\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9890\n",
      "Epoch 00098: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0360 - acc: 0.9889 - val_loss: 0.2421 - val_acc: 0.9457\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9872\n",
      "Epoch 00099: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0447 - acc: 0.9871 - val_loss: 0.2372 - val_acc: 0.9425\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9887\n",
      "Epoch 00100: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0383 - acc: 0.9887 - val_loss: 0.2194 - val_acc: 0.9513\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9908\n",
      "Epoch 00101: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0306 - acc: 0.9908 - val_loss: 0.2318 - val_acc: 0.9467\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9886\n",
      "Epoch 00102: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0367 - acc: 0.9887 - val_loss: 0.2480 - val_acc: 0.9441\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9891\n",
      "Epoch 00103: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0365 - acc: 0.9891 - val_loss: 0.2151 - val_acc: 0.9522\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9892\n",
      "Epoch 00104: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0367 - acc: 0.9892 - val_loss: 0.2184 - val_acc: 0.9488\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9884\n",
      "Epoch 00105: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0383 - acc: 0.9884 - val_loss: 0.2271 - val_acc: 0.9464\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9903\n",
      "Epoch 00106: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0321 - acc: 0.9902 - val_loss: 0.2385 - val_acc: 0.9443\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9869\n",
      "Epoch 00107: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0426 - acc: 0.9869 - val_loss: 0.2087 - val_acc: 0.9527\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9910\n",
      "Epoch 00108: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0293 - acc: 0.9910 - val_loss: 0.2069 - val_acc: 0.9513\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9900\n",
      "Epoch 00109: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0325 - acc: 0.9900 - val_loss: 0.2146 - val_acc: 0.9525\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9906\n",
      "Epoch 00110: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0306 - acc: 0.9906 - val_loss: 0.2335 - val_acc: 0.9478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9884\n",
      "Epoch 00111: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0366 - acc: 0.9885 - val_loss: 0.2469 - val_acc: 0.9448\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9921\n",
      "Epoch 00112: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0286 - acc: 0.9920 - val_loss: 0.2260 - val_acc: 0.9495\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9877\n",
      "Epoch 00113: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0392 - acc: 0.9877 - val_loss: 0.2094 - val_acc: 0.9529\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9895\n",
      "Epoch 00114: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0347 - acc: 0.9895 - val_loss: 0.2273 - val_acc: 0.9474\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9898\n",
      "Epoch 00115: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0334 - acc: 0.9898 - val_loss: 0.1880 - val_acc: 0.9567\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9897\n",
      "Epoch 00116: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0335 - acc: 0.9897 - val_loss: 0.2107 - val_acc: 0.9520\n",
      "Epoch 117/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9900\n",
      "Epoch 00117: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0349 - acc: 0.9899 - val_loss: 0.2215 - val_acc: 0.9509\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9928\n",
      "Epoch 00118: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.0255 - acc: 0.9928 - val_loss: 0.2291 - val_acc: 0.9469\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9916\n",
      "Epoch 00119: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0293 - acc: 0.9916 - val_loss: 0.2227 - val_acc: 0.9492\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9922\n",
      "Epoch 00120: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0272 - acc: 0.9923 - val_loss: 0.2445 - val_acc: 0.9467\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9919\n",
      "Epoch 00121: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0290 - acc: 0.9918 - val_loss: 0.3074 - val_acc: 0.9399\n",
      "Epoch 122/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9911\n",
      "Epoch 00122: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0299 - acc: 0.9910 - val_loss: 0.2765 - val_acc: 0.9394\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9881\n",
      "Epoch 00123: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0405 - acc: 0.9881 - val_loss: 0.2056 - val_acc: 0.9520\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9923\n",
      "Epoch 00124: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0265 - acc: 0.9923 - val_loss: 0.2044 - val_acc: 0.9550\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9922\n",
      "Epoch 00125: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0269 - acc: 0.9922 - val_loss: 0.2224 - val_acc: 0.9522\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9921\n",
      "Epoch 00126: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0265 - acc: 0.9921 - val_loss: 0.2254 - val_acc: 0.9520\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9917\n",
      "Epoch 00127: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0283 - acc: 0.9917 - val_loss: 0.2549 - val_acc: 0.9522\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9920\n",
      "Epoch 00128: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0279 - acc: 0.9920 - val_loss: 0.2527 - val_acc: 0.9464\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9910\n",
      "Epoch 00129: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 0.0296 - acc: 0.9910 - val_loss: 0.2648 - val_acc: 0.9492\n",
      "Epoch 130/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9932\n",
      "Epoch 00130: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0236 - acc: 0.9932 - val_loss: 0.2233 - val_acc: 0.9560\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9939\n",
      "Epoch 00131: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0231 - acc: 0.9939 - val_loss: 0.2283 - val_acc: 0.9534\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9927\n",
      "Epoch 00132: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.0255 - acc: 0.9927 - val_loss: 0.2718 - val_acc: 0.9408\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9902\n",
      "Epoch 00133: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0340 - acc: 0.9902 - val_loss: 0.2278 - val_acc: 0.9525\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9929\n",
      "Epoch 00134: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0246 - acc: 0.9929 - val_loss: 0.2826 - val_acc: 0.9420\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9894\n",
      "Epoch 00135: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0355 - acc: 0.9895 - val_loss: 0.2243 - val_acc: 0.9543\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9940\n",
      "Epoch 00136: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0218 - acc: 0.9940 - val_loss: 0.2508 - val_acc: 0.9506\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9945\n",
      "Epoch 00137: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0199 - acc: 0.9945 - val_loss: 0.2710 - val_acc: 0.9464\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9888\n",
      "Epoch 00138: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0368 - acc: 0.9888 - val_loss: 0.2438 - val_acc: 0.9474\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9938\n",
      "Epoch 00139: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0213 - acc: 0.9938 - val_loss: 0.2811 - val_acc: 0.9453\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9938\n",
      "Epoch 00140: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0216 - acc: 0.9937 - val_loss: 0.2452 - val_acc: 0.9495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9906\n",
      "Epoch 00141: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0310 - acc: 0.9906 - val_loss: 0.2500 - val_acc: 0.9467\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9934\n",
      "Epoch 00142: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0232 - acc: 0.9934 - val_loss: 0.2361 - val_acc: 0.9529\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9926\n",
      "Epoch 00143: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0252 - acc: 0.9926 - val_loss: 0.2291 - val_acc: 0.9497\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9937\n",
      "Epoch 00144: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0223 - acc: 0.9937 - val_loss: 0.2992 - val_acc: 0.9429\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9941\n",
      "Epoch 00145: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0212 - acc: 0.9941 - val_loss: 0.2369 - val_acc: 0.9488\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9926\n",
      "Epoch 00146: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0249 - acc: 0.9926 - val_loss: 0.2512 - val_acc: 0.9488\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9931\n",
      "Epoch 00147: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0229 - acc: 0.9931 - val_loss: 0.2561 - val_acc: 0.9502\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9901\n",
      "Epoch 00148: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0339 - acc: 0.9901 - val_loss: 0.2310 - val_acc: 0.9506\n",
      "Epoch 149/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9941\n",
      "Epoch 00149: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0218 - acc: 0.9941 - val_loss: 0.2715 - val_acc: 0.9443\n",
      "Epoch 150/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9939\n",
      "Epoch 00150: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0206 - acc: 0.9939 - val_loss: 0.2691 - val_acc: 0.9432\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9924\n",
      "Epoch 00151: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0251 - acc: 0.9923 - val_loss: 0.2616 - val_acc: 0.9455\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9906\n",
      "Epoch 00152: val_loss did not improve from 0.17716\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0305 - acc: 0.9906 - val_loss: 0.2249 - val_acc: 0.9506\n",
      "\n",
      "1D_CNN_BN_DO_5_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 435us/sample - loss: 0.2204 - acc: 0.9375\n",
      "Loss: 0.22042345849648196 Accuracy: 0.937487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_BN_DO_{}_only_conv'.format(i)\n",
    "    model = build_1d_cnn_BN_DO_only_conv(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_BN_DO_1_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 364us/sample - loss: 1.8043 - acc: 0.4490\n",
      "Loss: 1.8042701047281362 Accuracy: 0.4490135\n",
      "\n",
      "1D_CNN_BN_DO_2_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 423us/sample - loss: 1.0973 - acc: 0.6885\n",
      "Loss: 1.097267709664714 Accuracy: 0.6884735\n",
      "\n",
      "1D_CNN_BN_DO_3_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 435us/sample - loss: 0.5562 - acc: 0.8538\n",
      "Loss: 0.5561911058079293 Accuracy: 0.8537902\n",
      "\n",
      "1D_CNN_BN_DO_4_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 466us/sample - loss: 0.2656 - acc: 0.9238\n",
      "Loss: 0.26559395780196937 Accuracy: 0.92377985\n",
      "\n",
      "1D_CNN_BN_DO_5_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 498us/sample - loss: 0.2204 - acc: 0.9375\n",
      "Loss: 0.22042345849648196 Accuracy: 0.937487\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_BN_DO_{}_only_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
