{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_BN_only_conv(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=25, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=25, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model = build_1d_cnn_BN_only_conv(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4395 - acc: 0.2666\n",
      "Epoch 00001: val_loss improved from inf to 2.07729, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/001-2.0773.hdf5\n",
      "36805/36805 [==============================] - 10s 278us/sample - loss: 2.4389 - acc: 0.2667 - val_loss: 2.0773 - val_acc: 0.3315\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6488 - acc: 0.4769\n",
      "Epoch 00002: val_loss improved from 2.07729 to 1.86285, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/002-1.8628.hdf5\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 1.6488 - acc: 0.4769 - val_loss: 1.8628 - val_acc: 0.4249\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3517 - acc: 0.5717\n",
      "Epoch 00003: val_loss improved from 1.86285 to 1.84130, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/003-1.8413.hdf5\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 1.3523 - acc: 0.5713 - val_loss: 1.8413 - val_acc: 0.4393\n",
      "Epoch 4/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1510 - acc: 0.6348\n",
      "Epoch 00004: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 1.1513 - acc: 0.6350 - val_loss: 1.8748 - val_acc: 0.4386\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9986 - acc: 0.6851\n",
      "Epoch 00005: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.9989 - acc: 0.6850 - val_loss: 1.8520 - val_acc: 0.4584\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8783 - acc: 0.7297\n",
      "Epoch 00006: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.8785 - acc: 0.7297 - val_loss: 1.9132 - val_acc: 0.4507\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7808 - acc: 0.7627\n",
      "Epoch 00007: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.7807 - acc: 0.7627 - val_loss: 1.9310 - val_acc: 0.4610\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6940 - acc: 0.7900\n",
      "Epoch 00008: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.6939 - acc: 0.7901 - val_loss: 1.9965 - val_acc: 0.4563\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6205 - acc: 0.8196\n",
      "Epoch 00009: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.6206 - acc: 0.8196 - val_loss: 2.0374 - val_acc: 0.4559\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5579 - acc: 0.8398\n",
      "Epoch 00010: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.5585 - acc: 0.8395 - val_loss: 2.1070 - val_acc: 0.4507\n",
      "Epoch 11/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4991 - acc: 0.8602\n",
      "Epoch 00011: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.4995 - acc: 0.8600 - val_loss: 2.1632 - val_acc: 0.4479\n",
      "Epoch 12/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4541 - acc: 0.8771\n",
      "Epoch 00012: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.4543 - acc: 0.8769 - val_loss: 2.2094 - val_acc: 0.4594\n",
      "Epoch 13/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8924\n",
      "Epoch 00013: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.4086 - acc: 0.8924 - val_loss: 2.3026 - val_acc: 0.4468\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.9086\n",
      "Epoch 00014: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.3665 - acc: 0.9085 - val_loss: 2.3374 - val_acc: 0.4505\n",
      "Epoch 15/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.9179\n",
      "Epoch 00015: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.3336 - acc: 0.9178 - val_loss: 2.4362 - val_acc: 0.4437\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.9290\n",
      "Epoch 00016: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.3007 - acc: 0.9289 - val_loss: 2.4344 - val_acc: 0.4603\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9363\n",
      "Epoch 00017: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.2779 - acc: 0.9363 - val_loss: 2.5048 - val_acc: 0.4493\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9430\n",
      "Epoch 00018: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.2561 - acc: 0.9429 - val_loss: 2.5758 - val_acc: 0.4484\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9479\n",
      "Epoch 00019: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.2373 - acc: 0.9479 - val_loss: 2.6626 - val_acc: 0.4449\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9593\n",
      "Epoch 00020: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.2091 - acc: 0.9593 - val_loss: 2.7160 - val_acc: 0.4472\n",
      "Epoch 21/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9630\n",
      "Epoch 00021: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.1924 - acc: 0.9629 - val_loss: 2.7403 - val_acc: 0.4498\n",
      "Epoch 22/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9665\n",
      "Epoch 00022: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.1813 - acc: 0.9664 - val_loss: 2.7966 - val_acc: 0.4521\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9707\n",
      "Epoch 00023: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.1653 - acc: 0.9706 - val_loss: 2.9324 - val_acc: 0.4410\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9730\n",
      "Epoch 00024: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.1572 - acc: 0.9729 - val_loss: 2.9513 - val_acc: 0.4496\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9773\n",
      "Epoch 00025: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.1405 - acc: 0.9773 - val_loss: 3.0397 - val_acc: 0.4363\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9773\n",
      "Epoch 00026: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.1388 - acc: 0.9772 - val_loss: 3.0759 - val_acc: 0.4423\n",
      "Epoch 27/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9800\n",
      "Epoch 00027: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.1264 - acc: 0.9801 - val_loss: 3.1155 - val_acc: 0.4430\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9832\n",
      "Epoch 00028: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.1155 - acc: 0.9831 - val_loss: 3.1592 - val_acc: 0.4423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9849\n",
      "Epoch 00029: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.1071 - acc: 0.9849 - val_loss: 3.2129 - val_acc: 0.4493\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9872\n",
      "Epoch 00030: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.1004 - acc: 0.9872 - val_loss: 3.2956 - val_acc: 0.4340\n",
      "Epoch 31/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9875\n",
      "Epoch 00031: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0930 - acc: 0.9873 - val_loss: 3.2926 - val_acc: 0.4458\n",
      "Epoch 32/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9871\n",
      "Epoch 00032: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.0941 - acc: 0.9870 - val_loss: 3.3688 - val_acc: 0.4316\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9888\n",
      "Epoch 00033: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.0843 - acc: 0.9887 - val_loss: 3.4500 - val_acc: 0.4337\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9903\n",
      "Epoch 00034: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.0833 - acc: 0.9902 - val_loss: 3.4379 - val_acc: 0.4477\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9914\n",
      "Epoch 00035: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.0740 - acc: 0.9913 - val_loss: 3.5311 - val_acc: 0.4330\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9889\n",
      "Epoch 00036: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.0789 - acc: 0.9889 - val_loss: 3.5747 - val_acc: 0.4354\n",
      "Epoch 37/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9881\n",
      "Epoch 00037: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.0798 - acc: 0.9880 - val_loss: 3.5870 - val_acc: 0.4349\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9926\n",
      "Epoch 00038: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.0682 - acc: 0.9926 - val_loss: 3.6915 - val_acc: 0.4298\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9901\n",
      "Epoch 00039: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.0723 - acc: 0.9901 - val_loss: 3.6745 - val_acc: 0.4368\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9929\n",
      "Epoch 00040: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.0614 - acc: 0.9929 - val_loss: 3.6910 - val_acc: 0.4361\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9932\n",
      "Epoch 00041: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.0586 - acc: 0.9932 - val_loss: 3.7522 - val_acc: 0.4312\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9917\n",
      "Epoch 00042: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.0627 - acc: 0.9916 - val_loss: 3.8430 - val_acc: 0.4288\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9924\n",
      "Epoch 00043: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.0600 - acc: 0.9924 - val_loss: 3.8147 - val_acc: 0.4279\n",
      "Epoch 44/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9939\n",
      "Epoch 00044: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.0579 - acc: 0.9938 - val_loss: 3.8838 - val_acc: 0.4342\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9953\n",
      "Epoch 00045: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.0505 - acc: 0.9953 - val_loss: 3.8850 - val_acc: 0.4347\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9952\n",
      "Epoch 00046: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.0483 - acc: 0.9951 - val_loss: 3.9298 - val_acc: 0.4349\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9904\n",
      "Epoch 00047: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.0613 - acc: 0.9904 - val_loss: 3.9152 - val_acc: 0.4384\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9936\n",
      "Epoch 00048: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0505 - acc: 0.9936 - val_loss: 3.9410 - val_acc: 0.4356\n",
      "Epoch 49/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9958\n",
      "Epoch 00049: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.0443 - acc: 0.9958 - val_loss: 3.9895 - val_acc: 0.4337\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9945\n",
      "Epoch 00050: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.0485 - acc: 0.9945 - val_loss: 4.0759 - val_acc: 0.4309\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9943\n",
      "Epoch 00051: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.0448 - acc: 0.9943 - val_loss: 4.0670 - val_acc: 0.4379\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9959\n",
      "Epoch 00052: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0409 - acc: 0.9959 - val_loss: 4.1296 - val_acc: 0.4400\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9958\n",
      "Epoch 00053: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.0423 - acc: 0.9958 - val_loss: 4.1037 - val_acc: 0.4316\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9930\n",
      "Epoch 00054: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0488 - acc: 0.9930 - val_loss: 4.2076 - val_acc: 0.4253\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9940\n",
      "Epoch 00055: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.0428 - acc: 0.9940 - val_loss: 4.1504 - val_acc: 0.4351\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9959\n",
      "Epoch 00056: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.0364 - acc: 0.9959 - val_loss: 4.2513 - val_acc: 0.4284\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9947\n",
      "Epoch 00057: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.0427 - acc: 0.9946 - val_loss: 4.2751 - val_acc: 0.4347\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9952\n",
      "Epoch 00058: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.0396 - acc: 0.9951 - val_loss: 4.3042 - val_acc: 0.4263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9943\n",
      "Epoch 00059: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.0415 - acc: 0.9943 - val_loss: 4.3423 - val_acc: 0.4235\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9947\n",
      "Epoch 00060: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.0405 - acc: 0.9947 - val_loss: 4.3755 - val_acc: 0.4232\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9968\n",
      "Epoch 00061: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0331 - acc: 0.9968 - val_loss: 4.3693 - val_acc: 0.4326\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9935\n",
      "Epoch 00062: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0434 - acc: 0.9935 - val_loss: 4.4173 - val_acc: 0.4265\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9940\n",
      "Epoch 00063: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.0418 - acc: 0.9939 - val_loss: 4.4347 - val_acc: 0.4326\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9922\n",
      "Epoch 00064: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.0479 - acc: 0.9922 - val_loss: 4.4177 - val_acc: 0.4235\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9960\n",
      "Epoch 00065: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0325 - acc: 0.9960 - val_loss: 4.4315 - val_acc: 0.4260\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9961\n",
      "Epoch 00066: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.0334 - acc: 0.9961 - val_loss: 4.4938 - val_acc: 0.4246\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9977\n",
      "Epoch 00067: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.0261 - acc: 0.9977 - val_loss: 4.4959 - val_acc: 0.4272\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9950\n",
      "Epoch 00068: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.0364 - acc: 0.9950 - val_loss: 4.5135 - val_acc: 0.4235\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9956\n",
      "Epoch 00069: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0340 - acc: 0.9955 - val_loss: 4.5890 - val_acc: 0.4267\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9948\n",
      "Epoch 00070: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0359 - acc: 0.9947 - val_loss: 4.7158 - val_acc: 0.4142\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9912\n",
      "Epoch 00071: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.0473 - acc: 0.9913 - val_loss: 4.5502 - val_acc: 0.4218\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9970\n",
      "Epoch 00072: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.0292 - acc: 0.9970 - val_loss: 4.5232 - val_acc: 0.4270\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9982\n",
      "Epoch 00073: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.0234 - acc: 0.9982 - val_loss: 4.5494 - val_acc: 0.4274\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9971\n",
      "Epoch 00074: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0263 - acc: 0.9971 - val_loss: 4.5915 - val_acc: 0.4195\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9951\n",
      "Epoch 00075: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0335 - acc: 0.9951 - val_loss: 4.6273 - val_acc: 0.4302\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9948\n",
      "Epoch 00076: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.0339 - acc: 0.9947 - val_loss: 4.8291 - val_acc: 0.4137\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9958\n",
      "Epoch 00077: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0321 - acc: 0.9958 - val_loss: 4.6707 - val_acc: 0.4214\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9946\n",
      "Epoch 00078: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.0344 - acc: 0.9946 - val_loss: 4.7605 - val_acc: 0.4165\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9951\n",
      "Epoch 00079: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.0338 - acc: 0.9951 - val_loss: 4.7205 - val_acc: 0.4239\n",
      "Epoch 80/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9964\n",
      "Epoch 00080: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.0295 - acc: 0.9964 - val_loss: 4.8214 - val_acc: 0.4212\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9972\n",
      "Epoch 00081: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.0248 - acc: 0.9972 - val_loss: 4.7491 - val_acc: 0.4242\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9969\n",
      "Epoch 00082: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.0265 - acc: 0.9970 - val_loss: 4.7969 - val_acc: 0.4165\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9975\n",
      "Epoch 00083: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.0235 - acc: 0.9975 - val_loss: 4.8081 - val_acc: 0.4267\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9952\n",
      "Epoch 00084: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.0320 - acc: 0.9951 - val_loss: 5.0059 - val_acc: 0.4088\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9930\n",
      "Epoch 00085: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0412 - acc: 0.9929 - val_loss: 4.9964 - val_acc: 0.4160\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9927\n",
      "Epoch 00086: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.0409 - acc: 0.9927 - val_loss: 4.8072 - val_acc: 0.4209\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9971\n",
      "Epoch 00087: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0280 - acc: 0.9971 - val_loss: 4.8190 - val_acc: 0.4270\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9967\n",
      "Epoch 00088: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0266 - acc: 0.9967 - val_loss: 4.8908 - val_acc: 0.4212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9955\n",
      "Epoch 00089: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.0306 - acc: 0.9955 - val_loss: 4.9043 - val_acc: 0.4237\n",
      "Epoch 90/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9964\n",
      "Epoch 00090: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.0263 - acc: 0.9964 - val_loss: 4.9008 - val_acc: 0.4212\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9962\n",
      "Epoch 00091: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.0293 - acc: 0.9962 - val_loss: 4.9536 - val_acc: 0.4160\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9956\n",
      "Epoch 00092: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.0290 - acc: 0.9956 - val_loss: 4.9218 - val_acc: 0.4277\n",
      "Epoch 93/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9965\n",
      "Epoch 00093: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.0295 - acc: 0.9964 - val_loss: 4.9566 - val_acc: 0.4158\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9956\n",
      "Epoch 00094: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0309 - acc: 0.9957 - val_loss: 4.9233 - val_acc: 0.4235\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9983\n",
      "Epoch 00095: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.0206 - acc: 0.9983 - val_loss: 4.8946 - val_acc: 0.4239\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9985\n",
      "Epoch 00096: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.0186 - acc: 0.9985 - val_loss: 4.9451 - val_acc: 0.4223\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9924\n",
      "Epoch 00097: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.0391 - acc: 0.9923 - val_loss: 5.0800 - val_acc: 0.4167\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9956\n",
      "Epoch 00098: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.0307 - acc: 0.9955 - val_loss: 5.0393 - val_acc: 0.4165\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9957\n",
      "Epoch 00099: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.0321 - acc: 0.9957 - val_loss: 5.0098 - val_acc: 0.4214\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9965\n",
      "Epoch 00100: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.0255 - acc: 0.9965 - val_loss: 4.9900 - val_acc: 0.4165\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9979\n",
      "Epoch 00101: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.0206 - acc: 0.9979 - val_loss: 4.9766 - val_acc: 0.4246\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9958\n",
      "Epoch 00102: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.0268 - acc: 0.9958 - val_loss: 5.0748 - val_acc: 0.4223\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9945\n",
      "Epoch 00103: val_loss did not improve from 1.84130\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.0340 - acc: 0.9945 - val_loss: 5.1028 - val_acc: 0.4244\n",
      "\n",
      "1D_CNN_BN_1_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 211us/sample - loss: 1.9023 - acc: 0.4150\n",
      "Loss: 1.902310560425494 Accuracy: 0.41495326\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3290 - acc: 0.3170\n",
      "Epoch 00001: val_loss improved from inf to 1.92244, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/001-1.9224.hdf5\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 2.3277 - acc: 0.3174 - val_loss: 1.9224 - val_acc: 0.3823\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5560 - acc: 0.5151\n",
      "Epoch 00002: val_loss improved from 1.92244 to 1.53371, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/002-1.5337.hdf5\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 1.5552 - acc: 0.5153 - val_loss: 1.5337 - val_acc: 0.5243\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3223 - acc: 0.5902\n",
      "Epoch 00003: val_loss improved from 1.53371 to 1.47667, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/003-1.4767.hdf5\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 1.3221 - acc: 0.5901 - val_loss: 1.4767 - val_acc: 0.5444\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1633 - acc: 0.6408\n",
      "Epoch 00004: val_loss improved from 1.47667 to 1.44490, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/004-1.4449.hdf5\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 1.1642 - acc: 0.6405 - val_loss: 1.4449 - val_acc: 0.5595\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0232 - acc: 0.6874\n",
      "Epoch 00005: val_loss improved from 1.44490 to 1.38085, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/005-1.3809.hdf5\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 1.0240 - acc: 0.6871 - val_loss: 1.3809 - val_acc: 0.5949\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9010 - acc: 0.7228\n",
      "Epoch 00006: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.9014 - acc: 0.7229 - val_loss: 1.4143 - val_acc: 0.5856\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7923 - acc: 0.7542\n",
      "Epoch 00007: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.7925 - acc: 0.7542 - val_loss: 1.4416 - val_acc: 0.5816\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6812 - acc: 0.7907\n",
      "Epoch 00008: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.6810 - acc: 0.7908 - val_loss: 1.4172 - val_acc: 0.6091\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5915 - acc: 0.8202\n",
      "Epoch 00009: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.5915 - acc: 0.8203 - val_loss: 1.4300 - val_acc: 0.6084\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8509\n",
      "Epoch 00010: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.5024 - acc: 0.8508 - val_loss: 1.4763 - val_acc: 0.5959\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.8790\n",
      "Epoch 00011: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4254 - acc: 0.8789 - val_loss: 1.5272 - val_acc: 0.6049\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.9019\n",
      "Epoch 00012: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.3601 - acc: 0.9018 - val_loss: 1.5841 - val_acc: 0.6021\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3095 - acc: 0.9208\n",
      "Epoch 00013: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.3093 - acc: 0.9209 - val_loss: 1.5906 - val_acc: 0.6014\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9411\n",
      "Epoch 00014: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.2513 - acc: 0.9410 - val_loss: 1.6433 - val_acc: 0.5996\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9532\n",
      "Epoch 00015: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.2163 - acc: 0.9532 - val_loss: 1.6584 - val_acc: 0.6066\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9649\n",
      "Epoch 00016: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.1800 - acc: 0.9649 - val_loss: 1.7246 - val_acc: 0.6054\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9738\n",
      "Epoch 00017: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.1533 - acc: 0.9738 - val_loss: 1.7838 - val_acc: 0.6089\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9806\n",
      "Epoch 00018: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.1283 - acc: 0.9806 - val_loss: 1.8063 - val_acc: 0.6068\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9837\n",
      "Epoch 00019: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1125 - acc: 0.9838 - val_loss: 1.9036 - val_acc: 0.6045\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9885\n",
      "Epoch 00020: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0934 - acc: 0.9885 - val_loss: 1.9681 - val_acc: 0.5959\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9910\n",
      "Epoch 00021: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0829 - acc: 0.9909 - val_loss: 2.0060 - val_acc: 0.5977\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9913\n",
      "Epoch 00022: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0743 - acc: 0.9912 - val_loss: 2.0667 - val_acc: 0.5947\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9946\n",
      "Epoch 00023: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0612 - acc: 0.9946 - val_loss: 2.1171 - val_acc: 0.5905\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9950\n",
      "Epoch 00024: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0554 - acc: 0.9950 - val_loss: 2.1189 - val_acc: 0.5879\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9949\n",
      "Epoch 00025: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0547 - acc: 0.9949 - val_loss: 2.1541 - val_acc: 0.5984\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9970\n",
      "Epoch 00026: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0427 - acc: 0.9970 - val_loss: 2.1813 - val_acc: 0.5984\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9951\n",
      "Epoch 00027: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 312us/sample - loss: 0.0485 - acc: 0.9951 - val_loss: 2.2078 - val_acc: 0.5945\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9965\n",
      "Epoch 00028: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0411 - acc: 0.9965 - val_loss: 2.3402 - val_acc: 0.5877\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9976\n",
      "Epoch 00029: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0361 - acc: 0.9976 - val_loss: 2.2483 - val_acc: 0.6024\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9970\n",
      "Epoch 00030: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0361 - acc: 0.9970 - val_loss: 2.3460 - val_acc: 0.5991\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9959\n",
      "Epoch 00031: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.0375 - acc: 0.9959 - val_loss: 2.3407 - val_acc: 0.5942\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9974\n",
      "Epoch 00032: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0311 - acc: 0.9974 - val_loss: 2.4420 - val_acc: 0.5854\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9979\n",
      "Epoch 00033: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0280 - acc: 0.9979 - val_loss: 2.4199 - val_acc: 0.5872\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9973\n",
      "Epoch 00034: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0288 - acc: 0.9973 - val_loss: 2.4648 - val_acc: 0.5851\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9970\n",
      "Epoch 00035: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0309 - acc: 0.9970 - val_loss: 2.5515 - val_acc: 0.5882\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9964\n",
      "Epoch 00036: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0315 - acc: 0.9964 - val_loss: 2.5517 - val_acc: 0.5903\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9975\n",
      "Epoch 00037: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0270 - acc: 0.9975 - val_loss: 2.5278 - val_acc: 0.5907\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9971\n",
      "Epoch 00038: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0288 - acc: 0.9971 - val_loss: 2.5719 - val_acc: 0.5870\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9984\n",
      "Epoch 00039: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0221 - acc: 0.9984 - val_loss: 2.6194 - val_acc: 0.5882\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9967\n",
      "Epoch 00040: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0288 - acc: 0.9967 - val_loss: 2.5962 - val_acc: 0.5921\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9989\n",
      "Epoch 00041: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.0189 - acc: 0.9989 - val_loss: 2.5822 - val_acc: 0.5956\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9991\n",
      "Epoch 00042: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.0170 - acc: 0.9991 - val_loss: 2.6606 - val_acc: 0.5847\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9981\n",
      "Epoch 00043: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0205 - acc: 0.9981 - val_loss: 3.4803 - val_acc: 0.5132\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9950\n",
      "Epoch 00044: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0327 - acc: 0.9950 - val_loss: 2.7864 - val_acc: 0.5837\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9987\n",
      "Epoch 00045: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0182 - acc: 0.9987 - val_loss: 2.7295 - val_acc: 0.5849\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9990\n",
      "Epoch 00046: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0164 - acc: 0.9990 - val_loss: 2.7403 - val_acc: 0.5861\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9976\n",
      "Epoch 00047: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0235 - acc: 0.9976 - val_loss: 2.8818 - val_acc: 0.5842\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9973\n",
      "Epoch 00048: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0232 - acc: 0.9973 - val_loss: 3.2350 - val_acc: 0.5586\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9965\n",
      "Epoch 00049: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0255 - acc: 0.9965 - val_loss: 2.9412 - val_acc: 0.5714\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9987\n",
      "Epoch 00050: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0178 - acc: 0.9987 - val_loss: 2.8378 - val_acc: 0.5854\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9986\n",
      "Epoch 00051: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0167 - acc: 0.9986 - val_loss: 2.9582 - val_acc: 0.5823\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9961\n",
      "Epoch 00052: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0282 - acc: 0.9961 - val_loss: 2.8896 - val_acc: 0.5795\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9985\n",
      "Epoch 00053: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0172 - acc: 0.9985 - val_loss: 2.9339 - val_acc: 0.5786\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9985\n",
      "Epoch 00054: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0168 - acc: 0.9985 - val_loss: 2.9134 - val_acc: 0.5840\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9985\n",
      "Epoch 00055: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0171 - acc: 0.9985 - val_loss: 3.0117 - val_acc: 0.5802\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9960\n",
      "Epoch 00056: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0255 - acc: 0.9960 - val_loss: 3.0546 - val_acc: 0.5667\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9989\n",
      "Epoch 00057: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0150 - acc: 0.9989 - val_loss: 2.9308 - val_acc: 0.5870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9971\n",
      "Epoch 00058: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0222 - acc: 0.9970 - val_loss: 3.1239 - val_acc: 0.5667\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9953\n",
      "Epoch 00059: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0254 - acc: 0.9952 - val_loss: 3.0521 - val_acc: 0.5733\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9982\n",
      "Epoch 00060: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0172 - acc: 0.9982 - val_loss: 3.0463 - val_acc: 0.5793\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9991\n",
      "Epoch 00061: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0139 - acc: 0.9990 - val_loss: 3.0384 - val_acc: 0.5865\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9972\n",
      "Epoch 00062: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0223 - acc: 0.9973 - val_loss: 3.0316 - val_acc: 0.5870\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9987\n",
      "Epoch 00063: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0149 - acc: 0.9987 - val_loss: 3.1036 - val_acc: 0.5742\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9980\n",
      "Epoch 00064: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0181 - acc: 0.9980 - val_loss: 3.0477 - val_acc: 0.5849\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9992\n",
      "Epoch 00065: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.0133 - acc: 0.9992 - val_loss: 3.1117 - val_acc: 0.5837\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9972\n",
      "Epoch 00066: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0204 - acc: 0.9972 - val_loss: 3.2346 - val_acc: 0.5714\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9976\n",
      "Epoch 00067: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0186 - acc: 0.9976 - val_loss: 3.2518 - val_acc: 0.5751\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9990\n",
      "Epoch 00068: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0143 - acc: 0.9989 - val_loss: 3.2058 - val_acc: 0.5763\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9958\n",
      "Epoch 00069: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0238 - acc: 0.9958 - val_loss: 3.1628 - val_acc: 0.5795\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9992\n",
      "Epoch 00070: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0129 - acc: 0.9992 - val_loss: 3.1411 - val_acc: 0.5802\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9992\n",
      "Epoch 00071: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 312us/sample - loss: 0.0132 - acc: 0.9991 - val_loss: 3.1492 - val_acc: 0.5861\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9952\n",
      "Epoch 00072: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0277 - acc: 0.9952 - val_loss: 3.3084 - val_acc: 0.5702\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9990\n",
      "Epoch 00073: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0132 - acc: 0.9990 - val_loss: 3.2385 - val_acc: 0.5788\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9992\n",
      "Epoch 00074: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0127 - acc: 0.9992 - val_loss: 3.1821 - val_acc: 0.5877\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9993\n",
      "Epoch 00075: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0122 - acc: 0.9992 - val_loss: 3.2836 - val_acc: 0.5730\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9974\n",
      "Epoch 00076: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0195 - acc: 0.9974 - val_loss: 3.2606 - val_acc: 0.5768\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9977\n",
      "Epoch 00077: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0194 - acc: 0.9977 - val_loss: 3.3237 - val_acc: 0.5723\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9986\n",
      "Epoch 00078: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0149 - acc: 0.9986 - val_loss: 3.2442 - val_acc: 0.5772\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9985\n",
      "Epoch 00079: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0153 - acc: 0.9985 - val_loss: 3.3789 - val_acc: 0.5707\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9989\n",
      "Epoch 00080: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0144 - acc: 0.9989 - val_loss: 3.2801 - val_acc: 0.5791\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9990\n",
      "Epoch 00081: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0130 - acc: 0.9990 - val_loss: 3.3171 - val_acc: 0.5709\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9964\n",
      "Epoch 00082: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0238 - acc: 0.9964 - val_loss: 3.4211 - val_acc: 0.5714\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9980\n",
      "Epoch 00083: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.0172 - acc: 0.9980 - val_loss: 3.3800 - val_acc: 0.5691\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9990\n",
      "Epoch 00084: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0132 - acc: 0.9990 - val_loss: 3.3476 - val_acc: 0.5700\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9988\n",
      "Epoch 00085: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0137 - acc: 0.9988 - val_loss: 3.3054 - val_acc: 0.5777\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9967\n",
      "Epoch 00086: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0222 - acc: 0.9967 - val_loss: 3.3448 - val_acc: 0.5770\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9989\n",
      "Epoch 00087: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0130 - acc: 0.9989 - val_loss: 3.3835 - val_acc: 0.5744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9981\n",
      "Epoch 00088: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0177 - acc: 0.9982 - val_loss: 3.3726 - val_acc: 0.5756\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9979\n",
      "Epoch 00089: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.0178 - acc: 0.9979 - val_loss: 3.5054 - val_acc: 0.5761\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9987\n",
      "Epoch 00090: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0143 - acc: 0.9987 - val_loss: 3.4081 - val_acc: 0.5702\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9992\n",
      "Epoch 00091: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0120 - acc: 0.9992 - val_loss: 3.4288 - val_acc: 0.5744\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9991\n",
      "Epoch 00092: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0123 - acc: 0.9991 - val_loss: 3.4108 - val_acc: 0.5779\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9968\n",
      "Epoch 00093: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0206 - acc: 0.9968 - val_loss: 3.5234 - val_acc: 0.5744\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9987\n",
      "Epoch 00094: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0144 - acc: 0.9988 - val_loss: 3.7300 - val_acc: 0.5584\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9978\n",
      "Epoch 00095: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0190 - acc: 0.9979 - val_loss: 3.5074 - val_acc: 0.5779\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9990\n",
      "Epoch 00096: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0131 - acc: 0.9990 - val_loss: 3.4963 - val_acc: 0.5712\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9992\n",
      "Epoch 00097: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0121 - acc: 0.9992 - val_loss: 3.5977 - val_acc: 0.5658\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9989\n",
      "Epoch 00098: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0135 - acc: 0.9989 - val_loss: 3.6430 - val_acc: 0.5579\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9980\n",
      "Epoch 00099: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0172 - acc: 0.9980 - val_loss: 3.7406 - val_acc: 0.5516\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9969\n",
      "Epoch 00100: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0204 - acc: 0.9969 - val_loss: 3.7194 - val_acc: 0.5602\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9988\n",
      "Epoch 00101: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0140 - acc: 0.9988 - val_loss: 3.6217 - val_acc: 0.5639\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9974\n",
      "Epoch 00102: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0184 - acc: 0.9973 - val_loss: 3.6710 - val_acc: 0.5581\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9980\n",
      "Epoch 00103: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 12s 312us/sample - loss: 0.0168 - acc: 0.9980 - val_loss: 3.5426 - val_acc: 0.5709\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9991\n",
      "Epoch 00104: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0125 - acc: 0.9991 - val_loss: 3.6090 - val_acc: 0.5695\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9989\n",
      "Epoch 00105: val_loss did not improve from 1.38085\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.0128 - acc: 0.9989 - val_loss: 3.5747 - val_acc: 0.5693\n",
      "\n",
      "1D_CNN_BN_2_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 219us/sample - loss: 1.4395 - acc: 0.5483\n",
      "Loss: 1.4394703664264699 Accuracy: 0.5482866\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1988 - acc: 0.3227\n",
      "Epoch 00001: val_loss improved from inf to 3.76844, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/001-3.7684.hdf5\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 2.1984 - acc: 0.3229 - val_loss: 3.7684 - val_acc: 0.1412\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5327 - acc: 0.5147\n",
      "Epoch 00002: val_loss improved from 3.76844 to 1.39893, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/002-1.3989.hdf5\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 1.5328 - acc: 0.5146 - val_loss: 1.3989 - val_acc: 0.5670\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3042 - acc: 0.5961\n",
      "Epoch 00003: val_loss improved from 1.39893 to 1.28693, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/003-1.2869.hdf5\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 1.3039 - acc: 0.5962 - val_loss: 1.2869 - val_acc: 0.6103\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1579 - acc: 0.6451\n",
      "Epoch 00004: val_loss improved from 1.28693 to 1.11780, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/004-1.1178.hdf5\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 1.1582 - acc: 0.6452 - val_loss: 1.1178 - val_acc: 0.6685\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0419 - acc: 0.6853\n",
      "Epoch 00005: val_loss improved from 1.11780 to 1.09595, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/005-1.0959.hdf5\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 1.0416 - acc: 0.6854 - val_loss: 1.0959 - val_acc: 0.6695\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9409 - acc: 0.7149\n",
      "Epoch 00006: val_loss improved from 1.09595 to 1.05669, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/006-1.0567.hdf5\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.9410 - acc: 0.7149 - val_loss: 1.0567 - val_acc: 0.6834\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8571 - acc: 0.7430\n",
      "Epoch 00007: val_loss improved from 1.05669 to 0.99791, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/007-0.9979.hdf5\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.8571 - acc: 0.7430 - val_loss: 0.9979 - val_acc: 0.7107\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7839 - acc: 0.7664\n",
      "Epoch 00008: val_loss improved from 0.99791 to 0.94670, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/008-0.9467.hdf5\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.7838 - acc: 0.7664 - val_loss: 0.9467 - val_acc: 0.7282\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7129 - acc: 0.7869\n",
      "Epoch 00009: val_loss did not improve from 0.94670\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.7126 - acc: 0.7870 - val_loss: 0.9747 - val_acc: 0.7293\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6466 - acc: 0.8071\n",
      "Epoch 00010: val_loss improved from 0.94670 to 0.90386, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/010-0.9039.hdf5\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.6464 - acc: 0.8071 - val_loss: 0.9039 - val_acc: 0.7433\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5783 - acc: 0.8249\n",
      "Epoch 00011: val_loss did not improve from 0.90386\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.5789 - acc: 0.8248 - val_loss: 0.9443 - val_acc: 0.7331\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.8471\n",
      "Epoch 00012: val_loss did not improve from 0.90386\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.5151 - acc: 0.8470 - val_loss: 0.9198 - val_acc: 0.7386\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4549 - acc: 0.8638\n",
      "Epoch 00013: val_loss improved from 0.90386 to 0.89918, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/013-0.8992.hdf5\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.4550 - acc: 0.8638 - val_loss: 0.8992 - val_acc: 0.7473\n",
      "Epoch 14/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8817\n",
      "Epoch 00014: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.4024 - acc: 0.8816 - val_loss: 0.9094 - val_acc: 0.7498\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8980\n",
      "Epoch 00015: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.3498 - acc: 0.8981 - val_loss: 0.9824 - val_acc: 0.7251\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3067 - acc: 0.9129\n",
      "Epoch 00016: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.3068 - acc: 0.9129 - val_loss: 0.9348 - val_acc: 0.7498\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9265\n",
      "Epoch 00017: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.2635 - acc: 0.9266 - val_loss: 0.9436 - val_acc: 0.7352\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9418\n",
      "Epoch 00018: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.2253 - acc: 0.9418 - val_loss: 0.9410 - val_acc: 0.7540\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9515\n",
      "Epoch 00019: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.1922 - acc: 0.9516 - val_loss: 0.9518 - val_acc: 0.7538\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9604\n",
      "Epoch 00020: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.1646 - acc: 0.9604 - val_loss: 0.9763 - val_acc: 0.7459\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9682\n",
      "Epoch 00021: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.1430 - acc: 0.9682 - val_loss: 1.0119 - val_acc: 0.7426\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9737\n",
      "Epoch 00022: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.1247 - acc: 0.9737 - val_loss: 1.0078 - val_acc: 0.7484\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9781\n",
      "Epoch 00023: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.1114 - acc: 0.9781 - val_loss: 1.0284 - val_acc: 0.7510\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9834\n",
      "Epoch 00024: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0932 - acc: 0.9835 - val_loss: 1.0440 - val_acc: 0.7573\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9879\n",
      "Epoch 00025: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0772 - acc: 0.9879 - val_loss: 1.0637 - val_acc: 0.7559\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9887\n",
      "Epoch 00026: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0727 - acc: 0.9886 - val_loss: 1.1184 - val_acc: 0.7370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9895\n",
      "Epoch 00027: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0691 - acc: 0.9895 - val_loss: 1.1325 - val_acc: 0.7372\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9912\n",
      "Epoch 00028: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0594 - acc: 0.9911 - val_loss: 1.0848 - val_acc: 0.7529\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9906\n",
      "Epoch 00029: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0567 - acc: 0.9906 - val_loss: 1.1531 - val_acc: 0.7398\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9942\n",
      "Epoch 00030: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0455 - acc: 0.9942 - val_loss: 1.1250 - val_acc: 0.7517\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9950\n",
      "Epoch 00031: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0402 - acc: 0.9949 - val_loss: 1.1830 - val_acc: 0.7573\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9838\n",
      "Epoch 00032: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0808 - acc: 0.9838 - val_loss: 1.1433 - val_acc: 0.7496\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9967\n",
      "Epoch 00033: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0340 - acc: 0.9966 - val_loss: 1.1751 - val_acc: 0.7466\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9959\n",
      "Epoch 00034: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0332 - acc: 0.9959 - val_loss: 1.1619 - val_acc: 0.7501\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9971\n",
      "Epoch 00035: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0292 - acc: 0.9970 - val_loss: 1.1768 - val_acc: 0.7480\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9965\n",
      "Epoch 00036: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0302 - acc: 0.9964 - val_loss: 1.2248 - val_acc: 0.7522\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9936\n",
      "Epoch 00037: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0373 - acc: 0.9936 - val_loss: 1.2920 - val_acc: 0.7412\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9967\n",
      "Epoch 00038: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0277 - acc: 0.9967 - val_loss: 1.2076 - val_acc: 0.7524\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9954\n",
      "Epoch 00039: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0303 - acc: 0.9954 - val_loss: 1.2821 - val_acc: 0.7508\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9976\n",
      "Epoch 00040: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0223 - acc: 0.9976 - val_loss: 1.2701 - val_acc: 0.7531\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9975\n",
      "Epoch 00041: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0230 - acc: 0.9975 - val_loss: 1.2413 - val_acc: 0.7540\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9951\n",
      "Epoch 00042: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.0293 - acc: 0.9951 - val_loss: 1.3931 - val_acc: 0.7398\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9968\n",
      "Epoch 00043: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0231 - acc: 0.9968 - val_loss: 1.2920 - val_acc: 0.7449\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9983\n",
      "Epoch 00044: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0181 - acc: 0.9983 - val_loss: 1.3876 - val_acc: 0.7331\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9959\n",
      "Epoch 00045: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0252 - acc: 0.9959 - val_loss: 1.3465 - val_acc: 0.7503\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9952\n",
      "Epoch 00046: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.0263 - acc: 0.9952 - val_loss: 1.2787 - val_acc: 0.7531\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9987\n",
      "Epoch 00047: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0153 - acc: 0.9987 - val_loss: 1.3235 - val_acc: 0.7559\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9976\n",
      "Epoch 00048: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0188 - acc: 0.9976 - val_loss: 1.3216 - val_acc: 0.7577\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9979\n",
      "Epoch 00049: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 366us/sample - loss: 0.0178 - acc: 0.9979 - val_loss: 1.4278 - val_acc: 0.7361\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9972\n",
      "Epoch 00050: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0200 - acc: 0.9971 - val_loss: 1.3514 - val_acc: 0.7496\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9977\n",
      "Epoch 00051: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.0183 - acc: 0.9977 - val_loss: 1.4360 - val_acc: 0.7410\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9968\n",
      "Epoch 00052: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.0212 - acc: 0.9968 - val_loss: 1.4011 - val_acc: 0.7459\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9975\n",
      "Epoch 00053: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.0175 - acc: 0.9975 - val_loss: 1.3484 - val_acc: 0.7549\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9990\n",
      "Epoch 00054: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 353us/sample - loss: 0.0126 - acc: 0.9990 - val_loss: 1.3893 - val_acc: 0.7517\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9964\n",
      "Epoch 00055: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 349us/sample - loss: 0.0211 - acc: 0.9963 - val_loss: 1.4472 - val_acc: 0.7482\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9968\n",
      "Epoch 00056: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 354us/sample - loss: 0.0186 - acc: 0.9968 - val_loss: 1.3886 - val_acc: 0.7543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9982\n",
      "Epoch 00057: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0149 - acc: 0.9982 - val_loss: 1.3966 - val_acc: 0.7545\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9983\n",
      "Epoch 00058: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 354us/sample - loss: 0.0151 - acc: 0.9982 - val_loss: 1.4027 - val_acc: 0.7529\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9960\n",
      "Epoch 00059: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0201 - acc: 0.9961 - val_loss: 1.4819 - val_acc: 0.7412\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9982\n",
      "Epoch 00060: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0150 - acc: 0.9982 - val_loss: 1.4035 - val_acc: 0.7531\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9988\n",
      "Epoch 00061: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0117 - acc: 0.9988 - val_loss: 1.4711 - val_acc: 0.7519\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9989\n",
      "Epoch 00062: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 354us/sample - loss: 0.0119 - acc: 0.9988 - val_loss: 1.4428 - val_acc: 0.7519\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9963\n",
      "Epoch 00063: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0197 - acc: 0.9963 - val_loss: 1.4492 - val_acc: 0.7487\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9985\n",
      "Epoch 00064: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0126 - acc: 0.9985 - val_loss: 1.4871 - val_acc: 0.7484\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9979\n",
      "Epoch 00065: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0149 - acc: 0.9979 - val_loss: 1.4794 - val_acc: 0.7452\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9959\n",
      "Epoch 00066: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0206 - acc: 0.9959 - val_loss: 1.4808 - val_acc: 0.7426\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9970\n",
      "Epoch 00067: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.0173 - acc: 0.9970 - val_loss: 1.4614 - val_acc: 0.7512\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9986\n",
      "Epoch 00068: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.0116 - acc: 0.9986 - val_loss: 1.4278 - val_acc: 0.7603\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9964\n",
      "Epoch 00069: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0188 - acc: 0.9964 - val_loss: 1.4848 - val_acc: 0.7508\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9983\n",
      "Epoch 00070: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 350us/sample - loss: 0.0130 - acc: 0.9983 - val_loss: 1.5256 - val_acc: 0.7466\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9989\n",
      "Epoch 00071: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.0104 - acc: 0.9989 - val_loss: 1.4587 - val_acc: 0.7540\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9988\n",
      "Epoch 00072: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.0105 - acc: 0.9988 - val_loss: 1.5398 - val_acc: 0.7463\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9987\n",
      "Epoch 00073: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0109 - acc: 0.9987 - val_loss: 1.5472 - val_acc: 0.7477\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9957\n",
      "Epoch 00074: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0208 - acc: 0.9957 - val_loss: 1.5517 - val_acc: 0.7407\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9969\n",
      "Epoch 00075: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0173 - acc: 0.9969 - val_loss: 1.4934 - val_acc: 0.7626\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9992\n",
      "Epoch 00076: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0101 - acc: 0.9992 - val_loss: 1.5255 - val_acc: 0.7587\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9947\n",
      "Epoch 00077: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.0243 - acc: 0.9947 - val_loss: 1.5001 - val_acc: 0.7568\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9992\n",
      "Epoch 00078: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0095 - acc: 0.9992 - val_loss: 1.4999 - val_acc: 0.7598\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9994\n",
      "Epoch 00079: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 354us/sample - loss: 0.0085 - acc: 0.9994 - val_loss: 1.5324 - val_acc: 0.7526\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9987\n",
      "Epoch 00080: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0107 - acc: 0.9987 - val_loss: 1.5117 - val_acc: 0.7547\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9968\n",
      "Epoch 00081: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0167 - acc: 0.9968 - val_loss: 1.5519 - val_acc: 0.7533\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9992\n",
      "Epoch 00082: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0099 - acc: 0.9992 - val_loss: 1.5462 - val_acc: 0.7570\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9986\n",
      "Epoch 00083: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0111 - acc: 0.9986 - val_loss: 1.5507 - val_acc: 0.7519\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9983\n",
      "Epoch 00084: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.0124 - acc: 0.9982 - val_loss: 1.9574 - val_acc: 0.7060\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9961\n",
      "Epoch 00085: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.0198 - acc: 0.9960 - val_loss: 1.5705 - val_acc: 0.7512\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9981\n",
      "Epoch 00086: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0137 - acc: 0.9980 - val_loss: 1.6304 - val_acc: 0.7501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9977\n",
      "Epoch 00087: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.0137 - acc: 0.9977 - val_loss: 1.5679 - val_acc: 0.7522\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9995\n",
      "Epoch 00088: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0081 - acc: 0.9995 - val_loss: 1.5260 - val_acc: 0.7598\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9990\n",
      "Epoch 00089: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0093 - acc: 0.9990 - val_loss: 1.5419 - val_acc: 0.7650\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9965\n",
      "Epoch 00090: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0175 - acc: 0.9965 - val_loss: 1.6884 - val_acc: 0.7501\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9981\n",
      "Epoch 00091: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0127 - acc: 0.9981 - val_loss: 1.5270 - val_acc: 0.7617\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9981\n",
      "Epoch 00092: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.0119 - acc: 0.9981 - val_loss: 1.5735 - val_acc: 0.7596\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9995\n",
      "Epoch 00093: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.0078 - acc: 0.9995 - val_loss: 1.5665 - val_acc: 0.7540\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9987\n",
      "Epoch 00094: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0101 - acc: 0.9987 - val_loss: 1.6445 - val_acc: 0.7454\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9983\n",
      "Epoch 00095: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0123 - acc: 0.9983 - val_loss: 1.6408 - val_acc: 0.7433\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9971\n",
      "Epoch 00096: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 353us/sample - loss: 0.0154 - acc: 0.9970 - val_loss: 1.6883 - val_acc: 0.7494\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9948\n",
      "Epoch 00097: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 354us/sample - loss: 0.0234 - acc: 0.9948 - val_loss: 1.6537 - val_acc: 0.7496\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9985\n",
      "Epoch 00098: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0110 - acc: 0.9985 - val_loss: 1.6187 - val_acc: 0.7498\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9985\n",
      "Epoch 00099: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 353us/sample - loss: 0.0109 - acc: 0.9985 - val_loss: 1.6730 - val_acc: 0.7496\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9983\n",
      "Epoch 00100: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0120 - acc: 0.9983 - val_loss: 1.6356 - val_acc: 0.7533\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9970\n",
      "Epoch 00101: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0167 - acc: 0.9970 - val_loss: 1.5812 - val_acc: 0.7568\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9957\n",
      "Epoch 00102: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0201 - acc: 0.9957 - val_loss: 1.6679 - val_acc: 0.7526\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9993\n",
      "Epoch 00103: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.0084 - acc: 0.9993 - val_loss: 1.6087 - val_acc: 0.7587\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9992\n",
      "Epoch 00104: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0087 - acc: 0.9992 - val_loss: 1.6124 - val_acc: 0.7601\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9971\n",
      "Epoch 00105: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0154 - acc: 0.9971 - val_loss: 1.6392 - val_acc: 0.7517\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9993\n",
      "Epoch 00106: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0079 - acc: 0.9993 - val_loss: 1.6386 - val_acc: 0.7447\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9983\n",
      "Epoch 00107: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.0114 - acc: 0.9983 - val_loss: 1.6932 - val_acc: 0.7463\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9990\n",
      "Epoch 00108: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.0095 - acc: 0.9990 - val_loss: 1.7167 - val_acc: 0.7412\n",
      "Epoch 109/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9977\n",
      "Epoch 00109: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 352us/sample - loss: 0.0142 - acc: 0.9976 - val_loss: 1.7614 - val_acc: 0.7379\n",
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9954\n",
      "Epoch 00110: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 353us/sample - loss: 0.0203 - acc: 0.9954 - val_loss: 1.7075 - val_acc: 0.7496\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9990\n",
      "Epoch 00111: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 355us/sample - loss: 0.0089 - acc: 0.9990 - val_loss: 1.6225 - val_acc: 0.7538\n",
      "Epoch 112/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9993\n",
      "Epoch 00112: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0078 - acc: 0.9993 - val_loss: 1.5789 - val_acc: 0.7612\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9992\n",
      "Epoch 00113: val_loss did not improve from 0.89918\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0081 - acc: 0.9992 - val_loss: 1.6605 - val_acc: 0.7543\n",
      "\n",
      "1D_CNN_BN_3_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX9+PH3mT2TfSOEBEhA1rCETVFU3EVtcRer1lqruFVrbbVWv1q1WvfW2motoq1ad9SfG4oLIO4KCAKCrAkJW/Z99jm/P04mCZCEEDIEks/reebJzJ1zzz13ZnI/9yz3XKW1RgghhACwdHcBhBBCHDgkKAghhGgiQUEIIUQTCQpCCCGaSFAQQgjRRIKCEEKIJhIUhBBCNJGgIIQQookEBSGEEE1s3V2AvZWWlqZzcnK6uxhCCHFQWbJkSZnWOn1P6Q66oJCTk8PixYu7uxhCCHFQUUoVdiSdNB8JIYRoIkFBCCFEEwkKQgghmhx0fQqtCQQCFBcX4/V6u7soBy2Xy0V2djZ2u727iyKE6EY9IigUFxcTHx9PTk4OSqnuLs5BR2tNeXk5xcXF5ObmdndxhBDdqEc0H3m9XlJTUyUgdJJSitTUVKlpCSF6RlAAJCDsI/n8hBDQg4LCHnk8sGULBALdXRIhhDhg9a6gsG0bBINdnnVVVRWPP/54p9Y99dRTqaqq6nD6O+64g4ceeqhT2xJCiD3pPUHB0rir4XCXZ91eUAjuIQjNnTuXpKSkLi+TEEJ0Ru8JCpE2c627POubb76ZDRs2kJ+fz4033sjChQs56qijmD59OiNHjgTgjDPOYMKECeTl5TFr1qymdXNycigrK6OgoIARI0Zw+eWXk5eXx0knnYTH42l3u8uWLWPy5MmMGTOGM888k8rKSgAeffRRRo4cyZgxYzj//PMB+OSTT8jPzyc/P59x48ZRW1vb5Z+DEOLg1yOGpLa0bt311NUt2/2NUAgaGmCNG6zWvcozLi6fIUMeafP9++67j5UrV7JsmdnuwoULWbp0KStXrmwa4vn000+TkpKCx+Nh0qRJnH322aSmpu5S9nW8+OKLPPnkk5x33nm89tprXHTRRW1u9+KLL+Yf//gHU6dO5fbbb+fOO+/kkUce4b777mPTpk04nc6mpqmHHnqIxx57jClTplBXV4fL5dqrz0AI0Tv0nprCfnbooYfuNOb/0UcfZezYsUyePJmioiLWrVu32zq5ubnk5+cDMGHCBAoKCtrMv7q6mqqqKqZOnQrAL37xCxYtWgTAmDFjuPDCC/nf//6HzWbi/pQpU7jhhht49NFHqaqqalouhBAt9bgjQ5tn9HV1sGYNDBkCiYlRL0dsbGzT84ULF/LRRx/x5Zdf4na7OeaYY1q9JsDpdDY9t1qte2w+asu7777LokWLePvtt7nnnntYsWIFN998M6eddhpz585lypQpzJs3j+HDh3cqfyFEz9V7agpR7FOIj49vt42+urqa5ORk3G43a9as4auvvtrnbSYmJpKcnMynn34KwHPPPcfUqVMJh8MUFRVx7LHHcv/991NdXU1dXR0bNmxg9OjR/OEPf2DSpEmsWbNmn8sghOh5elxNoU2RoBCF0UepqalMmTKFUaNGccopp3Daaaft9P60adN44oknGDFiBMOGDWPy5Mldst1nnnmGK6+8koaGBgYNGsR//vMfQqEQF110EdXV1Witue6660hKSuK2225jwYIFWCwW8vLyOOWUU7qkDEKInkXpKJw5R9PEiRP1rjfZWb16NSNGjGh/Ra8XVq6E3FzYpYNXGB36HIUQByWl1BKt9cQ9pZPmIyGEEE0kKAghhGgStaCglHIppb5RSi1XSq1SSt3ZSppLlFKlSqlljY/LolUeCQpCCLFn0exo9gHHaa3rlFJ24DOl1Hta612H3rystf51FMthSFAQQog9ilpQ0KYHu67xpb3x0X1HZAkKQgixR1HtU1BKWZVSy4AS4EOt9detJDtbKfW9UmqOUqp/G/nMVEotVkotLi0t7WxhzF8JCkII0aaoBgWtdUhrnQ9kA4cqpUbtkuRtIEdrPQb4EHimjXxmaa0naq0npqend64wB1hQiIuL26vlQgixP+yX0Uda6ypgATBtl+XlWmtf48vZwISoFUIp84jCxWtCCNFTRHP0UbpSKqnxeQxwIrBmlzSZLV5OB1ZHqzyNG4za1NmPPfZY0+vIjXDq6uo4/vjjGT9+PKNHj+bNN9/scJ5aa2688UZGjRrF6NGjefnllwHYtm0bRx99NPn5+YwaNYpPP/2UUCjEJZdc0pT2b3/7W5fvoxCid4jm6KNM4BmllBUTfF7RWr+jlLoLWKy1fgu4Tik1HQgCFcAl+7zV66+HZa1MnQ1mUjy7HVpMPNch+fnwSNtTZ8+YMYPrr7+ea665BoBXXnmFefPm4XK5eOONN0hISKCsrIzJkyczffr0Dt0P+fXXX2fZsmUsX76csrIyJk2axNFHH80LL7zAySefzK233kooFKKhoYFly5axZcsWVq5cCbBXd3ITQoiWojn66HtgXCvLb2/x/I/AH6NVhlZFoaYwbtw4SkpK2Lp1K6WlpSQnJ9O/f38CgQC33HILixYtwmKxsGXLFnbs2EHfvn33mOdnn33Gz372M6xWKxkZGUydOpVvv/2WSZMmcemllxIIBDjjjDPIz89n0KBBbNy4kWuvvZbTTjuNk046qcv3UQjRO/S8CfHaOaNn+XIzbXZOTpdv9txzz2XOnDls376dGTNmAPD8889TWlrKkiVLsNvt5OTktDpl9t44+uijWbRoEe+++y6XXHIJN9xwAxdffDHLly9n3rx5PPHEE7zyyis8/fTTXbFbQohepvdMcwFR61MA04T00ksvMWfOHM4991zATJndp08f7HY7CxYsoLCwsMP5HXXUUbz88suEQiFKS0tZtGgRhx56KIWFhWRkZHD55Zdz2WWXsXTpUsrKygiHw5x99tncfffdLF26NCr7KITo+XpeTaE9FkvUgkJeXh61tbVkZWWRmWn6zy+88EJ++tOfMnr0aCZOnLhXN7U588wz+fLLLxk7dixKKR544AH69u3LM888w4MPPojdbicuLo5nn32WLVu28Mtf/pJw48iqe++9Nyr7KITo+XrP1NkAq1aZTuZDDolS6Q5uMnW2ED2XTJ3dmig2HwkhRE8gQUEIIUQTCQpCCCGaSFAQQgjRRIKCEEKIJhIUhBBCNJGg0AWqqqp4/PHHO7XuqaeeKnMVCSEOGL0rKFgsUZk6u72gEAwG21137ty5JCUldXmZhBCiM3pXUIji1NkbNmwgPz+fG2+8kYULF3LUUUcxffp0Ro4cCcAZZ5zBhAkTyMvLY9asWU3r5uTkUFZWRkFBASNGjODyyy8nLy+Pk046CY/Hs9u23n77bQ477DDGjRvHCSecwI4dOwCoq6vjl7/8JaNHj2bMmDG89tprALz//vuMHz+esWPHcvzxx3f5vgshepYeN81FezNn482EYDrs5c3N9jBzNvfddx8rV65kWeOGFy5cyNKlS1m5ciW5ubkAPP3006SkpODxeJg0aRJnn302qampO+Wzbt06XnzxRZ588knOO+88XnvtNS666KKd0hx55JF89dVXKKWYPXs2DzzwAA8//DB//vOfSUxMZMWKFQBUVlZSWlrK5ZdfzqJFi8jNzaWiomLvdlwI0ev0uKBwoDj00EObAgLAo48+yhtvvAFAUVER69at2y0o5Obmkp+fD8CECRMoKCjYLd/i4mJmzJjBtm3b8Pv9Tdv46KOPeOmll5rSJScn8/bbb3P00Uc3pUlJSenSfRRC9Dw9Lii0d0bP5hIoL4dxu93mocvFxsY2PV+4cCEfffQRX375JW63m2OOOabVKbSdLW7+Y7VaW20+uvbaa7nhhhuYPn06Cxcu5I477ohK+YUQvZP0KXSB+Ph4amtr23y/urqa5ORk3G43a9as4auvvur0tqqrq8nKygLgmWeeaVp+4okn7nRL0MrKSiZPnsyiRYvYtGkTgDQfCSH2KJr3aHYppb5RSi1XSq1SSt3ZShqnUuplpdR6pdTXSqmcaJUHiNrU2ampqUyZMoVRo0Zx44037vb+tGnTCAaDjBgxgptvvpnJkyd3elt33HEH5557LhMmTCAtLa1p+f/93/9RWVnJqFGjGDt2LAsWLCA9PZ1Zs2Zx1llnMXbs2Kab/wghRFuiNnW2MjcijtVa1yml7MBnwG+01l+1SHM1MEZrfaVS6nzgTK11u0eufZo6e+tW85gwwdQaxE5k6mwheq5unzpbG3WNL+2Nj10j0OlApA1kDnC86shd7TsrkrVc1SyEEK2Kap+CUsqqlFoGlAAfaq2/3iVJFlAEoLUOAtVAKtEiQUEIIdoV1aCgtQ5prfOBbOBQpdSozuSjlJqplFqslFpcWlra+QJJUBBCiHbtl9FHWusqYAEwbZe3tgD9AZRSNiARKG9l/Vla64la64np6emdL4gEBSGEaFc0Rx+lK6WSGp/HACcCa3ZJ9hbwi8bn5wDzdTRvGi1BQQgh2hXNi9cygWeUUlZM8HlFa/2OUuouYLHW+i3gKeA5pdR6oAI4P4rlkaAghBB7ELWgoLX+Htjt0mGt9e0tnnuBc6NVht1EgkIUZkrdW3FxcdTV1e05oRBC7Ee964pmS+PuSk1BCCFa1buCQpSaj26++eadppi44447eOihh6irq+P4449n/PjxjB49mjfffHOPebU1xXZrU2C3NV22EEJ0Vo+bEO/6969n2fY25s4OBsHjgeVusFo7nGd+33wemdb2THszZszg+uuv55prrgHglVdeYd68ebhcLt544w0SEhIoKytj8uTJTJ8+nfauz2ttiu1wONzqFNitTZcthBD7oscFhXZF6WLpcePGUVJSwtatWyktLSU5OZn+/fsTCAS45ZZbWLRoERaLhS1btrBjxw769u3bZl6tTbFdWlra6hTYrU2XLYQQ+6LHBYX2zuiprYUff4ShQyEhoUu3e+655zJnzhy2b9/eNPHc888/T2lpKUuWLMFut5OTk9PqlNkRHZ1iWwghokX6FLrIjBkzeOmll5gzZw7nnmsGVFVXV9OnTx/sdjsLFiygsLCw3TzammK7rSmwW5suWwgh9oUEhS6Sl5dHbW0tWVlZZGZmAnDhhReyePFiRo8ezbPPPsvw4cPbzaOtKbbbmgK7temyhRBiX0Rt6uxo2aepsz0eWLUKBg0CuTXlbmTqbCF6rm6fOvuAJFc0CyFEuyQoCCGEaNJjgkKHmsEkKLTpYGtGFEJER48ICi6Xi/Ly8j0f2CQotEprTXl5OS6Xq7uLIoToZj3iOoXs7GyKi4vZ4w14wmEoK4NQCMp3u21Dr+ZyucjOzu7uYgghulmPCAp2u73pat921dfDqFFw//1w003RL5gQQhxkekTzUYc5HOav39+95RBCiANU7woKtsaKUSDQveUQQogDVO8KCkqB3S41BSGEaEM079HcXym1QCn1g1JqlVLqN62kOUYpVa2UWtb4uL21vLqUwyE1BSGEaEM0O5qDwO+01kuVUvHAEqXUh1rrH3ZJ96nW+idRLMfOpKYghBBtilpNQWu9TWu9tPF5LbAayIrW9jpMagpCCNGm/dKnoJTKAcYBX7fy9uFKqeVKqfeUUnlRL4zDITUFIYRoQ9SvU1BKxQGvAddrrWt2eXspMFBrXaeUOhX4f8CQVvKYCcwEGDBgwL4VSJqPhBCiTVGtKSil7JiA8LzW+vVd39da12it6xqfzwXsSqm0VtLN0lpP1FpPTE9P37dCSfOREEK0KZqjjxTwFLBaa/3XNtL0bUyHUurQxvJEd/4JqSkIIUSbotl8NAX4ObBCKbWscdktwAAArfUTwDnAVUqpIOABztfRnq5TagpCCNGmqAUFrfVngNpDmn8C/4xWGVolNQUhhGhT77qiGWT0kRBCtKN3BgVpPhJCiFb1vqAgzUdCCNGm3hcUpKYghBBt6n1BQWoKQgjRpt4XFKSjWQgh2tQ7g4I0HwkhRKt6X1CQ5iMhhGhT7wsKUlMQQog29aqgoHUYbbNJTUEIIdrQa4JCScmrfPKJg6ClXmoKQgjRhl4TFGy2BCBEyBaUmoIQQrShFwWFVABCFj+EQhAOd3OJhBDiwNNrgoLdbu7dE7L6zAJpQhJCiN30vqCgvGaBNCEJIcRuek1QsFpjUcpJyNoYFKSmIIQQu+k1QUEphd2eRlB5zAKpKQghxG56TVAA04QUtDSYFxIUhBBiN1ELCkqp/kqpBUqpH5RSq5RSv2kljVJKPaqUWq+U+l4pNT5a5YFIUKg3L6T5SAghdhPNmkIQ+J3WeiQwGbhGKTVylzSnAEMaHzOBf0WxPNjtaQRUnXkhNQUhhNhNh4KCUuo3SqmExjP7p5RSS5VSJ7W3jtZ6m9Z6aePzWmA1kLVLstOBZ7XxFZCklMrsxH50iN2eSjASFKSmIIQQu+loTeFSrXUNcBKQDPwcuK+jG1FK5QDjgK93eSsLKGrxupjdAwdKqZlKqcVKqcWlpaUd3exuTEez1BSEEKItHQ0KqvHvqcBzWutVLZa1v6JSccBrwPWNgWWvaa1naa0naq0npqendyYLwASFsK3xhdQUhNj/tmyB7du7uxSiHbY9JwFgiVLqAyAX+KNSKh7Y4zwRSik7JiA8r7V+vZUkW4D+LV5nNy6LCrs9DW1vfCE1BSH2v/PPh4QEePfd7i6JaENHg8KvgHxgo9a6QSmVAvyyvRWUUgp4Clittf5rG8neAn6tlHoJOAyo1lpv62CZ9prdnoaO7LEEBSH2v9WrIT6+u0sh2tHRoHA4sExrXa+UuggYD/x9D+tMwfQ9rFBKLWtcdgswAEBr/QQwF9MktR5oYA+BZl9J85EQ3ai2FsrLobLS/P/Z7XteR+x3HQ0K/wLGKqXGAr8DZgPPAlPbWkFr/Rl76HfQWmvgmg6WYZ9JTUGIblRQYP6Gw7B5Mwwe3K3FEa3raEdzsPEAfjrwT631Y8BBVwe021OlpiBEd4kEBYCNG7utGKJ9HQ0KtUqpP2Kag95VSlmAg67uZ7W6UQ6neSE1BSH2r02bmp9v2NB95RDt6mhQmAH4MNcrbMeMEnowaqWKImtMinkiQUGI/augAGJiwOmUmsIBrENBoTEQPA8kKqV+Ani11s9GtWRRYo0xd2CT5iMh9rNNmyA31zwkKBywOjrNxXnAN8C5wHnA10qpc6JZsGixuhqDgtQUhNi/CgpMQBg0SILCAayjo49uBSZprUsAlFLpwEfAnGgVLFpsbnMHNqkpCLGfbdoEU6aAUvDZZ6C1ed6bHYCfQUf7FCyRgNCofC/WPaDYYhqnyZCaghD7T1UVVFc31xRqaqCiortL1b0KCmD8eDjnnAPqeNTRmsL7Sql5wIuNr2dgLjw76NjcfQAI+70HZ1QT4mAUGXmUk9N80drGjZCa2m1F6lbffAM//Sk0NMCyZXDhhfDii2Dr6CE5ejra0XwjMAsY0/iYpbX+QzQLFi12VzpagfZWd3dRhOg9ItcoRGoK0Dv7Ferr4e9/h2OOgdhY+PZbePhhmDMHLr3UXNjXzToclrTWr2Emtzuo2R3paBuEvDVYu7swQvQWkZpCbi44HOZ5Z65VCIVMM1RKSufLUlIC6en7ty3f44G//AUee8xM83HssfDSS9CnDwwfbmoMt90Go0bBTTftv3K1ot2aglKqVilV08qjVinVqWmwu1tk/qOwr7a7iyJE71FQYGZHTUoyZ8gZGZ2rKTz8sGmCqqzsXDn++lez7Z/8xEzj3ZrPPoOysrbzCARMB3FH+f1w7rlw992mhvD55zB/vgkIEbfeCqefDnfeaaYA6UbtBgWtdbzWOqGVR7zWOmF/FbIrReY/0l4JCkLsN5FrFCJn550Zlqo1PP20mVhvb6fe1hpuvhl+9zs48khYuBDy8uCZZ3Y+wD/2GBx1lDlj/+CD3fPxemHcODjttI51DodC8POfm/L++9/w+utwxBG7p1PKNCtpDb/97e7vh8MmaDz/fId3ubN6XV9rJCiEffXdXRQheo9Nm8wZfsTgwTsHBY9nz3ksXQo//miev97a7Vl28e9/mwP/EUfA4YfD/ffDFVeYgLB8OYweDZdcYg6227ebAPHrX8NJJ5kO8JNPNk05wWBzng8+CKtWwXvvmbx2rTH4/TB7NlxzDVx7rQker7wCDz0EM2e2X96BA+H2282+zd1lHM/s2fDWW/tnlJLW+qB6TJgwQe+LYNCjPX3QteeO36d8hBAdFA5rHRur9fXXNy+7/XatLRatfT6t//IXrR0Ord95p/18brhBa7td6wsu0DomRuu6urbTLlumtc2m9dixWp9wgtYTJpjthMPNaUIhrf/6V62dTq2Tk015TjhBa49H6/p6ra+4QmvQ+pJLTNpNm7R2ubQ+7zyt77jDvPenP2ldXW3e++9/tc7NNcuTk5sf99zT8c/K59N6+HCTz7ZtZtnWrVonJmp9zDE7l38vAYt1B46x3X6Q39vHvgYFrbVu6Kd0zfTh+5yPEKIDSkrMoeaRR5qX/fe/Ztmdd5q/LpfW8fFar1rVeh7BoNb9+ml9+ulaf/yxWee111pP6/drnZ+vdUaG1uXley7f6tVaH3mk1scfv3ugiRz8r7lG6zPO0Nrt1nrzZnNwvuQS817Lx/jxWs+du08Hb/3ZZ2Y7AwZovXy51uecYwLX2rWdz1N3PCh0/6DY7mC3of0dqK4KIfZdy+GoEZFhqX/6Exx9tOkrmDIFpk+Hr7/e/fqFTz6BrVvhggtM+tRU08xy1lnm/bo6sFjA7TbNRMuWwRtvdGyU0vDh8Omnrb93++0m74ceMq/vvRf6N95BeNYsmDDB9DOkpprmsWOO2fdRTVOmmPL89Kdw2GEm/3vugSFD9i3fDuqVQUHbbWift7uLIUTv0PLCtYhDDjF/hw1rPni/8YY5qJ5+Orz55s6B4YUXIC7OjBqy2UyaOXNMG/u775pg4fVCZiaUlsLPfgZnnLHvZVcKHnjAdPQuXrxzJ7DdbvogomH8eHMNw5lnmm3//vfR2U4rohYUlFJPAz8BSrTWo1p5/xjgTSAyyfrrWuu7olWenTjs6IAEBdHLeL1mbPzq1bBuHRQVmTH7FRWmY/Tee/ftLPfbb+Gii8xBOTvbDP2srm49KGRmwlNPwQknNJ/NH344/O9/ZrTOpEnw//6f6Qz+/nsTAM46y9QEwDx/+mm47DIzImfSJHNmvWGDGZ306KOd349dKWWGwu5v/frBV1+ZEUz78UrnaG7pv8A/MbftbMunWuufRLEMrbM7wFe33zcrxH4RDpuRNwMHwqmnNi+/7jp48klz8digQeb9kSPNmP/774e0tOYz0vfegyVL4Je/hKysnfOvqzPNKnPmmNE1V1xhpm249lro29ecpRcXw44d5oA/fLg5aCfsMor90kt3L/u555pynXmmCRLp6VBYaM7Kr7iiOd3xx0N8PDz3nCnDyy+b6x96GqX2/9QXHel46OwDyAFWtvHeMcA7e5tnV3Q0ew7N1ZVj0T5f6T7nJcQBpapK65/+1HR6Op1aL11qli9caJb99rem07alUMiMqAGtH31U67PPbu44dTi0vvpqrd9/X+t587R+5hmt+/fXWimtjzvOdBBH0p58stZlZV2zH1u3an3mmWZfZs/Wevv23dM8/LDWN92kdSDQNdvs4ehgR7MyaaNDKZXTeOBvq/noNaAY2Ar8Xmu9qo18ZgIzAQYMGDChsLBwn8oVOHYiDaVLCCx8m7S0/V9REb1ISYm5AnbXs+32VFSYC5lcLhg61HRshkLg85nnbd3wfs0a01G7aRP8+c/mQiy73VxBe8wxphwrVzY3wbTk9Zpx+YsWme3edpuZvfPhh+E//9l5qvlRo0wn6+GHm1rG88+bs9nLLwerTB5zoFJKLdFaT9xjwo5Ejs4+aL+mkADENT4/FVjXkTy7oqYQnnaSrh6G3rDhj/uclxBtqqsz480zMnY+g1671gwzfPvt3Ycufvpp85n4rsMdI4/DD9f63//eefhkYaHWWVla9+lj8tBa6y+/NOP609LMeh980H55Kyq0vusurdev33n51q1af/65GSr55ZdmyKc46HAgXKfQXlBoJW0BkLandF0RFPTpp+v6oTH6u++O2fe8RO/zww9av/HGntP95jfmX8xq1fqii8yy+nqtR41qPsAfc4zWL7xgxvBffrm5gGrwYK0XL9a6pkbrJUu0fustrd97T+v587V+8EGtR4406+bkmAN9aam54Ckx0Yxrb+kf/zBpL7646z8HcVA54IMC0Beamq8OBTZHXrf36JKgcM452js4WX/yiVuHQnLW0+ssWLD72fCrr5qrXj/+uP11f/yx+cz7qafaTvf55+Zs/+qrzdW7YGoGF19slr/zjtb//KfW6enNASI21lwQVV3dfhnCYbMPw4aZ9TIzTf/BokWtp/3wQxOMRK/W7UEBc0OebUAA02/wK+BK4MrG938NrAKWA18BR3Qk3y4JChdcoAO5ffWCBeiamsX7np84eBQWmiaVnBxzJq61adpJTW1usvn1r1ufQmHbNtMclJam9dFHmxrAe+/tnCYcbj5zHzDAbMPn03r0aHPQj0yNEFFbazqDS0v3/ipYj0frW27ROimpYzUX0at1e1CI1qNLgsIll+hwnzS9YD66qOjRfc9PRN/f/25G0LRUW9vcft5R11xj5sRRSuuZM82ymTPNAf7rr5ubfMaO3bkfoLLSTGHgdpt0NTVmKoXYWK1//3szUiYvT+u4uOYz/3nzmtdfvNhs48QTdx/9s6/2ZUoF0WtIUGjPM89oDfqHh1L1qlXn73t+Irq2bDEH8UGDdu7kvOAC8xOeOVNrr3fP+WzdappZLrtM6xtvNOvedZf5+7vfNaebO9ekGzfOdL6uWKH1kCEmmLSctG3rVq2HDjXLhw838/L85jdmqOQnn+y+/R9/NGf3QnQDCQrt8fm0zs7WdRPT9RdfDNz3/ER0PfJI89n3rFlm2dKl5nV+vvk7ebIJHu254QZztr5+vTk4Rzpss7Kam5Ii3nvPjNEfOdLUBvr2bb1WEgzKaBxxUOhoUOh191MAzBWtnrqqAAAgAElEQVSdN9xA7OJSHN8V4vO1cQcmcWB4+WUYMwYmTzbj730++MMfzNWyCxbAq6/CihXmFoe1bdw8qbQUnnjCzJEzeLAZi//MM2a6hccfN1fHtjRtGrz2mpkOYuxYc3XvkUfunq/V2nwjeiF6go5EjgPp0SU1Ba21rqnR4cR4XXI0eseOV7smT9H1CgrM2fxf/mKGX4IZ4w9mLvyIhQvNcM7zzjNt7OGw1n/7mznLj4kxbf1KmWmSW9pTe/z27XLFrOgRkKmz9yA+Hq6+mrT77qdw2Ztw0jndXaLeoa7OTKU8areL3Fv3yivm74wZZurlo44yc+4MHAhXX92cbupUM73wH/9o7rS1YQP84x/mKt3Ro82N0ceMMfPwtLSnCeAyMjq8a0L0BFGd5iIaJk6cqBcvXtw1me3YQXhAP3ZMj6HvK9UoJZfoR1UwaCYy++wzM4HahAlmeXW1adZJTDRzyR9zjLmNIsDEiWae/G++Ma8//dS8/7//mYnXWorcx/add8zr3/3OTHts6Z2tpEK01NFpLnpvTQEgIwP/CRNI/uRbqio/ITnluO4uUc92661mbp3YWLjqKvjyS9Mm/5vfwPvvmxk2X3zRpD39dHNP2yVLdp62+KijzOybaWm752+xmH6CyFz6V121f/ZLiB6k159COc7+Fa5SqFrYhfOvi929+aY5a7/ySjOt87ffmknVXn/dHMhvvdVMt1xQAHffDR9/bKZEBjOdckutBYSIlBSYN08CghCd1LubjwBKS9F9M9h8sYPs2VVYra6uy1sYy5ebNv8hQ0zTkcNhbq6yZImZXTMnx9QaWo7i2bHDBAe7Hf76124ruhA9RUebjyQoAIHDR+EtX4X389dITz+rS/Pu9T77zNxCMT7ePB840Cxfs8Z0/Fqt8N13u3cACyG6VEeDQq9vPgKwnvlz4tdBxfInu7soB75nnzXXDbQUDpu7Y4XDzctCITNK6KSTTF/B5583BwQwQeDVV02zkgQEIQ4YEhQAy+nmBt+Wdz4kEKjq5tIcQHatRT71FPziF3D++ebeuB4PLFtmhoDm5JhbJ551luno7dPH9AWMGGFGDA0YsHv+p59ugoYQ4oAhQQFg2DBChwwg9fMQJSXPd3dpDgzz5pkhojNnwtat8O675h65J58Mt9xiAkRenhlWunEj/OUv5iD/3XewcKG5ifrLL5smo/T07t4bIUQHSZ9CI33jjehHHmbpvFwmHLsOpXp4vAyH4cMPzcH/o4+gXz9zW8XUVCgqgnHjwOk000NEbhw+cqQ54MfFmSBx1VVwyilw332QnNytuyOEaJ/0KewlddZZWIKakb/YSP1frjAXVPVU4bBpBpo2zcz7k5ZmDvZHHWWGhM6YYeYXmj8fVq82Y/7z8kwgiIszeZx2GmzebIaXSkAQoseQoBBx+OGEX3qRUKKDuP+bbdrI33yzu0vV9bSGX//aXBF8++3mxuvz55saw5YtptP3yy9h9mwYNsxMHvfCC+a6ApnyQYgeT4JCC5YZ51M59x6WPAGh3Exzhvz730Mg0N1F6xpeL9x4I/zrX2aW0TvugJgY897Uqaa2kJYGv/2tqS0IIXqdqAUFpdTTSqkSpdTKNt5XSqlHlVLrlVLfK6XGR6sseyMz8zIaRsax7j/j4JprzBQLp50Gfn93F61zwmH4+mtTO+jXz+zP1VfDvffuPhncuHGmSUguFhOi14pmTeG/wLR23j8FGNL4mAn8K4pl6TC7PYm+fX/FjqpX8T50Izz5pOmQveyy3YdoHqjCYTNNxGWXQVaWuQ/B7NmmD+GDD+Cf/2x7dlCZPE6IXi1qE+JprRcppXLaSXI68GzjPN9fKaWSlFKZWutt0SpTR/Xv/3u2bn2CwsK7GXbZk7B9O9x2mxlrf/fd3V28ndXVmbP/qirT9BMMwnPPmamjExJMIJg+3dR2kpK6u7RCiANcd86SmgUUtXhd3Lis24OCy5VNv35XsGXLY/Tv/wfct95qrti95x7T2XrttV27wVDITPewt774Ai6+2FwnEBfXfNexo4+Gu+6Cs84i7HBRXg7bi6BkCZSVmYFFwaDZbCBgnvv9psvB6zUVIqvV/K2uNn3RtbUmXTBoLlA+7DDIz4cff4RPPoHvvzebjoxe9fubu2KsVrM8Ls7EKbvd5FddbcrgcJhlXq+57YHH07yLdrtZLy7OlMfjaS6nz2cqRbGx5uFymW1ZrSb/ykoTMyM3R7NaTXqtTYUokjYcNuWIlN9uNxWpyP76fM2fWWysmbHD5TLrhcNmP30+s8+RyqTFAm63eShl9quhofnzbnnxt9W68z7W1zd/BpHPLpKX1dr82fp85nMIBEx5YmNNWo+neVuh0M7bguZKosNhLkVJTDTL/H6TZ329eWhtLjFJTzd5lJSYEcqBQPN+Rj5Dh8N0T8XEmPfr600ZlGpOY7GYh8PRvD+BgEnn9Zq0FovZlsdjHi2/q9hY8xk5nTt/J2DSRT6TYNCsY7M1bzfyOTocZn23u/nzqqyE8nJThl0/p0h5nU7zN/LweMx6lZWmvJG0kXyh+XOMfNY2m0kbKXMk35gYs05MjPm9VlRATU3zb7VleX77W7jzzvYPC/vqoJg6Wyk1E9PExIDWroyNggED/si2bU9SWHgXI0Y8azpnS0vhuuvMN3XddeZX+J//mF/GTTft+YYtrXnjDTM89JFH4NJLO7ZOURHcfz/Bx2dR0O8I1t73BkWJo9lSGGRrcYiSKiclj8L2W8x1Z3vTT26xmN2I/CATE83Eo/HxzQfWpUvNxKYRqanmtgc2mzkIad18oAezLBg0P/gtW8zBJyHBrGezNR/kUlOhf3/zzxH5KP1+s15trSlbaqp53+Uy/1CRA25trTkghELm0aePGTwVH2/2xe83yyP7p3VzYIwcNKA5EITDzfvrdJqHzWb+yWtrzQEkcpCz25sPGpHWt1CoORBEApfbbdJEDlSRfYwcRCP7GDlAtPzsIgf6YLD5s3W5zMNmM+Wprzd5ud1mfaezuYyRbbU8yHi9JjBXV5v3Iwe8SJBVyvzkd+ww2xs/3gQIp7M5j8jn7fc3lzESyN1us71Imshzv7/5s7HbTbpInlqb7UYCTCRIBIMmfV2dKXfL7ySyb5HvwW5v3mbku4w8jwS+hgbzeXm9pqtt9GhTjojITcEjv53IepG/6enmpCgpaeffTuQz0Lr5O498x8Fgc3DSujkvj6f5RCA11XTtJSQ0f2+R3yvApEkd/1/urO4MCluA/i1eZzcu243WehYwC8zFa9EvGjidfcnK+jVFRQ8xYMAfiY0dYe4Cdv75Zv7/tWtN+/y6dWaFtDT41a+aMwgGm0+b21JcbNbx+czfsjITXKD59M5iQWsoXlrCsv9XwPJ3i1m5LMBKfSVrLY8Q2GKDP0SS2sjIsJGRYQ6KQ4aYLoV+/cytiPv0MT/mlmfUdnvzIyZm5yJH/kF3pbW5nGH5cjNiNS9PuiKE6DE6cs/Ozj6AHGBlG++dBrwHKGAy8E1H8uyyezR3gM9XqhctitMrVpzVvNDv1/rss82JxMiRWr/5ptbHH6+12631Dz+Ye/4+/rjWTqfW06ZpvWhR65mHQma92FitV63SesYMk+eJJ2o9fryucPbVL3K+vsj2gk5XJbr53EXr3IRS/dMT6vVNN2n91FNaf/aZ1kVFcithIUTb6O57NCulXgSOAdKUUsXAnwB7YyB6ApgLnAqsBxqAX0arLJ3lcKTRv/8fKCi4jfLy90lNnWZOqV96yQzznDzZnG5PnAhjx5paRH6+mUn0iCPM/QKOPto8v+oqOPtsczru88FDD5kRQk8+CSNHEvjv83yupzJvnuZjfTxL/EMIYyHVVscpuas4fMz35B+fxugzDyG+bzs3mRFCiH0gcx/tQTjs49tvx6J1gEmTVmK1xrSe8N13zX0DlII//cmMVvJ64emn4e9/h/XrzXQQOTmwciUEAlSedhFvn/ssb76l+PBD06Zss5lO3BNOMBOIHnZY5/qghRCiJbnJTheqrFzA8uXHMXDg/5Gb++e2E77wghmddPzxOy8Ph83VwrNnU7W1gTfcF/LS1qOYvyqDYFCRlQWnnmoexx1nOpmEEKIrdTQoHBSjj7pbcvKxZGT8nM2b76dPnwtMp3NrLrig1cXllRbe3nwcr9Uex7wvzEiEQYPMDBpnnmlan6SjVghxIJCg0EGDBz9Eefk7rFlzCePGfYbFYm83vdbmVgKPPmpGnYZC5tq3a681XQ8TJ3ZuBKsQQkSTBIUOcjj6MHToLH744VwKC/9Mbu5drabbvNmMXP3f/8yQzeRkuP56EwgmTJBAIIQ4sElQ2At9+pxDefkvKCy8h5SUaSQmHgGYC09efdX0KX/6qUk7YQLMmgUXXrjzRTFCCHEgk6Cwl4YMeZTq6kWsXn0RFsv3/Oc/cTz/vBk5NHSomRppxgw45JDuLqkQQuw9CQp7SesEvv/+fR5+uIa1a+NwueC888yEpEceGZ3mIU/Agz/kRymF0+rEaXPueSXMhYlV3ioKqwvxBr04rU7cdjeHpByC1bJ341y11pQ1lFFcU4zL5mJwymAcVkdndmePguEggVCAYDiIy+bCbm2//2ZXYR2mzl9Hja+GWl8tDYEGPEEPfWL7MDR16F7lVeGpwKIsJDgTsLRyi1ZPwEN9oN5c+IPGaXXisrlwWB2oXX4MkZF+uy7vrGpvNStLVlJQVcDglMGM7jOaWEfsbuki3/2etvtj2Y+8vvp1nDYnae40kl3JuO1uYuwxBMNBqr3V1PnryIjLYHDyYLITsvGH/DQEGgiGgzhtTpxWJw2BBso95dT4akiNSSUzPhOXzdXh/fIEPKyvWM/a8rUU1RSxrXYbFZ4KJmVN4pRDTiE7IZt1Fev4suhLyhrKiHXEEmuPpU9sH7ITsumf2J8EZ9cM4fMFfVR4Kqj0VhIMB0lzp5HmTqPeX8/m6s0U1xTTEGjAF/JhVVaGpg5leNrwVr+Hjor8r62vWM+6inUEQgEGJQ9iUPIgshOy9/p/d29JUOggj8fcufJvf4MtW4YyZEgp1113DVdffRRDh85gbflanlg8H7vVzoTMCeT1yaMh0EBxTTEFVQWsKlnFD2U/sLV2a9OB6rjc4/j9Eb8nOyGbDRUbePCLB1ldtppjc45l2iHTKKwq5Lnvn+P99e8T0qGmsvRP6M+wtGEMTBxIsiuZRFciVd4qimqK2Fq7lWpvNbX+Wsobyqn11+62L9kJ2Vw4+kLOHH4mbrsbjeaH0h94e+3bzFs/j1R3KsfmHMv4zPH8UPoDnxd9zoodK/CFfE15WJSFnKQc0t3pJLoSibHFUO2rpsJTgT/kJ9YeS4w9hnp/PaUNpdT6ahmRPoLDsg4jIzaDxdsW83Xx11R6K3HZXDitTnwhH3X+Ovyh5ntXuO1upg6cyomDTiTBmcC2OnOAyEvPY8qAKSS5knhj9RvMWT2HNWVrqPHVUOeva/N7nDpwKldPuhqH1cHHGz/mm63fkBmXSV56HlkJWZQ1lLGtdhvrKtaxomQFJfUlACgUCc4EkmOSSXaZ248W1RRR1lDW6nZibDFMyprE4dmHY7PY+Lzoc77Z8g0NgQYcVgd2i73pIG2z2EhyJZHkSsJlc6FQWJSFeGc8Sa4kEp2JxDviiXfGU+OrYVXpKlaVrKKopminbSoUAxIH0Ce2D+mx6dT4alhXvo4d9TtIcCYwNHUoAxIHEAwH8QV9uO3upgPN3HVz+XDjh3v6N+i0YanDeO2818jrkwfAih0r+PkbP6ekvgSH1YHVYm0KsDW+mp3WdVgdxDnimP3dbAASnAm7pdlVv/h+jEwfSW5SLnGOuKbf5+bqzWyp3YI/5G8K5JG/YD5DgFp/LRWeChoCDZ3a3+yEbAYnD2ZQ8iAcVgeeoIdgOEi6O51+8f3IiM0gOSaZRGcihdWFfFH0Bd9u/ZattVspaygjGA62mu/1h13P36b9rVNl6ii5TmEPQiFzgfLtt5upio44eRtjZsxhq+sjSioX4QvUUhrqQ3HtzpO7KlTTDy2iX3w/BiYOJN4Zj0Lx8aaPUSiOHHAknxR+gs1iIy89j2XblzWtm52Qzfl555MZn4nWmjp/Hesq1vFj+Y9sqdlClbcKT9BDjC2G/on9yYrPItFlDiJJriRyknIYmDgQt92NL2TOel5b/Rrz1s/bKdAApLnTmHbINCo9lSwqXEStv5YYWwyHZh3KxH4TGZg4kKyELDwBD2vL17KuYh3lnnKqvdV4gh4SnYmkxKRgt9qb/sHddjfp7nRi7bGsKFnBkm1LaAg0MDh5MJOzJ5MZl4k36MUb9OKyuYhzxOG2u7Fb7dgtdgqqCvhg4wesLV/bVE6XzYU3uPOUlsPThnNE9hFN+x75G++MN2e7thiWbV/Gvxb/i01VmwATcCb1m0RJfQlry9c2fR5p7jQGJg5kTMYY8tLzsCgLVd4qKr2V5uGpJKzDDEgcQP8Ec1YaOcD7Q368QS876nbw1ZavWLptKVpr8vvmc0T/I0h2JeMP+XcKfP6Qn2pfNZXeSnxBHxpNWIep9dVS6a2kyltFnb+u6Yx/eNpw8vrkMSp9FKMzRpOblMv6ivUs37GcdRXrKK0vpbShlFh7LENShpCTlGP2sWItxTXF2C12HFYHdf46NlZuxBfykRWfxVUTr+JX43+F0+qk3FNOpacST9CDJ+DBZrGR6Eok1h7LtrptbKjYwJbaLcTYYnDb3VgtVnxBH76QCTYpMSkkOBMobyhnS+0WHvv2MbxBL2+e/ya+oI+zXzmbeGc8px5yKoGwqRXG2GKIdcSSGpPK0NShDEk1ZY8E4dVlq5m7bi7rK9Yzsd9Ejuh/BFnxWTQEGqjz17GjfgfFNcUUVhXyQ9kPTYGzIdBAQ6CBBGcCAxIHkJ2Q3VRzUqimv0BTkEhwJpASk0KyK5mUmBRSYlKwWqyUN5RT2lCK2+5uyivOEdd0UvNj2Y+sLlvNuop1bKjYwKaqTYTCIWLsMViUhZL6klZPWhKcCRyWdRgDEweSHptORmwGh6QcwpDUIdgtdjZWbmRD5Yamk6HOkIvXusDHH8MNN5hpoUec/Ckxp9zJd1Xz0WiGpAwhxZWAt2E56TFxnJX/Z0465BQ0miVbl7CqdBWJzkSyE7IZkDiA4WnDSY7Z+Qb3BVUFPPTFQ7y77l3OGXEONxx+A5nxmZQ3lDN/03xS3alMHTh1j9VFf8i/05lnR+yo28Fnmz9Do1EoshKymNRvUtO2guEgBVUFDEwcuNfNN+0JhoPU+etIcu3dvR2KqosI6RB94/ritDpZW76Wz4s+p6S+hJ8M/Ql56Xkd2v9QOMSCggU4rA4mZ09uagLzh/yUN5ST5k7r0v31BDxoNG77vo82CIaDKFSXNh+EdZiS+hLS3GnYLNFrOCioKuCU509hY+VGwjrMiLQRzL1wLtkJ2VHbZkta6y5rtttXtb5aSupLqPJWUeWtIiMug5HpI1ttnuxKEhT2webN5nqCt971kzHpCzLPfoBl9e+RGZfJzAkzmZE3gxHp5gK2srK3WLnydDIzL2fYsFlRLZcQB7MKTwXnzzkfl83Fc2c+R6IrsbuL1KvIFc17qd5fz4NfPMiSNSV8+HGAYNJWHLd9wg7q8YeSeeCEB7jm0Gt2O+NLS5vOgAF/ZPPme0lIOIzMzF+1sQUhereUmBQ++PkH3V0MsQcSFDDV8hlzZjB33Vx0fSq2wXZyMpI4aejFnDjoRE4YdALxzvg218/N/TO1td+ydu01xMXlEx8/YT+WXgghuk6vDwpaa65652reXfcuvPMvLhp+Jf/+995dcKaUlREjXmTJkvGsXHk2EycuxW5PiV6hhRAiSnr1NGxaa26bfwezv3sSFt3KbadcybPPdu4KZIcjjby8V/H7t7JmzS/QOrznlYQQ4gDTa4OCP+Rn5jtXcM9nd8F3l3D/yX/mrrv27eKzhITDGDz4YcrL32Hz5ge6rrBCCLGf9MqgUOGp4OT/nczspU/Colu49/CnuOmmrhmulpX1a9LTz2PTpluprJzfJXkKIcT+EtWgoJSappT6USm1Xil1cyvvX6KUKlVKLWt8XBbN8kTcNv82Pt/8OXEfPMfhnnu46cau+xiUUgwbNhu3exirVp2Hx7Opy/IWQohoi1pQUEpZgceAU4CRwM+UUiNbSfqy1jq/8TE7WuVpaXXZapIaJuL79iJmzer6G9zYbPGMGvUmEGLlyjMIheq7dgNCCBEl0awpHAqs11pv1Fr7gZeA06O4vQ5bs72Q0nU5/OEPMGpUdLbhdg9h5MiXqK9fyZo1l0jHsxDioBDNoJAFtJyxq7hx2a7OVkp9r5Sao5TqH8XyAOay/u0NRcSHB3LrrdHdVkrKyQwadD+lpXPYtOn26G5MCCG6QHd3NL8N5GitxwAfAs+0lkgpNVMptVgptbi0tHSfNlhYsQ1tCTAudyCujs/m22n9+/+OzMzL2Lz5HrZtezr6GxRCiH0QzaCwBWh55p/duKyJ1rpcax2Zj3k20OqlwFrrWVrriVrrienp6ftUqLmfFwJw9NiB+5RPRymlGDLkcZKTT2Tt2iuoqPhov2xXCCE6I5pB4VtgiFIqVynlAM4H3mqZQCmV2eLldGB1FMsDwIffFgDwkyP3T1AAsFjs5OW9its9nFWrzqS6+ov9tm0hhNgbUQsKWusg8GtgHuZg/4rWepVS6i6l1PTGZNcppVYppZYD1wGXRKs8EUs2mJrCqOz9FxQAbLZExoyZh8ORyfffT6Om5uv9un0hhOiIXjV1dmUlpFx8JTET5tBwR+t3zIo2n28L3303lUCgjLFjPyIhYY8z2QohxD7r6NTZ3d3RvF/Nnw8kFtI/Yf/WElpyOrPIz5+P3Z7M99+fSG3td91WFiGE2FWvCgoffgiWlEJGZHZfUABwuQYwduwCrNYEli8/gbq677u1PEIIEdGrgsIHH2pUUiG5yd0bFABiYnLIz5+P1epm+fLjqan5pruLJIQQvScobNwIm7aXE7I2MDCp+4MCQEzMYMaOnY/VGs+yZVPZseOl7i6SEKKX6zVBYdEiINGMPBqYeGAEBTDTYYwf/zXx8ZNYvfpnbNp0m0yJIYToNr0mKPziF/CP5xqDwgFSU4hwONIZO/Yj+va9lMLCu1m16jyZRE8I0S16TVBQCgLuA6+mEGGxOBg2bDaDBz9MWdnrfPfd0Xi9xd1dLCFEL9NrggJAYXUhsfZYUmIOzPsnK6Xo3/8GRo16C49nLUuWjKO8/P3uLpYQohfpVUGhoKqAgUkDUftyz839IC3tJ4wf/y0ORyYrVpzChg03EQ77u7tYQoheoFcFhcLqwgOy6ag1sbHDGT/+a/r1u4qiogdZsmSCTI0hhIi63hUUqg6eoABgtcYwdOjjjBr1NsFgFUuXHs66ddcRCFR0d9GEED1UrwkKtb5aKr2VB9zIo45IS/sJkyatol+/q9iy5Z989dUgCgvvJRRq6O6iCSF6mF4TFAqrzcijnKSc7i1IJ9lsCQwd+hgTJy4nKeloNm26hS+/HMCGDX/A49nU3cUTQvQQvScoVB24w1H3RlzcaEaPfotx4z4jKWkqRUUP8/XXg1m27AS2bfsPwWB1dxdRCHEQs3V3AfaX5Jhkzhl5DoOSB3V3UbpEYuIUEhOn4PUWs23bbHbseI4ff7yUtWuvJDHxCJKSjiMl5STi4w894EdbCSEOHL3qfgo9mdaampqvKS2dQ1XVfOrqlgEalyuHPn0uoE+f84iNHSMBQoheqqP3U5Cg0EMFAuWUl7/Ljh3PU1n5ERDG5cohNfWnuN3DsNszcDjSsdlSsduTsdv7YLHYO5y/319CYeHdZGRcSELCYdHbESFEl5CgIJr4/TsoK3ub8vI3qaz8iHDYu1sapRzExuYRF5ePw9EPmy0emy2ZuLh84uLysVgcTWlrar5h1aqz8fmKsVhiyMubQ2rqqftzl4QQe+mACApKqWnA3wErMFtrfd8u7zuBZ4EJQDkwQ2td0F6eEhT2TTgcJBgsx+/fgd9fQjBYSSBQjte7ibq6ZdTVLScQKAGafxdKOYmNzcPpzMZmS6ak5EWczn4MHfoEGzfeQl3dcoYMeZSEhMNQyobF4sZuT8NmS0QpC1prwmEvXm8BXu9GgsFaYmNH4XYPa6qdaB1GqV4z7kGI/a6jQSFqHc1KKSvwGHAiUAx8q5R6S2v9Q4tkvwIqtdaHKKXOB+4HZkSrTAIsFhsORwYOR0abacxBvAG/v5S6uiXU1HxFff1KvN4C/P4vSEk5heHDn8JuTyUh4QhWrjyTdeuuaSUna2NQCLS6HaWc2GwJhEK1hMNe7PYM3O7hxMTkAgqtgwSD1Xi9hfh8m7FYYnC5BuB0DsTp7IfD0ReLJQavdxMezwYCgXK0DqB1kJiYQ0hIOJTY2NGEQg0Eg5WEw16UsmOx2AmHA4TD9YRCHiwWOxaLC7AQCtUSDFajlKWxaS0Vh6MPDkdfrNY4PJ711Nf/QCBQitUah9Uaj82WhN2egs2W0hQEtfbh92/H79+O1mEcjkwcjr6NwTIJmy0BMP07wWA1Hs+PNDSsRSk7MTGDcblyAU0oVE8wWIXPV4zPV0w47G3cXjIOR9/GQJ2Kz1eMx7OeUKgGpzMLp7M/dns6Vms8Fouz1b4kn287dXXfUVe3HAhjsyVjt6ficuUSE3MIoKms/IjKyo9QykFKyjSSk4/Fao3dLa9QqB6vtwifrwirNQ6XKxeHIwOlVOPnEURrP+Gw+Vw8nvV4vZuwWhNwuQbicGSilK3xNxrT+N3ufHjy+8toaFhNIFCC3W6+E3PiYUMpG1ZrXKsnFuFwgFConlCohmCwmmCwAo9nPQ0NawgGq4iPP5TExKNwu4c0fieqRXm9+P0l+P07CATKCAYrCQarcDgySEg4gpiYwTt9tsFgHV5vAaCJiRmM1ereqRymrOYzCcLIFzEAAAnfSURBVIXq8Pt3NH63Rfj92xp/t4fjdGa2+j+zP0StpqCUOhy4Q2t9cuPrPwJore9tkWZeY5ovlflFbAfSdTuFkprCgScc9lNVtYhwuAGtg4RC9QQC5QQCZUC48UDsxOkc0PiPEkt9/Qrq6pYRDNZisyVgscTg8xXT0PAjXm8BSqmmf3SncyAu14DG2oYJED7fVsJhc/GexRJDTMxg7PZ0LBYnoKiv/wGfr7CTe2TB1JTa+9+wAqFO5r+/WTHnaGafzP06On7PDqs1Ea0DhMMNKGXHZktCKTtKWQiFGgiHG9pskgTQujPzdllwODKwWJyEw37C4QaCwao9rmO3p2KzJRIOexsDQX2b21fKidXqJhis7ET5DJstFas1DggTDnsaf/PNHI5+AASDFY2fkcJicTelb4vd3ger1Y1SDpSyNn1nmZmXM2DAjZ0qa7fXFIAsoKjF62Jg1x7JpjRa66BSqhpIBXb6ZJVSM4GZAAMGDIhWeUUnWSwOUlJO2Kt14uLGkJFxYae3ac60agmHPdjtfdo8E/Z4fmw8m0/GYolprEkEUMqO1RrbuCxIOOxF62BjgHIDmmCwujG4leD3bycYrCYmZjBu9wgcjnTCYR/BYC3BYBXBYDmBQCXmYKuwWBzY7RmNZ3yqsdawjUCggmCwilCoBq01Sikslljc7qHExAxB6yAez4bGwGjDao3FZkvA6czG6czGYnE1nu1W4vNtw+crJhAoxensT0zMIdhsifh8W/D5iggEygmFagmF6prKBTQGCIXdnkpc3Dji4sailLOxKbEMr3cjHs96wmEvSUnHER8/Ca2DVFd/SlXVfILB6sYJGkNYLO7GMiY31uKyCYXq8Hg24fNtBixYLI6mEwOLxYndnkZMzBBcrlxCoVq83s34/duIBKpQqK5xH7Y0flcOLBYXMTGDGj/7TAKBUvz+bYRCtWgdQusAwWAVfn8poVA1FktM4/cb21iji22q1dlsSY21sQGABY9nLVVVn+L3b8UETd3YDOpoLG86DkcGdns6NlsKNlsi3v/f3t3HXF3WcRx/f+LJB5xIqStQwWQVtURrRlmNaX9gufQP7EnNubn+0aWtVtp6WG790dayWs1samExtQiLNdcTOso1UBArgZpkrnAotBQQRFQ+/XFd53i8uYF7Bw6H3zmf18bu8/uda2fXxfe+z/f8ruv8vteuJ9i27c9s376aPXt21zgeVT/AzEASO3c+xvPPb0Aaz4QJUxk37jjs3e1KBK0r9hLbU5k48SR27FjPtm0r2LlzLXv2vND+f25ddU+aNL3rv5mx6uWVwgJgvu2r6vHlwLtsX9PR5tHaZmM9/mdt89/RXhNypRAR0Y2xXin0cmXvSeCUjuPp9dyober00fGUBeeIiOiDXiaFh4BZkmaqTC5+DFg6os1S4Ir6eAFw3/7WEyIiord6tqZQ1wiuAX5LWZW73fZaSTcCq2wvBW4DfiJpA/A/SuKIiIg+6WntI9v3AveOOPeVjse7gEt62YeIiBi73C0UERFtSQoREdGWpBAREW1JChER0da4KqmStgDd1i94HSPulh4ggzq2jKt5BnVsTR/XabZPPFCjxiWFgyFp1Vju6GuiQR1bxtU8gzq2QR3XSJk+ioiItiSFiIhoG7ak8MN+d6CHBnVsGVfzDOrYBnVcrzJUawoREbF/w3alEBER+zE0SUHSfEn/kLRB0vX97k+3JJ0i6X5J6yStlXRtPT9V0u8lPVZ/ntDvvnZD0jhJayT9uh7PlLSyxu1utbbzahhJUyQtlvR3SeslvXsQYibpM/X38FFJd0o6qqkxk3S7pM11n5fWuVFjpOK7dYx/lXR2/3p+aA1FUujYL/oCYDbwcUmz+9urrr0EfNb2bGAucHUdy/XAMtuzgGX1uImuBdZ3HH8DuMn2GcAzlH29m+g7wG9svxk4kzLGRsdM0jTg08A7bb+NUg25tdd6E2P2Y2D+iHP7itEFwKz671PAzYepjz03FEkBOAfYYPtxlw1b7wIu6nOfumJ7k+2H6+PtlDeXaZTxLKzNFgIX96eH3ZM0HfgQcGs9FnAesLg2aeq4jgfeTykVj+3dtp9lAGJGqbR8dN0k6xhgEw2Nme0/Ukr4d9pXjC4C7nCxApgi6fWHp6e9NSxJYbT9oqf1qS+HjKQZwFnASuBk25vqU08BJ/epWwfj28DneWVX+dcCz9p+qR43NW4zgS3Aj+rU2K2SjqXhMbP9JPBN4N+UZLAVWM1gxKxlXzEayPcUGJ6kMHAkTQZ+AVxne1vnc3X3ukZ9rUzShcBm26v73ZceGA+cDdxs+yxgByOmihoasxMon5hnAm8AjmXv6ZeB0cQYdWNYksJY9otuDEkTKAlhke0l9fTTrcvX+nNzv/rXpXOBD0t6gjK9dx5lHn5KnZqA5sZtI7DR9sp6vJiSJJoesw8A/7K9xfaLwBJKHAchZi37itFAvad0GpakMJb9ohuhzrPfBqy3/a2Opzr3u74C+NXh7tvBsH2D7em2Z1Dic5/tS4H7Kft3QwPHBWD7KeA/kt5UT50PrKPhMaNMG82VdEz9vWyNq/Ex67CvGC0FPlm/hTQX2NoxzdRoQ3PzmqQPUuasW/tFf73PXeqKpPcCfwL+xitz71+krCv8DDiVUkX2I7ZHLpo1gqR5wOdsXyjpdMqVw1RgDXCZ7Rf62b9uSJpDWUCfCDwOXEn5UNbomEn6GvBRyrfi1gBXUebWGxczSXcC8yjVUJ8Gvgr8klFiVJPg9yjTZTuBK22v6ke/D7WhSQoREXFgwzJ9FBERY5CkEBERbUkKERHRlqQQERFtSQoREdGWpBBxGEma16oAG3EkSlKIiIi2JIWIUUi6TNKDkh6RdEvd5+E5STfV/QOWSTqxtp0jaUWtq39PR839MyT9QdJfJD0s6Y315Sd37K2wqN4IFXFESFKIGEHSWyh36Z5rew7wMnAppeDbKttvBZZT7ngFuAP4gu23U+40b51fBHzf9pnAeyiVRKFUtr2OsrfH6ZR6QRFHhPEHbhIxdM4H3gE8VD/EH00phLYHuLu2+SmwpO6VMMX28np+IfBzSccB02zfA2B7F0B9vQdtb6zHjwAzgAd6P6yIA0tSiNibgIW2b3jVSenLI9p1WyOmsw7Qy+TvMI4gmT6K2NsyYIGkk6C9T+9plL+XVvXPTwAP2N4KPCPpffX85cDyuiveRkkX19eYJOmYwzqKiC7kE0rECLbXSfoS8DtJrwFeBK6mbI5zTn1uM2XdAUpJ5R/UN/1WBVQoCeIWSTfW17jkMA4joiupkhoxRpKesz253/2I6KVMH0VERFuuFCIioi1XChER0ZakEBERbUkKERHRlqQQERFtSQoREdGWpBAREW3/B9AzlcUCku64AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 283us/sample - loss: 0.9750 - acc: 0.7198\n",
      "Loss: 0.9750070831857366 Accuracy: 0.71983385\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0002 - acc: 0.3864\n",
      "Epoch 00001: val_loss improved from inf to 1.95541, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/001-1.9554.hdf5\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 1.9993 - acc: 0.3866 - val_loss: 1.9554 - val_acc: 0.3590\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2072 - acc: 0.6340\n",
      "Epoch 00002: val_loss improved from 1.95541 to 1.07666, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/002-1.0767.hdf5\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 1.2072 - acc: 0.6340 - val_loss: 1.0767 - val_acc: 0.6685\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9114 - acc: 0.7294\n",
      "Epoch 00003: val_loss improved from 1.07666 to 0.84294, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/003-0.8429.hdf5\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.9113 - acc: 0.7296 - val_loss: 0.8429 - val_acc: 0.7496\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7181 - acc: 0.7870\n",
      "Epoch 00004: val_loss improved from 0.84294 to 0.70893, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/004-0.7089.hdf5\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.7181 - acc: 0.7870 - val_loss: 0.7089 - val_acc: 0.7932\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5953 - acc: 0.8243\n",
      "Epoch 00005: val_loss improved from 0.70893 to 0.60091, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/005-0.6009.hdf5\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.5952 - acc: 0.8243 - val_loss: 0.6009 - val_acc: 0.8244\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5014 - acc: 0.8513\n",
      "Epoch 00006: val_loss improved from 0.60091 to 0.53770, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/006-0.5377.hdf5\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.5016 - acc: 0.8512 - val_loss: 0.5377 - val_acc: 0.8467\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8739\n",
      "Epoch 00007: val_loss did not improve from 0.53770\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.4281 - acc: 0.8738 - val_loss: 0.5510 - val_acc: 0.8321\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8908\n",
      "Epoch 00008: val_loss improved from 0.53770 to 0.46870, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/008-0.4687.hdf5\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.3708 - acc: 0.8909 - val_loss: 0.4687 - val_acc: 0.8605\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.9050\n",
      "Epoch 00009: val_loss improved from 0.46870 to 0.45706, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/009-0.4571.hdf5\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.3224 - acc: 0.9049 - val_loss: 0.4571 - val_acc: 0.8703\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9184\n",
      "Epoch 00010: val_loss did not improve from 0.45706\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2781 - acc: 0.9183 - val_loss: 0.4925 - val_acc: 0.8542\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2420 - acc: 0.9300\n",
      "Epoch 00011: val_loss did not improve from 0.45706\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.2421 - acc: 0.9299 - val_loss: 0.5045 - val_acc: 0.8544\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9406\n",
      "Epoch 00012: val_loss improved from 0.45706 to 0.44472, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/012-0.4447.hdf5\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.2132 - acc: 0.9406 - val_loss: 0.4447 - val_acc: 0.8733\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9524\n",
      "Epoch 00013: val_loss did not improve from 0.44472\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.1747 - acc: 0.9524 - val_loss: 0.4766 - val_acc: 0.8640\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9562\n",
      "Epoch 00014: val_loss improved from 0.44472 to 0.41920, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/014-0.4192.hdf5\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.1583 - acc: 0.9562 - val_loss: 0.4192 - val_acc: 0.8807\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9673\n",
      "Epoch 00015: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1268 - acc: 0.9673 - val_loss: 0.5025 - val_acc: 0.8686\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9711\n",
      "Epoch 00016: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.1130 - acc: 0.9711 - val_loss: 0.4949 - val_acc: 0.8647\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9794\n",
      "Epoch 00017: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0903 - acc: 0.9794 - val_loss: 0.4585 - val_acc: 0.8710\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9809\n",
      "Epoch 00018: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0812 - acc: 0.9809 - val_loss: 0.4693 - val_acc: 0.8742\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9867\n",
      "Epoch 00019: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0664 - acc: 0.9867 - val_loss: 0.4691 - val_acc: 0.8763\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9870\n",
      "Epoch 00020: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0617 - acc: 0.9869 - val_loss: 0.5005 - val_acc: 0.8756\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9898\n",
      "Epoch 00021: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0524 - acc: 0.9898 - val_loss: 0.4692 - val_acc: 0.8847\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9893\n",
      "Epoch 00022: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0516 - acc: 0.9892 - val_loss: 0.4954 - val_acc: 0.8761\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9911\n",
      "Epoch 00023: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0447 - acc: 0.9911 - val_loss: 0.4882 - val_acc: 0.8754\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9954\n",
      "Epoch 00024: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0316 - acc: 0.9954 - val_loss: 0.4923 - val_acc: 0.8744\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9934\n",
      "Epoch 00025: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0349 - acc: 0.9934 - val_loss: 0.5323 - val_acc: 0.8758\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9962\n",
      "Epoch 00026: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0269 - acc: 0.9962 - val_loss: 0.4845 - val_acc: 0.8821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9944\n",
      "Epoch 00027: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 390us/sample - loss: 0.0311 - acc: 0.9944 - val_loss: 0.6102 - val_acc: 0.8602\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9947\n",
      "Epoch 00028: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0289 - acc: 0.9947 - val_loss: 0.5312 - val_acc: 0.8805\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9969\n",
      "Epoch 00029: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0210 - acc: 0.9969 - val_loss: 0.5375 - val_acc: 0.8775\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9947\n",
      "Epoch 00030: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0271 - acc: 0.9947 - val_loss: 0.5221 - val_acc: 0.8765\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9941\n",
      "Epoch 00031: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0263 - acc: 0.9941 - val_loss: 0.5207 - val_acc: 0.8819\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9972\n",
      "Epoch 00032: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0180 - acc: 0.9971 - val_loss: 0.5623 - val_acc: 0.8779\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9947\n",
      "Epoch 00033: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0252 - acc: 0.9947 - val_loss: 0.5154 - val_acc: 0.8859\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9976\n",
      "Epoch 00034: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0165 - acc: 0.9975 - val_loss: 0.6257 - val_acc: 0.8696\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9953\n",
      "Epoch 00035: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0231 - acc: 0.9952 - val_loss: 0.5652 - val_acc: 0.8714\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9968\n",
      "Epoch 00036: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0183 - acc: 0.9968 - val_loss: 0.5234 - val_acc: 0.8814\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9963\n",
      "Epoch 00037: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 388us/sample - loss: 0.0184 - acc: 0.9963 - val_loss: 0.5321 - val_acc: 0.8866\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9984\n",
      "Epoch 00038: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.0114 - acc: 0.9984 - val_loss: 0.5695 - val_acc: 0.8847\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9987\n",
      "Epoch 00039: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0110 - acc: 0.9986 - val_loss: 0.5994 - val_acc: 0.8735\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9919\n",
      "Epoch 00040: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0337 - acc: 0.9919 - val_loss: 0.5554 - val_acc: 0.8819\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9986\n",
      "Epoch 00041: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0099 - acc: 0.9986 - val_loss: 0.5324 - val_acc: 0.8919\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9990\n",
      "Epoch 00042: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 379us/sample - loss: 0.0088 - acc: 0.9990 - val_loss: 0.5256 - val_acc: 0.8854\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9986\n",
      "Epoch 00043: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 384us/sample - loss: 0.0112 - acc: 0.9985 - val_loss: 0.7130 - val_acc: 0.8467\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9944\n",
      "Epoch 00044: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0226 - acc: 0.9944 - val_loss: 0.5358 - val_acc: 0.8849\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9989\n",
      "Epoch 00045: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0088 - acc: 0.9989 - val_loss: 0.5903 - val_acc: 0.8747\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9962\n",
      "Epoch 00046: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0179 - acc: 0.9963 - val_loss: 0.5632 - val_acc: 0.8810\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9986\n",
      "Epoch 00047: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0095 - acc: 0.9986 - val_loss: 0.5584 - val_acc: 0.8777\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9974\n",
      "Epoch 00048: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0136 - acc: 0.9974 - val_loss: 0.5813 - val_acc: 0.8840\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9983\n",
      "Epoch 00049: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0101 - acc: 0.9983 - val_loss: 0.8077 - val_acc: 0.8442\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9943\n",
      "Epoch 00050: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0218 - acc: 0.9943 - val_loss: 0.5446 - val_acc: 0.8868\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9990\n",
      "Epoch 00051: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0072 - acc: 0.9990 - val_loss: 0.5844 - val_acc: 0.8870\n",
      "Epoch 52/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9966\n",
      "Epoch 00052: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0157 - acc: 0.9967 - val_loss: 0.5992 - val_acc: 0.8791\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9985\n",
      "Epoch 00053: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0089 - acc: 0.9985 - val_loss: 0.5477 - val_acc: 0.8887\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9990\n",
      "Epoch 00054: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0073 - acc: 0.9989 - val_loss: 0.7500 - val_acc: 0.8523\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9944\n",
      "Epoch 00055: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0219 - acc: 0.9943 - val_loss: 0.5541 - val_acc: 0.8898\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9965\n",
      "Epoch 00056: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0150 - acc: 0.9965 - val_loss: 0.5413 - val_acc: 0.8919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9994\n",
      "Epoch 00057: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0054 - acc: 0.9994 - val_loss: 0.5804 - val_acc: 0.8891\n",
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9996\n",
      "Epoch 00058: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0046 - acc: 0.9996 - val_loss: 0.5528 - val_acc: 0.8915\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9995\n",
      "Epoch 00059: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0046 - acc: 0.9995 - val_loss: 0.5569 - val_acc: 0.8898\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9952\n",
      "Epoch 00060: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0185 - acc: 0.9952 - val_loss: 0.5977 - val_acc: 0.8847\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9986\n",
      "Epoch 00061: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0079 - acc: 0.9986 - val_loss: 0.5962 - val_acc: 0.8824\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9953\n",
      "Epoch 00062: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0176 - acc: 0.9953 - val_loss: 0.5804 - val_acc: 0.8917\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9983\n",
      "Epoch 00063: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0082 - acc: 0.9983 - val_loss: 0.5762 - val_acc: 0.8894\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9984\n",
      "Epoch 00064: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0083 - acc: 0.9985 - val_loss: 0.6052 - val_acc: 0.8826\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9997\n",
      "Epoch 00065: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0036 - acc: 0.9997 - val_loss: 0.5813 - val_acc: 0.8928\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9982\n",
      "Epoch 00066: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0085 - acc: 0.9982 - val_loss: 0.6354 - val_acc: 0.8817\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9977\n",
      "Epoch 00067: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0099 - acc: 0.9977 - val_loss: 0.6402 - val_acc: 0.8730\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9974\n",
      "Epoch 00068: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0103 - acc: 0.9974 - val_loss: 0.6133 - val_acc: 0.8852\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9987\n",
      "Epoch 00069: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 389us/sample - loss: 0.0066 - acc: 0.9988 - val_loss: 0.6622 - val_acc: 0.8761\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9987\n",
      "Epoch 00070: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0061 - acc: 0.9988 - val_loss: 0.5813 - val_acc: 0.8915\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9992\n",
      "Epoch 00071: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0051 - acc: 0.9992 - val_loss: 0.6347 - val_acc: 0.8807\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9948\n",
      "Epoch 00072: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0184 - acc: 0.9948 - val_loss: 0.6549 - val_acc: 0.8758\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9965\n",
      "Epoch 00073: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0144 - acc: 0.9965 - val_loss: 0.6952 - val_acc: 0.8756\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9989\n",
      "Epoch 00074: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0058 - acc: 0.9989 - val_loss: 0.5859 - val_acc: 0.8952\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9996\n",
      "Epoch 00075: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0035 - acc: 0.9995 - val_loss: 0.5900 - val_acc: 0.8912\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9955\n",
      "Epoch 00076: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0178 - acc: 0.9955 - val_loss: 0.6056 - val_acc: 0.8880\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9974\n",
      "Epoch 00077: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0101 - acc: 0.9974 - val_loss: 0.5987 - val_acc: 0.8921\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9994\n",
      "Epoch 00078: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0042 - acc: 0.9994 - val_loss: 0.5742 - val_acc: 0.8966\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 00079: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0030 - acc: 0.9997 - val_loss: 0.5951 - val_acc: 0.8910\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9986\n",
      "Epoch 00080: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0072 - acc: 0.9986 - val_loss: 0.6378 - val_acc: 0.8831\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9941\n",
      "Epoch 00081: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0201 - acc: 0.9941 - val_loss: 0.5999 - val_acc: 0.8870\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9996\n",
      "Epoch 00082: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0037 - acc: 0.9996 - val_loss: 0.5735 - val_acc: 0.8991\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9992\n",
      "Epoch 00083: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0046 - acc: 0.9992 - val_loss: 0.5776 - val_acc: 0.8921\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9997\n",
      "Epoch 00084: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0025 - acc: 0.9997 - val_loss: 0.6221 - val_acc: 0.8868\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9965\n",
      "Epoch 00085: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0125 - acc: 0.9965 - val_loss: 0.6479 - val_acc: 0.8768\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9982\n",
      "Epoch 00086: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0079 - acc: 0.9982 - val_loss: 0.5608 - val_acc: 0.8935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9992\n",
      "Epoch 00087: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0045 - acc: 0.9992 - val_loss: 0.5803 - val_acc: 0.8917\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9977\n",
      "Epoch 00088: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0088 - acc: 0.9977 - val_loss: 0.7156 - val_acc: 0.8721\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9980\n",
      "Epoch 00089: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0083 - acc: 0.9980 - val_loss: 0.6891 - val_acc: 0.8737\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9986\n",
      "Epoch 00090: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 396us/sample - loss: 0.0057 - acc: 0.9986 - val_loss: 0.6300 - val_acc: 0.8905\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9988\n",
      "Epoch 00091: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0059 - acc: 0.9987 - val_loss: 0.6585 - val_acc: 0.8833\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9972\n",
      "Epoch 00092: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0114 - acc: 0.9972 - val_loss: 0.5766 - val_acc: 0.8928\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9984\n",
      "Epoch 00093: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0064 - acc: 0.9985 - val_loss: 0.6059 - val_acc: 0.8931\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9995\n",
      "Epoch 00094: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0035 - acc: 0.9995 - val_loss: 0.5786 - val_acc: 0.8987\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9991\n",
      "Epoch 00095: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0043 - acc: 0.9991 - val_loss: 0.6035 - val_acc: 0.8877\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9993\n",
      "Epoch 00096: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.6271 - val_acc: 0.8942\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9989\n",
      "Epoch 00097: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0056 - acc: 0.9989 - val_loss: 0.5871 - val_acc: 0.8924\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9984\n",
      "Epoch 00098: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 394us/sample - loss: 0.0070 - acc: 0.9984 - val_loss: 0.6323 - val_acc: 0.8891\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9958\n",
      "Epoch 00099: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 392us/sample - loss: 0.0151 - acc: 0.9958 - val_loss: 0.6714 - val_acc: 0.8770\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9994\n",
      "Epoch 00100: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 390us/sample - loss: 0.0037 - acc: 0.9994 - val_loss: 0.5672 - val_acc: 0.8961\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9994\n",
      "Epoch 00101: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 383us/sample - loss: 0.0040 - acc: 0.9994 - val_loss: 0.5967 - val_acc: 0.8928\n",
      "Epoch 102/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9984\n",
      "Epoch 00102: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0071 - acc: 0.9984 - val_loss: 0.6677 - val_acc: 0.8763\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9958\n",
      "Epoch 00103: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 381us/sample - loss: 0.0154 - acc: 0.9957 - val_loss: 0.6158 - val_acc: 0.8966\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9968\n",
      "Epoch 00104: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0111 - acc: 0.9968 - val_loss: 0.6298 - val_acc: 0.8896\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9995\n",
      "Epoch 00105: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 382us/sample - loss: 0.0033 - acc: 0.9995 - val_loss: 0.6007 - val_acc: 0.8935\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9997\n",
      "Epoch 00106: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0026 - acc: 0.9997 - val_loss: 0.6243 - val_acc: 0.8889\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9984\n",
      "Epoch 00107: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0065 - acc: 0.9984 - val_loss: 0.6851 - val_acc: 0.8826\n",
      "Epoch 108/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9963\n",
      "Epoch 00108: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 395us/sample - loss: 0.0145 - acc: 0.9964 - val_loss: 0.6168 - val_acc: 0.8924\n",
      "Epoch 109/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9993\n",
      "Epoch 00109: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 393us/sample - loss: 0.0039 - acc: 0.9993 - val_loss: 0.6636 - val_acc: 0.8868\n",
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9973\n",
      "Epoch 00110: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.0097 - acc: 0.9973 - val_loss: 0.6093 - val_acc: 0.8956\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9996\n",
      "Epoch 00111: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 387us/sample - loss: 0.0026 - acc: 0.9996 - val_loss: 0.5984 - val_acc: 0.8975\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9996\n",
      "Epoch 00112: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0026 - acc: 0.9996 - val_loss: 0.6129 - val_acc: 0.8959\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9969\n",
      "Epoch 00113: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 15s 394us/sample - loss: 0.0114 - acc: 0.9969 - val_loss: 0.6762 - val_acc: 0.8817\n",
      "Epoch 114/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9982\n",
      "Epoch 00114: val_loss did not improve from 0.41920\n",
      "36805/36805 [==============================] - 14s 391us/sample - loss: 0.0081 - acc: 0.9982 - val_loss: 0.6206 - val_acc: 0.8945\n",
      "\n",
      "1D_CNN_BN_4_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 296us/sample - loss: 0.4896 - acc: 0.8538\n",
      "Loss: 0.4895778525036567 Accuracy: 0.8537902\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5957 - acc: 0.5104\n",
      "Epoch 00001: val_loss improved from inf to 2.69424, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/001-2.6942.hdf5\n",
      "36805/36805 [==============================] - 23s 624us/sample - loss: 1.5956 - acc: 0.5104 - val_loss: 2.6942 - val_acc: 0.2541\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8136 - acc: 0.7517\n",
      "Epoch 00002: val_loss improved from 2.69424 to 0.64013, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/002-0.6401.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8134 - acc: 0.7518 - val_loss: 0.6401 - val_acc: 0.7992\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.8293\n",
      "Epoch 00003: val_loss improved from 0.64013 to 0.47979, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/003-0.4798.hdf5\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.5589 - acc: 0.8294 - val_loss: 0.4798 - val_acc: 0.8507\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4221 - acc: 0.8717\n",
      "Epoch 00004: val_loss improved from 0.47979 to 0.40450, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/004-0.4045.hdf5\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.4219 - acc: 0.8717 - val_loss: 0.4045 - val_acc: 0.8724\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.8978\n",
      "Epoch 00005: val_loss improved from 0.40450 to 0.35341, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/005-0.3534.hdf5\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.3349 - acc: 0.8979 - val_loss: 0.3534 - val_acc: 0.8933\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9175\n",
      "Epoch 00006: val_loss improved from 0.35341 to 0.33462, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/006-0.3346.hdf5\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.2750 - acc: 0.9175 - val_loss: 0.3346 - val_acc: 0.8963\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9339\n",
      "Epoch 00007: val_loss improved from 0.33462 to 0.30241, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/007-0.3024.hdf5\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.2246 - acc: 0.9339 - val_loss: 0.3024 - val_acc: 0.9073\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9462\n",
      "Epoch 00008: val_loss did not improve from 0.30241\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.1850 - acc: 0.9462 - val_loss: 0.3088 - val_acc: 0.9043\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9577\n",
      "Epoch 00009: val_loss did not improve from 0.30241\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.1518 - acc: 0.9576 - val_loss: 0.3026 - val_acc: 0.9026\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9610\n",
      "Epoch 00010: val_loss improved from 0.30241 to 0.27289, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/010-0.2729.hdf5\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.1371 - acc: 0.9610 - val_loss: 0.2729 - val_acc: 0.9147\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9720\n",
      "Epoch 00011: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.1058 - acc: 0.9720 - val_loss: 0.2871 - val_acc: 0.9161\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9798\n",
      "Epoch 00012: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.0824 - acc: 0.9798 - val_loss: 0.2911 - val_acc: 0.9131\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9822\n",
      "Epoch 00013: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.0748 - acc: 0.9822 - val_loss: 0.2872 - val_acc: 0.9140\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9882\n",
      "Epoch 00014: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0558 - acc: 0.9882 - val_loss: 0.2939 - val_acc: 0.9150\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9900\n",
      "Epoch 00015: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0480 - acc: 0.9900 - val_loss: 0.3438 - val_acc: 0.8998\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9865\n",
      "Epoch 00016: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.0547 - acc: 0.9865 - val_loss: 0.3194 - val_acc: 0.9110\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9874\n",
      "Epoch 00017: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.0524 - acc: 0.9874 - val_loss: 0.2928 - val_acc: 0.9187\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9928\n",
      "Epoch 00018: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0367 - acc: 0.9927 - val_loss: 0.3046 - val_acc: 0.9161\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9943\n",
      "Epoch 00019: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0284 - acc: 0.9943 - val_loss: 0.3102 - val_acc: 0.9185\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9913\n",
      "Epoch 00020: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.0386 - acc: 0.9913 - val_loss: 0.2873 - val_acc: 0.9220\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9968\n",
      "Epoch 00021: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0191 - acc: 0.9968 - val_loss: 0.3308 - val_acc: 0.9166\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9955\n",
      "Epoch 00022: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.0238 - acc: 0.9955 - val_loss: 0.3605 - val_acc: 0.9045\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9958\n",
      "Epoch 00023: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.0216 - acc: 0.9958 - val_loss: 0.3829 - val_acc: 0.9045\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9958\n",
      "Epoch 00024: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.0207 - acc: 0.9958 - val_loss: 0.3416 - val_acc: 0.9119\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9957\n",
      "Epoch 00025: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 448us/sample - loss: 0.0206 - acc: 0.9957 - val_loss: 0.3437 - val_acc: 0.9147\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9962\n",
      "Epoch 00026: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.0190 - acc: 0.9961 - val_loss: 0.3565 - val_acc: 0.9124\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9879\n",
      "Epoch 00027: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.0425 - acc: 0.9879 - val_loss: 0.3440 - val_acc: 0.9133\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9962\n",
      "Epoch 00028: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.0176 - acc: 0.9962 - val_loss: 0.3154 - val_acc: 0.9215\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9986\n",
      "Epoch 00029: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.0100 - acc: 0.9985 - val_loss: 0.3331 - val_acc: 0.9206\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9977\n",
      "Epoch 00030: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.0119 - acc: 0.9977 - val_loss: 0.3484 - val_acc: 0.9152\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9986\n",
      "Epoch 00031: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.0106 - acc: 0.9986 - val_loss: 0.3315 - val_acc: 0.9213\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9952\n",
      "Epoch 00032: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.0205 - acc: 0.9951 - val_loss: 0.3542 - val_acc: 0.9099\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9921\n",
      "Epoch 00033: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.0305 - acc: 0.9921 - val_loss: 0.3271 - val_acc: 0.9171\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9986\n",
      "Epoch 00034: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.0088 - acc: 0.9986 - val_loss: 0.2917 - val_acc: 0.9236\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9990\n",
      "Epoch 00035: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.0073 - acc: 0.9989 - val_loss: 0.3457 - val_acc: 0.9194\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9985\n",
      "Epoch 00036: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.0092 - acc: 0.9985 - val_loss: 0.3563 - val_acc: 0.9234\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9961\n",
      "Epoch 00037: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.0164 - acc: 0.9961 - val_loss: 0.3987 - val_acc: 0.9071\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9980\n",
      "Epoch 00038: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 456us/sample - loss: 0.0099 - acc: 0.9980 - val_loss: 0.3453 - val_acc: 0.9208\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9923\n",
      "Epoch 00039: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.0290 - acc: 0.9923 - val_loss: 0.3462 - val_acc: 0.9217\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9989\n",
      "Epoch 00040: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.0066 - acc: 0.9989 - val_loss: 0.3367 - val_acc: 0.9287\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9970\n",
      "Epoch 00041: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.0133 - acc: 0.9969 - val_loss: 0.3341 - val_acc: 0.9276\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9937\n",
      "Epoch 00042: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.0234 - acc: 0.9937 - val_loss: 0.3316 - val_acc: 0.9259\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9962\n",
      "Epoch 00043: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.0155 - acc: 0.9962 - val_loss: 0.3173 - val_acc: 0.9273\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9988\n",
      "Epoch 00044: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.0068 - acc: 0.9988 - val_loss: 0.3387 - val_acc: 0.9243\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9990\n",
      "Epoch 00045: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.0057 - acc: 0.9990 - val_loss: 0.4054 - val_acc: 0.9117\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9978\n",
      "Epoch 00046: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 448us/sample - loss: 0.0104 - acc: 0.9978 - val_loss: 0.3924 - val_acc: 0.9131\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9978\n",
      "Epoch 00047: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0098 - acc: 0.9978 - val_loss: 0.3767 - val_acc: 0.9178\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9983\n",
      "Epoch 00048: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0082 - acc: 0.9983 - val_loss: 0.4005 - val_acc: 0.9117\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9971\n",
      "Epoch 00049: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0126 - acc: 0.9970 - val_loss: 0.3738 - val_acc: 0.9161\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9969\n",
      "Epoch 00050: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0120 - acc: 0.9969 - val_loss: 0.3415 - val_acc: 0.9290\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9979\n",
      "Epoch 00051: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 0.0085 - acc: 0.9979 - val_loss: 0.3732 - val_acc: 0.9224\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9991\n",
      "Epoch 00052: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.0050 - acc: 0.9991 - val_loss: 0.3653 - val_acc: 0.9199\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9962\n",
      "Epoch 00053: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.0144 - acc: 0.9961 - val_loss: 0.3853 - val_acc: 0.9182\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9939\n",
      "Epoch 00054: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 0.0233 - acc: 0.9939 - val_loss: 0.3983 - val_acc: 0.9145\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9952\n",
      "Epoch 00055: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0163 - acc: 0.9952 - val_loss: 0.3600 - val_acc: 0.9257\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9993\n",
      "Epoch 00056: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.3598 - val_acc: 0.9220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9990\n",
      "Epoch 00057: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.0049 - acc: 0.9990 - val_loss: 0.3501 - val_acc: 0.9292\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9990\n",
      "Epoch 00058: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.0049 - acc: 0.9990 - val_loss: 0.3835 - val_acc: 0.9227\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9953\n",
      "Epoch 00059: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0167 - acc: 0.9953 - val_loss: 0.3731 - val_acc: 0.9194\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9959\n",
      "Epoch 00060: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0152 - acc: 0.9959 - val_loss: 0.3518 - val_acc: 0.9269\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9969\n",
      "Epoch 00061: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.0123 - acc: 0.9968 - val_loss: 0.3473 - val_acc: 0.9248\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9953\n",
      "Epoch 00062: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 419us/sample - loss: 0.0174 - acc: 0.9953 - val_loss: 0.3328 - val_acc: 0.9271\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9996\n",
      "Epoch 00063: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0036 - acc: 0.9996 - val_loss: 0.3348 - val_acc: 0.9311\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 00064: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.3414 - val_acc: 0.9273\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9997\n",
      "Epoch 00065: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0023 - acc: 0.9997 - val_loss: 0.3559 - val_acc: 0.9290\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9965\n",
      "Epoch 00066: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 418us/sample - loss: 0.0141 - acc: 0.9965 - val_loss: 0.3497 - val_acc: 0.9264\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9993\n",
      "Epoch 00067: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0045 - acc: 0.9993 - val_loss: 0.3427 - val_acc: 0.9273\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9989\n",
      "Epoch 00068: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0048 - acc: 0.9989 - val_loss: 0.3436 - val_acc: 0.9273\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9979\n",
      "Epoch 00069: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.0083 - acc: 0.9979 - val_loss: 0.4017 - val_acc: 0.9173\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9982\n",
      "Epoch 00070: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0071 - acc: 0.9982 - val_loss: 0.4566 - val_acc: 0.9068\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9972\n",
      "Epoch 00071: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0096 - acc: 0.9972 - val_loss: 0.4531 - val_acc: 0.9099\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9979\n",
      "Epoch 00072: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0088 - acc: 0.9979 - val_loss: 0.3456 - val_acc: 0.9287\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9987\n",
      "Epoch 00073: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0059 - acc: 0.9987 - val_loss: 0.3886 - val_acc: 0.9194\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9991\n",
      "Epoch 00074: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0042 - acc: 0.9991 - val_loss: 0.3427 - val_acc: 0.9294\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9984\n",
      "Epoch 00075: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.0073 - acc: 0.9984 - val_loss: 0.4205 - val_acc: 0.9143\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9989\n",
      "Epoch 00076: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.0051 - acc: 0.9989 - val_loss: 0.4180 - val_acc: 0.9199\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9946\n",
      "Epoch 00077: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.0182 - acc: 0.9946 - val_loss: 0.3330 - val_acc: 0.9311\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9971\n",
      "Epoch 00078: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.0115 - acc: 0.9971 - val_loss: 0.3361 - val_acc: 0.9294\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9973\n",
      "Epoch 00079: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.0095 - acc: 0.9973 - val_loss: 0.3493 - val_acc: 0.9262\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 00080: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 0.0028 - acc: 0.9996 - val_loss: 0.3302 - val_acc: 0.9292\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9996\n",
      "Epoch 00081: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 449us/sample - loss: 0.0026 - acc: 0.9995 - val_loss: 0.3368 - val_acc: 0.9264\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9966\n",
      "Epoch 00082: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0116 - acc: 0.9966 - val_loss: 0.3554 - val_acc: 0.9273\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9988\n",
      "Epoch 00083: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.0057 - acc: 0.9988 - val_loss: 0.3630 - val_acc: 0.9290\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9956\n",
      "Epoch 00084: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 0.0153 - acc: 0.9955 - val_loss: 0.3530 - val_acc: 0.9271\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9994\n",
      "Epoch 00085: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.0041 - acc: 0.9993 - val_loss: 0.3375 - val_acc: 0.9324\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9955\n",
      "Epoch 00086: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 0.0158 - acc: 0.9955 - val_loss: 0.3343 - val_acc: 0.9266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9995\n",
      "Epoch 00087: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.0031 - acc: 0.9995 - val_loss: 0.3427 - val_acc: 0.9278\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9995\n",
      "Epoch 00088: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 448us/sample - loss: 0.0028 - acc: 0.9995 - val_loss: 0.3480 - val_acc: 0.9299\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9953\n",
      "Epoch 00089: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.0155 - acc: 0.9954 - val_loss: 0.3673 - val_acc: 0.9222\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00090: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.0037 - acc: 0.9991 - val_loss: 0.3351 - val_acc: 0.9324\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9978\n",
      "Epoch 00091: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.0085 - acc: 0.9978 - val_loss: 0.3690 - val_acc: 0.9248\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n",
      "Epoch 00092: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 0.0041 - acc: 0.9990 - val_loss: 0.3547 - val_acc: 0.9324\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9996\n",
      "Epoch 00093: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.0020 - acc: 0.9996 - val_loss: 0.3731 - val_acc: 0.9273\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00094: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.3610 - val_acc: 0.9287\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9970\n",
      "Epoch 00095: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.0102 - acc: 0.9970 - val_loss: 0.3715 - val_acc: 0.9271\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 00096: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 449us/sample - loss: 0.0046 - acc: 0.9987 - val_loss: 0.3596 - val_acc: 0.9269\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9986\n",
      "Epoch 00097: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.3883 - val_acc: 0.9243\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979\n",
      "Epoch 00098: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 448us/sample - loss: 0.0080 - acc: 0.9979 - val_loss: 0.3562 - val_acc: 0.9269\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9986\n",
      "Epoch 00099: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 448us/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.4501 - val_acc: 0.9138\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9987\n",
      "Epoch 00100: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.0048 - acc: 0.9987 - val_loss: 0.3833 - val_acc: 0.9283\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9981\n",
      "Epoch 00101: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 447us/sample - loss: 0.0069 - acc: 0.9980 - val_loss: 0.4331 - val_acc: 0.9126\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9952\n",
      "Epoch 00102: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.0168 - acc: 0.9952 - val_loss: 0.3558 - val_acc: 0.9292\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9987\n",
      "Epoch 00103: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 447us/sample - loss: 0.0051 - acc: 0.9987 - val_loss: 0.3833 - val_acc: 0.9203\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9964\n",
      "Epoch 00104: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 447us/sample - loss: 0.0126 - acc: 0.9964 - val_loss: 0.3423 - val_acc: 0.9315\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9978\n",
      "Epoch 00105: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.0077 - acc: 0.9978 - val_loss: 0.3398 - val_acc: 0.9350\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 00106: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.0023 - acc: 0.9995 - val_loss: 0.3257 - val_acc: 0.9352\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 00107: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.0031 - acc: 0.9994 - val_loss: 0.3682 - val_acc: 0.9243\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9995\n",
      "Epoch 00108: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.0026 - acc: 0.9995 - val_loss: 0.3656 - val_acc: 0.9287\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993\n",
      "Epoch 00109: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.3983 - val_acc: 0.9208\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9984\n",
      "Epoch 00110: val_loss did not improve from 0.27289\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.0063 - acc: 0.9984 - val_loss: 0.3851 - val_acc: 0.9227\n",
      "\n",
      "1D_CNN_BN_5_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 354us/sample - loss: 0.3738 - acc: 0.8866\n",
      "Loss: 0.3738165969913003 Accuracy: 0.88660437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_BN_{}_only_conv'.format(i)\n",
    "    model = build_1d_cnn_BN_only_conv(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_BN_1_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 295us/sample - loss: 1.9023 - acc: 0.4150\n",
      "Loss: 1.902310560425494 Accuracy: 0.41495326\n",
      "\n",
      "1D_CNN_BN_2_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_31 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 255us/sample - loss: 1.4395 - acc: 0.5483\n",
      "Loss: 1.4394703664264699 Accuracy: 0.5482866\n",
      "\n",
      "1D_CNN_BN_3_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 342us/sample - loss: 0.9750 - acc: 0.7198\n",
      "Loss: 0.9750070831857366 Accuracy: 0.71983385\n",
      "\n",
      "1D_CNN_BN_4_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 329us/sample - loss: 0.4896 - acc: 0.8538\n",
      "Loss: 0.4895778525036567 Accuracy: 0.8537902\n",
      "\n",
      "1D_CNN_BN_5_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_40 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 370us/sample - loss: 0.3738 - acc: 0.8866\n",
      "Loss: 0.3738165969913003 Accuracy: 0.88660437\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_BN_{}_only_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_BN_DO_only_conv(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=25, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=25, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=4, strides=4, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model = build_1d_cnn_BN_only_conv(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 3.0254 - acc: 0.2082\n",
      "Epoch 00001: val_loss improved from inf to 2.01469, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/001-2.0147.hdf5\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 3.0230 - acc: 0.2087 - val_loss: 2.0147 - val_acc: 0.3683\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.0452 - acc: 0.3798\n",
      "Epoch 00002: val_loss improved from 2.01469 to 1.84213, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/002-1.8421.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 2.0447 - acc: 0.3799 - val_loss: 1.8421 - val_acc: 0.4258\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.7395 - acc: 0.4581\n",
      "Epoch 00003: val_loss improved from 1.84213 to 1.74729, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/003-1.7473.hdf5\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 1.7389 - acc: 0.4583 - val_loss: 1.7473 - val_acc: 0.4554\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5510 - acc: 0.5070\n",
      "Epoch 00004: val_loss improved from 1.74729 to 1.71685, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/004-1.7168.hdf5\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 1.5507 - acc: 0.5070 - val_loss: 1.7168 - val_acc: 0.4789\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4135 - acc: 0.5491\n",
      "Epoch 00005: val_loss did not improve from 1.71685\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 1.4142 - acc: 0.5489 - val_loss: 1.7188 - val_acc: 0.4752\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3149 - acc: 0.5753\n",
      "Epoch 00006: val_loss improved from 1.71685 to 1.68636, saving model to model/checkpoint/1D_CNN_BN_DO_1_only_conv_checkpoint/006-1.6864.hdf5\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 1.3148 - acc: 0.5753 - val_loss: 1.6864 - val_acc: 0.4931\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2302 - acc: 0.6037\n",
      "Epoch 00007: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 1.2306 - acc: 0.6036 - val_loss: 1.7175 - val_acc: 0.4764\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.1625 - acc: 0.6223\n",
      "Epoch 00008: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 1.1631 - acc: 0.6223 - val_loss: 1.7167 - val_acc: 0.4866\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1116 - acc: 0.6383\n",
      "Epoch 00009: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 1.1117 - acc: 0.6381 - val_loss: 1.7356 - val_acc: 0.4882\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0621 - acc: 0.6561\n",
      "Epoch 00010: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 1.0628 - acc: 0.6559 - val_loss: 1.7597 - val_acc: 0.4892\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0178 - acc: 0.6697\n",
      "Epoch 00011: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 1.0186 - acc: 0.6693 - val_loss: 1.7364 - val_acc: 0.4980\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9799 - acc: 0.6819\n",
      "Epoch 00012: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.9798 - acc: 0.6818 - val_loss: 1.7953 - val_acc: 0.4896\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9516 - acc: 0.6881\n",
      "Epoch 00013: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.9519 - acc: 0.6881 - val_loss: 1.8039 - val_acc: 0.4859\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9265 - acc: 0.6954\n",
      "Epoch 00014: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.9264 - acc: 0.6956 - val_loss: 1.8112 - val_acc: 0.4889\n",
      "Epoch 15/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8926 - acc: 0.7081\n",
      "Epoch 00015: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.8924 - acc: 0.7082 - val_loss: 1.8133 - val_acc: 0.4906\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8714 - acc: 0.7116\n",
      "Epoch 00016: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.8716 - acc: 0.7115 - val_loss: 1.8854 - val_acc: 0.4757\n",
      "Epoch 17/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8532 - acc: 0.7189\n",
      "Epoch 00017: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.8532 - acc: 0.7190 - val_loss: 1.8786 - val_acc: 0.4882\n",
      "Epoch 18/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8276 - acc: 0.7230\n",
      "Epoch 00018: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.8280 - acc: 0.7228 - val_loss: 1.9259 - val_acc: 0.4824\n",
      "Epoch 19/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.8154 - acc: 0.7295\n",
      "Epoch 00019: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.8150 - acc: 0.7295 - val_loss: 1.9129 - val_acc: 0.4903\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7939 - acc: 0.7379\n",
      "Epoch 00020: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.7937 - acc: 0.7380 - val_loss: 1.8919 - val_acc: 0.4922\n",
      "Epoch 21/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7820 - acc: 0.7399\n",
      "Epoch 00021: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.7834 - acc: 0.7397 - val_loss: 1.9221 - val_acc: 0.4899\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7748 - acc: 0.7430\n",
      "Epoch 00022: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.7747 - acc: 0.7432 - val_loss: 1.9786 - val_acc: 0.4815\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7531 - acc: 0.7515\n",
      "Epoch 00023: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.7529 - acc: 0.7514 - val_loss: 1.9408 - val_acc: 0.4931\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7414 - acc: 0.7513\n",
      "Epoch 00024: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7421 - acc: 0.7513 - val_loss: 2.0106 - val_acc: 0.4831\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7292 - acc: 0.7561\n",
      "Epoch 00025: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7290 - acc: 0.7561 - val_loss: 1.9845 - val_acc: 0.4922\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7232 - acc: 0.7592\n",
      "Epoch 00026: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.7235 - acc: 0.7591 - val_loss: 1.9822 - val_acc: 0.4941\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7123 - acc: 0.7610\n",
      "Epoch 00027: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7126 - acc: 0.7609 - val_loss: 1.9986 - val_acc: 0.5008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7004 - acc: 0.7660\n",
      "Epoch 00028: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7007 - acc: 0.7661 - val_loss: 1.9949 - val_acc: 0.4976\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7005 - acc: 0.7664\n",
      "Epoch 00029: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.7018 - acc: 0.7662 - val_loss: 2.0234 - val_acc: 0.4948\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6938 - acc: 0.7685\n",
      "Epoch 00030: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.6940 - acc: 0.7683 - val_loss: 2.0410 - val_acc: 0.4950\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.7714\n",
      "Epoch 00031: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.6796 - acc: 0.7717 - val_loss: 2.0482 - val_acc: 0.4950\n",
      "Epoch 32/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6649 - acc: 0.7795\n",
      "Epoch 00032: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.6653 - acc: 0.7792 - val_loss: 2.0374 - val_acc: 0.5038\n",
      "Epoch 33/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6714 - acc: 0.7752\n",
      "Epoch 00033: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.6713 - acc: 0.7751 - val_loss: 2.0684 - val_acc: 0.4980\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6575 - acc: 0.7816\n",
      "Epoch 00034: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.6579 - acc: 0.7814 - val_loss: 2.0840 - val_acc: 0.4929\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6531 - acc: 0.7823\n",
      "Epoch 00035: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.6530 - acc: 0.7824 - val_loss: 2.1258 - val_acc: 0.4889\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6552 - acc: 0.7804\n",
      "Epoch 00036: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.6556 - acc: 0.7803 - val_loss: 2.1139 - val_acc: 0.4990\n",
      "Epoch 37/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6424 - acc: 0.7860\n",
      "Epoch 00037: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.6423 - acc: 0.7861 - val_loss: 2.1242 - val_acc: 0.4983\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6419 - acc: 0.7853\n",
      "Epoch 00038: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.6421 - acc: 0.7852 - val_loss: 2.1064 - val_acc: 0.4973\n",
      "Epoch 39/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6316 - acc: 0.7878\n",
      "Epoch 00039: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.6317 - acc: 0.7877 - val_loss: 2.1213 - val_acc: 0.4938\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6292 - acc: 0.7886\n",
      "Epoch 00040: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.6290 - acc: 0.7886 - val_loss: 2.1467 - val_acc: 0.4948\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6210 - acc: 0.7920\n",
      "Epoch 00041: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.6214 - acc: 0.7918 - val_loss: 2.1341 - val_acc: 0.5020\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6165 - acc: 0.7933\n",
      "Epoch 00042: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.6171 - acc: 0.7931 - val_loss: 2.1680 - val_acc: 0.4973\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6219 - acc: 0.7919\n",
      "Epoch 00043: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.6217 - acc: 0.7919 - val_loss: 2.1605 - val_acc: 0.4941\n",
      "Epoch 44/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6078 - acc: 0.7961\n",
      "Epoch 00044: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.6076 - acc: 0.7962 - val_loss: 2.1884 - val_acc: 0.4910\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.7989\n",
      "Epoch 00045: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.6039 - acc: 0.7989 - val_loss: 2.1708 - val_acc: 0.5013\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6097 - acc: 0.7934\n",
      "Epoch 00046: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.6105 - acc: 0.7935 - val_loss: 2.1710 - val_acc: 0.5027\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.7969\n",
      "Epoch 00047: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.6021 - acc: 0.7968 - val_loss: 2.1819 - val_acc: 0.4987\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5983 - acc: 0.7978\n",
      "Epoch 00048: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.5983 - acc: 0.7978 - val_loss: 2.1680 - val_acc: 0.4983\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5928 - acc: 0.8009\n",
      "Epoch 00049: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.5928 - acc: 0.8009 - val_loss: 2.2129 - val_acc: 0.4931\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5876 - acc: 0.8037\n",
      "Epoch 00050: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5879 - acc: 0.8035 - val_loss: 2.1792 - val_acc: 0.5003\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5828 - acc: 0.8061\n",
      "Epoch 00051: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.5830 - acc: 0.8060 - val_loss: 2.1847 - val_acc: 0.5029\n",
      "Epoch 52/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5692 - acc: 0.8061\n",
      "Epoch 00052: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.5688 - acc: 0.8062 - val_loss: 2.1961 - val_acc: 0.5064\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5715 - acc: 0.8074\n",
      "Epoch 00053: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.5718 - acc: 0.8073 - val_loss: 2.2112 - val_acc: 0.5003\n",
      "Epoch 54/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.8078\n",
      "Epoch 00054: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5775 - acc: 0.8079 - val_loss: 2.2269 - val_acc: 0.5022\n",
      "Epoch 55/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5813 - acc: 0.8074\n",
      "Epoch 00055: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.5819 - acc: 0.8072 - val_loss: 2.2106 - val_acc: 0.5029\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5660 - acc: 0.8116\n",
      "Epoch 00056: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5660 - acc: 0.8116 - val_loss: 2.2074 - val_acc: 0.4966\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5666 - acc: 0.8094\n",
      "Epoch 00057: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.5674 - acc: 0.8094 - val_loss: 2.2480 - val_acc: 0.4985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.8085\n",
      "Epoch 00058: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.5665 - acc: 0.8085 - val_loss: 2.2712 - val_acc: 0.4938\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5563 - acc: 0.8141\n",
      "Epoch 00059: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5566 - acc: 0.8139 - val_loss: 2.2756 - val_acc: 0.4922\n",
      "Epoch 60/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5583 - acc: 0.8114\n",
      "Epoch 00060: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.5580 - acc: 0.8113 - val_loss: 2.2395 - val_acc: 0.4997\n",
      "Epoch 61/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5534 - acc: 0.8128\n",
      "Epoch 00061: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.5543 - acc: 0.8124 - val_loss: 2.2412 - val_acc: 0.5020\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.8143\n",
      "Epoch 00062: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.5521 - acc: 0.8143 - val_loss: 2.2634 - val_acc: 0.5003\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.8138\n",
      "Epoch 00063: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.5501 - acc: 0.8137 - val_loss: 2.2397 - val_acc: 0.5076\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.8175\n",
      "Epoch 00064: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.5466 - acc: 0.8175 - val_loss: 2.3190 - val_acc: 0.4929\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.8202\n",
      "Epoch 00065: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.5379 - acc: 0.8200 - val_loss: 2.2700 - val_acc: 0.5024\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.8206\n",
      "Epoch 00066: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.5408 - acc: 0.8204 - val_loss: 2.2639 - val_acc: 0.5076\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.8162\n",
      "Epoch 00067: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.5430 - acc: 0.8162 - val_loss: 2.3234 - val_acc: 0.4885\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.8184\n",
      "Epoch 00068: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5450 - acc: 0.8184 - val_loss: 2.2750 - val_acc: 0.5036\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.8190\n",
      "Epoch 00069: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.5405 - acc: 0.8190 - val_loss: 2.2728 - val_acc: 0.5010\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8201\n",
      "Epoch 00070: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.5346 - acc: 0.8201 - val_loss: 2.2473 - val_acc: 0.5076\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5284 - acc: 0.8238\n",
      "Epoch 00071: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.5285 - acc: 0.8238 - val_loss: 2.3072 - val_acc: 0.4941\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5244 - acc: 0.8243\n",
      "Epoch 00072: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5243 - acc: 0.8243 - val_loss: 2.2964 - val_acc: 0.5094\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.8168\n",
      "Epoch 00073: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.5423 - acc: 0.8165 - val_loss: 2.2992 - val_acc: 0.5041\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5341 - acc: 0.8238\n",
      "Epoch 00074: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.5337 - acc: 0.8238 - val_loss: 2.2743 - val_acc: 0.5073\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.8206\n",
      "Epoch 00075: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.5278 - acc: 0.8206 - val_loss: 2.3258 - val_acc: 0.4978\n",
      "Epoch 76/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5212 - acc: 0.8238\n",
      "Epoch 00076: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.5216 - acc: 0.8235 - val_loss: 2.3023 - val_acc: 0.5078\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5206 - acc: 0.8238\n",
      "Epoch 00077: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.5211 - acc: 0.8237 - val_loss: 2.2647 - val_acc: 0.5115\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.8271\n",
      "Epoch 00078: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.5231 - acc: 0.8268 - val_loss: 2.3348 - val_acc: 0.4878\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5147 - acc: 0.8288\n",
      "Epoch 00079: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5148 - acc: 0.8287 - val_loss: 2.4110 - val_acc: 0.4922\n",
      "Epoch 80/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.8277\n",
      "Epoch 00080: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.5186 - acc: 0.8276 - val_loss: 2.3240 - val_acc: 0.4997\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.8261\n",
      "Epoch 00081: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.5180 - acc: 0.8262 - val_loss: 2.3128 - val_acc: 0.5104\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.8261\n",
      "Epoch 00082: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.5133 - acc: 0.8260 - val_loss: 2.3551 - val_acc: 0.4987\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.8295\n",
      "Epoch 00083: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.5131 - acc: 0.8290 - val_loss: 2.3307 - val_acc: 0.4964\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.8293\n",
      "Epoch 00084: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.5114 - acc: 0.8293 - val_loss: 2.3203 - val_acc: 0.5048\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.8320\n",
      "Epoch 00085: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.5083 - acc: 0.8321 - val_loss: 2.3308 - val_acc: 0.5059\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8314\n",
      "Epoch 00086: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.5078 - acc: 0.8314 - val_loss: 2.3381 - val_acc: 0.5003\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.8306\n",
      "Epoch 00087: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5099 - acc: 0.8308 - val_loss: 2.3354 - val_acc: 0.5106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4993 - acc: 0.8340\n",
      "Epoch 00088: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.4991 - acc: 0.8340 - val_loss: 2.3179 - val_acc: 0.5087\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.8315\n",
      "Epoch 00089: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.5050 - acc: 0.8316 - val_loss: 2.3531 - val_acc: 0.5020\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5041 - acc: 0.8312\n",
      "Epoch 00090: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5040 - acc: 0.8313 - val_loss: 2.3435 - val_acc: 0.5048\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.8338\n",
      "Epoch 00091: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.4967 - acc: 0.8339 - val_loss: 2.3473 - val_acc: 0.5062\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.8321\n",
      "Epoch 00092: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.5001 - acc: 0.8322 - val_loss: 2.3298 - val_acc: 0.5083\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4959 - acc: 0.8353\n",
      "Epoch 00093: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.4968 - acc: 0.8352 - val_loss: 2.3722 - val_acc: 0.4994\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8338\n",
      "Epoch 00094: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.4964 - acc: 0.8337 - val_loss: 2.3655 - val_acc: 0.4969\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.8328\n",
      "Epoch 00095: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.4956 - acc: 0.8327 - val_loss: 2.3505 - val_acc: 0.5048\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4936 - acc: 0.8355\n",
      "Epoch 00096: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.4937 - acc: 0.8354 - val_loss: 2.3559 - val_acc: 0.5045\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.8335\n",
      "Epoch 00097: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5013 - acc: 0.8334 - val_loss: 2.3991 - val_acc: 0.4948\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8332\n",
      "Epoch 00098: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.4929 - acc: 0.8332 - val_loss: 2.3957 - val_acc: 0.5017\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4878 - acc: 0.8373\n",
      "Epoch 00099: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.4876 - acc: 0.8374 - val_loss: 2.3492 - val_acc: 0.5099\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4915 - acc: 0.8353\n",
      "Epoch 00100: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.4915 - acc: 0.8352 - val_loss: 2.4287 - val_acc: 0.4920\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4808 - acc: 0.8391\n",
      "Epoch 00101: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.4806 - acc: 0.8392 - val_loss: 2.3841 - val_acc: 0.5080\n",
      "Epoch 102/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8382\n",
      "Epoch 00102: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.4821 - acc: 0.8385 - val_loss: 2.4012 - val_acc: 0.5057\n",
      "Epoch 103/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4865 - acc: 0.8384\n",
      "Epoch 00103: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.4869 - acc: 0.8382 - val_loss: 2.3753 - val_acc: 0.5092\n",
      "Epoch 104/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4915 - acc: 0.8373\n",
      "Epoch 00104: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.4919 - acc: 0.8372 - val_loss: 2.3849 - val_acc: 0.5087\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4812 - acc: 0.8383\n",
      "Epoch 00105: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.4814 - acc: 0.8384 - val_loss: 2.3764 - val_acc: 0.5038\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4804 - acc: 0.8409\n",
      "Epoch 00106: val_loss did not improve from 1.68636\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.4811 - acc: 0.8408 - val_loss: 2.3761 - val_acc: 0.5092\n",
      "\n",
      "1D_CNN_BN_DO_1_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 332us/sample - loss: 1.7871 - acc: 0.4559\n",
      "Loss: 1.7870784911534991 Accuracy: 0.45586708\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 3.5113 - acc: 0.2055\n",
      "Epoch 00001: val_loss improved from inf to 2.01016, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/001-2.0102.hdf5\n",
      "36805/36805 [==============================] - 20s 550us/sample - loss: 3.5096 - acc: 0.2057 - val_loss: 2.0102 - val_acc: 0.3401\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3812 - acc: 0.3602\n",
      "Epoch 00002: val_loss improved from 2.01016 to 1.79065, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/002-1.7907.hdf5\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 2.3815 - acc: 0.3600 - val_loss: 1.7907 - val_acc: 0.4589\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9789 - acc: 0.4407\n",
      "Epoch 00003: val_loss improved from 1.79065 to 1.52554, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/003-1.5255.hdf5\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 1.9785 - acc: 0.4407 - val_loss: 1.5255 - val_acc: 0.5432\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7466 - acc: 0.4950\n",
      "Epoch 00004: val_loss improved from 1.52554 to 1.38963, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/004-1.3896.hdf5\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 1.7476 - acc: 0.4950 - val_loss: 1.3896 - val_acc: 0.5819\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5994 - acc: 0.5326\n",
      "Epoch 00005: val_loss improved from 1.38963 to 1.35329, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/005-1.3533.hdf5\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 1.5990 - acc: 0.5327 - val_loss: 1.3533 - val_acc: 0.5884\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4678 - acc: 0.5655\n",
      "Epoch 00006: val_loss improved from 1.35329 to 1.27951, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/006-1.2795.hdf5\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 1.4685 - acc: 0.5654 - val_loss: 1.2795 - val_acc: 0.6196\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3478 - acc: 0.5941\n",
      "Epoch 00007: val_loss improved from 1.27951 to 1.26704, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/007-1.2670.hdf5\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 1.3482 - acc: 0.5941 - val_loss: 1.2670 - val_acc: 0.6247\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2739 - acc: 0.6173\n",
      "Epoch 00008: val_loss improved from 1.26704 to 1.21985, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/008-1.2199.hdf5\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 1.2744 - acc: 0.6174 - val_loss: 1.2199 - val_acc: 0.6343\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1971 - acc: 0.6350\n",
      "Epoch 00009: val_loss did not improve from 1.21985\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 1.1964 - acc: 0.6352 - val_loss: 1.2213 - val_acc: 0.6378\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1294 - acc: 0.6540\n",
      "Epoch 00010: val_loss did not improve from 1.21985\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 1.1299 - acc: 0.6540 - val_loss: 1.2405 - val_acc: 0.6264\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0733 - acc: 0.6668\n",
      "Epoch 00011: val_loss improved from 1.21985 to 1.17237, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/011-1.1724.hdf5\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 1.0732 - acc: 0.6669 - val_loss: 1.1724 - val_acc: 0.6562\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0246 - acc: 0.6824\n",
      "Epoch 00012: val_loss did not improve from 1.17237\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 1.0245 - acc: 0.6824 - val_loss: 1.2012 - val_acc: 0.6520\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9854 - acc: 0.6938\n",
      "Epoch 00013: val_loss improved from 1.17237 to 1.16999, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/013-1.1700.hdf5\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.9855 - acc: 0.6938 - val_loss: 1.1700 - val_acc: 0.6632\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9390 - acc: 0.7071\n",
      "Epoch 00014: val_loss improved from 1.16999 to 1.15472, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/014-1.1547.hdf5\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.9393 - acc: 0.7070 - val_loss: 1.1547 - val_acc: 0.6629\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8956 - acc: 0.7198\n",
      "Epoch 00015: val_loss improved from 1.15472 to 1.14771, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/015-1.1477.hdf5\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.8951 - acc: 0.7199 - val_loss: 1.1477 - val_acc: 0.6636\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8526 - acc: 0.7318\n",
      "Epoch 00016: val_loss did not improve from 1.14771\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.8523 - acc: 0.7319 - val_loss: 1.1689 - val_acc: 0.6555\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8178 - acc: 0.7407\n",
      "Epoch 00017: val_loss did not improve from 1.14771\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.8181 - acc: 0.7404 - val_loss: 1.1656 - val_acc: 0.6664\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7850 - acc: 0.7500\n",
      "Epoch 00018: val_loss improved from 1.14771 to 1.12323, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/018-1.1232.hdf5\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.7848 - acc: 0.7500 - val_loss: 1.1232 - val_acc: 0.6811\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7499 - acc: 0.7578\n",
      "Epoch 00019: val_loss improved from 1.12323 to 1.11731, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/019-1.1173.hdf5\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.7496 - acc: 0.7578 - val_loss: 1.1173 - val_acc: 0.6834\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7115 - acc: 0.7692\n",
      "Epoch 00020: val_loss did not improve from 1.11731\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.7114 - acc: 0.7692 - val_loss: 1.1233 - val_acc: 0.6816\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6777 - acc: 0.7795\n",
      "Epoch 00021: val_loss did not improve from 1.11731\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.6776 - acc: 0.7795 - val_loss: 1.1199 - val_acc: 0.6874\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6510 - acc: 0.7888\n",
      "Epoch 00022: val_loss did not improve from 1.11731\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.6516 - acc: 0.7887 - val_loss: 1.1365 - val_acc: 0.6834\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6297 - acc: 0.7952\n",
      "Epoch 00023: val_loss improved from 1.11731 to 1.11486, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/023-1.1149.hdf5\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.6303 - acc: 0.7950 - val_loss: 1.1149 - val_acc: 0.6890\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.8003\n",
      "Epoch 00024: val_loss did not improve from 1.11486\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.6069 - acc: 0.8003 - val_loss: 1.1619 - val_acc: 0.6846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5880 - acc: 0.8074\n",
      "Epoch 00025: val_loss improved from 1.11486 to 1.11065, saving model to model/checkpoint/1D_CNN_BN_DO_2_only_conv_checkpoint/025-1.1107.hdf5\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.5883 - acc: 0.8074 - val_loss: 1.1107 - val_acc: 0.6986\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.8141\n",
      "Epoch 00026: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.5607 - acc: 0.8141 - val_loss: 1.1623 - val_acc: 0.6939\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.8226\n",
      "Epoch 00027: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.5394 - acc: 0.8226 - val_loss: 1.1474 - val_acc: 0.6928\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.8276\n",
      "Epoch 00028: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.5271 - acc: 0.8276 - val_loss: 1.1476 - val_acc: 0.6986\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.8292\n",
      "Epoch 00029: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.5150 - acc: 0.8291 - val_loss: 1.1497 - val_acc: 0.6932\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8383\n",
      "Epoch 00030: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4948 - acc: 0.8382 - val_loss: 1.1951 - val_acc: 0.6837\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4839 - acc: 0.8404\n",
      "Epoch 00031: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4842 - acc: 0.8404 - val_loss: 1.1543 - val_acc: 0.6942\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.8434\n",
      "Epoch 00032: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.4700 - acc: 0.8434 - val_loss: 1.1690 - val_acc: 0.6904\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4498 - acc: 0.8510\n",
      "Epoch 00033: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.4500 - acc: 0.8509 - val_loss: 1.1663 - val_acc: 0.7004\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.8532\n",
      "Epoch 00034: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.4378 - acc: 0.8531 - val_loss: 1.2251 - val_acc: 0.6946\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.8572\n",
      "Epoch 00035: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.4280 - acc: 0.8572 - val_loss: 1.1631 - val_acc: 0.7074\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4249 - acc: 0.8592\n",
      "Epoch 00036: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.4250 - acc: 0.8591 - val_loss: 1.2117 - val_acc: 0.6958\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8626\n",
      "Epoch 00037: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.4107 - acc: 0.8625 - val_loss: 1.1746 - val_acc: 0.7016\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.8656\n",
      "Epoch 00038: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.4016 - acc: 0.8656 - val_loss: 1.1814 - val_acc: 0.7056\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8675\n",
      "Epoch 00039: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.3923 - acc: 0.8674 - val_loss: 1.1941 - val_acc: 0.7100\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8721\n",
      "Epoch 00040: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3808 - acc: 0.8721 - val_loss: 1.1947 - val_acc: 0.7072\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8755\n",
      "Epoch 00041: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.3732 - acc: 0.8753 - val_loss: 1.2032 - val_acc: 0.7053\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8763\n",
      "Epoch 00042: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.3689 - acc: 0.8762 - val_loss: 1.2058 - val_acc: 0.7046\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8757\n",
      "Epoch 00043: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.3667 - acc: 0.8756 - val_loss: 1.1883 - val_acc: 0.7084\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8791\n",
      "Epoch 00044: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.3634 - acc: 0.8790 - val_loss: 1.2481 - val_acc: 0.7002\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8830\n",
      "Epoch 00045: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.3484 - acc: 0.8831 - val_loss: 1.2246 - val_acc: 0.7042\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.8828\n",
      "Epoch 00046: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3458 - acc: 0.8828 - val_loss: 1.2059 - val_acc: 0.7114\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.8904\n",
      "Epoch 00047: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3271 - acc: 0.8903 - val_loss: 1.2361 - val_acc: 0.7077\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3239 - acc: 0.8917\n",
      "Epoch 00048: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 323us/sample - loss: 0.3240 - acc: 0.8916 - val_loss: 1.2205 - val_acc: 0.7093\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8905\n",
      "Epoch 00049: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.3227 - acc: 0.8905 - val_loss: 1.2247 - val_acc: 0.7177\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8904\n",
      "Epoch 00050: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.3220 - acc: 0.8904 - val_loss: 1.2377 - val_acc: 0.7142\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.8948\n",
      "Epoch 00051: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.3159 - acc: 0.8947 - val_loss: 1.2060 - val_acc: 0.7193\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.8956\n",
      "Epoch 00052: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3109 - acc: 0.8956 - val_loss: 1.2383 - val_acc: 0.7109\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3155 - acc: 0.8939\n",
      "Epoch 00053: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.3158 - acc: 0.8937 - val_loss: 1.2496 - val_acc: 0.7160\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.8997\n",
      "Epoch 00054: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.2944 - acc: 0.8997 - val_loss: 1.2534 - val_acc: 0.7126\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.8991\n",
      "Epoch 00055: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.3023 - acc: 0.8991 - val_loss: 1.2905 - val_acc: 0.7126\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9039\n",
      "Epoch 00056: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.2896 - acc: 0.9039 - val_loss: 1.2897 - val_acc: 0.7114\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9025\n",
      "Epoch 00057: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.2885 - acc: 0.9026 - val_loss: 1.2454 - val_acc: 0.7181\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9051\n",
      "Epoch 00058: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2818 - acc: 0.9051 - val_loss: 1.2171 - val_acc: 0.7254\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9010\n",
      "Epoch 00059: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.2958 - acc: 0.9009 - val_loss: 1.2417 - val_acc: 0.7263\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9026\n",
      "Epoch 00060: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 329us/sample - loss: 0.2884 - acc: 0.9026 - val_loss: 1.2308 - val_acc: 0.7230\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9050\n",
      "Epoch 00061: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 334us/sample - loss: 0.2826 - acc: 0.9050 - val_loss: 1.3078 - val_acc: 0.7123\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9072\n",
      "Epoch 00062: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 324us/sample - loss: 0.2754 - acc: 0.9073 - val_loss: 1.2789 - val_acc: 0.7212\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9085\n",
      "Epoch 00063: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.2741 - acc: 0.9085 - val_loss: 1.2774 - val_acc: 0.7170\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9119\n",
      "Epoch 00064: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 327us/sample - loss: 0.2649 - acc: 0.9119 - val_loss: 1.2902 - val_acc: 0.7163\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9085\n",
      "Epoch 00065: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.2731 - acc: 0.9086 - val_loss: 1.2884 - val_acc: 0.7172\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9117\n",
      "Epoch 00066: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2610 - acc: 0.9116 - val_loss: 1.2973 - val_acc: 0.7098\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2722 - acc: 0.9106\n",
      "Epoch 00067: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.2722 - acc: 0.9107 - val_loss: 1.4010 - val_acc: 0.7011\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9115\n",
      "Epoch 00068: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.2677 - acc: 0.9114 - val_loss: 1.2580 - val_acc: 0.7240\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9150\n",
      "Epoch 00069: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.2620 - acc: 0.9149 - val_loss: 1.2857 - val_acc: 0.7263\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9144\n",
      "Epoch 00070: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.2529 - acc: 0.9143 - val_loss: 1.2705 - val_acc: 0.7256\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2556 - acc: 0.9142\n",
      "Epoch 00071: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.2556 - acc: 0.9142 - val_loss: 1.2698 - val_acc: 0.7256\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9164\n",
      "Epoch 00072: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.2594 - acc: 0.9165 - val_loss: 1.3600 - val_acc: 0.7170\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9160\n",
      "Epoch 00073: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.2487 - acc: 0.9159 - val_loss: 1.2988 - val_acc: 0.7170\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2530 - acc: 0.9158\n",
      "Epoch 00074: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.2527 - acc: 0.9159 - val_loss: 1.3116 - val_acc: 0.7205\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9189\n",
      "Epoch 00075: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2491 - acc: 0.9190 - val_loss: 1.2702 - val_acc: 0.7272\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9210\n",
      "Epoch 00076: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2390 - acc: 0.9209 - val_loss: 1.3156 - val_acc: 0.7174\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9202\n",
      "Epoch 00077: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.2399 - acc: 0.9201 - val_loss: 1.3288 - val_acc: 0.7207\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9208\n",
      "Epoch 00078: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2389 - acc: 0.9208 - val_loss: 1.2847 - val_acc: 0.7319\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9203\n",
      "Epoch 00079: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.2413 - acc: 0.9203 - val_loss: 1.4003 - val_acc: 0.6990\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9227\n",
      "Epoch 00080: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.2372 - acc: 0.9227 - val_loss: 1.3574 - val_acc: 0.7230\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9229\n",
      "Epoch 00081: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.2322 - acc: 0.9230 - val_loss: 1.2798 - val_acc: 0.7296\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9242\n",
      "Epoch 00082: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.2338 - acc: 0.9243 - val_loss: 1.2917 - val_acc: 0.7198\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9209\n",
      "Epoch 00083: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2335 - acc: 0.9209 - val_loss: 1.3594 - val_acc: 0.7212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9204\n",
      "Epoch 00084: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.2394 - acc: 0.9204 - val_loss: 1.3493 - val_acc: 0.7263\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9245\n",
      "Epoch 00085: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.2293 - acc: 0.9244 - val_loss: 1.3192 - val_acc: 0.7195\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9241\n",
      "Epoch 00086: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2261 - acc: 0.9240 - val_loss: 1.3886 - val_acc: 0.7133\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9253\n",
      "Epoch 00087: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.2316 - acc: 0.9253 - val_loss: 1.5135 - val_acc: 0.7079\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9252\n",
      "Epoch 00088: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.2267 - acc: 0.9252 - val_loss: 1.4070 - val_acc: 0.7244\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9236\n",
      "Epoch 00089: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.2285 - acc: 0.9236 - val_loss: 1.3143 - val_acc: 0.7226\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9263\n",
      "Epoch 00090: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.2232 - acc: 0.9263 - val_loss: 1.3556 - val_acc: 0.7324\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9286\n",
      "Epoch 00091: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.2201 - acc: 0.9286 - val_loss: 1.3600 - val_acc: 0.7233\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9266\n",
      "Epoch 00092: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2211 - acc: 0.9266 - val_loss: 1.3226 - val_acc: 0.7293\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9261\n",
      "Epoch 00093: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2218 - acc: 0.9260 - val_loss: 1.3542 - val_acc: 0.7226\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9283\n",
      "Epoch 00094: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.2179 - acc: 0.9282 - val_loss: 1.3658 - val_acc: 0.7181\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9267\n",
      "Epoch 00095: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.2222 - acc: 0.9267 - val_loss: 1.3393 - val_acc: 0.7279\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9284\n",
      "Epoch 00096: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.2187 - acc: 0.9284 - val_loss: 1.3548 - val_acc: 0.7212\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9310\n",
      "Epoch 00097: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.2129 - acc: 0.9309 - val_loss: 1.3840 - val_acc: 0.7205\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9278\n",
      "Epoch 00098: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2157 - acc: 0.9276 - val_loss: 1.3449 - val_acc: 0.7352\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9287\n",
      "Epoch 00099: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.2140 - acc: 0.9286 - val_loss: 1.3195 - val_acc: 0.7284\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9301\n",
      "Epoch 00100: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.2079 - acc: 0.9301 - val_loss: 1.4193 - val_acc: 0.7221\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9303\n",
      "Epoch 00101: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.2109 - acc: 0.9303 - val_loss: 1.3920 - val_acc: 0.7147\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9309\n",
      "Epoch 00102: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.2132 - acc: 0.9310 - val_loss: 1.3230 - val_acc: 0.7400\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9290\n",
      "Epoch 00103: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.2123 - acc: 0.9290 - val_loss: 1.3384 - val_acc: 0.7200\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9292\n",
      "Epoch 00104: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.2102 - acc: 0.9292 - val_loss: 1.3346 - val_acc: 0.7270\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9300\n",
      "Epoch 00105: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2110 - acc: 0.9301 - val_loss: 1.3177 - val_acc: 0.7358\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9321\n",
      "Epoch 00106: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.2074 - acc: 0.9322 - val_loss: 1.3514 - val_acc: 0.7361\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9330\n",
      "Epoch 00107: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.2043 - acc: 0.9330 - val_loss: 1.3518 - val_acc: 0.7319\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9349\n",
      "Epoch 00108: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.2000 - acc: 0.9350 - val_loss: 1.3474 - val_acc: 0.7198\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9324\n",
      "Epoch 00109: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.2011 - acc: 0.9323 - val_loss: 1.3092 - val_acc: 0.7447\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9347\n",
      "Epoch 00110: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.2000 - acc: 0.9345 - val_loss: 1.3959 - val_acc: 0.7305\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9319\n",
      "Epoch 00111: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.2087 - acc: 0.9319 - val_loss: 1.3439 - val_acc: 0.7345\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9329\n",
      "Epoch 00112: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.2078 - acc: 0.9328 - val_loss: 1.3755 - val_acc: 0.7275\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9340\n",
      "Epoch 00113: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.2016 - acc: 0.9340 - val_loss: 1.3630 - val_acc: 0.7407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9379\n",
      "Epoch 00114: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.1922 - acc: 0.9380 - val_loss: 1.3523 - val_acc: 0.7347\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9367\n",
      "Epoch 00115: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.1895 - acc: 0.9366 - val_loss: 1.3654 - val_acc: 0.7331\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9372\n",
      "Epoch 00116: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.1932 - acc: 0.9372 - val_loss: 1.4394 - val_acc: 0.7263\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9310\n",
      "Epoch 00117: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.2067 - acc: 0.9311 - val_loss: 1.3734 - val_acc: 0.7331\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9340\n",
      "Epoch 00118: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.2030 - acc: 0.9341 - val_loss: 1.3635 - val_acc: 0.7419\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9368\n",
      "Epoch 00119: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 322us/sample - loss: 0.1929 - acc: 0.9369 - val_loss: 1.3864 - val_acc: 0.7214\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9346\n",
      "Epoch 00120: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 326us/sample - loss: 0.2007 - acc: 0.9346 - val_loss: 1.4597 - val_acc: 0.7207\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9364\n",
      "Epoch 00121: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 0.1932 - acc: 0.9363 - val_loss: 1.3797 - val_acc: 0.7338\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9340\n",
      "Epoch 00122: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.2007 - acc: 0.9341 - val_loss: 1.3434 - val_acc: 0.7379\n",
      "Epoch 123/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9372\n",
      "Epoch 00123: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.1889 - acc: 0.9372 - val_loss: 1.4072 - val_acc: 0.7333\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9388\n",
      "Epoch 00124: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.1903 - acc: 0.9388 - val_loss: 1.3258 - val_acc: 0.7354\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9359\n",
      "Epoch 00125: val_loss did not improve from 1.11065\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.1942 - acc: 0.9358 - val_loss: 1.3743 - val_acc: 0.7328\n",
      "\n",
      "1D_CNN_BN_DO_2_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 354us/sample - loss: 1.2079 - acc: 0.6586\n",
      "Loss: 1.2078924180315043 Accuracy: 0.65856695\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 3.1552 - acc: 0.2279\n",
      "Epoch 00001: val_loss improved from inf to 3.31004, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/001-3.3100.hdf5\n",
      "36805/36805 [==============================] - 23s 627us/sample - loss: 3.1535 - acc: 0.2280 - val_loss: 3.3100 - val_acc: 0.1854\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0248 - acc: 0.4295\n",
      "Epoch 00002: val_loss improved from 3.31004 to 1.31739, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/002-1.3174.hdf5\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 2.0244 - acc: 0.4296 - val_loss: 1.3174 - val_acc: 0.6047\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6143 - acc: 0.5310\n",
      "Epoch 00003: val_loss improved from 1.31739 to 1.13620, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/003-1.1362.hdf5\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 1.6139 - acc: 0.5311 - val_loss: 1.1362 - val_acc: 0.6583\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3905 - acc: 0.5947\n",
      "Epoch 00004: val_loss improved from 1.13620 to 1.00487, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/004-1.0049.hdf5\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 1.3906 - acc: 0.5946 - val_loss: 1.0049 - val_acc: 0.7098\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2416 - acc: 0.6344\n",
      "Epoch 00005: val_loss improved from 1.00487 to 0.94003, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/005-0.9400.hdf5\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 1.2420 - acc: 0.6342 - val_loss: 0.9400 - val_acc: 0.7258\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1327 - acc: 0.6665\n",
      "Epoch 00006: val_loss improved from 0.94003 to 0.93020, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/006-0.9302.hdf5\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 1.1330 - acc: 0.6665 - val_loss: 0.9302 - val_acc: 0.7263\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0468 - acc: 0.6920\n",
      "Epoch 00007: val_loss improved from 0.93020 to 0.84797, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/007-0.8480.hdf5\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 1.0460 - acc: 0.6921 - val_loss: 0.8480 - val_acc: 0.7547\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9649 - acc: 0.7144\n",
      "Epoch 00008: val_loss did not improve from 0.84797\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.9652 - acc: 0.7143 - val_loss: 0.8655 - val_acc: 0.7591\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9061 - acc: 0.7333\n",
      "Epoch 00009: val_loss improved from 0.84797 to 0.78125, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/009-0.7812.hdf5\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.9062 - acc: 0.7332 - val_loss: 0.7812 - val_acc: 0.7782\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8386 - acc: 0.7522\n",
      "Epoch 00010: val_loss did not improve from 0.78125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.8389 - acc: 0.7520 - val_loss: 0.7852 - val_acc: 0.7822\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7902 - acc: 0.7645\n",
      "Epoch 00011: val_loss improved from 0.78125 to 0.72526, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/011-0.7253.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.7895 - acc: 0.7647 - val_loss: 0.7253 - val_acc: 0.8034\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7457 - acc: 0.7804\n",
      "Epoch 00012: val_loss improved from 0.72526 to 0.69375, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/012-0.6937.hdf5\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.7462 - acc: 0.7802 - val_loss: 0.6937 - val_acc: 0.8139\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7053 - acc: 0.7897\n",
      "Epoch 00013: val_loss improved from 0.69375 to 0.66785, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/013-0.6679.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.7053 - acc: 0.7898 - val_loss: 0.6679 - val_acc: 0.8169\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6664 - acc: 0.7995\n",
      "Epoch 00014: val_loss improved from 0.66785 to 0.65510, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/014-0.6551.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.6664 - acc: 0.7995 - val_loss: 0.6551 - val_acc: 0.8220\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6323 - acc: 0.8096\n",
      "Epoch 00015: val_loss did not improve from 0.65510\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.6325 - acc: 0.8095 - val_loss: 0.7591 - val_acc: 0.7927\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6043 - acc: 0.8185\n",
      "Epoch 00016: val_loss improved from 0.65510 to 0.58631, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/016-0.5863.hdf5\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.6039 - acc: 0.8186 - val_loss: 0.5863 - val_acc: 0.8395\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.8253\n",
      "Epoch 00017: val_loss did not improve from 0.58631\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.5793 - acc: 0.8253 - val_loss: 0.5933 - val_acc: 0.8372\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.8312\n",
      "Epoch 00018: val_loss improved from 0.58631 to 0.56744, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/018-0.5674.hdf5\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.5562 - acc: 0.8311 - val_loss: 0.5674 - val_acc: 0.8502\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.8382\n",
      "Epoch 00019: val_loss did not improve from 0.56744\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.5302 - acc: 0.8381 - val_loss: 0.6319 - val_acc: 0.8255\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8430\n",
      "Epoch 00020: val_loss improved from 0.56744 to 0.55180, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/020-0.5518.hdf5\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.5131 - acc: 0.8431 - val_loss: 0.5518 - val_acc: 0.8507\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4876 - acc: 0.8498\n",
      "Epoch 00021: val_loss did not improve from 0.55180\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.4874 - acc: 0.8498 - val_loss: 0.8819 - val_acc: 0.7701\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.8553\n",
      "Epoch 00022: val_loss improved from 0.55180 to 0.54906, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/022-0.5491.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.4706 - acc: 0.8553 - val_loss: 0.5491 - val_acc: 0.8514\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8593\n",
      "Epoch 00023: val_loss improved from 0.54906 to 0.54614, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/023-0.5461.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.4621 - acc: 0.8593 - val_loss: 0.5461 - val_acc: 0.8498\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4404 - acc: 0.8648\n",
      "Epoch 00024: val_loss did not improve from 0.54614\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.4406 - acc: 0.8649 - val_loss: 0.5486 - val_acc: 0.8512\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.8684\n",
      "Epoch 00025: val_loss improved from 0.54614 to 0.52287, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/025-0.5229.hdf5\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.4264 - acc: 0.8684 - val_loss: 0.5229 - val_acc: 0.8602\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8732\n",
      "Epoch 00026: val_loss did not improve from 0.52287\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.4112 - acc: 0.8733 - val_loss: 0.5426 - val_acc: 0.8570\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8774\n",
      "Epoch 00027: val_loss improved from 0.52287 to 0.51914, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/027-0.5191.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.3938 - acc: 0.8774 - val_loss: 0.5191 - val_acc: 0.8600\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8802\n",
      "Epoch 00028: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.3875 - acc: 0.8802 - val_loss: 0.5985 - val_acc: 0.8416\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3736 - acc: 0.8818\n",
      "Epoch 00029: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.3734 - acc: 0.8818 - val_loss: 0.5263 - val_acc: 0.8600\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3609 - acc: 0.8867\n",
      "Epoch 00030: val_loss did not improve from 0.51914\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.3612 - acc: 0.8866 - val_loss: 0.5298 - val_acc: 0.8633\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.8919\n",
      "Epoch 00031: val_loss improved from 0.51914 to 0.51782, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/031-0.5178.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.3451 - acc: 0.8919 - val_loss: 0.5178 - val_acc: 0.8679\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3326 - acc: 0.8941\n",
      "Epoch 00032: val_loss improved from 0.51782 to 0.50868, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/032-0.5087.hdf5\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.3329 - acc: 0.8940 - val_loss: 0.5087 - val_acc: 0.8693\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.8967\n",
      "Epoch 00033: val_loss did not improve from 0.50868\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.3291 - acc: 0.8966 - val_loss: 0.5470 - val_acc: 0.8577\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.9000\n",
      "Epoch 00034: val_loss did not improve from 0.50868\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.3159 - acc: 0.8999 - val_loss: 0.5198 - val_acc: 0.8628\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.9014\n",
      "Epoch 00035: val_loss did not improve from 0.50868\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.3088 - acc: 0.9013 - val_loss: 0.5353 - val_acc: 0.8670\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9052\n",
      "Epoch 00036: val_loss improved from 0.50868 to 0.50400, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/036-0.5040.hdf5\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.2969 - acc: 0.9051 - val_loss: 0.5040 - val_acc: 0.8710\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9071\n",
      "Epoch 00037: val_loss did not improve from 0.50400\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2943 - acc: 0.9072 - val_loss: 0.5305 - val_acc: 0.8684\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9101\n",
      "Epoch 00038: val_loss did not improve from 0.50400\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2829 - acc: 0.9101 - val_loss: 0.5145 - val_acc: 0.8677\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9103\n",
      "Epoch 00039: val_loss did not improve from 0.50400\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2800 - acc: 0.9103 - val_loss: 0.5487 - val_acc: 0.8635\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9140\n",
      "Epoch 00040: val_loss did not improve from 0.50400\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.2669 - acc: 0.9140 - val_loss: 0.5165 - val_acc: 0.8684\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9169\n",
      "Epoch 00041: val_loss did not improve from 0.50400\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2609 - acc: 0.9168 - val_loss: 0.5327 - val_acc: 0.8665\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9182\n",
      "Epoch 00042: val_loss improved from 0.50400 to 0.50125, saving model to model/checkpoint/1D_CNN_BN_DO_3_only_conv_checkpoint/042-0.5013.hdf5\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.2526 - acc: 0.9183 - val_loss: 0.5013 - val_acc: 0.8714\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.9191\n",
      "Epoch 00043: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.2476 - acc: 0.9190 - val_loss: 0.5619 - val_acc: 0.8623\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9203\n",
      "Epoch 00044: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2432 - acc: 0.9204 - val_loss: 0.5302 - val_acc: 0.8717\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9231\n",
      "Epoch 00045: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2373 - acc: 0.9229 - val_loss: 0.5271 - val_acc: 0.8665\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9250\n",
      "Epoch 00046: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2316 - acc: 0.9250 - val_loss: 0.5468 - val_acc: 0.8614\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9276\n",
      "Epoch 00047: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2265 - acc: 0.9276 - val_loss: 0.6161 - val_acc: 0.8593\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9293\n",
      "Epoch 00048: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.2189 - acc: 0.9293 - val_loss: 0.5252 - val_acc: 0.8719\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9297\n",
      "Epoch 00049: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.2165 - acc: 0.9297 - val_loss: 0.5315 - val_acc: 0.8707\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9319\n",
      "Epoch 00050: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2108 - acc: 0.9319 - val_loss: 0.5613 - val_acc: 0.8626\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9340\n",
      "Epoch 00051: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2065 - acc: 0.9340 - val_loss: 0.5385 - val_acc: 0.8744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9324\n",
      "Epoch 00052: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2055 - acc: 0.9324 - val_loss: 0.5519 - val_acc: 0.8707\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9338\n",
      "Epoch 00053: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2034 - acc: 0.9338 - val_loss: 0.5370 - val_acc: 0.8740\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9338\n",
      "Epoch 00054: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2023 - acc: 0.9338 - val_loss: 0.5701 - val_acc: 0.8647\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9353\n",
      "Epoch 00055: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1971 - acc: 0.9354 - val_loss: 0.5401 - val_acc: 0.8698\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9394\n",
      "Epoch 00056: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1864 - acc: 0.9394 - val_loss: 0.5493 - val_acc: 0.8733\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9403\n",
      "Epoch 00057: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1850 - acc: 0.9403 - val_loss: 0.5344 - val_acc: 0.8735\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1919 - acc: 0.9364\n",
      "Epoch 00058: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.1921 - acc: 0.9363 - val_loss: 0.5273 - val_acc: 0.8807\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9404\n",
      "Epoch 00059: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1801 - acc: 0.9404 - val_loss: 0.5371 - val_acc: 0.8765\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9411\n",
      "Epoch 00060: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1777 - acc: 0.9411 - val_loss: 0.5607 - val_acc: 0.8670\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9428\n",
      "Epoch 00061: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1763 - acc: 0.9429 - val_loss: 0.5545 - val_acc: 0.8744\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9428\n",
      "Epoch 00062: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1722 - acc: 0.9426 - val_loss: 0.5341 - val_acc: 0.8793\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9449\n",
      "Epoch 00063: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 351us/sample - loss: 0.1666 - acc: 0.9450 - val_loss: 0.6134 - val_acc: 0.8612\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9446\n",
      "Epoch 00064: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.1688 - acc: 0.9446 - val_loss: 0.6002 - val_acc: 0.8602\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9435\n",
      "Epoch 00065: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1721 - acc: 0.9435 - val_loss: 0.5497 - val_acc: 0.8768\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9433\n",
      "Epoch 00066: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1717 - acc: 0.9434 - val_loss: 0.5427 - val_acc: 0.8744\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9472\n",
      "Epoch 00067: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1561 - acc: 0.9472 - val_loss: 0.5586 - val_acc: 0.8737\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9456\n",
      "Epoch 00068: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1653 - acc: 0.9454 - val_loss: 0.6092 - val_acc: 0.8651\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9452\n",
      "Epoch 00069: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1621 - acc: 0.9453 - val_loss: 0.5468 - val_acc: 0.8770\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9490\n",
      "Epoch 00070: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1522 - acc: 0.9489 - val_loss: 0.6061 - val_acc: 0.8642\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9490\n",
      "Epoch 00071: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1552 - acc: 0.9490 - val_loss: 0.5794 - val_acc: 0.8730\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9519\n",
      "Epoch 00072: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1458 - acc: 0.9518 - val_loss: 0.5792 - val_acc: 0.8730\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9485\n",
      "Epoch 00073: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1550 - acc: 0.9486 - val_loss: 0.5751 - val_acc: 0.8747\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9491\n",
      "Epoch 00074: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1542 - acc: 0.9491 - val_loss: 0.5903 - val_acc: 0.8710\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9496\n",
      "Epoch 00075: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1520 - acc: 0.9494 - val_loss: 0.5649 - val_acc: 0.8812\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9524\n",
      "Epoch 00076: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1434 - acc: 0.9524 - val_loss: 0.5777 - val_acc: 0.8761\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9510\n",
      "Epoch 00077: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1460 - acc: 0.9510 - val_loss: 0.5572 - val_acc: 0.8779\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9504\n",
      "Epoch 00078: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1500 - acc: 0.9504 - val_loss: 0.5769 - val_acc: 0.8744\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9507\n",
      "Epoch 00079: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1477 - acc: 0.9508 - val_loss: 0.5576 - val_acc: 0.8842\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9536\n",
      "Epoch 00080: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1398 - acc: 0.9536 - val_loss: 0.6165 - val_acc: 0.8637\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9508\n",
      "Epoch 00081: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1496 - acc: 0.9509 - val_loss: 0.5780 - val_acc: 0.8793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9530\n",
      "Epoch 00082: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1426 - acc: 0.9530 - val_loss: 0.5553 - val_acc: 0.8821\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9552\n",
      "Epoch 00083: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1350 - acc: 0.9553 - val_loss: 0.5680 - val_acc: 0.8740\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9534\n",
      "Epoch 00084: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1396 - acc: 0.9534 - val_loss: 0.5756 - val_acc: 0.8754\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9553\n",
      "Epoch 00085: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.1336 - acc: 0.9552 - val_loss: 0.5578 - val_acc: 0.8793\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9542\n",
      "Epoch 00086: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 348us/sample - loss: 0.1368 - acc: 0.9541 - val_loss: 0.5727 - val_acc: 0.8737\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9551\n",
      "Epoch 00087: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1331 - acc: 0.9550 - val_loss: 0.5927 - val_acc: 0.8742\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9539\n",
      "Epoch 00088: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1380 - acc: 0.9539 - val_loss: 0.5849 - val_acc: 0.8765\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9560\n",
      "Epoch 00089: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1312 - acc: 0.9559 - val_loss: 0.5516 - val_acc: 0.8791\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9557\n",
      "Epoch 00090: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1336 - acc: 0.9557 - val_loss: 0.5737 - val_acc: 0.8833\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9564\n",
      "Epoch 00091: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1312 - acc: 0.9563 - val_loss: 0.6437 - val_acc: 0.8682\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9575\n",
      "Epoch 00092: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1273 - acc: 0.9575 - val_loss: 0.5716 - val_acc: 0.8793\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9592\n",
      "Epoch 00093: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1238 - acc: 0.9591 - val_loss: 0.5801 - val_acc: 0.8807\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9581\n",
      "Epoch 00094: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1234 - acc: 0.9580 - val_loss: 0.5733 - val_acc: 0.8779\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9594\n",
      "Epoch 00095: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1241 - acc: 0.9594 - val_loss: 0.6034 - val_acc: 0.8733\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9574\n",
      "Epoch 00096: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1296 - acc: 0.9574 - val_loss: 0.5720 - val_acc: 0.8800\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9569\n",
      "Epoch 00097: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1280 - acc: 0.9569 - val_loss: 0.5748 - val_acc: 0.8800\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9588\n",
      "Epoch 00098: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1261 - acc: 0.9587 - val_loss: 0.6090 - val_acc: 0.8805\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9597\n",
      "Epoch 00099: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1235 - acc: 0.9597 - val_loss: 0.6113 - val_acc: 0.8765\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9600\n",
      "Epoch 00100: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1232 - acc: 0.9601 - val_loss: 0.6016 - val_acc: 0.8782\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9610\n",
      "Epoch 00101: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1185 - acc: 0.9610 - val_loss: 0.5879 - val_acc: 0.8765\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9602\n",
      "Epoch 00102: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1181 - acc: 0.9602 - val_loss: 0.6314 - val_acc: 0.8768\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9611\n",
      "Epoch 00103: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1155 - acc: 0.9610 - val_loss: 0.5775 - val_acc: 0.8805\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9569\n",
      "Epoch 00104: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1313 - acc: 0.9570 - val_loss: 0.6133 - val_acc: 0.8728\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9631\n",
      "Epoch 00105: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1137 - acc: 0.9632 - val_loss: 0.5814 - val_acc: 0.8831\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9604\n",
      "Epoch 00106: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1182 - acc: 0.9604 - val_loss: 0.5815 - val_acc: 0.8786\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9605\n",
      "Epoch 00107: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1198 - acc: 0.9605 - val_loss: 0.6366 - val_acc: 0.8765\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9619\n",
      "Epoch 00108: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1135 - acc: 0.9619 - val_loss: 0.5835 - val_acc: 0.8838\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9629\n",
      "Epoch 00109: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1133 - acc: 0.9629 - val_loss: 0.5908 - val_acc: 0.8852\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9612\n",
      "Epoch 00110: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1133 - acc: 0.9612 - val_loss: 0.5848 - val_acc: 0.8789\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9630\n",
      "Epoch 00111: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1119 - acc: 0.9630 - val_loss: 0.6296 - val_acc: 0.8717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9622\n",
      "Epoch 00112: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1108 - acc: 0.9623 - val_loss: 0.5934 - val_acc: 0.8777\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9615\n",
      "Epoch 00113: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1144 - acc: 0.9615 - val_loss: 0.6040 - val_acc: 0.8784\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9615\n",
      "Epoch 00114: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1154 - acc: 0.9615 - val_loss: 0.5752 - val_acc: 0.8821\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9628\n",
      "Epoch 00115: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1116 - acc: 0.9627 - val_loss: 0.5820 - val_acc: 0.8807\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9644\n",
      "Epoch 00116: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1065 - acc: 0.9645 - val_loss: 0.6468 - val_acc: 0.8700\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9630\n",
      "Epoch 00117: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1112 - acc: 0.9631 - val_loss: 0.5883 - val_acc: 0.8842\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9658\n",
      "Epoch 00118: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1060 - acc: 0.9657 - val_loss: 0.6008 - val_acc: 0.8826\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9604\n",
      "Epoch 00119: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1200 - acc: 0.9605 - val_loss: 0.5959 - val_acc: 0.8847\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9647\n",
      "Epoch 00120: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1087 - acc: 0.9647 - val_loss: 0.6273 - val_acc: 0.8821\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9633\n",
      "Epoch 00121: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.1125 - acc: 0.9633 - val_loss: 0.5999 - val_acc: 0.8777\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9639\n",
      "Epoch 00122: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1091 - acc: 0.9639 - val_loss: 0.6395 - val_acc: 0.8737\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9642\n",
      "Epoch 00123: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1090 - acc: 0.9642 - val_loss: 0.6097 - val_acc: 0.8831\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9636\n",
      "Epoch 00124: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1122 - acc: 0.9635 - val_loss: 0.6148 - val_acc: 0.8812\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9634\n",
      "Epoch 00125: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1086 - acc: 0.9634 - val_loss: 0.6328 - val_acc: 0.8786\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9644\n",
      "Epoch 00126: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1081 - acc: 0.9645 - val_loss: 0.6190 - val_acc: 0.8812\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9643\n",
      "Epoch 00127: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1090 - acc: 0.9644 - val_loss: 0.6026 - val_acc: 0.8758\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9663\n",
      "Epoch 00128: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1034 - acc: 0.9663 - val_loss: 0.6139 - val_acc: 0.8796\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9649\n",
      "Epoch 00129: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.1078 - acc: 0.9649 - val_loss: 0.6411 - val_acc: 0.8735\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9671\n",
      "Epoch 00130: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0998 - acc: 0.9670 - val_loss: 0.6342 - val_acc: 0.8812\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9676\n",
      "Epoch 00131: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0985 - acc: 0.9676 - val_loss: 0.6191 - val_acc: 0.8793\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9673\n",
      "Epoch 00132: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.0998 - acc: 0.9674 - val_loss: 0.5900 - val_acc: 0.8824\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9654\n",
      "Epoch 00133: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1063 - acc: 0.9654 - val_loss: 0.6346 - val_acc: 0.8810\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9658\n",
      "Epoch 00134: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1047 - acc: 0.9657 - val_loss: 0.6015 - val_acc: 0.8856\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9646\n",
      "Epoch 00135: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1072 - acc: 0.9646 - val_loss: 0.5961 - val_acc: 0.8819\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9658\n",
      "Epoch 00136: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1007 - acc: 0.9659 - val_loss: 0.6154 - val_acc: 0.8849\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9660\n",
      "Epoch 00137: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1042 - acc: 0.9660 - val_loss: 0.6675 - val_acc: 0.8719\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9663\n",
      "Epoch 00138: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0996 - acc: 0.9663 - val_loss: 0.6439 - val_acc: 0.8777\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9671\n",
      "Epoch 00139: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1015 - acc: 0.9670 - val_loss: 0.6154 - val_acc: 0.8849\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9668\n",
      "Epoch 00140: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0996 - acc: 0.9668 - val_loss: 0.6268 - val_acc: 0.8875\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9663\n",
      "Epoch 00141: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1041 - acc: 0.9663 - val_loss: 0.6110 - val_acc: 0.8863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9675\n",
      "Epoch 00142: val_loss did not improve from 0.50125\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.0989 - acc: 0.9675 - val_loss: 0.6169 - val_acc: 0.8782\n",
      "\n",
      "1D_CNN_BN_DO_3_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 406us/sample - loss: 0.5594 - acc: 0.8538\n",
      "Loss: 0.5594111439099811 Accuracy: 0.8537902\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 3.0288 - acc: 0.2362\n",
      "Epoch 00001: val_loss improved from inf to 4.79983, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/001-4.7998.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 3.0263 - acc: 0.2366 - val_loss: 4.7998 - val_acc: 0.1379\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9429 - acc: 0.4286\n",
      "Epoch 00002: val_loss improved from 4.79983 to 1.22896, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/002-1.2290.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 1.9428 - acc: 0.4286 - val_loss: 1.2290 - val_acc: 0.6180\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5312 - acc: 0.5443\n",
      "Epoch 00003: val_loss improved from 1.22896 to 0.99150, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/003-0.9915.hdf5\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 1.5310 - acc: 0.5444 - val_loss: 0.9915 - val_acc: 0.7021\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2544 - acc: 0.6198\n",
      "Epoch 00004: val_loss improved from 0.99150 to 0.83959, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/004-0.8396.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 1.2547 - acc: 0.6199 - val_loss: 0.8396 - val_acc: 0.7508\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0556 - acc: 0.6810\n",
      "Epoch 00005: val_loss improved from 0.83959 to 0.73785, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/005-0.7378.hdf5\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 1.0554 - acc: 0.6811 - val_loss: 0.7378 - val_acc: 0.7773\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9062 - acc: 0.7242\n",
      "Epoch 00006: val_loss improved from 0.73785 to 0.72907, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/006-0.7291.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.9057 - acc: 0.7242 - val_loss: 0.7291 - val_acc: 0.7876\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7739 - acc: 0.7641\n",
      "Epoch 00007: val_loss improved from 0.72907 to 0.57630, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/007-0.5763.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.7740 - acc: 0.7640 - val_loss: 0.5763 - val_acc: 0.8358\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6720 - acc: 0.7972\n",
      "Epoch 00008: val_loss improved from 0.57630 to 0.48847, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/008-0.4885.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.6719 - acc: 0.7972 - val_loss: 0.4885 - val_acc: 0.8560\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5978 - acc: 0.8186\n",
      "Epoch 00009: val_loss improved from 0.48847 to 0.45067, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/009-0.4507.hdf5\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.5982 - acc: 0.8184 - val_loss: 0.4507 - val_acc: 0.8693\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8386\n",
      "Epoch 00010: val_loss improved from 0.45067 to 0.41337, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/010-0.4134.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.5319 - acc: 0.8384 - val_loss: 0.4134 - val_acc: 0.8826\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4892 - acc: 0.8504\n",
      "Epoch 00011: val_loss improved from 0.41337 to 0.36892, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/011-0.3689.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.4896 - acc: 0.8503 - val_loss: 0.3689 - val_acc: 0.8959\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4465 - acc: 0.8624\n",
      "Epoch 00012: val_loss improved from 0.36892 to 0.34301, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/012-0.3430.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.4469 - acc: 0.8623 - val_loss: 0.3430 - val_acc: 0.9085\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8759\n",
      "Epoch 00013: val_loss improved from 0.34301 to 0.33195, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/013-0.3319.hdf5\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.4063 - acc: 0.8760 - val_loss: 0.3319 - val_acc: 0.9064\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3813 - acc: 0.8807\n",
      "Epoch 00014: val_loss improved from 0.33195 to 0.32083, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/014-0.3208.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.3811 - acc: 0.8807 - val_loss: 0.3208 - val_acc: 0.9122\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8916\n",
      "Epoch 00015: val_loss improved from 0.32083 to 0.30424, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/015-0.3042.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.3485 - acc: 0.8916 - val_loss: 0.3042 - val_acc: 0.9168\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.8961\n",
      "Epoch 00016: val_loss did not improve from 0.30424\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.3339 - acc: 0.8962 - val_loss: 0.3072 - val_acc: 0.9136\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.9026\n",
      "Epoch 00017: val_loss improved from 0.30424 to 0.28863, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/017-0.2886.hdf5\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.3140 - acc: 0.9025 - val_loss: 0.2886 - val_acc: 0.9217\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2972 - acc: 0.9079\n",
      "Epoch 00018: val_loss did not improve from 0.28863\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.2972 - acc: 0.9079 - val_loss: 0.2914 - val_acc: 0.9182\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9121\n",
      "Epoch 00019: val_loss improved from 0.28863 to 0.26090, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/019-0.2609.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.2789 - acc: 0.9121 - val_loss: 0.2609 - val_acc: 0.9241\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9181\n",
      "Epoch 00020: val_loss did not improve from 0.26090\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.2604 - acc: 0.9181 - val_loss: 0.2875 - val_acc: 0.9194\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2495 - acc: 0.9206\n",
      "Epoch 00021: val_loss improved from 0.26090 to 0.25062, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/021-0.2506.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.2494 - acc: 0.9207 - val_loss: 0.2506 - val_acc: 0.9299\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9251\n",
      "Epoch 00022: val_loss did not improve from 0.25062\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.2345 - acc: 0.9251 - val_loss: 0.2673 - val_acc: 0.9262\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9272\n",
      "Epoch 00023: val_loss did not improve from 0.25062\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.2195 - acc: 0.9273 - val_loss: 0.2791 - val_acc: 0.9206\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9303\n",
      "Epoch 00024: val_loss did not improve from 0.25062\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.2130 - acc: 0.9302 - val_loss: 0.2623 - val_acc: 0.9250\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9349\n",
      "Epoch 00025: val_loss did not improve from 0.25062\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.2064 - acc: 0.9346 - val_loss: 0.2598 - val_acc: 0.9280\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9347\n",
      "Epoch 00026: val_loss improved from 0.25062 to 0.24860, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/026-0.2486.hdf5\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.2012 - acc: 0.9346 - val_loss: 0.2486 - val_acc: 0.9313\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9392\n",
      "Epoch 00027: val_loss did not improve from 0.24860\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1914 - acc: 0.9392 - val_loss: 0.2529 - val_acc: 0.9324\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9419\n",
      "Epoch 00028: val_loss improved from 0.24860 to 0.24399, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/028-0.2440.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.1778 - acc: 0.9418 - val_loss: 0.2440 - val_acc: 0.9315\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9437\n",
      "Epoch 00029: val_loss did not improve from 0.24399\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.1708 - acc: 0.9437 - val_loss: 0.2461 - val_acc: 0.9301\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9461\n",
      "Epoch 00030: val_loss did not improve from 0.24399\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.1663 - acc: 0.9461 - val_loss: 0.2539 - val_acc: 0.9311\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9476\n",
      "Epoch 00031: val_loss improved from 0.24399 to 0.24054, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/031-0.2405.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.1599 - acc: 0.9476 - val_loss: 0.2405 - val_acc: 0.9324\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9483\n",
      "Epoch 00032: val_loss improved from 0.24054 to 0.23977, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/032-0.2398.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.1545 - acc: 0.9483 - val_loss: 0.2398 - val_acc: 0.9338\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9508\n",
      "Epoch 00033: val_loss improved from 0.23977 to 0.23650, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/033-0.2365.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.1486 - acc: 0.9508 - val_loss: 0.2365 - val_acc: 0.9324\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9523\n",
      "Epoch 00034: val_loss did not improve from 0.23650\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.1438 - acc: 0.9524 - val_loss: 0.2375 - val_acc: 0.9369\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9530\n",
      "Epoch 00035: val_loss improved from 0.23650 to 0.23583, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/035-0.2358.hdf5\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.1401 - acc: 0.9529 - val_loss: 0.2358 - val_acc: 0.9313\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9546\n",
      "Epoch 00036: val_loss did not improve from 0.23583\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.1387 - acc: 0.9546 - val_loss: 0.2436 - val_acc: 0.9355\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9560\n",
      "Epoch 00037: val_loss did not improve from 0.23583\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1310 - acc: 0.9560 - val_loss: 0.2480 - val_acc: 0.9345\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9582\n",
      "Epoch 00038: val_loss did not improve from 0.23583\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1265 - acc: 0.9582 - val_loss: 0.2490 - val_acc: 0.9338\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9590\n",
      "Epoch 00039: val_loss improved from 0.23583 to 0.23479, saving model to model/checkpoint/1D_CNN_BN_DO_4_only_conv_checkpoint/039-0.2348.hdf5\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1233 - acc: 0.9590 - val_loss: 0.2348 - val_acc: 0.9404\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9608\n",
      "Epoch 00040: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1191 - acc: 0.9608 - val_loss: 0.2504 - val_acc: 0.9371\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9621\n",
      "Epoch 00041: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1115 - acc: 0.9622 - val_loss: 0.2468 - val_acc: 0.9331\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9615\n",
      "Epoch 00042: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1139 - acc: 0.9615 - val_loss: 0.2701 - val_acc: 0.9320\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9639\n",
      "Epoch 00043: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1098 - acc: 0.9639 - val_loss: 0.2681 - val_acc: 0.9306\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9616\n",
      "Epoch 00044: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.1111 - acc: 0.9616 - val_loss: 0.2471 - val_acc: 0.9341\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9644\n",
      "Epoch 00045: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1055 - acc: 0.9644 - val_loss: 0.2559 - val_acc: 0.9329\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9652\n",
      "Epoch 00046: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1034 - acc: 0.9652 - val_loss: 0.2500 - val_acc: 0.9385\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9650\n",
      "Epoch 00047: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.1050 - acc: 0.9650 - val_loss: 0.2622 - val_acc: 0.9359\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9695\n",
      "Epoch 00048: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0923 - acc: 0.9696 - val_loss: 0.2640 - val_acc: 0.9362\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9687\n",
      "Epoch 00049: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.0933 - acc: 0.9687 - val_loss: 0.2990 - val_acc: 0.9266\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9688\n",
      "Epoch 00050: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.0943 - acc: 0.9688 - val_loss: 0.2435 - val_acc: 0.9362\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9692\n",
      "Epoch 00051: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0913 - acc: 0.9692 - val_loss: 0.2594 - val_acc: 0.9322\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9725\n",
      "Epoch 00052: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0843 - acc: 0.9724 - val_loss: 0.2739 - val_acc: 0.9320\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9693\n",
      "Epoch 00053: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0903 - acc: 0.9692 - val_loss: 0.2539 - val_acc: 0.9373\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9699\n",
      "Epoch 00054: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0912 - acc: 0.9699 - val_loss: 0.2765 - val_acc: 0.9306\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9716\n",
      "Epoch 00055: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0828 - acc: 0.9716 - val_loss: 0.2595 - val_acc: 0.9345\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9713\n",
      "Epoch 00056: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0853 - acc: 0.9713 - val_loss: 0.2514 - val_acc: 0.9399\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9720\n",
      "Epoch 00057: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0837 - acc: 0.9720 - val_loss: 0.2591 - val_acc: 0.9348\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9735\n",
      "Epoch 00058: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0786 - acc: 0.9734 - val_loss: 0.2899 - val_acc: 0.9329\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9722\n",
      "Epoch 00059: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0831 - acc: 0.9722 - val_loss: 0.2729 - val_acc: 0.9371\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9713\n",
      "Epoch 00060: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0839 - acc: 0.9712 - val_loss: 0.2885 - val_acc: 0.9320\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9754\n",
      "Epoch 00061: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0735 - acc: 0.9754 - val_loss: 0.2694 - val_acc: 0.9311\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9743\n",
      "Epoch 00062: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0760 - acc: 0.9743 - val_loss: 0.2703 - val_acc: 0.9378\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9735\n",
      "Epoch 00063: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0789 - acc: 0.9734 - val_loss: 0.2628 - val_acc: 0.9394\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9760\n",
      "Epoch 00064: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0712 - acc: 0.9760 - val_loss: 0.2545 - val_acc: 0.9404\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9748\n",
      "Epoch 00065: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0717 - acc: 0.9748 - val_loss: 0.2806 - val_acc: 0.9324\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9769\n",
      "Epoch 00066: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0694 - acc: 0.9769 - val_loss: 0.2713 - val_acc: 0.9385\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9754\n",
      "Epoch 00067: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0715 - acc: 0.9754 - val_loss: 0.2609 - val_acc: 0.9397\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9762\n",
      "Epoch 00068: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0693 - acc: 0.9763 - val_loss: 0.2771 - val_acc: 0.9394\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9781\n",
      "Epoch 00069: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.0634 - acc: 0.9781 - val_loss: 0.2608 - val_acc: 0.9362\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9768\n",
      "Epoch 00070: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0667 - acc: 0.9767 - val_loss: 0.2736 - val_acc: 0.9364\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9767\n",
      "Epoch 00071: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0686 - acc: 0.9767 - val_loss: 0.2604 - val_acc: 0.9369\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9774\n",
      "Epoch 00072: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0681 - acc: 0.9773 - val_loss: 0.2769 - val_acc: 0.9350\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9782\n",
      "Epoch 00073: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0648 - acc: 0.9782 - val_loss: 0.2871 - val_acc: 0.9385\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9788\n",
      "Epoch 00074: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0635 - acc: 0.9788 - val_loss: 0.2723 - val_acc: 0.9348\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9778\n",
      "Epoch 00075: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0666 - acc: 0.9779 - val_loss: 0.2652 - val_acc: 0.9394\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9785\n",
      "Epoch 00076: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0630 - acc: 0.9785 - val_loss: 0.2688 - val_acc: 0.9385\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9784\n",
      "Epoch 00077: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0623 - acc: 0.9784 - val_loss: 0.2928 - val_acc: 0.9297\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9793\n",
      "Epoch 00078: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0589 - acc: 0.9793 - val_loss: 0.2877 - val_acc: 0.9357\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9802\n",
      "Epoch 00079: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0617 - acc: 0.9801 - val_loss: 0.2960 - val_acc: 0.9334\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9792\n",
      "Epoch 00080: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0601 - acc: 0.9792 - val_loss: 0.3086 - val_acc: 0.9313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9821\n",
      "Epoch 00081: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0561 - acc: 0.9821 - val_loss: 0.2883 - val_acc: 0.9366\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9795\n",
      "Epoch 00082: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0602 - acc: 0.9794 - val_loss: 0.2710 - val_acc: 0.9362\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9808\n",
      "Epoch 00083: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0581 - acc: 0.9808 - val_loss: 0.2811 - val_acc: 0.9369\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9800\n",
      "Epoch 00084: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0594 - acc: 0.9799 - val_loss: 0.2997 - val_acc: 0.9306\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9818\n",
      "Epoch 00085: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0560 - acc: 0.9819 - val_loss: 0.2711 - val_acc: 0.9390\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9804\n",
      "Epoch 00086: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0593 - acc: 0.9802 - val_loss: 0.2967 - val_acc: 0.9373\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9786\n",
      "Epoch 00087: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0661 - acc: 0.9786 - val_loss: 0.2829 - val_acc: 0.9415\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9828\n",
      "Epoch 00088: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0512 - acc: 0.9827 - val_loss: 0.2569 - val_acc: 0.9408\n",
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9819\n",
      "Epoch 00089: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0541 - acc: 0.9818 - val_loss: 0.3040 - val_acc: 0.9322\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9817\n",
      "Epoch 00090: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0530 - acc: 0.9817 - val_loss: 0.3130 - val_acc: 0.9348\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9841\n",
      "Epoch 00091: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0484 - acc: 0.9840 - val_loss: 0.2909 - val_acc: 0.9355\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9790\n",
      "Epoch 00092: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0642 - acc: 0.9790 - val_loss: 0.2916 - val_acc: 0.9383\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9822\n",
      "Epoch 00093: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0527 - acc: 0.9823 - val_loss: 0.2957 - val_acc: 0.9380\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9816\n",
      "Epoch 00094: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0567 - acc: 0.9815 - val_loss: 0.2834 - val_acc: 0.9432\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9823\n",
      "Epoch 00095: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0504 - acc: 0.9822 - val_loss: 0.2774 - val_acc: 0.9439\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9830\n",
      "Epoch 00096: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0518 - acc: 0.9829 - val_loss: 0.2941 - val_acc: 0.9385\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9838\n",
      "Epoch 00097: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0501 - acc: 0.9838 - val_loss: 0.2996 - val_acc: 0.9390\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9844\n",
      "Epoch 00098: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0492 - acc: 0.9843 - val_loss: 0.2994 - val_acc: 0.9317\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9829\n",
      "Epoch 00099: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0517 - acc: 0.9828 - val_loss: 0.2957 - val_acc: 0.9371\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9800\n",
      "Epoch 00100: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0615 - acc: 0.9800 - val_loss: 0.2904 - val_acc: 0.9401\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9837\n",
      "Epoch 00101: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0475 - acc: 0.9837 - val_loss: 0.2725 - val_acc: 0.9413\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9839\n",
      "Epoch 00102: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0470 - acc: 0.9840 - val_loss: 0.3255 - val_acc: 0.9334\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9838\n",
      "Epoch 00103: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0480 - acc: 0.9838 - val_loss: 0.3028 - val_acc: 0.9357\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9848\n",
      "Epoch 00104: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0443 - acc: 0.9848 - val_loss: 0.2941 - val_acc: 0.9366\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9840\n",
      "Epoch 00105: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0460 - acc: 0.9839 - val_loss: 0.2987 - val_acc: 0.9397\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9833\n",
      "Epoch 00106: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0500 - acc: 0.9833 - val_loss: 0.3201 - val_acc: 0.9320\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9831\n",
      "Epoch 00107: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0493 - acc: 0.9831 - val_loss: 0.2874 - val_acc: 0.9404\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9835\n",
      "Epoch 00108: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0503 - acc: 0.9835 - val_loss: 0.2863 - val_acc: 0.9399\n",
      "Epoch 109/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9842\n",
      "Epoch 00109: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0464 - acc: 0.9842 - val_loss: 0.3145 - val_acc: 0.9357\n",
      "Epoch 110/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9821\n",
      "Epoch 00110: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0516 - acc: 0.9821 - val_loss: 0.2890 - val_acc: 0.9415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9851\n",
      "Epoch 00111: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0438 - acc: 0.9851 - val_loss: 0.2698 - val_acc: 0.9408\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9828\n",
      "Epoch 00112: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0544 - acc: 0.9828 - val_loss: 0.2844 - val_acc: 0.9406\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9864\n",
      "Epoch 00113: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0411 - acc: 0.9864 - val_loss: 0.2925 - val_acc: 0.9422\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9852\n",
      "Epoch 00114: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0456 - acc: 0.9853 - val_loss: 0.2874 - val_acc: 0.9397\n",
      "Epoch 115/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9860\n",
      "Epoch 00115: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0417 - acc: 0.9860 - val_loss: 0.2885 - val_acc: 0.9343\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9861\n",
      "Epoch 00116: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0414 - acc: 0.9861 - val_loss: 0.2998 - val_acc: 0.9383\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9836\n",
      "Epoch 00117: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0490 - acc: 0.9835 - val_loss: 0.3062 - val_acc: 0.9394\n",
      "Epoch 118/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9845\n",
      "Epoch 00118: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0468 - acc: 0.9846 - val_loss: 0.3102 - val_acc: 0.9373\n",
      "Epoch 119/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9857\n",
      "Epoch 00119: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0436 - acc: 0.9857 - val_loss: 0.3088 - val_acc: 0.9420\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9858\n",
      "Epoch 00120: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0415 - acc: 0.9858 - val_loss: 0.3114 - val_acc: 0.9387\n",
      "Epoch 121/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9838\n",
      "Epoch 00121: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0477 - acc: 0.9838 - val_loss: 0.3105 - val_acc: 0.9399\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9851\n",
      "Epoch 00122: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0456 - acc: 0.9851 - val_loss: 0.2760 - val_acc: 0.9406\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9848\n",
      "Epoch 00123: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0447 - acc: 0.9848 - val_loss: 0.3188 - val_acc: 0.9394\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9854\n",
      "Epoch 00124: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0450 - acc: 0.9854 - val_loss: 0.3155 - val_acc: 0.9376\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9869\n",
      "Epoch 00125: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0399 - acc: 0.9868 - val_loss: 0.3017 - val_acc: 0.9413\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9849\n",
      "Epoch 00126: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0452 - acc: 0.9849 - val_loss: 0.2954 - val_acc: 0.9401\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9869\n",
      "Epoch 00127: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0388 - acc: 0.9868 - val_loss: 0.2999 - val_acc: 0.9399\n",
      "Epoch 128/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9848\n",
      "Epoch 00128: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0454 - acc: 0.9849 - val_loss: 0.2918 - val_acc: 0.9429\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9852\n",
      "Epoch 00129: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0443 - acc: 0.9853 - val_loss: 0.3164 - val_acc: 0.9350\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9875\n",
      "Epoch 00130: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0377 - acc: 0.9874 - val_loss: 0.2848 - val_acc: 0.9413\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9879\n",
      "Epoch 00131: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0365 - acc: 0.9878 - val_loss: 0.3176 - val_acc: 0.9380\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9860\n",
      "Epoch 00132: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0427 - acc: 0.9860 - val_loss: 0.3001 - val_acc: 0.9390\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9851\n",
      "Epoch 00133: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0429 - acc: 0.9851 - val_loss: 0.3000 - val_acc: 0.9418\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9866\n",
      "Epoch 00134: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0402 - acc: 0.9866 - val_loss: 0.2954 - val_acc: 0.9425\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9873\n",
      "Epoch 00135: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0379 - acc: 0.9872 - val_loss: 0.3253 - val_acc: 0.9373\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9845\n",
      "Epoch 00136: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0485 - acc: 0.9845 - val_loss: 0.3014 - val_acc: 0.9427\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9877\n",
      "Epoch 00137: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0373 - acc: 0.9877 - val_loss: 0.3229 - val_acc: 0.9369\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9877\n",
      "Epoch 00138: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0366 - acc: 0.9877 - val_loss: 0.3006 - val_acc: 0.9420\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9874\n",
      "Epoch 00139: val_loss did not improve from 0.23479\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0361 - acc: 0.9874 - val_loss: 0.3081 - val_acc: 0.9380\n",
      "\n",
      "1D_CNN_BN_DO_4_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 409us/sample - loss: 0.3009 - acc: 0.9169\n",
      "Loss: 0.30086840958114974 Accuracy: 0.91692626\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6689 - acc: 0.2896\n",
      "Epoch 00001: val_loss improved from inf to 4.54738, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/001-4.5474.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 2.6687 - acc: 0.2897 - val_loss: 4.5474 - val_acc: 0.1689\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5197 - acc: 0.5347\n",
      "Epoch 00002: val_loss improved from 4.54738 to 0.87122, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/002-0.8712.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 1.5191 - acc: 0.5349 - val_loss: 0.8712 - val_acc: 0.7321\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1081 - acc: 0.6605\n",
      "Epoch 00003: val_loss improved from 0.87122 to 0.66135, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/003-0.6613.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 1.1081 - acc: 0.6605 - val_loss: 0.6613 - val_acc: 0.7994\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8729 - acc: 0.7291\n",
      "Epoch 00004: val_loss improved from 0.66135 to 0.56655, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/004-0.5665.hdf5\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.8727 - acc: 0.7292 - val_loss: 0.5665 - val_acc: 0.8281\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6962 - acc: 0.7809\n",
      "Epoch 00005: val_loss improved from 0.56655 to 0.44050, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/005-0.4405.hdf5\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.6967 - acc: 0.7808 - val_loss: 0.4405 - val_acc: 0.8679\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5827 - acc: 0.8174\n",
      "Epoch 00006: val_loss improved from 0.44050 to 0.37113, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/006-0.3711.hdf5\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.5826 - acc: 0.8174 - val_loss: 0.3711 - val_acc: 0.8884\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4921 - acc: 0.8459\n",
      "Epoch 00007: val_loss improved from 0.37113 to 0.32192, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/007-0.3219.hdf5\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.4922 - acc: 0.8459 - val_loss: 0.3219 - val_acc: 0.9033\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4339 - acc: 0.8612\n",
      "Epoch 00008: val_loss improved from 0.32192 to 0.31392, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/008-0.3139.hdf5\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.4339 - acc: 0.8612 - val_loss: 0.3139 - val_acc: 0.9047\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3755 - acc: 0.8800\n",
      "Epoch 00009: val_loss improved from 0.31392 to 0.29273, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/009-0.2927.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.3757 - acc: 0.8800 - val_loss: 0.2927 - val_acc: 0.9126\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8888\n",
      "Epoch 00010: val_loss improved from 0.29273 to 0.27807, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/010-0.2781.hdf5\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.3473 - acc: 0.8888 - val_loss: 0.2781 - val_acc: 0.9140\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.8972\n",
      "Epoch 00011: val_loss improved from 0.27807 to 0.26564, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/011-0.2656.hdf5\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.3207 - acc: 0.8972 - val_loss: 0.2656 - val_acc: 0.9203\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9081\n",
      "Epoch 00012: val_loss improved from 0.26564 to 0.24642, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/012-0.2464.hdf5\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.2832 - acc: 0.9081 - val_loss: 0.2464 - val_acc: 0.9215\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9127\n",
      "Epoch 00013: val_loss improved from 0.24642 to 0.24199, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/013-0.2420.hdf5\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.2673 - acc: 0.9125 - val_loss: 0.2420 - val_acc: 0.9304\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9207\n",
      "Epoch 00014: val_loss improved from 0.24199 to 0.22314, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/014-0.2231.hdf5\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.2461 - acc: 0.9207 - val_loss: 0.2231 - val_acc: 0.9334\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9279\n",
      "Epoch 00015: val_loss did not improve from 0.22314\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.2249 - acc: 0.9278 - val_loss: 0.2415 - val_acc: 0.9280\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9332\n",
      "Epoch 00016: val_loss improved from 0.22314 to 0.22308, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/016-0.2231.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.2076 - acc: 0.9332 - val_loss: 0.2231 - val_acc: 0.9313\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9376\n",
      "Epoch 00017: val_loss did not improve from 0.22308\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.1896 - acc: 0.9375 - val_loss: 0.2330 - val_acc: 0.9297\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9401\n",
      "Epoch 00018: val_loss improved from 0.22308 to 0.21034, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/018-0.2103.hdf5\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.1820 - acc: 0.9401 - val_loss: 0.2103 - val_acc: 0.9362\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9437\n",
      "Epoch 00019: val_loss did not improve from 0.21034\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.1776 - acc: 0.9437 - val_loss: 0.2141 - val_acc: 0.9355\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9478\n",
      "Epoch 00020: val_loss did not improve from 0.21034\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.1617 - acc: 0.9478 - val_loss: 0.2336 - val_acc: 0.9304\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9502\n",
      "Epoch 00021: val_loss did not improve from 0.21034\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.1501 - acc: 0.9502 - val_loss: 0.2127 - val_acc: 0.9385\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9541\n",
      "Epoch 00022: val_loss improved from 0.21034 to 0.20492, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/022-0.2049.hdf5\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.1448 - acc: 0.9541 - val_loss: 0.2049 - val_acc: 0.9394\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9553\n",
      "Epoch 00023: val_loss did not improve from 0.20492\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.1403 - acc: 0.9553 - val_loss: 0.2201 - val_acc: 0.9380\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9588\n",
      "Epoch 00024: val_loss did not improve from 0.20492\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.1256 - acc: 0.9588 - val_loss: 0.2140 - val_acc: 0.9380\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9625\n",
      "Epoch 00025: val_loss did not improve from 0.20492\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.1188 - acc: 0.9624 - val_loss: 0.2226 - val_acc: 0.9348\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9578\n",
      "Epoch 00026: val_loss did not improve from 0.20492\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.1277 - acc: 0.9578 - val_loss: 0.2235 - val_acc: 0.9383\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9642\n",
      "Epoch 00027: val_loss did not improve from 0.20492\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.1107 - acc: 0.9641 - val_loss: 0.2349 - val_acc: 0.9357\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9652\n",
      "Epoch 00028: val_loss did not improve from 0.20492\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.1075 - acc: 0.9652 - val_loss: 0.2064 - val_acc: 0.9406\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9669\n",
      "Epoch 00029: val_loss improved from 0.20492 to 0.20428, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/029-0.2043.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.1040 - acc: 0.9669 - val_loss: 0.2043 - val_acc: 0.9448\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9704\n",
      "Epoch 00030: val_loss did not improve from 0.20428\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0920 - acc: 0.9704 - val_loss: 0.2434 - val_acc: 0.9301\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9724\n",
      "Epoch 00031: val_loss did not improve from 0.20428\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0885 - acc: 0.9724 - val_loss: 0.2297 - val_acc: 0.9373\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9718\n",
      "Epoch 00032: val_loss did not improve from 0.20428\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0857 - acc: 0.9718 - val_loss: 0.2470 - val_acc: 0.9378\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9728\n",
      "Epoch 00033: val_loss improved from 0.20428 to 0.20277, saving model to model/checkpoint/1D_CNN_BN_DO_5_only_conv_checkpoint/033-0.2028.hdf5\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.0840 - acc: 0.9727 - val_loss: 0.2028 - val_acc: 0.9415\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9744\n",
      "Epoch 00034: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.0784 - acc: 0.9744 - val_loss: 0.2383 - val_acc: 0.9336\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9704\n",
      "Epoch 00035: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.0905 - acc: 0.9705 - val_loss: 0.2108 - val_acc: 0.9415\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9766\n",
      "Epoch 00036: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0713 - acc: 0.9764 - val_loss: 0.2278 - val_acc: 0.9394\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9766\n",
      "Epoch 00037: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0720 - acc: 0.9766 - val_loss: 0.2234 - val_acc: 0.9432\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9778\n",
      "Epoch 00038: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 0.0708 - acc: 0.9778 - val_loss: 0.2189 - val_acc: 0.9418\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9774\n",
      "Epoch 00039: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0703 - acc: 0.9774 - val_loss: 0.2180 - val_acc: 0.9436\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9777\n",
      "Epoch 00040: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0671 - acc: 0.9777 - val_loss: 0.2255 - val_acc: 0.9404\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9829\n",
      "Epoch 00041: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0554 - acc: 0.9829 - val_loss: 0.2298 - val_acc: 0.9425\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9823\n",
      "Epoch 00042: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0561 - acc: 0.9823 - val_loss: 0.2382 - val_acc: 0.9404\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9818\n",
      "Epoch 00043: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0572 - acc: 0.9819 - val_loss: 0.2307 - val_acc: 0.9427\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9816\n",
      "Epoch 00044: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.0580 - acc: 0.9815 - val_loss: 0.2486 - val_acc: 0.9373\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9815\n",
      "Epoch 00045: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0585 - acc: 0.9816 - val_loss: 0.2300 - val_acc: 0.9427\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9831\n",
      "Epoch 00046: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0521 - acc: 0.9831 - val_loss: 0.2773 - val_acc: 0.9252\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9783\n",
      "Epoch 00047: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0672 - acc: 0.9783 - val_loss: 0.2229 - val_acc: 0.9406\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9847\n",
      "Epoch 00048: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0496 - acc: 0.9847 - val_loss: 0.2579 - val_acc: 0.9387\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9847\n",
      "Epoch 00049: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.0474 - acc: 0.9848 - val_loss: 0.2241 - val_acc: 0.9457\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9870\n",
      "Epoch 00050: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0428 - acc: 0.9868 - val_loss: 0.2734 - val_acc: 0.9380\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9815\n",
      "Epoch 00051: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.0574 - acc: 0.9815 - val_loss: 0.2333 - val_acc: 0.9427\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9854\n",
      "Epoch 00052: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0460 - acc: 0.9854 - val_loss: 0.2408 - val_acc: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9883\n",
      "Epoch 00053: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0374 - acc: 0.9883 - val_loss: 0.2467 - val_acc: 0.9425\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9858\n",
      "Epoch 00054: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0459 - acc: 0.9859 - val_loss: 0.2572 - val_acc: 0.9383\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9862\n",
      "Epoch 00055: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0426 - acc: 0.9862 - val_loss: 0.2321 - val_acc: 0.9434\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9843\n",
      "Epoch 00056: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0493 - acc: 0.9842 - val_loss: 0.2516 - val_acc: 0.9429\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9854\n",
      "Epoch 00057: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0453 - acc: 0.9854 - val_loss: 0.2404 - val_acc: 0.9397\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9842\n",
      "Epoch 00058: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0502 - acc: 0.9842 - val_loss: 0.2621 - val_acc: 0.9439\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9896\n",
      "Epoch 00059: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0346 - acc: 0.9896 - val_loss: 0.2310 - val_acc: 0.9471\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9888\n",
      "Epoch 00060: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0359 - acc: 0.9888 - val_loss: 0.2910 - val_acc: 0.9373\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9887\n",
      "Epoch 00061: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0382 - acc: 0.9887 - val_loss: 0.2627 - val_acc: 0.9408\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9881\n",
      "Epoch 00062: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0387 - acc: 0.9881 - val_loss: 0.2286 - val_acc: 0.9453\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9875\n",
      "Epoch 00063: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0402 - acc: 0.9874 - val_loss: 0.2566 - val_acc: 0.9411\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9861\n",
      "Epoch 00064: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0435 - acc: 0.9861 - val_loss: 0.2380 - val_acc: 0.9425\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9840\n",
      "Epoch 00065: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0494 - acc: 0.9840 - val_loss: 0.2256 - val_acc: 0.9469\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9901\n",
      "Epoch 00066: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0327 - acc: 0.9901 - val_loss: 0.2346 - val_acc: 0.9469\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9902\n",
      "Epoch 00067: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0325 - acc: 0.9901 - val_loss: 0.2715 - val_acc: 0.9408\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9896\n",
      "Epoch 00068: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0332 - acc: 0.9896 - val_loss: 0.2484 - val_acc: 0.9450\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9917\n",
      "Epoch 00069: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0288 - acc: 0.9917 - val_loss: 0.2515 - val_acc: 0.9443\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9900\n",
      "Epoch 00070: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0319 - acc: 0.9899 - val_loss: 0.2534 - val_acc: 0.9457\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9846\n",
      "Epoch 00071: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.0524 - acc: 0.9845 - val_loss: 0.2705 - val_acc: 0.9380\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9896\n",
      "Epoch 00072: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0339 - acc: 0.9896 - val_loss: 0.2337 - val_acc: 0.9457\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9919\n",
      "Epoch 00073: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0276 - acc: 0.9919 - val_loss: 0.2503 - val_acc: 0.9422\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9902\n",
      "Epoch 00074: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0308 - acc: 0.9902 - val_loss: 0.2572 - val_acc: 0.9453\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9899\n",
      "Epoch 00075: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.0332 - acc: 0.9899 - val_loss: 0.2508 - val_acc: 0.9462\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9907\n",
      "Epoch 00076: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0294 - acc: 0.9907 - val_loss: 0.2466 - val_acc: 0.9469\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9903\n",
      "Epoch 00077: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0291 - acc: 0.9903 - val_loss: 0.3320 - val_acc: 0.9283\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9884\n",
      "Epoch 00078: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.0375 - acc: 0.9883 - val_loss: 0.2568 - val_acc: 0.9446\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9898\n",
      "Epoch 00079: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0336 - acc: 0.9898 - val_loss: 0.2667 - val_acc: 0.9418\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9896\n",
      "Epoch 00080: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.0336 - acc: 0.9896 - val_loss: 0.2441 - val_acc: 0.9474\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9923\n",
      "Epoch 00081: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.0252 - acc: 0.9923 - val_loss: 0.2646 - val_acc: 0.9434\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9873\n",
      "Epoch 00082: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0413 - acc: 0.9873 - val_loss: 0.2454 - val_acc: 0.9460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9925\n",
      "Epoch 00083: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0238 - acc: 0.9924 - val_loss: 0.2669 - val_acc: 0.9455\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9920\n",
      "Epoch 00084: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 0.0262 - acc: 0.9920 - val_loss: 0.2570 - val_acc: 0.9450\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9865\n",
      "Epoch 00085: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0412 - acc: 0.9865 - val_loss: 0.2604 - val_acc: 0.9450\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9926\n",
      "Epoch 00086: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0232 - acc: 0.9926 - val_loss: 0.2573 - val_acc: 0.9467\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9929\n",
      "Epoch 00087: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0237 - acc: 0.9929 - val_loss: 0.2641 - val_acc: 0.9460\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9929\n",
      "Epoch 00088: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 0.0242 - acc: 0.9927 - val_loss: 0.2460 - val_acc: 0.9439\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9916\n",
      "Epoch 00089: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0260 - acc: 0.9916 - val_loss: 0.2612 - val_acc: 0.9464\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9909\n",
      "Epoch 00090: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0282 - acc: 0.9909 - val_loss: 0.2817 - val_acc: 0.9413\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9904\n",
      "Epoch 00091: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0311 - acc: 0.9904 - val_loss: 0.2706 - val_acc: 0.9443\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9918\n",
      "Epoch 00092: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0271 - acc: 0.9918 - val_loss: 0.2554 - val_acc: 0.9455\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9920\n",
      "Epoch 00093: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.0267 - acc: 0.9920 - val_loss: 0.2923 - val_acc: 0.9415\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9929\n",
      "Epoch 00094: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0220 - acc: 0.9929 - val_loss: 0.2633 - val_acc: 0.9460\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9908\n",
      "Epoch 00095: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0306 - acc: 0.9907 - val_loss: 0.2553 - val_acc: 0.9448\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9916\n",
      "Epoch 00096: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0272 - acc: 0.9916 - val_loss: 0.2517 - val_acc: 0.9460\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9938\n",
      "Epoch 00097: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0200 - acc: 0.9938 - val_loss: 0.2496 - val_acc: 0.9462\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9926\n",
      "Epoch 00098: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0236 - acc: 0.9926 - val_loss: 0.2951 - val_acc: 0.9406\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9900\n",
      "Epoch 00099: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0300 - acc: 0.9900 - val_loss: 0.2321 - val_acc: 0.9497\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9925\n",
      "Epoch 00100: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0233 - acc: 0.9925 - val_loss: 0.2675 - val_acc: 0.9469\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9927\n",
      "Epoch 00101: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0219 - acc: 0.9927 - val_loss: 0.2679 - val_acc: 0.9415\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9930\n",
      "Epoch 00102: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0234 - acc: 0.9930 - val_loss: 0.2923 - val_acc: 0.9394\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9904\n",
      "Epoch 00103: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.0304 - acc: 0.9904 - val_loss: 0.2843 - val_acc: 0.9436\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9927\n",
      "Epoch 00104: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0240 - acc: 0.9927 - val_loss: 0.2733 - val_acc: 0.9436\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9917\n",
      "Epoch 00105: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0265 - acc: 0.9917 - val_loss: 0.2695 - val_acc: 0.9453\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9934\n",
      "Epoch 00106: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0197 - acc: 0.9934 - val_loss: 0.2813 - val_acc: 0.9455\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9943\n",
      "Epoch 00107: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0176 - acc: 0.9943 - val_loss: 0.2766 - val_acc: 0.9455\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9936\n",
      "Epoch 00108: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0209 - acc: 0.9936 - val_loss: 0.2748 - val_acc: 0.9432\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9910\n",
      "Epoch 00109: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0291 - acc: 0.9910 - val_loss: 0.2898 - val_acc: 0.9406\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9935\n",
      "Epoch 00110: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0209 - acc: 0.9935 - val_loss: 0.2521 - val_acc: 0.9471\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9937\n",
      "Epoch 00111: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0216 - acc: 0.9937 - val_loss: 0.2841 - val_acc: 0.9413\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9941\n",
      "Epoch 00112: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0195 - acc: 0.9940 - val_loss: 0.2979 - val_acc: 0.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9887\n",
      "Epoch 00113: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0347 - acc: 0.9887 - val_loss: 0.2591 - val_acc: 0.9448\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9929\n",
      "Epoch 00114: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0232 - acc: 0.9929 - val_loss: 0.2637 - val_acc: 0.9448\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9942\n",
      "Epoch 00115: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0183 - acc: 0.9941 - val_loss: 0.2623 - val_acc: 0.9485\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9920\n",
      "Epoch 00116: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0252 - acc: 0.9920 - val_loss: 0.2703 - val_acc: 0.9453\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9935\n",
      "Epoch 00117: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.0195 - acc: 0.9935 - val_loss: 0.2720 - val_acc: 0.9434\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9951\n",
      "Epoch 00118: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0160 - acc: 0.9951 - val_loss: 0.2942 - val_acc: 0.9434\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9912\n",
      "Epoch 00119: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0268 - acc: 0.9912 - val_loss: 0.2586 - val_acc: 0.9471\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9928\n",
      "Epoch 00120: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0232 - acc: 0.9928 - val_loss: 0.2864 - val_acc: 0.9446\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9943\n",
      "Epoch 00121: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.2676 - val_acc: 0.9478\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9947\n",
      "Epoch 00122: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0186 - acc: 0.9946 - val_loss: 0.2721 - val_acc: 0.9457\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9898\n",
      "Epoch 00123: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0348 - acc: 0.9897 - val_loss: 0.2511 - val_acc: 0.9504\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9919\n",
      "Epoch 00124: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.0260 - acc: 0.9919 - val_loss: 0.2867 - val_acc: 0.9432\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9947\n",
      "Epoch 00125: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.0168 - acc: 0.9947 - val_loss: 0.2571 - val_acc: 0.9483\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9951\n",
      "Epoch 00126: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0169 - acc: 0.9951 - val_loss: 0.2855 - val_acc: 0.9457\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9958\n",
      "Epoch 00127: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0151 - acc: 0.9958 - val_loss: 0.2733 - val_acc: 0.9485\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9952\n",
      "Epoch 00128: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0154 - acc: 0.9952 - val_loss: 0.3150 - val_acc: 0.9397\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9934\n",
      "Epoch 00129: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 0.0204 - acc: 0.9934 - val_loss: 0.3129 - val_acc: 0.9436\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9938\n",
      "Epoch 00130: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 0.0199 - acc: 0.9938 - val_loss: 0.2922 - val_acc: 0.9455\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9937\n",
      "Epoch 00131: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.0196 - acc: 0.9937 - val_loss: 0.2982 - val_acc: 0.9418\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9953\n",
      "Epoch 00132: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 0.0164 - acc: 0.9953 - val_loss: 0.2926 - val_acc: 0.9429\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9944\n",
      "Epoch 00133: val_loss did not improve from 0.20277\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.0184 - acc: 0.9944 - val_loss: 0.2883 - val_acc: 0.9467\n",
      "\n",
      "1D_CNN_BN_DO_5_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 469us/sample - loss: 0.2710 - acc: 0.9221\n",
      "Loss: 0.2710222844517986 Accuracy: 0.92211837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_BN_DO_{}_only_conv'.format(i)\n",
    "    model = build_1d_cnn_BN_DO_only_conv(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_BN_DO_1_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 416us/sample - loss: 1.7871 - acc: 0.4559\n",
      "Loss: 1.7870784911534991 Accuracy: 0.45586708\n",
      "\n",
      "1D_CNN_BN_DO_2_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_61 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 474us/sample - loss: 1.2079 - acc: 0.6586\n",
      "Loss: 1.2078924180315043 Accuracy: 0.65856695\n",
      "\n",
      "1D_CNN_BN_DO_3_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_63 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 479us/sample - loss: 0.5594 - acc: 0.8538\n",
      "Loss: 0.5594111439099811 Accuracy: 0.8537902\n",
      "\n",
      "1D_CNN_BN_DO_4_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 519us/sample - loss: 0.3009 - acc: 0.9169\n",
      "Loss: 0.30086840958114974 Accuracy: 0.91692626\n",
      "\n",
      "1D_CNN_BN_DO_5_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_70 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_72 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 550us/sample - loss: 0.2710 - acc: 0.9221\n",
      "Loss: 0.2710222844517986 Accuracy: 0.92211837\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    model_name = '1D_CNN_BN_DO_{}_only_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
