{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 56864)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 18944)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 75808)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 75808)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1212944     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,223,440\n",
      "Trainable params: 1,223,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 18944)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 6304)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 25248)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 25248)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           403984      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 419,632\n",
      "Trainable params: 419,632\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 6304)         0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4160)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10464)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10464)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           167440      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 193,392\n",
      "Trainable params: 193,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4160)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 1344)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 5504)         0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 5504)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           88080       dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 134,576\n",
      "Trainable params: 134,576\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 1344)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 448)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1792)         0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1792)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           28688       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 95,728\n",
      "Trainable params: 95,728\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 448)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 576)          0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 576)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           9232        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 96,816\n",
      "Trainable params: 96,816\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2375 - acc: 0.2989\n",
      "Epoch 00001: val_loss improved from inf to 1.81266, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv_checkpoint/001-1.8127.hdf5\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 2.2375 - acc: 0.2989 - val_loss: 1.8127 - val_acc: 0.4435\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6690 - acc: 0.4851\n",
      "Epoch 00002: val_loss improved from 1.81266 to 1.58161, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv_checkpoint/002-1.5816.hdf5\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 1.6691 - acc: 0.4851 - val_loss: 1.5816 - val_acc: 0.5167\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4584 - acc: 0.5523\n",
      "Epoch 00003: val_loss improved from 1.58161 to 1.49693, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv_checkpoint/003-1.4969.hdf5\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 1.4584 - acc: 0.5523 - val_loss: 1.4969 - val_acc: 0.5295\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3289 - acc: 0.5894\n",
      "Epoch 00004: val_loss improved from 1.49693 to 1.46596, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv_checkpoint/004-1.4660.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 1.3289 - acc: 0.5894 - val_loss: 1.4660 - val_acc: 0.5486\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2312 - acc: 0.6201\n",
      "Epoch 00005: val_loss improved from 1.46596 to 1.41227, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv_checkpoint/005-1.4123.hdf5\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 1.2313 - acc: 0.6200 - val_loss: 1.4123 - val_acc: 0.5507\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1556 - acc: 0.6438\n",
      "Epoch 00006: val_loss improved from 1.41227 to 1.40293, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv_checkpoint/006-1.4029.hdf5\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 1.1556 - acc: 0.6438 - val_loss: 1.4029 - val_acc: 0.5567\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0878 - acc: 0.6677\n",
      "Epoch 00007: val_loss improved from 1.40293 to 1.39946, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv_checkpoint/007-1.3995.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 1.0878 - acc: 0.6677 - val_loss: 1.3995 - val_acc: 0.5586\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0303 - acc: 0.6813\n",
      "Epoch 00008: val_loss improved from 1.39946 to 1.37989, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv_checkpoint/008-1.3799.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 1.0302 - acc: 0.6813 - val_loss: 1.3799 - val_acc: 0.5639\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9757 - acc: 0.7009\n",
      "Epoch 00009: val_loss did not improve from 1.37989\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.9756 - acc: 0.7009 - val_loss: 1.3957 - val_acc: 0.5607\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9246 - acc: 0.7121\n",
      "Epoch 00010: val_loss improved from 1.37989 to 1.35146, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv_checkpoint/010-1.3515.hdf5\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.9246 - acc: 0.7121 - val_loss: 1.3515 - val_acc: 0.5777\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8791 - acc: 0.7236\n",
      "Epoch 00011: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.8791 - acc: 0.7236 - val_loss: 1.4028 - val_acc: 0.5602\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8422 - acc: 0.7388\n",
      "Epoch 00012: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.8421 - acc: 0.7388 - val_loss: 1.3824 - val_acc: 0.5751\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7989 - acc: 0.7510\n",
      "Epoch 00013: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.7988 - acc: 0.7510 - val_loss: 1.3746 - val_acc: 0.5840\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7655 - acc: 0.7614\n",
      "Epoch 00014: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.7655 - acc: 0.7614 - val_loss: 1.3860 - val_acc: 0.5814\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7282 - acc: 0.7739\n",
      "Epoch 00015: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.7281 - acc: 0.7739 - val_loss: 1.3944 - val_acc: 0.5856\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6953 - acc: 0.7836\n",
      "Epoch 00016: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.6953 - acc: 0.7836 - val_loss: 1.3850 - val_acc: 0.5814\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6609 - acc: 0.7943\n",
      "Epoch 00017: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.6610 - acc: 0.7942 - val_loss: 1.3726 - val_acc: 0.5968\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6301 - acc: 0.8046\n",
      "Epoch 00018: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.6301 - acc: 0.8046 - val_loss: 1.3868 - val_acc: 0.5886\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.8114\n",
      "Epoch 00019: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.6017 - acc: 0.8114 - val_loss: 1.4039 - val_acc: 0.5900\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5824 - acc: 0.8139\n",
      "Epoch 00020: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.5823 - acc: 0.8139 - val_loss: 1.4292 - val_acc: 0.5919\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.8268\n",
      "Epoch 00021: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.5531 - acc: 0.8268 - val_loss: 1.4165 - val_acc: 0.6026\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.8317\n",
      "Epoch 00022: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.5368 - acc: 0.8317 - val_loss: 1.4039 - val_acc: 0.6033\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.8402\n",
      "Epoch 00023: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5097 - acc: 0.8402 - val_loss: 1.4051 - val_acc: 0.5970\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4890 - acc: 0.8461\n",
      "Epoch 00024: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.4890 - acc: 0.8461 - val_loss: 1.4072 - val_acc: 0.6084\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4690 - acc: 0.8535\n",
      "Epoch 00025: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.4690 - acc: 0.8535 - val_loss: 1.4611 - val_acc: 0.5973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.8579\n",
      "Epoch 00026: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.4532 - acc: 0.8579 - val_loss: 1.4553 - val_acc: 0.6096\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.8660\n",
      "Epoch 00027: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4323 - acc: 0.8660 - val_loss: 1.4397 - val_acc: 0.6131\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8681\n",
      "Epoch 00028: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4183 - acc: 0.8681 - val_loss: 1.4285 - val_acc: 0.6147\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.8715\n",
      "Epoch 00029: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.4059 - acc: 0.8715 - val_loss: 1.4541 - val_acc: 0.6061\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8778\n",
      "Epoch 00030: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3864 - acc: 0.8777 - val_loss: 1.4565 - val_acc: 0.6175\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8843\n",
      "Epoch 00031: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3702 - acc: 0.8844 - val_loss: 1.4753 - val_acc: 0.6098\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8878\n",
      "Epoch 00032: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.3582 - acc: 0.8878 - val_loss: 1.4728 - val_acc: 0.6138\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8862\n",
      "Epoch 00033: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.3533 - acc: 0.8862 - val_loss: 1.4890 - val_acc: 0.6224\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8929\n",
      "Epoch 00034: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.3390 - acc: 0.8929 - val_loss: 1.4861 - val_acc: 0.6236\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.8995\n",
      "Epoch 00035: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.3249 - acc: 0.8996 - val_loss: 1.4878 - val_acc: 0.6343\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8972\n",
      "Epoch 00036: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.3211 - acc: 0.8972 - val_loss: 1.5231 - val_acc: 0.6210\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9040\n",
      "Epoch 00037: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.3045 - acc: 0.9040 - val_loss: 1.5089 - val_acc: 0.6313\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9070\n",
      "Epoch 00038: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 469us/sample - loss: 0.2940 - acc: 0.9071 - val_loss: 1.5206 - val_acc: 0.6387\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.9089\n",
      "Epoch 00039: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2873 - acc: 0.9089 - val_loss: 1.5131 - val_acc: 0.6357\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9143\n",
      "Epoch 00040: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.2737 - acc: 0.9143 - val_loss: 1.5473 - val_acc: 0.6261\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9126\n",
      "Epoch 00041: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.2772 - acc: 0.9126 - val_loss: 1.5384 - val_acc: 0.6278\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9194\n",
      "Epoch 00042: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.2581 - acc: 0.9194 - val_loss: 1.5345 - val_acc: 0.6373\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9208\n",
      "Epoch 00043: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.2517 - acc: 0.9208 - val_loss: 1.5809 - val_acc: 0.6324\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9222\n",
      "Epoch 00044: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.2487 - acc: 0.9222 - val_loss: 1.5542 - val_acc: 0.6401\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9241\n",
      "Epoch 00045: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2385 - acc: 0.9241 - val_loss: 1.5603 - val_acc: 0.6373\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9267\n",
      "Epoch 00046: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.2321 - acc: 0.9267 - val_loss: 1.6005 - val_acc: 0.6299\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9286\n",
      "Epoch 00047: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2267 - acc: 0.9286 - val_loss: 1.5816 - val_acc: 0.6452\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9316\n",
      "Epoch 00048: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.2201 - acc: 0.9316 - val_loss: 1.5938 - val_acc: 0.6429\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9315\n",
      "Epoch 00049: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2156 - acc: 0.9315 - val_loss: 1.6105 - val_acc: 0.6429\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9341\n",
      "Epoch 00050: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.2101 - acc: 0.9341 - val_loss: 1.6404 - val_acc: 0.6455\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9361\n",
      "Epoch 00051: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2026 - acc: 0.9361 - val_loss: 1.6048 - val_acc: 0.6560\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9361\n",
      "Epoch 00052: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.2013 - acc: 0.9361 - val_loss: 1.6193 - val_acc: 0.6508\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9407\n",
      "Epoch 00053: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.1936 - acc: 0.9407 - val_loss: 1.6315 - val_acc: 0.6448\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9410\n",
      "Epoch 00054: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.1894 - acc: 0.9410 - val_loss: 1.6640 - val_acc: 0.6483\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9433\n",
      "Epoch 00055: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.1856 - acc: 0.9433 - val_loss: 1.6329 - val_acc: 0.6557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9430\n",
      "Epoch 00056: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.1854 - acc: 0.9429 - val_loss: 1.6294 - val_acc: 0.6567\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9442\n",
      "Epoch 00057: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.1784 - acc: 0.9442 - val_loss: 1.6371 - val_acc: 0.6557\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9477\n",
      "Epoch 00058: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.1719 - acc: 0.9478 - val_loss: 1.6221 - val_acc: 0.6578\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9468\n",
      "Epoch 00059: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.1711 - acc: 0.9467 - val_loss: 1.6503 - val_acc: 0.6534\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9473\n",
      "Epoch 00060: val_loss did not improve from 1.35146\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.1680 - acc: 0.9473 - val_loss: 1.6663 - val_acc: 0.6536\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 339us/sample - loss: 1.4481 - acc: 0.5468\n",
      "Loss: 1.4480572943870647 Accuracy: 0.5468328\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2460 - acc: 0.2726\n",
      "Epoch 00001: val_loss improved from inf to 1.79598, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/001-1.7960.hdf5\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 2.2459 - acc: 0.2726 - val_loss: 1.7960 - val_acc: 0.4230\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6802 - acc: 0.4685\n",
      "Epoch 00002: val_loss improved from 1.79598 to 1.50594, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/002-1.5059.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 1.6801 - acc: 0.4685 - val_loss: 1.5059 - val_acc: 0.5323\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4852 - acc: 0.5361\n",
      "Epoch 00003: val_loss improved from 1.50594 to 1.40094, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/003-1.4009.hdf5\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 1.4853 - acc: 0.5360 - val_loss: 1.4009 - val_acc: 0.5670\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3697 - acc: 0.5768\n",
      "Epoch 00004: val_loss improved from 1.40094 to 1.32774, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/004-1.3277.hdf5\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 1.3696 - acc: 0.5768 - val_loss: 1.3277 - val_acc: 0.5949\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2799 - acc: 0.6075\n",
      "Epoch 00005: val_loss improved from 1.32774 to 1.24687, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/005-1.2469.hdf5\n",
      "36805/36805 [==============================] - 17s 469us/sample - loss: 1.2799 - acc: 0.6074 - val_loss: 1.2469 - val_acc: 0.6271\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2072 - acc: 0.6316\n",
      "Epoch 00006: val_loss improved from 1.24687 to 1.20102, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/006-1.2010.hdf5\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 1.2071 - acc: 0.6316 - val_loss: 1.2010 - val_acc: 0.6382\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1418 - acc: 0.6526\n",
      "Epoch 00007: val_loss improved from 1.20102 to 1.15749, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/007-1.1575.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 1.1418 - acc: 0.6526 - val_loss: 1.1575 - val_acc: 0.6583\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0848 - acc: 0.6700\n",
      "Epoch 00008: val_loss improved from 1.15749 to 1.12178, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/008-1.1218.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 1.0848 - acc: 0.6700 - val_loss: 1.1218 - val_acc: 0.6625\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0400 - acc: 0.6858\n",
      "Epoch 00009: val_loss improved from 1.12178 to 1.08521, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/009-1.0852.hdf5\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 1.0399 - acc: 0.6859 - val_loss: 1.0852 - val_acc: 0.6704\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9897 - acc: 0.7001\n",
      "Epoch 00010: val_loss improved from 1.08521 to 1.08057, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/010-1.0806.hdf5\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.9898 - acc: 0.7000 - val_loss: 1.0806 - val_acc: 0.6718\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9494 - acc: 0.7134\n",
      "Epoch 00011: val_loss improved from 1.08057 to 1.03752, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/011-1.0375.hdf5\n",
      "36805/36805 [==============================] - 17s 469us/sample - loss: 0.9493 - acc: 0.7134 - val_loss: 1.0375 - val_acc: 0.6839\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9081 - acc: 0.7260\n",
      "Epoch 00012: val_loss did not improve from 1.03752\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.9080 - acc: 0.7260 - val_loss: 1.0727 - val_acc: 0.6751\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8747 - acc: 0.7361\n",
      "Epoch 00013: val_loss improved from 1.03752 to 1.01084, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/013-1.0108.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.8746 - acc: 0.7361 - val_loss: 1.0108 - val_acc: 0.6881\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8379 - acc: 0.7448\n",
      "Epoch 00014: val_loss did not improve from 1.01084\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.8380 - acc: 0.7448 - val_loss: 1.0159 - val_acc: 0.6904\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8097 - acc: 0.7555\n",
      "Epoch 00015: val_loss improved from 1.01084 to 0.99176, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/015-0.9918.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.8097 - acc: 0.7554 - val_loss: 0.9918 - val_acc: 0.7056\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7723 - acc: 0.7667\n",
      "Epoch 00016: val_loss did not improve from 0.99176\n",
      "36805/36805 [==============================] - 17s 469us/sample - loss: 0.7726 - acc: 0.7667 - val_loss: 0.9955 - val_acc: 0.6988\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7460 - acc: 0.7715\n",
      "Epoch 00017: val_loss improved from 0.99176 to 0.95167, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/017-0.9517.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.7461 - acc: 0.7715 - val_loss: 0.9517 - val_acc: 0.7163\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7166 - acc: 0.7817\n",
      "Epoch 00018: val_loss improved from 0.95167 to 0.94281, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/018-0.9428.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.7166 - acc: 0.7817 - val_loss: 0.9428 - val_acc: 0.7151\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6988 - acc: 0.7882\n",
      "Epoch 00019: val_loss improved from 0.94281 to 0.94196, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/019-0.9420.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.6988 - acc: 0.7882 - val_loss: 0.9420 - val_acc: 0.7165\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6689 - acc: 0.7970\n",
      "Epoch 00020: val_loss improved from 0.94196 to 0.93707, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/020-0.9371.hdf5\n",
      "36805/36805 [==============================] - 17s 469us/sample - loss: 0.6689 - acc: 0.7969 - val_loss: 0.9371 - val_acc: 0.7177\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6437 - acc: 0.8013\n",
      "Epoch 00021: val_loss improved from 0.93707 to 0.91266, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/021-0.9127.hdf5\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.6436 - acc: 0.8014 - val_loss: 0.9127 - val_acc: 0.7289\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6246 - acc: 0.8073\n",
      "Epoch 00022: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.6245 - acc: 0.8073 - val_loss: 0.9302 - val_acc: 0.7133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.8133\n",
      "Epoch 00023: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.6029 - acc: 0.8133 - val_loss: 0.9455 - val_acc: 0.7186\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5837 - acc: 0.8208\n",
      "Epoch 00024: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.5837 - acc: 0.8209 - val_loss: 0.9318 - val_acc: 0.7305\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.8229\n",
      "Epoch 00025: val_loss improved from 0.91266 to 0.90152, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/025-0.9015.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.5635 - acc: 0.8229 - val_loss: 0.9015 - val_acc: 0.7338\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5488 - acc: 0.8285\n",
      "Epoch 00026: val_loss did not improve from 0.90152\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5488 - acc: 0.8285 - val_loss: 0.9206 - val_acc: 0.7270\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.8366\n",
      "Epoch 00027: val_loss did not improve from 0.90152\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.5271 - acc: 0.8366 - val_loss: 0.9125 - val_acc: 0.7326\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.8374\n",
      "Epoch 00028: val_loss did not improve from 0.90152\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.5176 - acc: 0.8374 - val_loss: 0.9069 - val_acc: 0.7303\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4973 - acc: 0.8437\n",
      "Epoch 00029: val_loss did not improve from 0.90152\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4973 - acc: 0.8437 - val_loss: 0.9127 - val_acc: 0.7321\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4846 - acc: 0.8498\n",
      "Epoch 00030: val_loss improved from 0.90152 to 0.90087, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/030-0.9009.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4845 - acc: 0.8499 - val_loss: 0.9009 - val_acc: 0.7365\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4654 - acc: 0.8525\n",
      "Epoch 00031: val_loss improved from 0.90087 to 0.89921, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/031-0.8992.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4654 - acc: 0.8525 - val_loss: 0.8992 - val_acc: 0.7379\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8548\n",
      "Epoch 00032: val_loss did not improve from 0.89921\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.4605 - acc: 0.8547 - val_loss: 0.9473 - val_acc: 0.7270\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4515 - acc: 0.8573\n",
      "Epoch 00033: val_loss did not improve from 0.89921\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4516 - acc: 0.8572 - val_loss: 0.9078 - val_acc: 0.7396\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4289 - acc: 0.8652\n",
      "Epoch 00034: val_loss did not improve from 0.89921\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4289 - acc: 0.8652 - val_loss: 0.9074 - val_acc: 0.7466\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.8664\n",
      "Epoch 00035: val_loss improved from 0.89921 to 0.89210, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv_checkpoint/035-0.8921.hdf5\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.4200 - acc: 0.8664 - val_loss: 0.8921 - val_acc: 0.7461\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8706\n",
      "Epoch 00036: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.4092 - acc: 0.8706 - val_loss: 0.8982 - val_acc: 0.7435\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8722\n",
      "Epoch 00037: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4003 - acc: 0.8722 - val_loss: 0.9062 - val_acc: 0.7433\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8745\n",
      "Epoch 00038: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.3904 - acc: 0.8744 - val_loss: 0.9167 - val_acc: 0.7456\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8795\n",
      "Epoch 00039: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3786 - acc: 0.8795 - val_loss: 0.9020 - val_acc: 0.7510\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8810\n",
      "Epoch 00040: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3711 - acc: 0.8810 - val_loss: 0.9191 - val_acc: 0.7431\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8820\n",
      "Epoch 00041: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3624 - acc: 0.8820 - val_loss: 0.9409 - val_acc: 0.7424\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8878\n",
      "Epoch 00042: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.3522 - acc: 0.8878 - val_loss: 0.9254 - val_acc: 0.7491\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3430 - acc: 0.8896\n",
      "Epoch 00043: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.3431 - acc: 0.8896 - val_loss: 0.9558 - val_acc: 0.7396\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.8886\n",
      "Epoch 00044: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3406 - acc: 0.8887 - val_loss: 0.9369 - val_acc: 0.7410\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8940\n",
      "Epoch 00045: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3266 - acc: 0.8941 - val_loss: 0.9223 - val_acc: 0.7554\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.8951\n",
      "Epoch 00046: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3162 - acc: 0.8950 - val_loss: 0.9353 - val_acc: 0.7447\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3194 - acc: 0.8964\n",
      "Epoch 00047: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.3194 - acc: 0.8965 - val_loss: 0.9251 - val_acc: 0.7533\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9001\n",
      "Epoch 00048: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.3086 - acc: 0.9001 - val_loss: 0.9344 - val_acc: 0.7580\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.8997\n",
      "Epoch 00049: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3046 - acc: 0.8997 - val_loss: 0.9319 - val_acc: 0.7563\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9026\n",
      "Epoch 00050: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.2957 - acc: 0.9025 - val_loss: 0.9222 - val_acc: 0.7675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9048\n",
      "Epoch 00051: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.2872 - acc: 0.9048 - val_loss: 0.9459 - val_acc: 0.7559\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.9052\n",
      "Epoch 00052: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.2882 - acc: 0.9052 - val_loss: 0.9400 - val_acc: 0.7591\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9086\n",
      "Epoch 00053: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2788 - acc: 0.9086 - val_loss: 0.9395 - val_acc: 0.7638\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9115\n",
      "Epoch 00054: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.2712 - acc: 0.9115 - val_loss: 0.9557 - val_acc: 0.7570\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9126\n",
      "Epoch 00055: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 0.2669 - acc: 0.9126 - val_loss: 0.9343 - val_acc: 0.7673\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2665 - acc: 0.9117\n",
      "Epoch 00056: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.2666 - acc: 0.9115 - val_loss: 0.9810 - val_acc: 0.7561\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9133\n",
      "Epoch 00057: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.2654 - acc: 0.9132 - val_loss: 0.9630 - val_acc: 0.7629\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9152\n",
      "Epoch 00058: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2558 - acc: 0.9152 - val_loss: 1.0037 - val_acc: 0.7468\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9178\n",
      "Epoch 00059: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 456us/sample - loss: 0.2480 - acc: 0.9178 - val_loss: 0.9577 - val_acc: 0.7643\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9204\n",
      "Epoch 00060: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.2434 - acc: 0.9204 - val_loss: 0.9582 - val_acc: 0.7629\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9207\n",
      "Epoch 00061: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2421 - acc: 0.9207 - val_loss: 0.9647 - val_acc: 0.7612\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9214\n",
      "Epoch 00062: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2348 - acc: 0.9214 - val_loss: 0.9825 - val_acc: 0.7685\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9208\n",
      "Epoch 00063: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2379 - acc: 0.9208 - val_loss: 0.9834 - val_acc: 0.7617\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9246\n",
      "Epoch 00064: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.2281 - acc: 0.9247 - val_loss: 0.9624 - val_acc: 0.7745\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9260\n",
      "Epoch 00065: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2243 - acc: 0.9260 - val_loss: 0.9777 - val_acc: 0.7654\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9283\n",
      "Epoch 00066: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2186 - acc: 0.9283 - val_loss: 1.0507 - val_acc: 0.7547\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9282\n",
      "Epoch 00067: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.2176 - acc: 0.9282 - val_loss: 0.9989 - val_acc: 0.7608\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2093 - acc: 0.9314\n",
      "Epoch 00068: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2093 - acc: 0.9314 - val_loss: 0.9884 - val_acc: 0.7638\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9285\n",
      "Epoch 00069: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.2157 - acc: 0.9284 - val_loss: 0.9887 - val_acc: 0.7682\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9314\n",
      "Epoch 00070: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.2062 - acc: 0.9314 - val_loss: 1.0010 - val_acc: 0.7629\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9307\n",
      "Epoch 00071: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.2077 - acc: 0.9307 - val_loss: 1.0152 - val_acc: 0.7608\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9316\n",
      "Epoch 00072: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.2042 - acc: 0.9316 - val_loss: 1.0065 - val_acc: 0.7591\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9333\n",
      "Epoch 00073: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.1994 - acc: 0.9334 - val_loss: 1.0136 - val_acc: 0.7615\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9359\n",
      "Epoch 00074: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.1948 - acc: 0.9359 - val_loss: 0.9956 - val_acc: 0.7696\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9335\n",
      "Epoch 00075: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.1992 - acc: 0.9334 - val_loss: 0.9944 - val_acc: 0.7722\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9354\n",
      "Epoch 00076: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.1934 - acc: 0.9353 - val_loss: 1.0025 - val_acc: 0.7713\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9355\n",
      "Epoch 00077: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 448us/sample - loss: 0.1979 - acc: 0.9355 - val_loss: 1.0177 - val_acc: 0.7638\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9388\n",
      "Epoch 00078: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.1850 - acc: 0.9388 - val_loss: 1.0201 - val_acc: 0.7701\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9384\n",
      "Epoch 00079: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.1849 - acc: 0.9384 - val_loss: 1.0517 - val_acc: 0.7661\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9382\n",
      "Epoch 00080: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.1847 - acc: 0.9382 - val_loss: 1.0215 - val_acc: 0.7771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9397\n",
      "Epoch 00081: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.1833 - acc: 0.9397 - val_loss: 1.0204 - val_acc: 0.7710\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9414\n",
      "Epoch 00082: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1763 - acc: 0.9414 - val_loss: 0.9932 - val_acc: 0.7789\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9404\n",
      "Epoch 00083: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 456us/sample - loss: 0.1781 - acc: 0.9404 - val_loss: 1.0382 - val_acc: 0.7678\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9437\n",
      "Epoch 00084: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1701 - acc: 0.9437 - val_loss: 1.0273 - val_acc: 0.7699\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9439\n",
      "Epoch 00085: val_loss did not improve from 0.89210\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.1729 - acc: 0.9439 - val_loss: 1.0071 - val_acc: 0.7720\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 262us/sample - loss: 1.0127 - acc: 0.7124\n",
      "Loss: 1.0127461109082152 Accuracy: 0.7123572\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3966 - acc: 0.2155\n",
      "Epoch 00001: val_loss improved from inf to 1.85210, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/001-1.8521.hdf5\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 2.3965 - acc: 0.2156 - val_loss: 1.8521 - val_acc: 0.4093\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7607 - acc: 0.4265\n",
      "Epoch 00002: val_loss improved from 1.85210 to 1.57533, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/002-1.5753.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 1.7609 - acc: 0.4265 - val_loss: 1.5753 - val_acc: 0.5020\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5675 - acc: 0.4985\n",
      "Epoch 00003: val_loss improved from 1.57533 to 1.46043, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/003-1.4604.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 1.5674 - acc: 0.4984 - val_loss: 1.4604 - val_acc: 0.5411\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4463 - acc: 0.5425\n",
      "Epoch 00004: val_loss improved from 1.46043 to 1.33171, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/004-1.3317.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 1.4463 - acc: 0.5426 - val_loss: 1.3317 - val_acc: 0.6087\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3597 - acc: 0.5765\n",
      "Epoch 00005: val_loss improved from 1.33171 to 1.26571, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/005-1.2657.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 1.3596 - acc: 0.5766 - val_loss: 1.2657 - val_acc: 0.6229\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2761 - acc: 0.6054\n",
      "Epoch 00006: val_loss improved from 1.26571 to 1.20015, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/006-1.2001.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 1.2761 - acc: 0.6054 - val_loss: 1.2001 - val_acc: 0.6355\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2114 - acc: 0.6279\n",
      "Epoch 00007: val_loss improved from 1.20015 to 1.13060, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/007-1.1306.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 1.2113 - acc: 0.6279 - val_loss: 1.1306 - val_acc: 0.6709\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1529 - acc: 0.6489\n",
      "Epoch 00008: val_loss improved from 1.13060 to 1.08905, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/008-1.0891.hdf5\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 1.1529 - acc: 0.6489 - val_loss: 1.0891 - val_acc: 0.6758\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1061 - acc: 0.6631\n",
      "Epoch 00009: val_loss improved from 1.08905 to 1.08373, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/009-1.0837.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 1.1062 - acc: 0.6631 - val_loss: 1.0837 - val_acc: 0.6790\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0637 - acc: 0.6749\n",
      "Epoch 00010: val_loss improved from 1.08373 to 0.98774, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/010-0.9877.hdf5\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 1.0637 - acc: 0.6749 - val_loss: 0.9877 - val_acc: 0.7049\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0150 - acc: 0.6917\n",
      "Epoch 00011: val_loss improved from 0.98774 to 0.96252, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/011-0.9625.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 1.0149 - acc: 0.6917 - val_loss: 0.9625 - val_acc: 0.7114\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9781 - acc: 0.7044\n",
      "Epoch 00012: val_loss improved from 0.96252 to 0.95395, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/012-0.9539.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.9780 - acc: 0.7044 - val_loss: 0.9539 - val_acc: 0.7200\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9435 - acc: 0.7133\n",
      "Epoch 00013: val_loss improved from 0.95395 to 0.90524, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/013-0.9052.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.9434 - acc: 0.7133 - val_loss: 0.9052 - val_acc: 0.7300\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9123 - acc: 0.7233\n",
      "Epoch 00014: val_loss improved from 0.90524 to 0.89480, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/014-0.8948.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.9122 - acc: 0.7233 - val_loss: 0.8948 - val_acc: 0.7298\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8812 - acc: 0.7332\n",
      "Epoch 00015: val_loss improved from 0.89480 to 0.86704, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/015-0.8670.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.8812 - acc: 0.7332 - val_loss: 0.8670 - val_acc: 0.7331\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8527 - acc: 0.7418\n",
      "Epoch 00016: val_loss improved from 0.86704 to 0.82381, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/016-0.8238.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.8527 - acc: 0.7418 - val_loss: 0.8238 - val_acc: 0.7552\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8255 - acc: 0.7471\n",
      "Epoch 00017: val_loss improved from 0.82381 to 0.79913, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/017-0.7991.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.8254 - acc: 0.7471 - val_loss: 0.7991 - val_acc: 0.7638\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7941 - acc: 0.7608\n",
      "Epoch 00018: val_loss improved from 0.79913 to 0.78268, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/018-0.7827.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.7941 - acc: 0.7608 - val_loss: 0.7827 - val_acc: 0.7706\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7730 - acc: 0.7646\n",
      "Epoch 00019: val_loss improved from 0.78268 to 0.75798, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/019-0.7580.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.7729 - acc: 0.7646 - val_loss: 0.7580 - val_acc: 0.7771\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7427 - acc: 0.7738\n",
      "Epoch 00020: val_loss did not improve from 0.75798\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.7427 - acc: 0.7738 - val_loss: 0.7677 - val_acc: 0.7720\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7251 - acc: 0.7782\n",
      "Epoch 00021: val_loss improved from 0.75798 to 0.73492, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/021-0.7349.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.7251 - acc: 0.7782 - val_loss: 0.7349 - val_acc: 0.7824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7032 - acc: 0.7877\n",
      "Epoch 00022: val_loss improved from 0.73492 to 0.71091, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/022-0.7109.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.7031 - acc: 0.7877 - val_loss: 0.7109 - val_acc: 0.7939\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6807 - acc: 0.7924\n",
      "Epoch 00023: val_loss improved from 0.71091 to 0.69184, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/023-0.6918.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.6806 - acc: 0.7925 - val_loss: 0.6918 - val_acc: 0.7990\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6624 - acc: 0.8003\n",
      "Epoch 00024: val_loss improved from 0.69184 to 0.69110, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/024-0.6911.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.6624 - acc: 0.8003 - val_loss: 0.6911 - val_acc: 0.8001\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6440 - acc: 0.8041\n",
      "Epoch 00025: val_loss improved from 0.69110 to 0.66676, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/025-0.6668.hdf5\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.6441 - acc: 0.8041 - val_loss: 0.6668 - val_acc: 0.8074\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6272 - acc: 0.8123\n",
      "Epoch 00026: val_loss did not improve from 0.66676\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.6272 - acc: 0.8123 - val_loss: 0.6760 - val_acc: 0.8020\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.8160\n",
      "Epoch 00027: val_loss improved from 0.66676 to 0.64066, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/027-0.6407.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.6103 - acc: 0.8161 - val_loss: 0.6407 - val_acc: 0.8083\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5950 - acc: 0.8219\n",
      "Epoch 00028: val_loss did not improve from 0.64066\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5950 - acc: 0.8219 - val_loss: 0.6617 - val_acc: 0.8095\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5782 - acc: 0.8257\n",
      "Epoch 00029: val_loss did not improve from 0.64066\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.5782 - acc: 0.8256 - val_loss: 0.6492 - val_acc: 0.8083\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5620 - acc: 0.8281\n",
      "Epoch 00030: val_loss improved from 0.64066 to 0.62503, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/030-0.6250.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.5620 - acc: 0.8281 - val_loss: 0.6250 - val_acc: 0.8169\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5552 - acc: 0.8334\n",
      "Epoch 00031: val_loss did not improve from 0.62503\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.5552 - acc: 0.8334 - val_loss: 0.6299 - val_acc: 0.8164\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.8375\n",
      "Epoch 00032: val_loss improved from 0.62503 to 0.62406, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/032-0.6241.hdf5\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.5434 - acc: 0.8375 - val_loss: 0.6241 - val_acc: 0.8202\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5249 - acc: 0.8427\n",
      "Epoch 00033: val_loss improved from 0.62406 to 0.59482, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/033-0.5948.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.5249 - acc: 0.8427 - val_loss: 0.5948 - val_acc: 0.8255\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5165 - acc: 0.8454\n",
      "Epoch 00034: val_loss did not improve from 0.59482\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.5166 - acc: 0.8454 - val_loss: 0.5988 - val_acc: 0.8251\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.8476\n",
      "Epoch 00035: val_loss improved from 0.59482 to 0.57612, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/035-0.5761.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.5027 - acc: 0.8476 - val_loss: 0.5761 - val_acc: 0.8311\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4942 - acc: 0.8496\n",
      "Epoch 00036: val_loss did not improve from 0.57612\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.4942 - acc: 0.8496 - val_loss: 0.5779 - val_acc: 0.8314\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4837 - acc: 0.8526\n",
      "Epoch 00037: val_loss did not improve from 0.57612\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4837 - acc: 0.8526 - val_loss: 0.5864 - val_acc: 0.8323\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4681 - acc: 0.8593\n",
      "Epoch 00038: val_loss improved from 0.57612 to 0.56297, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/038-0.5630.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4681 - acc: 0.8593 - val_loss: 0.5630 - val_acc: 0.8395\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4624 - acc: 0.8592\n",
      "Epoch 00039: val_loss did not improve from 0.56297\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4623 - acc: 0.8593 - val_loss: 0.5645 - val_acc: 0.8407\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4511 - acc: 0.8636\n",
      "Epoch 00040: val_loss did not improve from 0.56297\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4511 - acc: 0.8636 - val_loss: 0.5733 - val_acc: 0.8379\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.8660\n",
      "Epoch 00041: val_loss did not improve from 0.56297\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4430 - acc: 0.8660 - val_loss: 0.5634 - val_acc: 0.8439\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8675\n",
      "Epoch 00042: val_loss improved from 0.56297 to 0.54455, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/042-0.5446.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4338 - acc: 0.8675 - val_loss: 0.5446 - val_acc: 0.8470\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.8695\n",
      "Epoch 00043: val_loss did not improve from 0.54455\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4269 - acc: 0.8696 - val_loss: 0.5472 - val_acc: 0.8439\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.8728\n",
      "Epoch 00044: val_loss did not improve from 0.54455\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.4166 - acc: 0.8728 - val_loss: 0.5477 - val_acc: 0.8465\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.8741\n",
      "Epoch 00045: val_loss improved from 0.54455 to 0.52743, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/045-0.5274.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4107 - acc: 0.8741 - val_loss: 0.5274 - val_acc: 0.8521\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4017 - acc: 0.8757\n",
      "Epoch 00046: val_loss did not improve from 0.52743\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4017 - acc: 0.8757 - val_loss: 0.5503 - val_acc: 0.8479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3991 - acc: 0.8765\n",
      "Epoch 00047: val_loss improved from 0.52743 to 0.52591, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/047-0.5259.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3991 - acc: 0.8765 - val_loss: 0.5259 - val_acc: 0.8574\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8811\n",
      "Epoch 00048: val_loss did not improve from 0.52591\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3863 - acc: 0.8812 - val_loss: 0.5260 - val_acc: 0.8542\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8853\n",
      "Epoch 00049: val_loss improved from 0.52591 to 0.52430, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/049-0.5243.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3746 - acc: 0.8853 - val_loss: 0.5243 - val_acc: 0.8544\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8857\n",
      "Epoch 00050: val_loss did not improve from 0.52430\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.3705 - acc: 0.8857 - val_loss: 0.5244 - val_acc: 0.8512\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.8866\n",
      "Epoch 00051: val_loss did not improve from 0.52430\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3663 - acc: 0.8866 - val_loss: 0.5345 - val_acc: 0.8521\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8887\n",
      "Epoch 00052: val_loss improved from 0.52430 to 0.51924, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/052-0.5192.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3622 - acc: 0.8887 - val_loss: 0.5192 - val_acc: 0.8549\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.8915\n",
      "Epoch 00053: val_loss did not improve from 0.51924\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3555 - acc: 0.8916 - val_loss: 0.5289 - val_acc: 0.8479\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8915\n",
      "Epoch 00054: val_loss improved from 0.51924 to 0.50733, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/054-0.5073.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3484 - acc: 0.8915 - val_loss: 0.5073 - val_acc: 0.8586\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3377 - acc: 0.8943\n",
      "Epoch 00055: val_loss did not improve from 0.50733\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3379 - acc: 0.8943 - val_loss: 0.5110 - val_acc: 0.8572\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8947\n",
      "Epoch 00056: val_loss did not improve from 0.50733\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.3362 - acc: 0.8947 - val_loss: 0.5143 - val_acc: 0.8623\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8961\n",
      "Epoch 00057: val_loss did not improve from 0.50733\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3321 - acc: 0.8961 - val_loss: 0.5086 - val_acc: 0.8567\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3273 - acc: 0.8986\n",
      "Epoch 00058: val_loss did not improve from 0.50733\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3272 - acc: 0.8986 - val_loss: 0.5214 - val_acc: 0.8553\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8999\n",
      "Epoch 00059: val_loss did not improve from 0.50733\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.3220 - acc: 0.8999 - val_loss: 0.5154 - val_acc: 0.8600\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3166 - acc: 0.9011\n",
      "Epoch 00060: val_loss did not improve from 0.50733\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3169 - acc: 0.9010 - val_loss: 0.5244 - val_acc: 0.8581\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.9012\n",
      "Epoch 00061: val_loss improved from 0.50733 to 0.50127, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/061-0.5013.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3138 - acc: 0.9012 - val_loss: 0.5013 - val_acc: 0.8623\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9021\n",
      "Epoch 00062: val_loss improved from 0.50127 to 0.50060, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/062-0.5006.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3070 - acc: 0.9021 - val_loss: 0.5006 - val_acc: 0.8637\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9047\n",
      "Epoch 00063: val_loss did not improve from 0.50060\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.2997 - acc: 0.9047 - val_loss: 0.5030 - val_acc: 0.8684\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9070\n",
      "Epoch 00064: val_loss improved from 0.50060 to 0.49899, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/064-0.4990.hdf5\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.2931 - acc: 0.9071 - val_loss: 0.4990 - val_acc: 0.8649\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9082\n",
      "Epoch 00065: val_loss did not improve from 0.49899\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2911 - acc: 0.9082 - val_loss: 0.5016 - val_acc: 0.8668\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9077\n",
      "Epoch 00066: val_loss did not improve from 0.49899\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.2915 - acc: 0.9077 - val_loss: 0.5009 - val_acc: 0.8686\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9118\n",
      "Epoch 00067: val_loss improved from 0.49899 to 0.49718, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/067-0.4972.hdf5\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2810 - acc: 0.9118 - val_loss: 0.4972 - val_acc: 0.8654\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9099\n",
      "Epoch 00068: val_loss did not improve from 0.49718\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2799 - acc: 0.9100 - val_loss: 0.5042 - val_acc: 0.8640\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9121\n",
      "Epoch 00069: val_loss did not improve from 0.49718\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2764 - acc: 0.9121 - val_loss: 0.5104 - val_acc: 0.8616\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9115\n",
      "Epoch 00070: val_loss did not improve from 0.49718\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.2757 - acc: 0.9115 - val_loss: 0.5084 - val_acc: 0.8707\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9139\n",
      "Epoch 00071: val_loss did not improve from 0.49718\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2660 - acc: 0.9138 - val_loss: 0.5054 - val_acc: 0.8647\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9147\n",
      "Epoch 00072: val_loss did not improve from 0.49718\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.2689 - acc: 0.9147 - val_loss: 0.4995 - val_acc: 0.8705\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9177\n",
      "Epoch 00073: val_loss did not improve from 0.49718\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.2581 - acc: 0.9177 - val_loss: 0.5063 - val_acc: 0.8665\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9168\n",
      "Epoch 00074: val_loss improved from 0.49718 to 0.49600, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/074-0.4960.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.2602 - acc: 0.9168 - val_loss: 0.4960 - val_acc: 0.8686\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9163\n",
      "Epoch 00075: val_loss did not improve from 0.49600\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.2562 - acc: 0.9163 - val_loss: 0.5000 - val_acc: 0.8677\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9190\n",
      "Epoch 00076: val_loss improved from 0.49600 to 0.49340, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/076-0.4934.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.2538 - acc: 0.9190 - val_loss: 0.4934 - val_acc: 0.8744\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9217\n",
      "Epoch 00077: val_loss improved from 0.49340 to 0.48867, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/077-0.4887.hdf5\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2439 - acc: 0.9217 - val_loss: 0.4887 - val_acc: 0.8740\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9214\n",
      "Epoch 00078: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.2438 - acc: 0.9214 - val_loss: 0.5060 - val_acc: 0.8740\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9238\n",
      "Epoch 00079: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.2379 - acc: 0.9238 - val_loss: 0.5224 - val_acc: 0.8682\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9231\n",
      "Epoch 00080: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.2389 - acc: 0.9231 - val_loss: 0.5291 - val_acc: 0.8661\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9236\n",
      "Epoch 00081: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.2354 - acc: 0.9236 - val_loss: 0.5217 - val_acc: 0.8686\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9246\n",
      "Epoch 00082: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.2315 - acc: 0.9246 - val_loss: 0.4975 - val_acc: 0.8758\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9261\n",
      "Epoch 00083: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2298 - acc: 0.9261 - val_loss: 0.5071 - val_acc: 0.8696\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9282\n",
      "Epoch 00084: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.2213 - acc: 0.9282 - val_loss: 0.5112 - val_acc: 0.8724\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9265\n",
      "Epoch 00085: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.2255 - acc: 0.9265 - val_loss: 0.4980 - val_acc: 0.8777\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9264\n",
      "Epoch 00086: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2227 - acc: 0.9264 - val_loss: 0.4949 - val_acc: 0.8756\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9273\n",
      "Epoch 00087: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2255 - acc: 0.9273 - val_loss: 0.4960 - val_acc: 0.8770\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9280\n",
      "Epoch 00088: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2187 - acc: 0.9280 - val_loss: 0.5055 - val_acc: 0.8714\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9293\n",
      "Epoch 00089: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.2171 - acc: 0.9294 - val_loss: 0.5051 - val_acc: 0.8756\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9292\n",
      "Epoch 00090: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2141 - acc: 0.9292 - val_loss: 0.5097 - val_acc: 0.8751\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9308\n",
      "Epoch 00091: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.2134 - acc: 0.9308 - val_loss: 0.4940 - val_acc: 0.8765\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9317\n",
      "Epoch 00092: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.2107 - acc: 0.9317 - val_loss: 0.5102 - val_acc: 0.8751\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9322\n",
      "Epoch 00093: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2070 - acc: 0.9322 - val_loss: 0.5023 - val_acc: 0.8803\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9328\n",
      "Epoch 00094: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.2048 - acc: 0.9328 - val_loss: 0.5047 - val_acc: 0.8798\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9322\n",
      "Epoch 00095: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.2041 - acc: 0.9322 - val_loss: 0.4898 - val_acc: 0.8798\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9335\n",
      "Epoch 00096: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1998 - acc: 0.9335 - val_loss: 0.5027 - val_acc: 0.8758\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9352\n",
      "Epoch 00097: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.1972 - acc: 0.9352 - val_loss: 0.4950 - val_acc: 0.8768\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9358\n",
      "Epoch 00098: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.1973 - acc: 0.9358 - val_loss: 0.4896 - val_acc: 0.8810\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9358\n",
      "Epoch 00099: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.1936 - acc: 0.9359 - val_loss: 0.4971 - val_acc: 0.8784\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9387\n",
      "Epoch 00100: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1879 - acc: 0.9387 - val_loss: 0.4994 - val_acc: 0.8803\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9380\n",
      "Epoch 00101: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.1890 - acc: 0.9380 - val_loss: 0.5064 - val_acc: 0.8768\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9371\n",
      "Epoch 00102: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.1869 - acc: 0.9372 - val_loss: 0.5069 - val_acc: 0.8803\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9375\n",
      "Epoch 00103: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.1863 - acc: 0.9375 - val_loss: 0.4960 - val_acc: 0.8812\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9381\n",
      "Epoch 00104: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1883 - acc: 0.9381 - val_loss: 0.4954 - val_acc: 0.8793\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9399\n",
      "Epoch 00105: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.1815 - acc: 0.9399 - val_loss: 0.4941 - val_acc: 0.8793\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9389\n",
      "Epoch 00106: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.1842 - acc: 0.9389 - val_loss: 0.5026 - val_acc: 0.8852\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9418\n",
      "Epoch 00107: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.1756 - acc: 0.9418 - val_loss: 0.4935 - val_acc: 0.8849\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9409\n",
      "Epoch 00108: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.1776 - acc: 0.9409 - val_loss: 0.4923 - val_acc: 0.8849\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9413\n",
      "Epoch 00109: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.1786 - acc: 0.9413 - val_loss: 0.4984 - val_acc: 0.8826\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9435\n",
      "Epoch 00110: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1720 - acc: 0.9435 - val_loss: 0.5066 - val_acc: 0.8817\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9420\n",
      "Epoch 00111: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1760 - acc: 0.9419 - val_loss: 0.5015 - val_acc: 0.8814\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9420\n",
      "Epoch 00112: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1725 - acc: 0.9420 - val_loss: 0.4960 - val_acc: 0.8831\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9442\n",
      "Epoch 00113: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.1692 - acc: 0.9442 - val_loss: 0.5076 - val_acc: 0.8786\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9440\n",
      "Epoch 00114: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.1694 - acc: 0.9440 - val_loss: 0.5026 - val_acc: 0.8793\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9456\n",
      "Epoch 00115: val_loss did not improve from 0.48867\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.1656 - acc: 0.9456 - val_loss: 0.5158 - val_acc: 0.8828\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9433\n",
      "Epoch 00116: val_loss improved from 0.48867 to 0.48315, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv_checkpoint/116-0.4831.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.1698 - acc: 0.9433 - val_loss: 0.4831 - val_acc: 0.8889\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9445\n",
      "Epoch 00117: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1644 - acc: 0.9445 - val_loss: 0.5481 - val_acc: 0.8733\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9431\n",
      "Epoch 00118: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.1710 - acc: 0.9431 - val_loss: 0.5236 - val_acc: 0.8796\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9461\n",
      "Epoch 00119: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.1615 - acc: 0.9461 - val_loss: 0.5193 - val_acc: 0.8819\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9470\n",
      "Epoch 00120: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.1611 - acc: 0.9470 - val_loss: 0.5182 - val_acc: 0.8819\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9480\n",
      "Epoch 00121: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.1604 - acc: 0.9480 - val_loss: 0.4961 - val_acc: 0.8824\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9463\n",
      "Epoch 00122: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.1575 - acc: 0.9463 - val_loss: 0.4940 - val_acc: 0.8854\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9488\n",
      "Epoch 00123: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.1524 - acc: 0.9488 - val_loss: 0.5004 - val_acc: 0.8835\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9490\n",
      "Epoch 00124: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.1551 - acc: 0.9491 - val_loss: 0.5062 - val_acc: 0.8831\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9465\n",
      "Epoch 00125: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1602 - acc: 0.9465 - val_loss: 0.4983 - val_acc: 0.8810\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9499\n",
      "Epoch 00126: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1516 - acc: 0.9499 - val_loss: 0.4957 - val_acc: 0.8861\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9488\n",
      "Epoch 00127: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1521 - acc: 0.9487 - val_loss: 0.5155 - val_acc: 0.8826\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9486\n",
      "Epoch 00128: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.1524 - acc: 0.9486 - val_loss: 0.5139 - val_acc: 0.8840\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9485\n",
      "Epoch 00129: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1533 - acc: 0.9485 - val_loss: 0.5213 - val_acc: 0.8817\n",
      "Epoch 130/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9504\n",
      "Epoch 00130: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1488 - acc: 0.9504 - val_loss: 0.5115 - val_acc: 0.8852\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9506\n",
      "Epoch 00131: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1478 - acc: 0.9506 - val_loss: 0.5044 - val_acc: 0.8891\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9519\n",
      "Epoch 00132: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1447 - acc: 0.9519 - val_loss: 0.5039 - val_acc: 0.8877\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9525\n",
      "Epoch 00133: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.1464 - acc: 0.9525 - val_loss: 0.5163 - val_acc: 0.8819\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9533\n",
      "Epoch 00134: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.1440 - acc: 0.9533 - val_loss: 0.5037 - val_acc: 0.8854\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9517\n",
      "Epoch 00135: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.1443 - acc: 0.9517 - val_loss: 0.5171 - val_acc: 0.8859\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9507\n",
      "Epoch 00136: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.1451 - acc: 0.9507 - val_loss: 0.5076 - val_acc: 0.8828\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9535\n",
      "Epoch 00137: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1401 - acc: 0.9535 - val_loss: 0.5028 - val_acc: 0.8896\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9546\n",
      "Epoch 00138: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.1388 - acc: 0.9547 - val_loss: 0.5605 - val_acc: 0.8689\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9526\n",
      "Epoch 00139: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.1420 - acc: 0.9526 - val_loss: 0.5010 - val_acc: 0.8840\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9551\n",
      "Epoch 00140: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.1357 - acc: 0.9551 - val_loss: 0.5245 - val_acc: 0.8800\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9524\n",
      "Epoch 00141: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.1414 - acc: 0.9524 - val_loss: 0.5158 - val_acc: 0.8842\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9531\n",
      "Epoch 00142: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.1404 - acc: 0.9531 - val_loss: 0.5002 - val_acc: 0.8887\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9532\n",
      "Epoch 00143: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.1365 - acc: 0.9532 - val_loss: 0.4943 - val_acc: 0.8873\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9542\n",
      "Epoch 00144: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.1352 - acc: 0.9541 - val_loss: 0.5115 - val_acc: 0.8884\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9550\n",
      "Epoch 00145: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.1351 - acc: 0.9550 - val_loss: 0.5322 - val_acc: 0.8796\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9536\n",
      "Epoch 00146: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.1387 - acc: 0.9536 - val_loss: 0.5141 - val_acc: 0.8877\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9567\n",
      "Epoch 00147: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.1293 - acc: 0.9567 - val_loss: 0.5166 - val_acc: 0.8901\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9568\n",
      "Epoch 00148: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.1294 - acc: 0.9569 - val_loss: 0.5172 - val_acc: 0.8863\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9564\n",
      "Epoch 00149: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.1281 - acc: 0.9564 - val_loss: 0.5088 - val_acc: 0.8880\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9561\n",
      "Epoch 00150: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.1330 - acc: 0.9561 - val_loss: 0.5106 - val_acc: 0.8842\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9555\n",
      "Epoch 00151: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1305 - acc: 0.9555 - val_loss: 0.5064 - val_acc: 0.8854\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9556\n",
      "Epoch 00152: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.1297 - acc: 0.9556 - val_loss: 0.5027 - val_acc: 0.8833\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9570\n",
      "Epoch 00153: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.1284 - acc: 0.9570 - val_loss: 0.5189 - val_acc: 0.8887\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9565\n",
      "Epoch 00154: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1293 - acc: 0.9565 - val_loss: 0.5205 - val_acc: 0.8870\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9557\n",
      "Epoch 00155: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.1306 - acc: 0.9557 - val_loss: 0.5107 - val_acc: 0.8866\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9593\n",
      "Epoch 00156: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.1236 - acc: 0.9593 - val_loss: 0.5092 - val_acc: 0.8901\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9567\n",
      "Epoch 00157: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1262 - acc: 0.9567 - val_loss: 0.5025 - val_acc: 0.8889\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9593\n",
      "Epoch 00158: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1220 - acc: 0.9593 - val_loss: 0.5049 - val_acc: 0.8931\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9596\n",
      "Epoch 00159: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.1221 - acc: 0.9595 - val_loss: 0.5263 - val_acc: 0.8849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9596\n",
      "Epoch 00160: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.1224 - acc: 0.9596 - val_loss: 0.5200 - val_acc: 0.8861\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9600\n",
      "Epoch 00161: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.1221 - acc: 0.9600 - val_loss: 0.5213 - val_acc: 0.8903\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9593\n",
      "Epoch 00162: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.1216 - acc: 0.9594 - val_loss: 0.5123 - val_acc: 0.8891\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9611\n",
      "Epoch 00163: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.1203 - acc: 0.9611 - val_loss: 0.5082 - val_acc: 0.8921\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9624\n",
      "Epoch 00164: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.1160 - acc: 0.9624 - val_loss: 0.5203 - val_acc: 0.8894\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9626\n",
      "Epoch 00165: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.1132 - acc: 0.9626 - val_loss: 0.5209 - val_acc: 0.8903\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9608\n",
      "Epoch 00166: val_loss did not improve from 0.48315\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.1167 - acc: 0.9608 - val_loss: 0.5289 - val_acc: 0.8921\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 284us/sample - loss: 0.6059 - acc: 0.8536\n",
      "Loss: 0.6059250388561379 Accuracy: 0.85358256\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3579 - acc: 0.2267\n",
      "Epoch 00001: val_loss improved from inf to 1.79336, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/001-1.7934.hdf5\n",
      "36805/36805 [==============================] - 20s 549us/sample - loss: 2.3577 - acc: 0.2268 - val_loss: 1.7934 - val_acc: 0.4405\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7049 - acc: 0.4499\n",
      "Epoch 00002: val_loss improved from 1.79336 to 1.45283, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/002-1.4528.hdf5\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 1.7049 - acc: 0.4500 - val_loss: 1.4528 - val_acc: 0.5609\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4965 - acc: 0.5211\n",
      "Epoch 00003: val_loss improved from 1.45283 to 1.30247, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/003-1.3025.hdf5\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 1.4966 - acc: 0.5211 - val_loss: 1.3025 - val_acc: 0.5998\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3656 - acc: 0.5672\n",
      "Epoch 00004: val_loss improved from 1.30247 to 1.19747, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/004-1.1975.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 1.3656 - acc: 0.5672 - val_loss: 1.1975 - val_acc: 0.6455\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2738 - acc: 0.5995\n",
      "Epoch 00005: val_loss improved from 1.19747 to 1.11909, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/005-1.1191.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 1.2737 - acc: 0.5995 - val_loss: 1.1191 - val_acc: 0.6643\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1893 - acc: 0.6271\n",
      "Epoch 00006: val_loss improved from 1.11909 to 1.06963, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/006-1.0696.hdf5\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 1.1892 - acc: 0.6272 - val_loss: 1.0696 - val_acc: 0.6771\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1110 - acc: 0.6564\n",
      "Epoch 00007: val_loss improved from 1.06963 to 0.98893, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/007-0.9889.hdf5\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 1.1110 - acc: 0.6564 - val_loss: 0.9889 - val_acc: 0.7154\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0430 - acc: 0.6760\n",
      "Epoch 00008: val_loss improved from 0.98893 to 0.89512, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/008-0.8951.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 1.0430 - acc: 0.6759 - val_loss: 0.8951 - val_acc: 0.7368\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9793 - acc: 0.6978\n",
      "Epoch 00009: val_loss improved from 0.89512 to 0.84756, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/009-0.8476.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.9792 - acc: 0.6978 - val_loss: 0.8476 - val_acc: 0.7480\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9236 - acc: 0.7173\n",
      "Epoch 00010: val_loss improved from 0.84756 to 0.78942, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/010-0.7894.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.9236 - acc: 0.7173 - val_loss: 0.7894 - val_acc: 0.7706\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8744 - acc: 0.7320\n",
      "Epoch 00011: val_loss improved from 0.78942 to 0.76630, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/011-0.7663.hdf5\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 0.8743 - acc: 0.7320 - val_loss: 0.7663 - val_acc: 0.7717\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8286 - acc: 0.7464\n",
      "Epoch 00012: val_loss improved from 0.76630 to 0.70177, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/012-0.7018.hdf5\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.8286 - acc: 0.7464 - val_loss: 0.7018 - val_acc: 0.7955\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7901 - acc: 0.7591\n",
      "Epoch 00013: val_loss improved from 0.70177 to 0.66954, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/013-0.6695.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.7901 - acc: 0.7591 - val_loss: 0.6695 - val_acc: 0.8008\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7514 - acc: 0.7705\n",
      "Epoch 00014: val_loss improved from 0.66954 to 0.64809, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/014-0.6481.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.7513 - acc: 0.7705 - val_loss: 0.6481 - val_acc: 0.8116\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7144 - acc: 0.7826\n",
      "Epoch 00015: val_loss improved from 0.64809 to 0.60629, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/015-0.6063.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.7143 - acc: 0.7826 - val_loss: 0.6063 - val_acc: 0.8220\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6907 - acc: 0.7898\n",
      "Epoch 00016: val_loss improved from 0.60629 to 0.58388, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/016-0.5839.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.6907 - acc: 0.7898 - val_loss: 0.5839 - val_acc: 0.8269\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6569 - acc: 0.7998\n",
      "Epoch 00017: val_loss did not improve from 0.58388\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.6569 - acc: 0.7998 - val_loss: 0.5947 - val_acc: 0.8251\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6274 - acc: 0.8090\n",
      "Epoch 00018: val_loss improved from 0.58388 to 0.54313, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/018-0.5431.hdf5\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.6274 - acc: 0.8090 - val_loss: 0.5431 - val_acc: 0.8372\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.8178\n",
      "Epoch 00019: val_loss improved from 0.54313 to 0.53301, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/019-0.5330.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.6059 - acc: 0.8179 - val_loss: 0.5330 - val_acc: 0.8423\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5817 - acc: 0.8256\n",
      "Epoch 00020: val_loss improved from 0.53301 to 0.49247, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/020-0.4925.hdf5\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.5817 - acc: 0.8256 - val_loss: 0.4925 - val_acc: 0.8602\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5648 - acc: 0.8284\n",
      "Epoch 00021: val_loss did not improve from 0.49247\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.5649 - acc: 0.8284 - val_loss: 0.5127 - val_acc: 0.8495\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.8328\n",
      "Epoch 00022: val_loss improved from 0.49247 to 0.47317, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/022-0.4732.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.5486 - acc: 0.8328 - val_loss: 0.4732 - val_acc: 0.8633\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.8375\n",
      "Epoch 00023: val_loss improved from 0.47317 to 0.46507, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/023-0.4651.hdf5\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.5305 - acc: 0.8375 - val_loss: 0.4651 - val_acc: 0.8614\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5129 - acc: 0.8454\n",
      "Epoch 00024: val_loss improved from 0.46507 to 0.45830, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/024-0.4583.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.5128 - acc: 0.8454 - val_loss: 0.4583 - val_acc: 0.8696\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.8483\n",
      "Epoch 00025: val_loss improved from 0.45830 to 0.43712, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/025-0.4371.hdf5\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.4986 - acc: 0.8483 - val_loss: 0.4371 - val_acc: 0.8730\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.8525\n",
      "Epoch 00026: val_loss improved from 0.43712 to 0.41737, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/026-0.4174.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.4832 - acc: 0.8525 - val_loss: 0.4174 - val_acc: 0.8817\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4677 - acc: 0.8570\n",
      "Epoch 00027: val_loss improved from 0.41737 to 0.41730, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/027-0.4173.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.4677 - acc: 0.8570 - val_loss: 0.4173 - val_acc: 0.8758\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4603 - acc: 0.8594\n",
      "Epoch 00028: val_loss improved from 0.41730 to 0.39600, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/028-0.3960.hdf5\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.4603 - acc: 0.8594 - val_loss: 0.3960 - val_acc: 0.8873\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.8640\n",
      "Epoch 00029: val_loss did not improve from 0.39600\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.4460 - acc: 0.8640 - val_loss: 0.4002 - val_acc: 0.8842\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.8663\n",
      "Epoch 00030: val_loss did not improve from 0.39600\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.4376 - acc: 0.8663 - val_loss: 0.4055 - val_acc: 0.8786\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8688\n",
      "Epoch 00031: val_loss improved from 0.39600 to 0.38407, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/031-0.3841.hdf5\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.4273 - acc: 0.8688 - val_loss: 0.3841 - val_acc: 0.8870\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8699\n",
      "Epoch 00032: val_loss improved from 0.38407 to 0.36109, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/032-0.3611.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.4132 - acc: 0.8699 - val_loss: 0.3611 - val_acc: 0.8949\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8741\n",
      "Epoch 00033: val_loss did not improve from 0.36109\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.4066 - acc: 0.8741 - val_loss: 0.3903 - val_acc: 0.8854\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8788\n",
      "Epoch 00034: val_loss improved from 0.36109 to 0.35660, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/034-0.3566.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.3982 - acc: 0.8788 - val_loss: 0.3566 - val_acc: 0.8987\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.8801\n",
      "Epoch 00035: val_loss improved from 0.35660 to 0.35045, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/035-0.3505.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.3903 - acc: 0.8801 - val_loss: 0.3505 - val_acc: 0.8980\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8814\n",
      "Epoch 00036: val_loss improved from 0.35045 to 0.34602, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/036-0.3460.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.3809 - acc: 0.8814 - val_loss: 0.3460 - val_acc: 0.9019\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8862\n",
      "Epoch 00037: val_loss did not improve from 0.34602\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.3661 - acc: 0.8862 - val_loss: 0.3605 - val_acc: 0.8952\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8859\n",
      "Epoch 00038: val_loss improved from 0.34602 to 0.33483, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/038-0.3348.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.3650 - acc: 0.8859 - val_loss: 0.3348 - val_acc: 0.9064\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8904\n",
      "Epoch 00039: val_loss improved from 0.33483 to 0.33155, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/039-0.3315.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.3520 - acc: 0.8904 - val_loss: 0.3315 - val_acc: 0.9066\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8908\n",
      "Epoch 00040: val_loss did not improve from 0.33155\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.3538 - acc: 0.8909 - val_loss: 0.3364 - val_acc: 0.9012\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8939\n",
      "Epoch 00041: val_loss improved from 0.33155 to 0.32186, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/041-0.3219.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.3395 - acc: 0.8939 - val_loss: 0.3219 - val_acc: 0.9078\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.8963\n",
      "Epoch 00042: val_loss improved from 0.32186 to 0.31234, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/042-0.3123.hdf5\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.3353 - acc: 0.8963 - val_loss: 0.3123 - val_acc: 0.9099\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.8990\n",
      "Epoch 00043: val_loss did not improve from 0.31234\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.3309 - acc: 0.8990 - val_loss: 0.3252 - val_acc: 0.9050\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8992\n",
      "Epoch 00044: val_loss did not improve from 0.31234\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.3253 - acc: 0.8992 - val_loss: 0.3242 - val_acc: 0.9050\n",
      "Epoch 45/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.9021\n",
      "Epoch 00045: val_loss did not improve from 0.31234\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.3132 - acc: 0.9021 - val_loss: 0.3224 - val_acc: 0.9085\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.9014\n",
      "Epoch 00046: val_loss improved from 0.31234 to 0.31150, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/046-0.3115.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.3105 - acc: 0.9014 - val_loss: 0.3115 - val_acc: 0.9096\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3051 - acc: 0.9041\n",
      "Epoch 00047: val_loss improved from 0.31150 to 0.29692, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/047-0.2969.hdf5\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 0.3050 - acc: 0.9041 - val_loss: 0.2969 - val_acc: 0.9178\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.9058\n",
      "Epoch 00048: val_loss did not improve from 0.29692\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.3004 - acc: 0.9059 - val_loss: 0.3007 - val_acc: 0.9133\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9056\n",
      "Epoch 00049: val_loss did not improve from 0.29692\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2969 - acc: 0.9056 - val_loss: 0.3125 - val_acc: 0.9115\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9063\n",
      "Epoch 00050: val_loss improved from 0.29692 to 0.29606, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/050-0.2961.hdf5\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.2944 - acc: 0.9063 - val_loss: 0.2961 - val_acc: 0.9103\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9105\n",
      "Epoch 00051: val_loss improved from 0.29606 to 0.29094, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/051-0.2909.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2860 - acc: 0.9105 - val_loss: 0.2909 - val_acc: 0.9194\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9090\n",
      "Epoch 00052: val_loss improved from 0.29094 to 0.28548, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/052-0.2855.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.2848 - acc: 0.9090 - val_loss: 0.2855 - val_acc: 0.9201\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9115\n",
      "Epoch 00053: val_loss did not improve from 0.28548\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.2774 - acc: 0.9115 - val_loss: 0.3094 - val_acc: 0.9094\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9127\n",
      "Epoch 00054: val_loss did not improve from 0.28548\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.2789 - acc: 0.9127 - val_loss: 0.2939 - val_acc: 0.9159\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9140\n",
      "Epoch 00055: val_loss improved from 0.28548 to 0.28323, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/055-0.2832.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2717 - acc: 0.9140 - val_loss: 0.2832 - val_acc: 0.9196\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9161\n",
      "Epoch 00056: val_loss improved from 0.28323 to 0.27572, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/056-0.2757.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.2634 - acc: 0.9161 - val_loss: 0.2757 - val_acc: 0.9227\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9181\n",
      "Epoch 00057: val_loss did not improve from 0.27572\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2596 - acc: 0.9181 - val_loss: 0.2759 - val_acc: 0.9229\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9187\n",
      "Epoch 00058: val_loss did not improve from 0.27572\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2577 - acc: 0.9188 - val_loss: 0.2949 - val_acc: 0.9138\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9211\n",
      "Epoch 00059: val_loss did not improve from 0.27572\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2509 - acc: 0.9211 - val_loss: 0.2814 - val_acc: 0.9192\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9199\n",
      "Epoch 00060: val_loss did not improve from 0.27572\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.2516 - acc: 0.9199 - val_loss: 0.2966 - val_acc: 0.9119\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9193\n",
      "Epoch 00061: val_loss did not improve from 0.27572\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.2553 - acc: 0.9194 - val_loss: 0.2923 - val_acc: 0.9143\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9222\n",
      "Epoch 00062: val_loss improved from 0.27572 to 0.26613, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/062-0.2661.hdf5\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2407 - acc: 0.9222 - val_loss: 0.2661 - val_acc: 0.9276\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9228\n",
      "Epoch 00063: val_loss did not improve from 0.26613\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.2397 - acc: 0.9228 - val_loss: 0.2679 - val_acc: 0.9248\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9257\n",
      "Epoch 00064: val_loss did not improve from 0.26613\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 0.2331 - acc: 0.9257 - val_loss: 0.2781 - val_acc: 0.9243\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9247\n",
      "Epoch 00065: val_loss did not improve from 0.26613\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2327 - acc: 0.9247 - val_loss: 0.2890 - val_acc: 0.9161\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9247\n",
      "Epoch 00066: val_loss improved from 0.26613 to 0.26560, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/066-0.2656.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2350 - acc: 0.9247 - val_loss: 0.2656 - val_acc: 0.9269\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9256\n",
      "Epoch 00067: val_loss improved from 0.26560 to 0.26268, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/067-0.2627.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2294 - acc: 0.9256 - val_loss: 0.2627 - val_acc: 0.9290\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9268\n",
      "Epoch 00068: val_loss did not improve from 0.26268\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.2246 - acc: 0.9267 - val_loss: 0.2644 - val_acc: 0.9257\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9268\n",
      "Epoch 00069: val_loss did not improve from 0.26268\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2236 - acc: 0.9268 - val_loss: 0.2641 - val_acc: 0.9294\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9294\n",
      "Epoch 00070: val_loss did not improve from 0.26268\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.2163 - acc: 0.9294 - val_loss: 0.2838 - val_acc: 0.9173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2182 - acc: 0.9291\n",
      "Epoch 00071: val_loss improved from 0.26268 to 0.25711, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/071-0.2571.hdf5\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.2181 - acc: 0.9291 - val_loss: 0.2571 - val_acc: 0.9329\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9325\n",
      "Epoch 00072: val_loss did not improve from 0.25711\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.2105 - acc: 0.9325 - val_loss: 0.2623 - val_acc: 0.9290\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9303\n",
      "Epoch 00073: val_loss did not improve from 0.25711\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.2141 - acc: 0.9303 - val_loss: 0.2648 - val_acc: 0.9299\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9342\n",
      "Epoch 00074: val_loss did not improve from 0.25711\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2068 - acc: 0.9342 - val_loss: 0.2616 - val_acc: 0.9292\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9343\n",
      "Epoch 00075: val_loss did not improve from 0.25711\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.2057 - acc: 0.9343 - val_loss: 0.2579 - val_acc: 0.9262\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9348\n",
      "Epoch 00076: val_loss did not improve from 0.25711\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2029 - acc: 0.9348 - val_loss: 0.2613 - val_acc: 0.9297\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9339\n",
      "Epoch 00077: val_loss did not improve from 0.25711\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.2028 - acc: 0.9339 - val_loss: 0.2621 - val_acc: 0.9301\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9360\n",
      "Epoch 00078: val_loss improved from 0.25711 to 0.25677, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/078-0.2568.hdf5\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.1947 - acc: 0.9360 - val_loss: 0.2568 - val_acc: 0.9308\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9370\n",
      "Epoch 00079: val_loss improved from 0.25677 to 0.24787, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/079-0.2479.hdf5\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1946 - acc: 0.9370 - val_loss: 0.2479 - val_acc: 0.9334\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9361\n",
      "Epoch 00080: val_loss did not improve from 0.24787\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1952 - acc: 0.9361 - val_loss: 0.2544 - val_acc: 0.9350\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9394\n",
      "Epoch 00081: val_loss did not improve from 0.24787\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.1898 - acc: 0.9394 - val_loss: 0.2601 - val_acc: 0.9276\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9379\n",
      "Epoch 00082: val_loss did not improve from 0.24787\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.1872 - acc: 0.9379 - val_loss: 0.2578 - val_acc: 0.9283\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9403\n",
      "Epoch 00083: val_loss did not improve from 0.24787\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.1828 - acc: 0.9403 - val_loss: 0.2768 - val_acc: 0.9206\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9404\n",
      "Epoch 00084: val_loss did not improve from 0.24787\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1843 - acc: 0.9404 - val_loss: 0.2531 - val_acc: 0.9329\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9390\n",
      "Epoch 00085: val_loss did not improve from 0.24787\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1832 - acc: 0.9390 - val_loss: 0.2540 - val_acc: 0.9329\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9406\n",
      "Epoch 00086: val_loss did not improve from 0.24787\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1798 - acc: 0.9406 - val_loss: 0.2595 - val_acc: 0.9317\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9415\n",
      "Epoch 00087: val_loss did not improve from 0.24787\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.1797 - acc: 0.9416 - val_loss: 0.2532 - val_acc: 0.9311\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9428\n",
      "Epoch 00088: val_loss did not improve from 0.24787\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1733 - acc: 0.9428 - val_loss: 0.2512 - val_acc: 0.9336\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9434\n",
      "Epoch 00089: val_loss did not improve from 0.24787\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1725 - acc: 0.9434 - val_loss: 0.2514 - val_acc: 0.9362\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9442\n",
      "Epoch 00090: val_loss improved from 0.24787 to 0.24505, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/090-0.2450.hdf5\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.1692 - acc: 0.9442 - val_loss: 0.2450 - val_acc: 0.9338\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9449\n",
      "Epoch 00091: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1687 - acc: 0.9448 - val_loss: 0.2643 - val_acc: 0.9292\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9440\n",
      "Epoch 00092: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1668 - acc: 0.9441 - val_loss: 0.2637 - val_acc: 0.9324\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9455\n",
      "Epoch 00093: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.1643 - acc: 0.9456 - val_loss: 0.2559 - val_acc: 0.9317\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9443\n",
      "Epoch 00094: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.1670 - acc: 0.9444 - val_loss: 0.2594 - val_acc: 0.9317\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9450\n",
      "Epoch 00095: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.1598 - acc: 0.9450 - val_loss: 0.2684 - val_acc: 0.9290\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.9449\n",
      "Epoch 00096: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1621 - acc: 0.9449 - val_loss: 0.2592 - val_acc: 0.9317\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9472\n",
      "Epoch 00097: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1599 - acc: 0.9472 - val_loss: 0.2579 - val_acc: 0.9345\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9474\n",
      "Epoch 00098: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1604 - acc: 0.9475 - val_loss: 0.2602 - val_acc: 0.9320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9480\n",
      "Epoch 00099: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1587 - acc: 0.9480 - val_loss: 0.2534 - val_acc: 0.9320\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9479\n",
      "Epoch 00100: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1560 - acc: 0.9479 - val_loss: 0.2464 - val_acc: 0.9355\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9473\n",
      "Epoch 00101: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1555 - acc: 0.9473 - val_loss: 0.2562 - val_acc: 0.9345\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9486\n",
      "Epoch 00102: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1538 - acc: 0.9486 - val_loss: 0.2590 - val_acc: 0.9317\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9476\n",
      "Epoch 00103: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.1534 - acc: 0.9476 - val_loss: 0.2517 - val_acc: 0.9350\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9514\n",
      "Epoch 00104: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.1467 - acc: 0.9514 - val_loss: 0.2618 - val_acc: 0.9364\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9519\n",
      "Epoch 00105: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1436 - acc: 0.9519 - val_loss: 0.2538 - val_acc: 0.9341\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9501\n",
      "Epoch 00106: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.1472 - acc: 0.9501 - val_loss: 0.2629 - val_acc: 0.9334\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9519\n",
      "Epoch 00107: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1445 - acc: 0.9519 - val_loss: 0.2659 - val_acc: 0.9338\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9513\n",
      "Epoch 00108: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.1444 - acc: 0.9512 - val_loss: 0.2527 - val_acc: 0.9366\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9526\n",
      "Epoch 00109: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.1408 - acc: 0.9526 - val_loss: 0.2752 - val_acc: 0.9306\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9548\n",
      "Epoch 00110: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.1357 - acc: 0.9548 - val_loss: 0.2726 - val_acc: 0.9297\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9540\n",
      "Epoch 00111: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.1406 - acc: 0.9541 - val_loss: 0.2574 - val_acc: 0.9357\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9530\n",
      "Epoch 00112: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.1387 - acc: 0.9530 - val_loss: 0.2581 - val_acc: 0.9357\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9545\n",
      "Epoch 00113: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1341 - acc: 0.9545 - val_loss: 0.2600 - val_acc: 0.9343\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9553\n",
      "Epoch 00114: val_loss did not improve from 0.24505\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1308 - acc: 0.9553 - val_loss: 0.2532 - val_acc: 0.9369\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9553\n",
      "Epoch 00115: val_loss improved from 0.24505 to 0.24243, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv_checkpoint/115-0.2424.hdf5\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.1323 - acc: 0.9553 - val_loss: 0.2424 - val_acc: 0.9359\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9563\n",
      "Epoch 00116: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1311 - acc: 0.9563 - val_loss: 0.2696 - val_acc: 0.9299\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9557\n",
      "Epoch 00117: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.1332 - acc: 0.9557 - val_loss: 0.2565 - val_acc: 0.9366\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9568\n",
      "Epoch 00118: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.1279 - acc: 0.9568 - val_loss: 0.2556 - val_acc: 0.9350\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9566\n",
      "Epoch 00119: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1285 - acc: 0.9566 - val_loss: 0.2679 - val_acc: 0.9329\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9566\n",
      "Epoch 00120: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1292 - acc: 0.9566 - val_loss: 0.2700 - val_acc: 0.9350\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9569\n",
      "Epoch 00121: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1257 - acc: 0.9569 - val_loss: 0.2556 - val_acc: 0.9385\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9574\n",
      "Epoch 00122: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1250 - acc: 0.9574 - val_loss: 0.2580 - val_acc: 0.9390\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9566\n",
      "Epoch 00123: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1281 - acc: 0.9566 - val_loss: 0.2481 - val_acc: 0.9373\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9580\n",
      "Epoch 00124: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1256 - acc: 0.9580 - val_loss: 0.2564 - val_acc: 0.9378\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9601\n",
      "Epoch 00125: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1207 - acc: 0.9601 - val_loss: 0.2664 - val_acc: 0.9352\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9582\n",
      "Epoch 00126: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1226 - acc: 0.9582 - val_loss: 0.2591 - val_acc: 0.9364\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9588\n",
      "Epoch 00127: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1214 - acc: 0.9588 - val_loss: 0.2469 - val_acc: 0.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9614\n",
      "Epoch 00128: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1179 - acc: 0.9614 - val_loss: 0.2585 - val_acc: 0.9362\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9588\n",
      "Epoch 00129: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1198 - acc: 0.9588 - val_loss: 0.2676 - val_acc: 0.9364\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9598\n",
      "Epoch 00130: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.1167 - acc: 0.9598 - val_loss: 0.2837 - val_acc: 0.9299\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9600\n",
      "Epoch 00131: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1182 - acc: 0.9600 - val_loss: 0.2612 - val_acc: 0.9376\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9616\n",
      "Epoch 00132: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1142 - acc: 0.9616 - val_loss: 0.2606 - val_acc: 0.9371\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9608\n",
      "Epoch 00133: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1153 - acc: 0.9607 - val_loss: 0.2612 - val_acc: 0.9345\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9614\n",
      "Epoch 00134: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1131 - acc: 0.9613 - val_loss: 0.2590 - val_acc: 0.9341\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9608\n",
      "Epoch 00135: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1139 - acc: 0.9608 - val_loss: 0.2661 - val_acc: 0.9383\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9629\n",
      "Epoch 00136: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.1125 - acc: 0.9629 - val_loss: 0.2571 - val_acc: 0.9376\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9636\n",
      "Epoch 00137: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.1110 - acc: 0.9636 - val_loss: 0.2572 - val_acc: 0.9369\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9626\n",
      "Epoch 00138: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.1092 - acc: 0.9625 - val_loss: 0.2643 - val_acc: 0.9383\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9627\n",
      "Epoch 00139: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.1109 - acc: 0.9627 - val_loss: 0.2631 - val_acc: 0.9406\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9655\n",
      "Epoch 00140: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1055 - acc: 0.9655 - val_loss: 0.2645 - val_acc: 0.9376\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9639\n",
      "Epoch 00141: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.1080 - acc: 0.9639 - val_loss: 0.2665 - val_acc: 0.9359\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9638\n",
      "Epoch 00142: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1085 - acc: 0.9638 - val_loss: 0.2666 - val_acc: 0.9380\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9635\n",
      "Epoch 00143: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.1066 - acc: 0.9635 - val_loss: 0.2727 - val_acc: 0.9348\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9636\n",
      "Epoch 00144: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.1042 - acc: 0.9636 - val_loss: 0.2522 - val_acc: 0.9364\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9648\n",
      "Epoch 00145: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1047 - acc: 0.9648 - val_loss: 0.2580 - val_acc: 0.9387\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9660\n",
      "Epoch 00146: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.1002 - acc: 0.9660 - val_loss: 0.2576 - val_acc: 0.9392\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9648\n",
      "Epoch 00147: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.1042 - acc: 0.9648 - val_loss: 0.2603 - val_acc: 0.9397\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9652\n",
      "Epoch 00148: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1026 - acc: 0.9652 - val_loss: 0.2510 - val_acc: 0.9380\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9648\n",
      "Epoch 00149: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.1029 - acc: 0.9648 - val_loss: 0.2564 - val_acc: 0.9380\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9667\n",
      "Epoch 00150: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.1012 - acc: 0.9667 - val_loss: 0.2597 - val_acc: 0.9392\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9659\n",
      "Epoch 00151: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.1010 - acc: 0.9659 - val_loss: 0.2547 - val_acc: 0.9380\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9665\n",
      "Epoch 00152: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.0988 - acc: 0.9665 - val_loss: 0.2706 - val_acc: 0.9343\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9671\n",
      "Epoch 00153: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.0985 - acc: 0.9672 - val_loss: 0.2570 - val_acc: 0.9387\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9664\n",
      "Epoch 00154: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.0969 - acc: 0.9664 - val_loss: 0.2564 - val_acc: 0.9401\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9664\n",
      "Epoch 00155: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.0953 - acc: 0.9664 - val_loss: 0.2663 - val_acc: 0.9366\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9670\n",
      "Epoch 00156: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0975 - acc: 0.9670 - val_loss: 0.2535 - val_acc: 0.9411\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9677\n",
      "Epoch 00157: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0961 - acc: 0.9677 - val_loss: 0.2598 - val_acc: 0.9397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9689\n",
      "Epoch 00158: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0934 - acc: 0.9689 - val_loss: 0.2759 - val_acc: 0.9394\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9691\n",
      "Epoch 00159: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0916 - acc: 0.9691 - val_loss: 0.2581 - val_acc: 0.9383\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9673\n",
      "Epoch 00160: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0942 - acc: 0.9673 - val_loss: 0.2679 - val_acc: 0.9397\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9693\n",
      "Epoch 00161: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0909 - acc: 0.9693 - val_loss: 0.2722 - val_acc: 0.9378\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9704\n",
      "Epoch 00162: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.0896 - acc: 0.9704 - val_loss: 0.2664 - val_acc: 0.9392\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9697\n",
      "Epoch 00163: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0912 - acc: 0.9697 - val_loss: 0.2699 - val_acc: 0.9387\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9693\n",
      "Epoch 00164: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0914 - acc: 0.9693 - val_loss: 0.2688 - val_acc: 0.9369\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9686\n",
      "Epoch 00165: val_loss did not improve from 0.24243\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0928 - acc: 0.9686 - val_loss: 0.2548 - val_acc: 0.9378\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 292us/sample - loss: 0.2991 - acc: 0.9184\n",
      "Loss: 0.2990781063602721 Accuracy: 0.9183801\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5098 - acc: 0.1788\n",
      "Epoch 00001: val_loss improved from inf to 1.91540, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/001-1.9154.hdf5\n",
      "36805/36805 [==============================] - 22s 595us/sample - loss: 2.5098 - acc: 0.1788 - val_loss: 1.9154 - val_acc: 0.4132\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7599 - acc: 0.4234\n",
      "Epoch 00002: val_loss improved from 1.91540 to 1.38253, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/002-1.3825.hdf5\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 1.7598 - acc: 0.4234 - val_loss: 1.3825 - val_acc: 0.5563\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4755 - acc: 0.5125\n",
      "Epoch 00003: val_loss improved from 1.38253 to 1.20754, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/003-1.2075.hdf5\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 1.4755 - acc: 0.5125 - val_loss: 1.2075 - val_acc: 0.6268\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3081 - acc: 0.5691\n",
      "Epoch 00004: val_loss improved from 1.20754 to 1.05518, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/004-1.0552.hdf5\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 1.3081 - acc: 0.5691 - val_loss: 1.0552 - val_acc: 0.6711\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1873 - acc: 0.6174\n",
      "Epoch 00005: val_loss improved from 1.05518 to 0.95457, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/005-0.9546.hdf5\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 1.1878 - acc: 0.6172 - val_loss: 0.9546 - val_acc: 0.7119\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0908 - acc: 0.6507\n",
      "Epoch 00006: val_loss improved from 0.95457 to 0.86746, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/006-0.8675.hdf5\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 1.0909 - acc: 0.6507 - val_loss: 0.8675 - val_acc: 0.7524\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9994 - acc: 0.6828\n",
      "Epoch 00007: val_loss improved from 0.86746 to 0.79307, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/007-0.7931.hdf5\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.9996 - acc: 0.6828 - val_loss: 0.7931 - val_acc: 0.7708\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9185 - acc: 0.7105\n",
      "Epoch 00008: val_loss improved from 0.79307 to 0.74853, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/008-0.7485.hdf5\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.9186 - acc: 0.7105 - val_loss: 0.7485 - val_acc: 0.7745\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8542 - acc: 0.7313\n",
      "Epoch 00009: val_loss improved from 0.74853 to 0.65953, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/009-0.6595.hdf5\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.8542 - acc: 0.7313 - val_loss: 0.6595 - val_acc: 0.8088\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7892 - acc: 0.7535\n",
      "Epoch 00010: val_loss improved from 0.65953 to 0.61683, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/010-0.6168.hdf5\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.7892 - acc: 0.7534 - val_loss: 0.6168 - val_acc: 0.8227\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7448 - acc: 0.7664\n",
      "Epoch 00011: val_loss improved from 0.61683 to 0.56304, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/011-0.5630.hdf5\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.7445 - acc: 0.7665 - val_loss: 0.5630 - val_acc: 0.8367\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.7836\n",
      "Epoch 00012: val_loss improved from 0.56304 to 0.52591, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/012-0.5259.hdf5\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.6938 - acc: 0.7836 - val_loss: 0.5259 - val_acc: 0.8472\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6535 - acc: 0.7961\n",
      "Epoch 00013: val_loss improved from 0.52591 to 0.51196, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/013-0.5120.hdf5\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.6534 - acc: 0.7961 - val_loss: 0.5120 - val_acc: 0.8523\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6208 - acc: 0.8067\n",
      "Epoch 00014: val_loss improved from 0.51196 to 0.46159, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/014-0.4616.hdf5\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.6208 - acc: 0.8067 - val_loss: 0.4616 - val_acc: 0.8717\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5827 - acc: 0.8187\n",
      "Epoch 00015: val_loss improved from 0.46159 to 0.45071, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/015-0.4507.hdf5\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.5827 - acc: 0.8186 - val_loss: 0.4507 - val_acc: 0.8733\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5559 - acc: 0.8275\n",
      "Epoch 00016: val_loss improved from 0.45071 to 0.42040, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/016-0.4204.hdf5\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.5559 - acc: 0.8275 - val_loss: 0.4204 - val_acc: 0.8805\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.8371\n",
      "Epoch 00017: val_loss improved from 0.42040 to 0.39163, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/017-0.3916.hdf5\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.5278 - acc: 0.8371 - val_loss: 0.3916 - val_acc: 0.8889\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.8413\n",
      "Epoch 00018: val_loss improved from 0.39163 to 0.37671, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/018-0.3767.hdf5\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.5069 - acc: 0.8413 - val_loss: 0.3767 - val_acc: 0.8924\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4842 - acc: 0.8514\n",
      "Epoch 00019: val_loss improved from 0.37671 to 0.35529, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/019-0.3553.hdf5\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.4841 - acc: 0.8514 - val_loss: 0.3553 - val_acc: 0.8982\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4580 - acc: 0.8580\n",
      "Epoch 00020: val_loss improved from 0.35529 to 0.34945, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/020-0.3494.hdf5\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.4578 - acc: 0.8581 - val_loss: 0.3494 - val_acc: 0.8970\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8608\n",
      "Epoch 00021: val_loss improved from 0.34945 to 0.33619, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/021-0.3362.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.4467 - acc: 0.8608 - val_loss: 0.3362 - val_acc: 0.9040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4268 - acc: 0.8658\n",
      "Epoch 00022: val_loss improved from 0.33619 to 0.31545, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/022-0.3155.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.4267 - acc: 0.8658 - val_loss: 0.3155 - val_acc: 0.9073\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4170 - acc: 0.8708\n",
      "Epoch 00023: val_loss did not improve from 0.31545\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.4170 - acc: 0.8708 - val_loss: 0.3197 - val_acc: 0.9071\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8739\n",
      "Epoch 00024: val_loss improved from 0.31545 to 0.31308, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/024-0.3131.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.4006 - acc: 0.8739 - val_loss: 0.3131 - val_acc: 0.9052\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8795\n",
      "Epoch 00025: val_loss improved from 0.31308 to 0.29293, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/025-0.2929.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.3839 - acc: 0.8795 - val_loss: 0.2929 - val_acc: 0.9150\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8835\n",
      "Epoch 00026: val_loss improved from 0.29293 to 0.27617, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/026-0.2762.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.3745 - acc: 0.8834 - val_loss: 0.2762 - val_acc: 0.9189\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8870\n",
      "Epoch 00027: val_loss did not improve from 0.27617\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.3630 - acc: 0.8870 - val_loss: 0.2803 - val_acc: 0.9140\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8892\n",
      "Epoch 00028: val_loss improved from 0.27617 to 0.25924, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/028-0.2592.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.3495 - acc: 0.8892 - val_loss: 0.2592 - val_acc: 0.9206\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.8924\n",
      "Epoch 00029: val_loss improved from 0.25924 to 0.25250, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/029-0.2525.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.3418 - acc: 0.8924 - val_loss: 0.2525 - val_acc: 0.9236\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8963\n",
      "Epoch 00030: val_loss did not improve from 0.25250\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.3299 - acc: 0.8963 - val_loss: 0.2571 - val_acc: 0.9229\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.8979\n",
      "Epoch 00031: val_loss improved from 0.25250 to 0.24667, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/031-0.2467.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3214 - acc: 0.8979 - val_loss: 0.2467 - val_acc: 0.9257\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.9021\n",
      "Epoch 00032: val_loss improved from 0.24667 to 0.23650, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/032-0.2365.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.3140 - acc: 0.9021 - val_loss: 0.2365 - val_acc: 0.9280\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3084 - acc: 0.9019\n",
      "Epoch 00033: val_loss did not improve from 0.23650\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.3084 - acc: 0.9019 - val_loss: 0.2385 - val_acc: 0.9271\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.9073\n",
      "Epoch 00034: val_loss did not improve from 0.23650\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2978 - acc: 0.9073 - val_loss: 0.2383 - val_acc: 0.9283\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9071\n",
      "Epoch 00035: val_loss improved from 0.23650 to 0.22726, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/035-0.2273.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2921 - acc: 0.9071 - val_loss: 0.2273 - val_acc: 0.9285\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9091\n",
      "Epoch 00036: val_loss did not improve from 0.22726\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2868 - acc: 0.9091 - val_loss: 0.2398 - val_acc: 0.9241\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9098\n",
      "Epoch 00037: val_loss did not improve from 0.22726\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2832 - acc: 0.9098 - val_loss: 0.2296 - val_acc: 0.9262\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9138\n",
      "Epoch 00038: val_loss improved from 0.22726 to 0.21911, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/038-0.2191.hdf5\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.2709 - acc: 0.9138 - val_loss: 0.2191 - val_acc: 0.9334\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9163\n",
      "Epoch 00039: val_loss did not improve from 0.21911\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.2668 - acc: 0.9163 - val_loss: 0.2452 - val_acc: 0.9241\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.9164\n",
      "Epoch 00040: val_loss did not improve from 0.21911\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.2625 - acc: 0.9164 - val_loss: 0.2201 - val_acc: 0.9304\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.9171\n",
      "Epoch 00041: val_loss improved from 0.21911 to 0.21449, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/041-0.2145.hdf5\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.2600 - acc: 0.9170 - val_loss: 0.2145 - val_acc: 0.9343\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9185\n",
      "Epoch 00042: val_loss improved from 0.21449 to 0.20411, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/042-0.2041.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2507 - acc: 0.9185 - val_loss: 0.2041 - val_acc: 0.9427\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9230\n",
      "Epoch 00043: val_loss did not improve from 0.20411\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.2462 - acc: 0.9230 - val_loss: 0.2145 - val_acc: 0.9348\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9224\n",
      "Epoch 00044: val_loss improved from 0.20411 to 0.19595, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/044-0.1960.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.2457 - acc: 0.9224 - val_loss: 0.1960 - val_acc: 0.9427\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9225\n",
      "Epoch 00045: val_loss did not improve from 0.19595\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.2423 - acc: 0.9225 - val_loss: 0.2000 - val_acc: 0.9411\n",
      "Epoch 46/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9235\n",
      "Epoch 00046: val_loss improved from 0.19595 to 0.19123, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/046-0.1912.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.2369 - acc: 0.9235 - val_loss: 0.1912 - val_acc: 0.9425\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9230\n",
      "Epoch 00047: val_loss did not improve from 0.19123\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.2386 - acc: 0.9231 - val_loss: 0.2028 - val_acc: 0.9383\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9279\n",
      "Epoch 00048: val_loss did not improve from 0.19123\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.2256 - acc: 0.9279 - val_loss: 0.1945 - val_acc: 0.9413\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9284\n",
      "Epoch 00049: val_loss did not improve from 0.19123\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2278 - acc: 0.9284 - val_loss: 0.1981 - val_acc: 0.9390\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9319\n",
      "Epoch 00050: val_loss improved from 0.19123 to 0.18074, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/050-0.1807.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2156 - acc: 0.9319 - val_loss: 0.1807 - val_acc: 0.9455\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9305\n",
      "Epoch 00051: val_loss did not improve from 0.18074\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.2175 - acc: 0.9305 - val_loss: 0.1884 - val_acc: 0.9429\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9315\n",
      "Epoch 00052: val_loss did not improve from 0.18074\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2152 - acc: 0.9316 - val_loss: 0.1825 - val_acc: 0.9450\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9332\n",
      "Epoch 00053: val_loss improved from 0.18074 to 0.17900, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/053-0.1790.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.2108 - acc: 0.9332 - val_loss: 0.1790 - val_acc: 0.9488\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9331\n",
      "Epoch 00054: val_loss improved from 0.17900 to 0.17367, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/054-0.1737.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.2079 - acc: 0.9331 - val_loss: 0.1737 - val_acc: 0.9495\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9360\n",
      "Epoch 00055: val_loss improved from 0.17367 to 0.17357, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/055-0.1736.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.1979 - acc: 0.9360 - val_loss: 0.1736 - val_acc: 0.9511\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9355\n",
      "Epoch 00056: val_loss improved from 0.17357 to 0.17342, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/056-0.1734.hdf5\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.1994 - acc: 0.9355 - val_loss: 0.1734 - val_acc: 0.9488\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9364\n",
      "Epoch 00057: val_loss did not improve from 0.17342\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1949 - acc: 0.9364 - val_loss: 0.1904 - val_acc: 0.9422\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9370\n",
      "Epoch 00058: val_loss did not improve from 0.17342\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1951 - acc: 0.9370 - val_loss: 0.1875 - val_acc: 0.9434\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9377\n",
      "Epoch 00059: val_loss improved from 0.17342 to 0.17124, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/059-0.1712.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1921 - acc: 0.9376 - val_loss: 0.1712 - val_acc: 0.9520\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9389\n",
      "Epoch 00060: val_loss improved from 0.17124 to 0.16661, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/060-0.1666.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1916 - acc: 0.9389 - val_loss: 0.1666 - val_acc: 0.9509\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9408\n",
      "Epoch 00061: val_loss did not improve from 0.16661\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1845 - acc: 0.9409 - val_loss: 0.1741 - val_acc: 0.9474\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9411\n",
      "Epoch 00062: val_loss did not improve from 0.16661\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.1796 - acc: 0.9410 - val_loss: 0.1757 - val_acc: 0.9457\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9410\n",
      "Epoch 00063: val_loss did not improve from 0.16661\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.1801 - acc: 0.9410 - val_loss: 0.1697 - val_acc: 0.9509\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9415\n",
      "Epoch 00064: val_loss did not improve from 0.16661\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.1795 - acc: 0.9415 - val_loss: 0.1669 - val_acc: 0.9499\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9440\n",
      "Epoch 00065: val_loss did not improve from 0.16661\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1711 - acc: 0.9440 - val_loss: 0.1742 - val_acc: 0.9490\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9447\n",
      "Epoch 00066: val_loss did not improve from 0.16661\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.1699 - acc: 0.9447 - val_loss: 0.1722 - val_acc: 0.9460\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9439\n",
      "Epoch 00067: val_loss did not improve from 0.16661\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.1725 - acc: 0.9439 - val_loss: 0.1951 - val_acc: 0.9436\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9435\n",
      "Epoch 00068: val_loss improved from 0.16661 to 0.16524, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/068-0.1652.hdf5\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.1698 - acc: 0.9435 - val_loss: 0.1652 - val_acc: 0.9525\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9466\n",
      "Epoch 00069: val_loss did not improve from 0.16524\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.1641 - acc: 0.9466 - val_loss: 0.1685 - val_acc: 0.9485\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9459\n",
      "Epoch 00070: val_loss did not improve from 0.16524\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.1640 - acc: 0.9459 - val_loss: 0.2017 - val_acc: 0.9401\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9457\n",
      "Epoch 00071: val_loss improved from 0.16524 to 0.15630, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/071-0.1563.hdf5\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.1670 - acc: 0.9457 - val_loss: 0.1563 - val_acc: 0.9525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9475\n",
      "Epoch 00072: val_loss did not improve from 0.15630\n",
      "36805/36805 [==============================] - 18s 503us/sample - loss: 0.1592 - acc: 0.9475 - val_loss: 0.1710 - val_acc: 0.9483\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9475\n",
      "Epoch 00073: val_loss improved from 0.15630 to 0.15606, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/073-0.1561.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.1597 - acc: 0.9475 - val_loss: 0.1561 - val_acc: 0.9541\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9466\n",
      "Epoch 00074: val_loss did not improve from 0.15606\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.1605 - acc: 0.9466 - val_loss: 0.1648 - val_acc: 0.9506\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9494\n",
      "Epoch 00075: val_loss did not improve from 0.15606\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.1542 - acc: 0.9494 - val_loss: 0.1615 - val_acc: 0.9527\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9503\n",
      "Epoch 00076: val_loss improved from 0.15606 to 0.15255, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/076-0.1526.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.1502 - acc: 0.9503 - val_loss: 0.1526 - val_acc: 0.9553\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9488\n",
      "Epoch 00077: val_loss did not improve from 0.15255\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1541 - acc: 0.9488 - val_loss: 0.1545 - val_acc: 0.9560\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9504\n",
      "Epoch 00078: val_loss did not improve from 0.15255\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1478 - acc: 0.9504 - val_loss: 0.1542 - val_acc: 0.9562\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9492\n",
      "Epoch 00079: val_loss did not improve from 0.15255\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1505 - acc: 0.9492 - val_loss: 0.1648 - val_acc: 0.9504\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9519\n",
      "Epoch 00080: val_loss improved from 0.15255 to 0.14846, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/080-0.1485.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.1426 - acc: 0.9519 - val_loss: 0.1485 - val_acc: 0.9583\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9525\n",
      "Epoch 00081: val_loss did not improve from 0.14846\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1422 - acc: 0.9525 - val_loss: 0.1574 - val_acc: 0.9536\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9544\n",
      "Epoch 00082: val_loss did not improve from 0.14846\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1399 - acc: 0.9544 - val_loss: 0.1558 - val_acc: 0.9518\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9539\n",
      "Epoch 00083: val_loss did not improve from 0.14846\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1394 - acc: 0.9539 - val_loss: 0.1617 - val_acc: 0.9553\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9535\n",
      "Epoch 00084: val_loss did not improve from 0.14846\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1394 - acc: 0.9535 - val_loss: 0.1619 - val_acc: 0.9553\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9548\n",
      "Epoch 00085: val_loss did not improve from 0.14846\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1345 - acc: 0.9548 - val_loss: 0.1550 - val_acc: 0.9578\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9556\n",
      "Epoch 00086: val_loss did not improve from 0.14846\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1342 - acc: 0.9556 - val_loss: 0.1520 - val_acc: 0.9576\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9535\n",
      "Epoch 00087: val_loss did not improve from 0.14846\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1367 - acc: 0.9535 - val_loss: 0.1582 - val_acc: 0.9515\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9545\n",
      "Epoch 00088: val_loss improved from 0.14846 to 0.14716, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/088-0.1472.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1332 - acc: 0.9545 - val_loss: 0.1472 - val_acc: 0.9574\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9574\n",
      "Epoch 00089: val_loss did not improve from 0.14716\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1314 - acc: 0.9574 - val_loss: 0.1617 - val_acc: 0.9560\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9566\n",
      "Epoch 00090: val_loss did not improve from 0.14716\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1285 - acc: 0.9566 - val_loss: 0.1552 - val_acc: 0.9527\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9567\n",
      "Epoch 00091: val_loss did not improve from 0.14716\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1301 - acc: 0.9567 - val_loss: 0.1524 - val_acc: 0.9604\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9576\n",
      "Epoch 00092: val_loss did not improve from 0.14716\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1260 - acc: 0.9576 - val_loss: 0.1530 - val_acc: 0.9576\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9556\n",
      "Epoch 00093: val_loss did not improve from 0.14716\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1277 - acc: 0.9556 - val_loss: 0.1556 - val_acc: 0.9557\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9578\n",
      "Epoch 00094: val_loss did not improve from 0.14716\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.1243 - acc: 0.9578 - val_loss: 0.1552 - val_acc: 0.9578\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9581\n",
      "Epoch 00095: val_loss did not improve from 0.14716\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.1212 - acc: 0.9581 - val_loss: 0.1746 - val_acc: 0.9462\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9597\n",
      "Epoch 00096: val_loss did not improve from 0.14716\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1214 - acc: 0.9597 - val_loss: 0.1532 - val_acc: 0.9585\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9606\n",
      "Epoch 00097: val_loss did not improve from 0.14716\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1174 - acc: 0.9606 - val_loss: 0.1485 - val_acc: 0.9550\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9601\n",
      "Epoch 00098: val_loss improved from 0.14716 to 0.14698, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/098-0.1470.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1187 - acc: 0.9601 - val_loss: 0.1470 - val_acc: 0.9595\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9599\n",
      "Epoch 00099: val_loss improved from 0.14698 to 0.14187, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv_checkpoint/099-0.1419.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1167 - acc: 0.9599 - val_loss: 0.1419 - val_acc: 0.9581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9601\n",
      "Epoch 00100: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.1186 - acc: 0.9601 - val_loss: 0.1441 - val_acc: 0.9581\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9597\n",
      "Epoch 00101: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.1157 - acc: 0.9597 - val_loss: 0.1429 - val_acc: 0.9613\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9616\n",
      "Epoch 00102: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.1131 - acc: 0.9616 - val_loss: 0.1621 - val_acc: 0.9560\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9616\n",
      "Epoch 00103: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.1115 - acc: 0.9616 - val_loss: 0.1518 - val_acc: 0.9583\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9624\n",
      "Epoch 00104: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.1101 - acc: 0.9624 - val_loss: 0.1510 - val_acc: 0.9583\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9620\n",
      "Epoch 00105: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.1094 - acc: 0.9620 - val_loss: 0.1529 - val_acc: 0.9564\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9641\n",
      "Epoch 00106: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.1053 - acc: 0.9641 - val_loss: 0.1455 - val_acc: 0.9585\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9611\n",
      "Epoch 00107: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.1120 - acc: 0.9611 - val_loss: 0.1532 - val_acc: 0.9546\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9626\n",
      "Epoch 00108: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.1091 - acc: 0.9626 - val_loss: 0.1483 - val_acc: 0.9571\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9635\n",
      "Epoch 00109: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.1060 - acc: 0.9635 - val_loss: 0.1497 - val_acc: 0.9599\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9648\n",
      "Epoch 00110: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.1010 - acc: 0.9648 - val_loss: 0.1581 - val_acc: 0.9595\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9641\n",
      "Epoch 00111: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.1058 - acc: 0.9641 - val_loss: 0.1481 - val_acc: 0.9602\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9642\n",
      "Epoch 00112: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.1053 - acc: 0.9642 - val_loss: 0.1538 - val_acc: 0.9555\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9646\n",
      "Epoch 00113: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.1025 - acc: 0.9646 - val_loss: 0.1474 - val_acc: 0.9597\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9652\n",
      "Epoch 00114: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.1034 - acc: 0.9652 - val_loss: 0.1556 - val_acc: 0.9585\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9666\n",
      "Epoch 00115: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1007 - acc: 0.9666 - val_loss: 0.1525 - val_acc: 0.9604\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9660\n",
      "Epoch 00116: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.0995 - acc: 0.9660 - val_loss: 0.1635 - val_acc: 0.9564\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9668\n",
      "Epoch 00117: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0985 - acc: 0.9668 - val_loss: 0.1474 - val_acc: 0.9590\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9661\n",
      "Epoch 00118: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0999 - acc: 0.9661 - val_loss: 0.1560 - val_acc: 0.9574\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9665\n",
      "Epoch 00119: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0937 - acc: 0.9665 - val_loss: 0.1732 - val_acc: 0.9550\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9658\n",
      "Epoch 00120: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.0990 - acc: 0.9658 - val_loss: 0.1516 - val_acc: 0.9583\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9654\n",
      "Epoch 00121: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.0983 - acc: 0.9654 - val_loss: 0.1567 - val_acc: 0.9592\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9667\n",
      "Epoch 00122: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0958 - acc: 0.9667 - val_loss: 0.1475 - val_acc: 0.9609\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9677\n",
      "Epoch 00123: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0922 - acc: 0.9677 - val_loss: 0.1488 - val_acc: 0.9595\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9686\n",
      "Epoch 00124: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0913 - acc: 0.9686 - val_loss: 0.1502 - val_acc: 0.9571\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9674\n",
      "Epoch 00125: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0939 - acc: 0.9674 - val_loss: 0.1511 - val_acc: 0.9620\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9675\n",
      "Epoch 00126: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0932 - acc: 0.9675 - val_loss: 0.1589 - val_acc: 0.9611\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9692\n",
      "Epoch 00127: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0883 - acc: 0.9692 - val_loss: 0.1549 - val_acc: 0.9581\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9678\n",
      "Epoch 00128: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.0937 - acc: 0.9678 - val_loss: 0.1557 - val_acc: 0.9557\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9695\n",
      "Epoch 00129: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0891 - acc: 0.9695 - val_loss: 0.1451 - val_acc: 0.9625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9686\n",
      "Epoch 00130: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0920 - acc: 0.9686 - val_loss: 0.1462 - val_acc: 0.9609\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9689\n",
      "Epoch 00131: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0890 - acc: 0.9689 - val_loss: 0.1695 - val_acc: 0.9548\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9717\n",
      "Epoch 00132: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0844 - acc: 0.9717 - val_loss: 0.1508 - val_acc: 0.9590\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9704\n",
      "Epoch 00133: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.0856 - acc: 0.9704 - val_loss: 0.1566 - val_acc: 0.9604\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9708\n",
      "Epoch 00134: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0854 - acc: 0.9708 - val_loss: 0.1521 - val_acc: 0.9571\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9705\n",
      "Epoch 00135: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0872 - acc: 0.9705 - val_loss: 0.1520 - val_acc: 0.9595\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9697\n",
      "Epoch 00136: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.0855 - acc: 0.9697 - val_loss: 0.1495 - val_acc: 0.9606\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9708\n",
      "Epoch 00137: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0857 - acc: 0.9708 - val_loss: 0.1544 - val_acc: 0.9604\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9711\n",
      "Epoch 00138: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.0842 - acc: 0.9711 - val_loss: 0.1521 - val_acc: 0.9588\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9708\n",
      "Epoch 00139: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.0845 - acc: 0.9708 - val_loss: 0.1463 - val_acc: 0.9604\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9704\n",
      "Epoch 00140: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0835 - acc: 0.9704 - val_loss: 0.1539 - val_acc: 0.9597\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9721\n",
      "Epoch 00141: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.0807 - acc: 0.9721 - val_loss: 0.1595 - val_acc: 0.9599\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9711\n",
      "Epoch 00142: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.0794 - acc: 0.9711 - val_loss: 0.1650 - val_acc: 0.9569\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9718\n",
      "Epoch 00143: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0823 - acc: 0.9718 - val_loss: 0.1455 - val_acc: 0.9597\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9720\n",
      "Epoch 00144: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0821 - acc: 0.9720 - val_loss: 0.1595 - val_acc: 0.9578\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9740\n",
      "Epoch 00145: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.0776 - acc: 0.9740 - val_loss: 0.1474 - val_acc: 0.9604\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9734\n",
      "Epoch 00146: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0763 - acc: 0.9734 - val_loss: 0.1609 - val_acc: 0.9604\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9724\n",
      "Epoch 00147: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0793 - acc: 0.9724 - val_loss: 0.1458 - val_acc: 0.9602\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9720\n",
      "Epoch 00148: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.0792 - acc: 0.9720 - val_loss: 0.1540 - val_acc: 0.9590\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9735\n",
      "Epoch 00149: val_loss did not improve from 0.14187\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.0769 - acc: 0.9735 - val_loss: 0.1493 - val_acc: 0.9613\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 319us/sample - loss: 0.2009 - acc: 0.9396\n",
      "Loss: 0.200893817667154 Accuracy: 0.9395639\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4652 - acc: 0.1885\n",
      "Epoch 00001: val_loss improved from inf to 1.73186, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/001-1.7319.hdf5\n",
      "36805/36805 [==============================] - 25s 676us/sample - loss: 2.4643 - acc: 0.1887 - val_loss: 1.7319 - val_acc: 0.4563\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6584 - acc: 0.4522\n",
      "Epoch 00002: val_loss improved from 1.73186 to 1.24683, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/002-1.2468.hdf5\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 1.6583 - acc: 0.4522 - val_loss: 1.2468 - val_acc: 0.6280\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3184 - acc: 0.5688\n",
      "Epoch 00003: val_loss improved from 1.24683 to 1.03085, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/003-1.0309.hdf5\n",
      "36805/36805 [==============================] - 20s 535us/sample - loss: 1.3174 - acc: 0.5692 - val_loss: 1.0309 - val_acc: 0.6893\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1301 - acc: 0.6290\n",
      "Epoch 00004: val_loss improved from 1.03085 to 0.90033, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/004-0.9003.hdf5\n",
      "36805/36805 [==============================] - 20s 534us/sample - loss: 1.1300 - acc: 0.6290 - val_loss: 0.9003 - val_acc: 0.7242\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9890 - acc: 0.6785\n",
      "Epoch 00005: val_loss improved from 0.90033 to 0.72819, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/005-0.7282.hdf5\n",
      "36805/36805 [==============================] - 20s 535us/sample - loss: 0.9890 - acc: 0.6784 - val_loss: 0.7282 - val_acc: 0.7778\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8809 - acc: 0.7126\n",
      "Epoch 00006: val_loss improved from 0.72819 to 0.64178, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/006-0.6418.hdf5\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.8808 - acc: 0.7126 - val_loss: 0.6418 - val_acc: 0.8050\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7880 - acc: 0.7447\n",
      "Epoch 00007: val_loss improved from 0.64178 to 0.57237, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/007-0.5724.hdf5\n",
      "36805/36805 [==============================] - 20s 533us/sample - loss: 0.7879 - acc: 0.7448 - val_loss: 0.5724 - val_acc: 0.8255\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7250 - acc: 0.7681\n",
      "Epoch 00008: val_loss improved from 0.57237 to 0.52494, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/008-0.5249.hdf5\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.7251 - acc: 0.7680 - val_loss: 0.5249 - val_acc: 0.8442\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6654 - acc: 0.7858\n",
      "Epoch 00009: val_loss improved from 0.52494 to 0.48551, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/009-0.4855.hdf5\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.6653 - acc: 0.7858 - val_loss: 0.4855 - val_acc: 0.8500\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6129 - acc: 0.8050\n",
      "Epoch 00010: val_loss improved from 0.48551 to 0.42867, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/010-0.4287.hdf5\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.6129 - acc: 0.8049 - val_loss: 0.4287 - val_acc: 0.8682\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5729 - acc: 0.8173\n",
      "Epoch 00011: val_loss improved from 0.42867 to 0.39936, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/011-0.3994.hdf5\n",
      "36805/36805 [==============================] - 20s 534us/sample - loss: 0.5728 - acc: 0.8173 - val_loss: 0.3994 - val_acc: 0.8807\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.8312\n",
      "Epoch 00012: val_loss improved from 0.39936 to 0.37213, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/012-0.3721.hdf5\n",
      "36805/36805 [==============================] - 20s 533us/sample - loss: 0.5296 - acc: 0.8312 - val_loss: 0.3721 - val_acc: 0.8887\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5048 - acc: 0.8389\n",
      "Epoch 00013: val_loss improved from 0.37213 to 0.35583, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/013-0.3558.hdf5\n",
      "36805/36805 [==============================] - 20s 538us/sample - loss: 0.5048 - acc: 0.8389 - val_loss: 0.3558 - val_acc: 0.8924\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4812 - acc: 0.8448\n",
      "Epoch 00014: val_loss improved from 0.35583 to 0.33650, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/014-0.3365.hdf5\n",
      "36805/36805 [==============================] - 20s 534us/sample - loss: 0.4812 - acc: 0.8448 - val_loss: 0.3365 - val_acc: 0.8938\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8580\n",
      "Epoch 00015: val_loss improved from 0.33650 to 0.31864, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/015-0.3186.hdf5\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.4493 - acc: 0.8580 - val_loss: 0.3186 - val_acc: 0.9031\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.8654\n",
      "Epoch 00016: val_loss improved from 0.31864 to 0.30945, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/016-0.3095.hdf5\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.4246 - acc: 0.8654 - val_loss: 0.3095 - val_acc: 0.8973\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8696\n",
      "Epoch 00017: val_loss improved from 0.30945 to 0.28877, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/017-0.2888.hdf5\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.4056 - acc: 0.8696 - val_loss: 0.2888 - val_acc: 0.9147\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8760\n",
      "Epoch 00018: val_loss did not improve from 0.28877\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.3901 - acc: 0.8760 - val_loss: 0.3068 - val_acc: 0.9066\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8818\n",
      "Epoch 00019: val_loss improved from 0.28877 to 0.26714, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/019-0.2671.hdf5\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.3756 - acc: 0.8818 - val_loss: 0.2671 - val_acc: 0.9145\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8861\n",
      "Epoch 00020: val_loss did not improve from 0.26714\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.3561 - acc: 0.8861 - val_loss: 0.2687 - val_acc: 0.9159\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8912\n",
      "Epoch 00021: val_loss improved from 0.26714 to 0.23881, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/021-0.2388.hdf5\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.3438 - acc: 0.8912 - val_loss: 0.2388 - val_acc: 0.9285\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8938\n",
      "Epoch 00022: val_loss improved from 0.23881 to 0.23874, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/022-0.2387.hdf5\n",
      "36805/36805 [==============================] - 20s 535us/sample - loss: 0.3351 - acc: 0.8938 - val_loss: 0.2387 - val_acc: 0.9287\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.8994\n",
      "Epoch 00023: val_loss did not improve from 0.23874\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.3192 - acc: 0.8994 - val_loss: 0.2413 - val_acc: 0.9245\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.9008\n",
      "Epoch 00024: val_loss improved from 0.23874 to 0.22198, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/024-0.2220.hdf5\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.3136 - acc: 0.9008 - val_loss: 0.2220 - val_acc: 0.9304\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9044\n",
      "Epoch 00025: val_loss improved from 0.22198 to 0.21923, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/025-0.2192.hdf5\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.3031 - acc: 0.9044 - val_loss: 0.2192 - val_acc: 0.9338\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2898 - acc: 0.9081\n",
      "Epoch 00026: val_loss did not improve from 0.21923\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.2898 - acc: 0.9081 - val_loss: 0.2214 - val_acc: 0.9297\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.9094\n",
      "Epoch 00027: val_loss did not improve from 0.21923\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.2812 - acc: 0.9093 - val_loss: 0.2228 - val_acc: 0.9257\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9145\n",
      "Epoch 00028: val_loss improved from 0.21923 to 0.20851, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/028-0.2085.hdf5\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.2740 - acc: 0.9145 - val_loss: 0.2085 - val_acc: 0.9345\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9168\n",
      "Epoch 00029: val_loss improved from 0.20851 to 0.18844, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/029-0.1884.hdf5\n",
      "36805/36805 [==============================] - 20s 533us/sample - loss: 0.2631 - acc: 0.9167 - val_loss: 0.1884 - val_acc: 0.9399\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9186\n",
      "Epoch 00030: val_loss improved from 0.18844 to 0.18808, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/030-0.1881.hdf5\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.2589 - acc: 0.9187 - val_loss: 0.1881 - val_acc: 0.9420\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9202\n",
      "Epoch 00031: val_loss improved from 0.18808 to 0.18666, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/031-0.1867.hdf5\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.2505 - acc: 0.9202 - val_loss: 0.1867 - val_acc: 0.9432\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9212\n",
      "Epoch 00032: val_loss improved from 0.18666 to 0.18570, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/032-0.1857.hdf5\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.2434 - acc: 0.9212 - val_loss: 0.1857 - val_acc: 0.9415\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9257\n",
      "Epoch 00033: val_loss improved from 0.18570 to 0.18098, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/033-0.1810.hdf5\n",
      "36805/36805 [==============================] - 19s 530us/sample - loss: 0.2351 - acc: 0.9257 - val_loss: 0.1810 - val_acc: 0.9446\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9250\n",
      "Epoch 00034: val_loss did not improve from 0.18098\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.2299 - acc: 0.9250 - val_loss: 0.1929 - val_acc: 0.9408\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9269\n",
      "Epoch 00035: val_loss did not improve from 0.18098\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.2276 - acc: 0.9269 - val_loss: 0.1847 - val_acc: 0.9418\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9278\n",
      "Epoch 00036: val_loss improved from 0.18098 to 0.17509, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/036-0.1751.hdf5\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.2227 - acc: 0.9278 - val_loss: 0.1751 - val_acc: 0.9460\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9313\n",
      "Epoch 00037: val_loss did not improve from 0.17509\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.2162 - acc: 0.9313 - val_loss: 0.1778 - val_acc: 0.9450\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9334\n",
      "Epoch 00038: val_loss did not improve from 0.17509\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.2079 - acc: 0.9334 - val_loss: 0.1754 - val_acc: 0.9455\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9349\n",
      "Epoch 00039: val_loss improved from 0.17509 to 0.17230, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/039-0.1723.hdf5\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.2064 - acc: 0.9348 - val_loss: 0.1723 - val_acc: 0.9422\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9353\n",
      "Epoch 00040: val_loss did not improve from 0.17230\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.2008 - acc: 0.9353 - val_loss: 0.1752 - val_acc: 0.9462\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9377\n",
      "Epoch 00041: val_loss did not improve from 0.17230\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.1947 - acc: 0.9377 - val_loss: 0.1742 - val_acc: 0.9469\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9381\n",
      "Epoch 00042: val_loss improved from 0.17230 to 0.16125, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/042-0.1613.hdf5\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.1938 - acc: 0.9381 - val_loss: 0.1613 - val_acc: 0.9478\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9403\n",
      "Epoch 00043: val_loss did not improve from 0.16125\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.1896 - acc: 0.9403 - val_loss: 0.1657 - val_acc: 0.9471\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9402\n",
      "Epoch 00044: val_loss did not improve from 0.16125\n",
      "36805/36805 [==============================] - 20s 534us/sample - loss: 0.1861 - acc: 0.9402 - val_loss: 0.1657 - val_acc: 0.9495\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9404\n",
      "Epoch 00045: val_loss did not improve from 0.16125\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.1853 - acc: 0.9404 - val_loss: 0.1665 - val_acc: 0.9485\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9427\n",
      "Epoch 00046: val_loss improved from 0.16125 to 0.15773, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/046-0.1577.hdf5\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.1788 - acc: 0.9427 - val_loss: 0.1577 - val_acc: 0.9515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9428\n",
      "Epoch 00047: val_loss improved from 0.15773 to 0.15460, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/047-0.1546.hdf5\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.1758 - acc: 0.9428 - val_loss: 0.1546 - val_acc: 0.9506\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9435\n",
      "Epoch 00048: val_loss did not improve from 0.15460\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.1735 - acc: 0.9435 - val_loss: 0.1638 - val_acc: 0.9497\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9470\n",
      "Epoch 00049: val_loss improved from 0.15460 to 0.15324, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/049-0.1532.hdf5\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.1679 - acc: 0.9470 - val_loss: 0.1532 - val_acc: 0.9534\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9443\n",
      "Epoch 00050: val_loss did not improve from 0.15324\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.1666 - acc: 0.9443 - val_loss: 0.1682 - val_acc: 0.9453\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9447\n",
      "Epoch 00051: val_loss did not improve from 0.15324\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.1682 - acc: 0.9447 - val_loss: 0.1730 - val_acc: 0.9441\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9463\n",
      "Epoch 00052: val_loss improved from 0.15324 to 0.15076, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/052-0.1508.hdf5\n",
      "36805/36805 [==============================] - 20s 534us/sample - loss: 0.1613 - acc: 0.9463 - val_loss: 0.1508 - val_acc: 0.9509\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9486\n",
      "Epoch 00053: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.1575 - acc: 0.9486 - val_loss: 0.1624 - val_acc: 0.9513\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9513\n",
      "Epoch 00054: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.1516 - acc: 0.9513 - val_loss: 0.1676 - val_acc: 0.9481\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9485\n",
      "Epoch 00055: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.1541 - acc: 0.9485 - val_loss: 0.1548 - val_acc: 0.9511\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9516\n",
      "Epoch 00056: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.1491 - acc: 0.9516 - val_loss: 0.1574 - val_acc: 0.9518\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9520\n",
      "Epoch 00057: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.1484 - acc: 0.9520 - val_loss: 0.1522 - val_acc: 0.9525\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9525\n",
      "Epoch 00058: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.1427 - acc: 0.9525 - val_loss: 0.1585 - val_acc: 0.9502\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9527\n",
      "Epoch 00059: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.1438 - acc: 0.9528 - val_loss: 0.1522 - val_acc: 0.9518\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9546\n",
      "Epoch 00060: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1397 - acc: 0.9546 - val_loss: 0.1717 - val_acc: 0.9478\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.9551\n",
      "Epoch 00061: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.1382 - acc: 0.9551 - val_loss: 0.1538 - val_acc: 0.9513\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9555\n",
      "Epoch 00062: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.1347 - acc: 0.9555 - val_loss: 0.1536 - val_acc: 0.9522\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9554\n",
      "Epoch 00063: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.1341 - acc: 0.9554 - val_loss: 0.1519 - val_acc: 0.9532\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9561\n",
      "Epoch 00064: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.1326 - acc: 0.9561 - val_loss: 0.1571 - val_acc: 0.9534\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9559\n",
      "Epoch 00065: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.1289 - acc: 0.9559 - val_loss: 0.1530 - val_acc: 0.9529\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9573\n",
      "Epoch 00066: val_loss did not improve from 0.15076\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1285 - acc: 0.9573 - val_loss: 0.1625 - val_acc: 0.9529\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9587\n",
      "Epoch 00067: val_loss improved from 0.15076 to 0.14380, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/067-0.1438.hdf5\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.1230 - acc: 0.9587 - val_loss: 0.1438 - val_acc: 0.9548\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9602\n",
      "Epoch 00068: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.1191 - acc: 0.9602 - val_loss: 0.1482 - val_acc: 0.9562\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9606\n",
      "Epoch 00069: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.1196 - acc: 0.9605 - val_loss: 0.1525 - val_acc: 0.9541\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9609\n",
      "Epoch 00070: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.1189 - acc: 0.9609 - val_loss: 0.1592 - val_acc: 0.9539\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9596\n",
      "Epoch 00071: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.1203 - acc: 0.9597 - val_loss: 0.1575 - val_acc: 0.9527\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9614\n",
      "Epoch 00072: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.1150 - acc: 0.9614 - val_loss: 0.1591 - val_acc: 0.9513\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9613\n",
      "Epoch 00073: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.1157 - acc: 0.9614 - val_loss: 0.1504 - val_acc: 0.9525\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9624\n",
      "Epoch 00074: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1122 - acc: 0.9624 - val_loss: 0.1604 - val_acc: 0.9518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9633\n",
      "Epoch 00075: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.1089 - acc: 0.9633 - val_loss: 0.1512 - val_acc: 0.9560\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9639\n",
      "Epoch 00076: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.1084 - acc: 0.9639 - val_loss: 0.1522 - val_acc: 0.9567\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9645\n",
      "Epoch 00077: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.1088 - acc: 0.9645 - val_loss: 0.1612 - val_acc: 0.9536\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9651\n",
      "Epoch 00078: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.1050 - acc: 0.9651 - val_loss: 0.1621 - val_acc: 0.9536\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9645\n",
      "Epoch 00079: val_loss did not improve from 0.14380\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.1041 - acc: 0.9645 - val_loss: 0.1668 - val_acc: 0.9569\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9655\n",
      "Epoch 00080: val_loss improved from 0.14380 to 0.14371, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/080-0.1437.hdf5\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.1028 - acc: 0.9655 - val_loss: 0.1437 - val_acc: 0.9555\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9661\n",
      "Epoch 00081: val_loss did not improve from 0.14371\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.0978 - acc: 0.9661 - val_loss: 0.1525 - val_acc: 0.9553\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9653\n",
      "Epoch 00082: val_loss did not improve from 0.14371\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1034 - acc: 0.9653 - val_loss: 0.1507 - val_acc: 0.9550\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9674\n",
      "Epoch 00083: val_loss improved from 0.14371 to 0.14313, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv_checkpoint/083-0.1431.hdf5\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0999 - acc: 0.9674 - val_loss: 0.1431 - val_acc: 0.9562\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9660\n",
      "Epoch 00084: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 20s 533us/sample - loss: 0.1003 - acc: 0.9660 - val_loss: 0.1523 - val_acc: 0.9569\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9686\n",
      "Epoch 00085: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.0945 - acc: 0.9686 - val_loss: 0.1553 - val_acc: 0.9564\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9689\n",
      "Epoch 00086: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0929 - acc: 0.9689 - val_loss: 0.1707 - val_acc: 0.9539\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9694\n",
      "Epoch 00087: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0902 - acc: 0.9694 - val_loss: 0.1718 - val_acc: 0.9541\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9680\n",
      "Epoch 00088: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0938 - acc: 0.9680 - val_loss: 0.1534 - val_acc: 0.9548\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9692\n",
      "Epoch 00089: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.0912 - acc: 0.9692 - val_loss: 0.1529 - val_acc: 0.9555\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9695\n",
      "Epoch 00090: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0906 - acc: 0.9695 - val_loss: 0.1600 - val_acc: 0.9557\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9700\n",
      "Epoch 00091: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0894 - acc: 0.9700 - val_loss: 0.1583 - val_acc: 0.9574\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9710\n",
      "Epoch 00092: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.0862 - acc: 0.9710 - val_loss: 0.1581 - val_acc: 0.9562\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9705\n",
      "Epoch 00093: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0871 - acc: 0.9705 - val_loss: 0.1583 - val_acc: 0.9543\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9690\n",
      "Epoch 00094: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.0911 - acc: 0.9690 - val_loss: 0.1545 - val_acc: 0.9555\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9714\n",
      "Epoch 00095: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.0844 - acc: 0.9714 - val_loss: 0.1685 - val_acc: 0.9548\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9723\n",
      "Epoch 00096: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.0804 - acc: 0.9723 - val_loss: 0.1552 - val_acc: 0.9599\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9725\n",
      "Epoch 00097: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0814 - acc: 0.9725 - val_loss: 0.1571 - val_acc: 0.9560\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9729\n",
      "Epoch 00098: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.0803 - acc: 0.9729 - val_loss: 0.1840 - val_acc: 0.9509\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9729\n",
      "Epoch 00099: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.0822 - acc: 0.9729 - val_loss: 0.1532 - val_acc: 0.9555\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9730\n",
      "Epoch 00100: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.0787 - acc: 0.9730 - val_loss: 0.1525 - val_acc: 0.9592\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9743\n",
      "Epoch 00101: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.0742 - acc: 0.9743 - val_loss: 0.1664 - val_acc: 0.9564\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9728\n",
      "Epoch 00102: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0807 - acc: 0.9728 - val_loss: 0.1648 - val_acc: 0.9588\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9747\n",
      "Epoch 00103: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0756 - acc: 0.9747 - val_loss: 0.1571 - val_acc: 0.9597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9757\n",
      "Epoch 00104: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.0719 - acc: 0.9757 - val_loss: 0.1595 - val_acc: 0.9574\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9755\n",
      "Epoch 00105: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.0720 - acc: 0.9755 - val_loss: 0.1556 - val_acc: 0.9574\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9765\n",
      "Epoch 00106: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 530us/sample - loss: 0.0702 - acc: 0.9765 - val_loss: 0.1657 - val_acc: 0.9546\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9747\n",
      "Epoch 00107: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.0728 - acc: 0.9747 - val_loss: 0.1817 - val_acc: 0.9541\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9764\n",
      "Epoch 00108: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.0692 - acc: 0.9764 - val_loss: 0.1651 - val_acc: 0.9585\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9768\n",
      "Epoch 00109: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.0703 - acc: 0.9768 - val_loss: 0.1668 - val_acc: 0.9569\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9742\n",
      "Epoch 00110: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0742 - acc: 0.9742 - val_loss: 0.1635 - val_acc: 0.9583\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9769\n",
      "Epoch 00111: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0646 - acc: 0.9770 - val_loss: 0.1686 - val_acc: 0.9592\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9776\n",
      "Epoch 00112: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0666 - acc: 0.9776 - val_loss: 0.1579 - val_acc: 0.9574\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9766\n",
      "Epoch 00113: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 20s 533us/sample - loss: 0.0676 - acc: 0.9766 - val_loss: 0.1676 - val_acc: 0.9578\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9773\n",
      "Epoch 00114: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.0669 - acc: 0.9773 - val_loss: 0.1643 - val_acc: 0.9583\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9781\n",
      "Epoch 00115: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0650 - acc: 0.9782 - val_loss: 0.1672 - val_acc: 0.9567\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9780\n",
      "Epoch 00116: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0623 - acc: 0.9780 - val_loss: 0.1623 - val_acc: 0.9576\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9781\n",
      "Epoch 00117: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.0638 - acc: 0.9781 - val_loss: 0.1677 - val_acc: 0.9606\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9783\n",
      "Epoch 00118: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0639 - acc: 0.9783 - val_loss: 0.1653 - val_acc: 0.9616\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9789\n",
      "Epoch 00119: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0612 - acc: 0.9789 - val_loss: 0.1723 - val_acc: 0.9578\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9784\n",
      "Epoch 00120: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.0638 - acc: 0.9784 - val_loss: 0.1538 - val_acc: 0.9616\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9801\n",
      "Epoch 00121: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.0606 - acc: 0.9801 - val_loss: 0.1593 - val_acc: 0.9595\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9788\n",
      "Epoch 00122: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 530us/sample - loss: 0.0614 - acc: 0.9788 - val_loss: 0.1809 - val_acc: 0.9562\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9789\n",
      "Epoch 00123: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0617 - acc: 0.9789 - val_loss: 0.1679 - val_acc: 0.9599\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9801\n",
      "Epoch 00124: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0585 - acc: 0.9801 - val_loss: 0.1667 - val_acc: 0.9602\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9789\n",
      "Epoch 00125: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0609 - acc: 0.9789 - val_loss: 0.1733 - val_acc: 0.9578\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9796\n",
      "Epoch 00126: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0590 - acc: 0.9796 - val_loss: 0.1675 - val_acc: 0.9611\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9803\n",
      "Epoch 00127: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0582 - acc: 0.9803 - val_loss: 0.1872 - val_acc: 0.9550\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9818\n",
      "Epoch 00128: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0541 - acc: 0.9817 - val_loss: 0.1873 - val_acc: 0.9546\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9785\n",
      "Epoch 00129: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0604 - acc: 0.9785 - val_loss: 0.1615 - val_acc: 0.9578\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9802\n",
      "Epoch 00130: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.0561 - acc: 0.9802 - val_loss: 0.1596 - val_acc: 0.9581\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9813\n",
      "Epoch 00131: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.0545 - acc: 0.9813 - val_loss: 0.1683 - val_acc: 0.9585\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9811\n",
      "Epoch 00132: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.0548 - acc: 0.9811 - val_loss: 0.1737 - val_acc: 0.9592\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9810\n",
      "Epoch 00133: val_loss did not improve from 0.14313\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.0529 - acc: 0.9810 - val_loss: 0.1858 - val_acc: 0.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 315us/sample - loss: 0.2448 - acc: 0.9329\n",
      "Loss: 0.2447861482359291 Accuracy: 0.93291795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_concat_ch_32_DO'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 75808)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 75808)        0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1212944     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,223,440\n",
      "Trainable params: 1,223,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 322us/sample - loss: 1.4481 - acc: 0.5468\n",
      "Loss: 1.4480572943870647 Accuracy: 0.5468328\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 25248)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 25248)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           403984      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 419,632\n",
      "Trainable params: 419,632\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 310us/sample - loss: 1.0127 - acc: 0.7124\n",
      "Loss: 1.0127461109082152 Accuracy: 0.7123572\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 10464)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 10464)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           167440      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 193,392\n",
      "Trainable params: 193,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 325us/sample - loss: 0.6059 - acc: 0.8536\n",
      "Loss: 0.6059250388561379 Accuracy: 0.85358256\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 5504)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 5504)         0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           88080       dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 134,576\n",
      "Trainable params: 134,576\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 348us/sample - loss: 0.2991 - acc: 0.9184\n",
      "Loss: 0.2990781063602721 Accuracy: 0.9183801\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1792)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1792)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           28688       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 95,728\n",
      "Trainable params: 95,728\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 348us/sample - loss: 0.2009 - acc: 0.9396\n",
      "Loss: 0.200893817667154 Accuracy: 0.9395639\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 576)          0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 576)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           9232        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 96,816\n",
      "Trainable params: 96,816\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 367us/sample - loss: 0.2448 - acc: 0.9329\n",
      "Loss: 0.2447861482359291 Accuracy: 0.93291795\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_concat_ch_32_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 75808)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 75808)        0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1212944     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,223,440\n",
      "Trainable params: 1,223,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 372us/sample - loss: 1.8088 - acc: 0.6214\n",
      "Loss: 1.8087964842252642 Accuracy: 0.6213915\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 25248)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 25248)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           403984      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 419,632\n",
      "Trainable params: 419,632\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 366us/sample - loss: 1.1247 - acc: 0.7416\n",
      "Loss: 1.124701316581956 Accuracy: 0.7416407\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 10464)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 10464)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           167440      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 193,392\n",
      "Trainable params: 193,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 388us/sample - loss: 0.6556 - acc: 0.8638\n",
      "Loss: 0.6555810355261231 Accuracy: 0.8637591\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 5504)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 5504)         0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           88080       dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 134,576\n",
      "Trainable params: 134,576\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 386us/sample - loss: 0.2980 - acc: 0.9223\n",
      "Loss: 0.29804111167030295 Accuracy: 0.9223261\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1792)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1792)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           28688       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 95,728\n",
      "Trainable params: 95,728\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 421us/sample - loss: 0.1996 - acc: 0.9462\n",
      "Loss: 0.1996189715675352 Accuracy: 0.9462098\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_32_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 576)          0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 576)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           9232        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 96,816\n",
      "Trainable params: 96,816\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 428us/sample - loss: 0.2931 - acc: 0.9385\n",
      "Loss: 0.2931289756053581 Accuracy: 0.93852544\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
