{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-3:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 341312)       0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 113728)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 37888)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 492928)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 492928)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           7886864     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,929,104\n",
      "Trainable params: 7,928,720\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 113728)       0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 37888)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12608)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 164224)       0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 164224)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2627600     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,690,640\n",
      "Trainable params: 2,690,128\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 37888)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12608)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 8320)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 58816)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 58816)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           941072      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,045,712\n",
      "Trainable params: 1,044,944\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 12608)        0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 8320)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 2688)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 23616)        0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 23616)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           377872      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 565,072\n",
      "Trainable params: 564,048\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 8320)         0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 2688)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 896)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 11904)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 11904)        0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           190480      dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 460,240\n",
      "Trainable params: 458,960\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 2688)         0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 896)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 3840)         0           flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 3840)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           61456       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 413,776\n",
      "Trainable params: 412,240\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.5658 - acc: 0.3431\n",
      "Epoch 00001: val_loss improved from inf to 2.19252, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_3_conv_checkpoint/001-2.1925.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 3.5656 - acc: 0.3431 - val_loss: 2.1925 - val_acc: 0.4684\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0146 - acc: 0.5830\n",
      "Epoch 00002: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 2.0145 - acc: 0.5830 - val_loss: 2.5331 - val_acc: 0.4701\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5926 - acc: 0.6807\n",
      "Epoch 00003: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 1.5924 - acc: 0.6807 - val_loss: 2.5857 - val_acc: 0.4927\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3533 - acc: 0.7398\n",
      "Epoch 00004: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 1.3532 - acc: 0.7398 - val_loss: 2.8740 - val_acc: 0.4694\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2155 - acc: 0.7824\n",
      "Epoch 00005: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 1.2153 - acc: 0.7824 - val_loss: 2.2668 - val_acc: 0.5693\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1222 - acc: 0.8109\n",
      "Epoch 00006: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 1.1220 - acc: 0.8109 - val_loss: 2.8498 - val_acc: 0.5150\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0742 - acc: 0.8250\n",
      "Epoch 00007: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 1.0741 - acc: 0.8250 - val_loss: 2.9066 - val_acc: 0.5299\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0221 - acc: 0.8408\n",
      "Epoch 00008: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 1.0221 - acc: 0.8407 - val_loss: 2.7638 - val_acc: 0.5577\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9897 - acc: 0.8515\n",
      "Epoch 00009: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.9896 - acc: 0.8515 - val_loss: 2.8419 - val_acc: 0.5479\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9389 - acc: 0.8692\n",
      "Epoch 00010: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.9388 - acc: 0.8692 - val_loss: 2.6344 - val_acc: 0.5879\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9308 - acc: 0.8722\n",
      "Epoch 00011: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.9308 - acc: 0.8722 - val_loss: 3.2245 - val_acc: 0.5134\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9057 - acc: 0.8822\n",
      "Epoch 00012: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.9057 - acc: 0.8822 - val_loss: 2.9031 - val_acc: 0.5660\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8688 - acc: 0.8920\n",
      "Epoch 00013: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8688 - acc: 0.8920 - val_loss: 3.2266 - val_acc: 0.5372\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8771 - acc: 0.8886\n",
      "Epoch 00014: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8776 - acc: 0.8885 - val_loss: 4.2683 - val_acc: 0.4745\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8830 - acc: 0.8908\n",
      "Epoch 00015: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8831 - acc: 0.8908 - val_loss: 2.8523 - val_acc: 0.5986\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8270 - acc: 0.9031- ETA: 2s - los\n",
      "Epoch 00016: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8269 - acc: 0.9031 - val_loss: 3.0608 - val_acc: 0.5926\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8278 - acc: 0.9062\n",
      "Epoch 00017: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8277 - acc: 0.9062 - val_loss: 2.8706 - val_acc: 0.6010\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8032 - acc: 0.9145\n",
      "Epoch 00018: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8031 - acc: 0.9145 - val_loss: 3.1233 - val_acc: 0.5823\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8206 - acc: 0.9075\n",
      "Epoch 00019: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8207 - acc: 0.9074 - val_loss: 2.9372 - val_acc: 0.6082\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8183 - acc: 0.9120\n",
      "Epoch 00020: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8182 - acc: 0.9120 - val_loss: 3.3172 - val_acc: 0.5993\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7845 - acc: 0.9200\n",
      "Epoch 00021: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7845 - acc: 0.9200 - val_loss: 2.9177 - val_acc: 0.6084\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7918 - acc: 0.9162\n",
      "Epoch 00022: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7918 - acc: 0.9162 - val_loss: 2.9418 - val_acc: 0.6075\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7935 - acc: 0.9202\n",
      "Epoch 00023: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7938 - acc: 0.9202 - val_loss: 2.9100 - val_acc: 0.6268\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7733 - acc: 0.9236\n",
      "Epoch 00024: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7734 - acc: 0.9236 - val_loss: 4.8615 - val_acc: 0.4948\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7873 - acc: 0.9189\n",
      "Epoch 00025: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7872 - acc: 0.9189 - val_loss: 3.4399 - val_acc: 0.5812\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7567 - acc: 0.9273\n",
      "Epoch 00026: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7566 - acc: 0.9273 - val_loss: 3.2919 - val_acc: 0.5900\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7616 - acc: 0.9267\n",
      "Epoch 00027: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7615 - acc: 0.9267 - val_loss: 2.8251 - val_acc: 0.6429\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7600 - acc: 0.9270\n",
      "Epoch 00028: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7599 - acc: 0.9270 - val_loss: 3.2176 - val_acc: 0.6005\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7479 - acc: 0.9306\n",
      "Epoch 00029: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7478 - acc: 0.9306 - val_loss: 3.1060 - val_acc: 0.6231\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7569 - acc: 0.9297\n",
      "Epoch 00030: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7568 - acc: 0.9297 - val_loss: 3.0120 - val_acc: 0.6331\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7602 - acc: 0.9292\n",
      "Epoch 00031: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7602 - acc: 0.9291 - val_loss: 3.1740 - val_acc: 0.6150\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7412 - acc: 0.9343\n",
      "Epoch 00032: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7412 - acc: 0.9343 - val_loss: 3.7218 - val_acc: 0.5607\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7608 - acc: 0.9289\n",
      "Epoch 00033: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7607 - acc: 0.9289 - val_loss: 2.9443 - val_acc: 0.6508\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7424 - acc: 0.9333\n",
      "Epoch 00034: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7428 - acc: 0.9333 - val_loss: 3.2645 - val_acc: 0.6226\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7467 - acc: 0.9328\n",
      "Epoch 00035: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7466 - acc: 0.9328 - val_loss: 3.1577 - val_acc: 0.6334\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7244 - acc: 0.9375\n",
      "Epoch 00036: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7243 - acc: 0.9375 - val_loss: 3.1574 - val_acc: 0.6175\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7327 - acc: 0.9366\n",
      "Epoch 00037: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7326 - acc: 0.9366 - val_loss: 3.1510 - val_acc: 0.6401\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7430 - acc: 0.9341\n",
      "Epoch 00038: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7429 - acc: 0.9341 - val_loss: 3.1487 - val_acc: 0.6448\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7348 - acc: 0.9369\n",
      "Epoch 00039: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7348 - acc: 0.9369 - val_loss: 3.2646 - val_acc: 0.6345\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7223 - acc: 0.9394\n",
      "Epoch 00040: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7222 - acc: 0.9394 - val_loss: 3.6433 - val_acc: 0.5973\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7167 - acc: 0.9400\n",
      "Epoch 00041: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7166 - acc: 0.9400 - val_loss: 3.3685 - val_acc: 0.6159\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7232 - acc: 0.9389\n",
      "Epoch 00042: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7231 - acc: 0.9389 - val_loss: 3.1778 - val_acc: 0.6371\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7139 - acc: 0.9415\n",
      "Epoch 00043: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7142 - acc: 0.9415 - val_loss: 3.2304 - val_acc: 0.6378\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7379 - acc: 0.9358\n",
      "Epoch 00044: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7379 - acc: 0.9358 - val_loss: 3.8604 - val_acc: 0.5977\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7201 - acc: 0.9407\n",
      "Epoch 00045: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7204 - acc: 0.9407 - val_loss: 3.9486 - val_acc: 0.5714\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7155 - acc: 0.9415\n",
      "Epoch 00046: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7158 - acc: 0.9414 - val_loss: 3.7147 - val_acc: 0.5926\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7121 - acc: 0.9428\n",
      "Epoch 00047: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7125 - acc: 0.9428 - val_loss: 3.5396 - val_acc: 0.6098\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7282 - acc: 0.9394\n",
      "Epoch 00048: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7281 - acc: 0.9394 - val_loss: 3.5213 - val_acc: 0.6217\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7134 - acc: 0.9416\n",
      "Epoch 00049: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7134 - acc: 0.9416 - val_loss: 4.4229 - val_acc: 0.5511\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7112 - acc: 0.9422\n",
      "Epoch 00050: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7111 - acc: 0.9422 - val_loss: 4.5839 - val_acc: 0.5283\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6999 - acc: 0.9458\n",
      "Epoch 00051: val_loss did not improve from 2.19252\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.6998 - acc: 0.9458 - val_loss: 3.2985 - val_acc: 0.6306\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 660us/sample - loss: 2.2415 - acc: 0.4447\n",
      "Loss: 2.241532568433946 Accuracy: 0.44465214\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3601 - acc: 0.3850\n",
      "Epoch 00001: val_loss improved from inf to 2.27844, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_4_conv_checkpoint/001-2.2784.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 2.3602 - acc: 0.3850 - val_loss: 2.2784 - val_acc: 0.3988\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4244 - acc: 0.5933\n",
      "Epoch 00002: val_loss improved from 2.27844 to 1.42281, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_4_conv_checkpoint/002-1.4228.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.4243 - acc: 0.5933 - val_loss: 1.4228 - val_acc: 0.6007\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0687 - acc: 0.6837\n",
      "Epoch 00003: val_loss did not improve from 1.42281\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 1.0687 - acc: 0.6837 - val_loss: 1.6923 - val_acc: 0.5758\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8675 - acc: 0.7404\n",
      "Epoch 00004: val_loss did not improve from 1.42281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.8674 - acc: 0.7403 - val_loss: 1.6515 - val_acc: 0.5877\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7104 - acc: 0.7821\n",
      "Epoch 00005: val_loss did not improve from 1.42281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.7104 - acc: 0.7821 - val_loss: 1.4984 - val_acc: 0.6117\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5993 - acc: 0.8128\n",
      "Epoch 00006: val_loss did not improve from 1.42281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5996 - acc: 0.8127 - val_loss: 1.8461 - val_acc: 0.5695\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8333\n",
      "Epoch 00007: val_loss improved from 1.42281 to 1.41120, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_4_conv_checkpoint/007-1.4112.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5452 - acc: 0.8333 - val_loss: 1.4112 - val_acc: 0.6674\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4297 - acc: 0.8599\n",
      "Epoch 00008: val_loss did not improve from 1.41120\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.4298 - acc: 0.8598 - val_loss: 1.8956 - val_acc: 0.5816\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8767\n",
      "Epoch 00009: val_loss improved from 1.41120 to 1.30239, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_4_conv_checkpoint/009-1.3024.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3816 - acc: 0.8768 - val_loss: 1.3024 - val_acc: 0.7035\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8917\n",
      "Epoch 00010: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.3323 - acc: 0.8917 - val_loss: 1.5132 - val_acc: 0.6790\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.8997\n",
      "Epoch 00011: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3186 - acc: 0.8997 - val_loss: 1.5378 - val_acc: 0.6837\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9086\n",
      "Epoch 00012: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2821 - acc: 0.9085 - val_loss: 1.6220 - val_acc: 0.6655\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9165\n",
      "Epoch 00013: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.2558 - acc: 0.9165 - val_loss: 1.7013 - val_acc: 0.6639\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9284\n",
      "Epoch 00014: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.2251 - acc: 0.9284 - val_loss: 1.6217 - val_acc: 0.6813\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2182 - acc: 0.9312\n",
      "Epoch 00015: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2182 - acc: 0.9313 - val_loss: 1.5890 - val_acc: 0.6881\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9329\n",
      "Epoch 00016: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2089 - acc: 0.9329 - val_loss: 1.6646 - val_acc: 0.6904\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9382\n",
      "Epoch 00017: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1932 - acc: 0.9382 - val_loss: 1.8567 - val_acc: 0.6820\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9432\n",
      "Epoch 00018: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1767 - acc: 0.9432 - val_loss: 1.7582 - val_acc: 0.6869\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9479\n",
      "Epoch 00019: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1664 - acc: 0.9478 - val_loss: 2.1132 - val_acc: 0.6406\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9516\n",
      "Epoch 00020: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1556 - acc: 0.9516 - val_loss: 1.5073 - val_acc: 0.7193\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9539\n",
      "Epoch 00021: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1453 - acc: 0.9539 - val_loss: 2.2425 - val_acc: 0.6350\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9529\n",
      "Epoch 00022: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1525 - acc: 0.9529 - val_loss: 1.7124 - val_acc: 0.6960\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9576\n",
      "Epoch 00023: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1326 - acc: 0.9575 - val_loss: 1.7380 - val_acc: 0.6995\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9565\n",
      "Epoch 00024: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1493 - acc: 0.9565 - val_loss: 2.0095 - val_acc: 0.6751\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9603\n",
      "Epoch 00025: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1319 - acc: 0.9603 - val_loss: 1.7571 - val_acc: 0.6949\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9665\n",
      "Epoch 00026: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1104 - acc: 0.9665 - val_loss: 2.4465 - val_acc: 0.6278\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9653\n",
      "Epoch 00027: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1126 - acc: 0.9653 - val_loss: 1.7220 - val_acc: 0.7107\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9633\n",
      "Epoch 00028: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1262 - acc: 0.9633 - val_loss: 2.3413 - val_acc: 0.6443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9673\n",
      "Epoch 00029: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1101 - acc: 0.9673 - val_loss: 1.8767 - val_acc: 0.7058\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9678\n",
      "Epoch 00030: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1054 - acc: 0.9678 - val_loss: 1.8164 - val_acc: 0.7116\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9690\n",
      "Epoch 00031: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1021 - acc: 0.9690 - val_loss: 2.3195 - val_acc: 0.6625\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9702\n",
      "Epoch 00032: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0972 - acc: 0.9702 - val_loss: 1.9009 - val_acc: 0.7137\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9692\n",
      "Epoch 00033: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1062 - acc: 0.9692 - val_loss: 1.6703 - val_acc: 0.7205\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9738\n",
      "Epoch 00034: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0901 - acc: 0.9738 - val_loss: 2.4872 - val_acc: 0.6401\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9725\n",
      "Epoch 00035: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0929 - acc: 0.9724 - val_loss: 2.1818 - val_acc: 0.6765\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9742\n",
      "Epoch 00036: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0918 - acc: 0.9742 - val_loss: 1.8666 - val_acc: 0.7021\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9779\n",
      "Epoch 00037: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0758 - acc: 0.9779 - val_loss: 1.8113 - val_acc: 0.7174\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9783\n",
      "Epoch 00038: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0755 - acc: 0.9783 - val_loss: 2.0407 - val_acc: 0.6979\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9765\n",
      "Epoch 00039: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0836 - acc: 0.9765 - val_loss: 1.9454 - val_acc: 0.7109\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9740\n",
      "Epoch 00040: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0932 - acc: 0.9740 - val_loss: 2.7066 - val_acc: 0.6401\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9782\n",
      "Epoch 00041: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0782 - acc: 0.9782 - val_loss: 2.0436 - val_acc: 0.7021\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9785\n",
      "Epoch 00042: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0728 - acc: 0.9785 - val_loss: 1.8628 - val_acc: 0.7163\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9795\n",
      "Epoch 00043: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0755 - acc: 0.9795 - val_loss: 1.9618 - val_acc: 0.7174\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9807\n",
      "Epoch 00044: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0681 - acc: 0.9807 - val_loss: 1.7567 - val_acc: 0.7398\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9785\n",
      "Epoch 00045: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0706 - acc: 0.9784 - val_loss: 1.9630 - val_acc: 0.7156\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9787\n",
      "Epoch 00046: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0749 - acc: 0.9787 - val_loss: 2.0571 - val_acc: 0.7058\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9811\n",
      "Epoch 00047: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0698 - acc: 0.9811 - val_loss: 2.0038 - val_acc: 0.7074\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9817\n",
      "Epoch 00048: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0639 - acc: 0.9817 - val_loss: 2.2114 - val_acc: 0.6785\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9802\n",
      "Epoch 00049: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0700 - acc: 0.9802 - val_loss: 1.8826 - val_acc: 0.7144\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9817\n",
      "Epoch 00050: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0648 - acc: 0.9817 - val_loss: 3.1686 - val_acc: 0.5851\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9835\n",
      "Epoch 00051: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0602 - acc: 0.9835 - val_loss: 1.8748 - val_acc: 0.7314\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9810\n",
      "Epoch 00052: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0678 - acc: 0.9810 - val_loss: 2.1078 - val_acc: 0.6993\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9835\n",
      "Epoch 00053: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0630 - acc: 0.9835 - val_loss: 2.5742 - val_acc: 0.6515\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9836\n",
      "Epoch 00054: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0609 - acc: 0.9836 - val_loss: 1.8570 - val_acc: 0.7268\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9812\n",
      "Epoch 00055: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0668 - acc: 0.9811 - val_loss: 2.4865 - val_acc: 0.6769\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9828\n",
      "Epoch 00056: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0650 - acc: 0.9828 - val_loss: 2.0054 - val_acc: 0.7160\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9861\n",
      "Epoch 00057: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0505 - acc: 0.9861 - val_loss: 1.9048 - val_acc: 0.7391\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9867\n",
      "Epoch 00058: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0492 - acc: 0.9867 - val_loss: 1.9179 - val_acc: 0.7270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9835\n",
      "Epoch 00059: val_loss did not improve from 1.30239\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0622 - acc: 0.9835 - val_loss: 2.5591 - val_acc: 0.6560\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 630us/sample - loss: 1.3796 - acc: 0.6681\n",
      "Loss: 1.3796283983862412 Accuracy: 0.66812044\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1167 - acc: 0.4002\n",
      "Epoch 00001: val_loss improved from inf to 1.50457, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_5_conv_checkpoint/001-1.5046.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 2.1166 - acc: 0.4002 - val_loss: 1.5046 - val_acc: 0.5043\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2824 - acc: 0.6165\n",
      "Epoch 00002: val_loss improved from 1.50457 to 1.09245, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_5_conv_checkpoint/002-1.0925.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 1.2824 - acc: 0.6164 - val_loss: 1.0925 - val_acc: 0.6692\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0453 - acc: 0.6839\n",
      "Epoch 00003: val_loss improved from 1.09245 to 0.95681, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_5_conv_checkpoint/003-0.9568.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 1.0452 - acc: 0.6840 - val_loss: 0.9568 - val_acc: 0.7214\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8775 - acc: 0.7337\n",
      "Epoch 00004: val_loss did not improve from 0.95681\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.8775 - acc: 0.7337 - val_loss: 1.1187 - val_acc: 0.7007\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7703 - acc: 0.7650\n",
      "Epoch 00005: val_loss did not improve from 0.95681\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.7703 - acc: 0.7650 - val_loss: 0.9833 - val_acc: 0.7184\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6789 - acc: 0.7926\n",
      "Epoch 00006: val_loss improved from 0.95681 to 0.84860, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_5_conv_checkpoint/006-0.8486.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.6789 - acc: 0.7926 - val_loss: 0.8486 - val_acc: 0.7668\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6097 - acc: 0.8136\n",
      "Epoch 00007: val_loss did not improve from 0.84860\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.6096 - acc: 0.8136 - val_loss: 0.9621 - val_acc: 0.7440\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.8315\n",
      "Epoch 00008: val_loss improved from 0.84860 to 0.79910, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_5_conv_checkpoint/008-0.7991.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5445 - acc: 0.8315 - val_loss: 0.7991 - val_acc: 0.7908\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8439\n",
      "Epoch 00009: val_loss did not improve from 0.79910\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5018 - acc: 0.8439 - val_loss: 0.8418 - val_acc: 0.7741\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4554 - acc: 0.8576\n",
      "Epoch 00010: val_loss improved from 0.79910 to 0.77064, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_5_conv_checkpoint/010-0.7706.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4553 - acc: 0.8577 - val_loss: 0.7706 - val_acc: 0.7962\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8741\n",
      "Epoch 00011: val_loss did not improve from 0.77064\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3955 - acc: 0.8741 - val_loss: 0.8091 - val_acc: 0.7841\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3740 - acc: 0.8822\n",
      "Epoch 00012: val_loss improved from 0.77064 to 0.72221, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_5_conv_checkpoint/012-0.7222.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3740 - acc: 0.8822 - val_loss: 0.7222 - val_acc: 0.8132\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8928\n",
      "Epoch 00013: val_loss did not improve from 0.72221\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3338 - acc: 0.8928 - val_loss: 0.9593 - val_acc: 0.7598\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9009\n",
      "Epoch 00014: val_loss did not improve from 0.72221\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3008 - acc: 0.9009 - val_loss: 0.9172 - val_acc: 0.7680\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9073\n",
      "Epoch 00015: val_loss improved from 0.72221 to 0.71568, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_5_conv_checkpoint/015-0.7157.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2871 - acc: 0.9073 - val_loss: 0.7157 - val_acc: 0.8211\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.9166\n",
      "Epoch 00016: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2587 - acc: 0.9166 - val_loss: 0.8270 - val_acc: 0.7901\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9210\n",
      "Epoch 00017: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2462 - acc: 0.9210 - val_loss: 0.8199 - val_acc: 0.8013\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9244\n",
      "Epoch 00018: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2340 - acc: 0.9244 - val_loss: 0.8884 - val_acc: 0.7957\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9310\n",
      "Epoch 00019: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2117 - acc: 0.9310 - val_loss: 0.7623 - val_acc: 0.8195\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9331\n",
      "Epoch 00020: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2003 - acc: 0.9331 - val_loss: 0.7345 - val_acc: 0.8223\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9353\n",
      "Epoch 00021: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1957 - acc: 0.9353 - val_loss: 0.7466 - val_acc: 0.8220\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9410\n",
      "Epoch 00022: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1821 - acc: 0.9410 - val_loss: 0.7767 - val_acc: 0.8255\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9445\n",
      "Epoch 00023: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1693 - acc: 0.9445 - val_loss: 0.8507 - val_acc: 0.7973\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9485\n",
      "Epoch 00024: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1597 - acc: 0.9485 - val_loss: 0.8713 - val_acc: 0.8099\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9473\n",
      "Epoch 00025: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1561 - acc: 0.9473 - val_loss: 0.8286 - val_acc: 0.8127\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9530\n",
      "Epoch 00026: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1430 - acc: 0.9530 - val_loss: 0.8152 - val_acc: 0.8218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9523\n",
      "Epoch 00027: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1443 - acc: 0.9523 - val_loss: 0.7812 - val_acc: 0.8295\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9533\n",
      "Epoch 00028: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1433 - acc: 0.9533 - val_loss: 0.7175 - val_acc: 0.8362\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9592\n",
      "Epoch 00029: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1254 - acc: 0.9592 - val_loss: 0.8479 - val_acc: 0.8137\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9619\n",
      "Epoch 00030: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1194 - acc: 0.9619 - val_loss: 0.8247 - val_acc: 0.8160\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9634\n",
      "Epoch 00031: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1139 - acc: 0.9634 - val_loss: 0.9243 - val_acc: 0.8069\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9629\n",
      "Epoch 00032: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1175 - acc: 0.9628 - val_loss: 1.1554 - val_acc: 0.7687\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9625\n",
      "Epoch 00033: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1172 - acc: 0.9625 - val_loss: 0.8717 - val_acc: 0.8246\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9667\n",
      "Epoch 00034: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1043 - acc: 0.9667 - val_loss: 0.8005 - val_acc: 0.8258\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9675\n",
      "Epoch 00035: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1015 - acc: 0.9675 - val_loss: 0.8135 - val_acc: 0.8358\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9672\n",
      "Epoch 00036: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1021 - acc: 0.9672 - val_loss: 0.8013 - val_acc: 0.8339\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9684\n",
      "Epoch 00037: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0995 - acc: 0.9684 - val_loss: 0.8332 - val_acc: 0.8288\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9693\n",
      "Epoch 00038: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0965 - acc: 0.9693 - val_loss: 0.8307 - val_acc: 0.8304\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9715\n",
      "Epoch 00039: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0891 - acc: 0.9715 - val_loss: 0.8747 - val_acc: 0.8265\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9721\n",
      "Epoch 00040: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0883 - acc: 0.9721 - val_loss: 0.7978 - val_acc: 0.8444\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9674\n",
      "Epoch 00041: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1027 - acc: 0.9675 - val_loss: 0.8277 - val_acc: 0.8295\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9736\n",
      "Epoch 00042: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0833 - acc: 0.9736 - val_loss: 0.8939 - val_acc: 0.8272\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9734\n",
      "Epoch 00043: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0845 - acc: 0.9734 - val_loss: 1.0616 - val_acc: 0.7969\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9760\n",
      "Epoch 00044: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0748 - acc: 0.9760 - val_loss: 0.7709 - val_acc: 0.8437\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9754\n",
      "Epoch 00045: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0796 - acc: 0.9754 - val_loss: 0.9528 - val_acc: 0.8267\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9743\n",
      "Epoch 00046: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0808 - acc: 0.9743 - val_loss: 0.8293 - val_acc: 0.8397\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9765\n",
      "Epoch 00047: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0759 - acc: 0.9765 - val_loss: 0.8626 - val_acc: 0.8318\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9794\n",
      "Epoch 00048: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0694 - acc: 0.9794 - val_loss: 0.7533 - val_acc: 0.8446\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9786\n",
      "Epoch 00049: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0717 - acc: 0.9786 - val_loss: 0.8428 - val_acc: 0.8381\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9782\n",
      "Epoch 00050: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0705 - acc: 0.9782 - val_loss: 0.9244 - val_acc: 0.8314\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9793\n",
      "Epoch 00051: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0684 - acc: 0.9792 - val_loss: 0.7913 - val_acc: 0.8477\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9776\n",
      "Epoch 00052: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0712 - acc: 0.9776 - val_loss: 0.7980 - val_acc: 0.8474\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9804\n",
      "Epoch 00053: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0634 - acc: 0.9804 - val_loss: 0.8160 - val_acc: 0.8491\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9799\n",
      "Epoch 00054: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0671 - acc: 0.9799 - val_loss: 0.8058 - val_acc: 0.8460\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9820\n",
      "Epoch 00055: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0618 - acc: 0.9820 - val_loss: 0.8569 - val_acc: 0.8514\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9804\n",
      "Epoch 00056: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0641 - acc: 0.9804 - val_loss: 1.0099 - val_acc: 0.8197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9810\n",
      "Epoch 00057: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0642 - acc: 0.9810 - val_loss: 0.8138 - val_acc: 0.8493\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9800\n",
      "Epoch 00058: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0656 - acc: 0.9800 - val_loss: 0.8157 - val_acc: 0.8430\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9816\n",
      "Epoch 00059: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0624 - acc: 0.9816 - val_loss: 0.8141 - val_acc: 0.8512\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9828\n",
      "Epoch 00060: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0555 - acc: 0.9828 - val_loss: 0.8806 - val_acc: 0.8341\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9844\n",
      "Epoch 00061: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0533 - acc: 0.9844 - val_loss: 0.7911 - val_acc: 0.8514\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9825\n",
      "Epoch 00062: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0584 - acc: 0.9825 - val_loss: 0.8275 - val_acc: 0.8442\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9817\n",
      "Epoch 00063: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0630 - acc: 0.9817 - val_loss: 0.8770 - val_acc: 0.8376\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9822\n",
      "Epoch 00064: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0611 - acc: 0.9821 - val_loss: 0.8667 - val_acc: 0.8444\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9827\n",
      "Epoch 00065: val_loss did not improve from 0.71568\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0577 - acc: 0.9827 - val_loss: 0.8844 - val_acc: 0.8362\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd8VFX6+PHPmckkk94p0hJQpBcpgihYsaAoi4quyuJa1rK6fC0/seO69rpY1lXXwtpQERFwRVEQpUkRpCmRHmo66cnMPL8/TiYFkhAgQ4B53q/XfU1m5s69ZyYz57n3lOcaEUEppZQCcDR1AZRSSh05NCgopZSqpEFBKaVUJQ0KSimlKmlQUEopVUmDglJKqUoaFJRSSlXSoKCUUqqSBgWllFKVQpq6AAcqKSlJUlJSmroYSil1VFm6dGmmiCTvb72jLiikpKSwZMmSpi6GUkodVYwxmxuynjYfKaWUqqRBQSmlVCUNCkoppSoddX0KtSkvLyc9PZ2SkpKmLspRy+1207p1a1wuV1MXRSnVhI6JoJCenk50dDQpKSkYY5q6OEcdESErK4v09HRSU1ObujhKqSZ0TDQflZSUkJiYqAHhIBljSExM1DMtpdSxERQADQiHSD8/pRQcQ0Fhf7zeYkpLt+HzlTd1UZRS6ogVNEHB5yuhrGwHIo0fFHJzc3n11VcP6rUXXHABubm5DV5//PjxPPvsswe1L6WU2p+gCQrGOAEQ8Tb6tusLCh6Pp97Xfvnll8TFxTV6mZRS6mBoUGgE48aNY/369fTq1Yu7776bOXPmcNpppzF8+HC6dOkCwCWXXEKfPn3o2rUrr7/+euVrU1JSyMzMZNOmTXTu3JkbbriBrl27MnToUIqLi+vd7/LlyxkwYAA9evRgxIgR5OTkADBhwgS6dOlCjx49uOKKKwD4/vvv6dWrF7169aJ3797k5+c3+ueglDr6HRNDUqtLSxtLQcHyWp7x4fUW4nC4MebAxuJHRfXihBNerPP5J598klWrVrF8ud3vnDlzWLZsGatWraoc4vnWW2+RkJBAcXEx/fr1Y+TIkSQmJu5V9jQ+/PBD3njjDS6//HImT57M1VdfXed+R48ezUsvvcSQIUN46KGHeOSRR3jxxRd58skn2bhxI2FhYZVNU88++yyvvPIKgwYNoqCgALfbfUCfgVIqOATNmQIc3tE1/fv3rzHmf8KECfTs2ZMBAwawdetW0tLS9nlNamoqvXr1AqBPnz5s2rSpzu3n5eWRm5vLkCFDAPjTn/7E3LlzAejRowdXXXUV7733HiEhNu4PGjSIO+64gwkTJpCbm1v5uFJKVXfM1Qx1HdGL+CgoWEZoaCvCwloGvByRkZGVf8+ZM4dZs2axYMECIiIiOP3002udExAWFlb5t9Pp3G/zUV1mzJjB3LlzmTZtGo899hgrV65k3LhxDBs2jC+//JJBgwYxc+ZMOnXqdFDbV0odu4LmTMEYB2AC0qcQHR1dbxt9Xl4e8fHxRERE8Ouvv7Jw4cJD3mdsbCzx8fH88MMPAPz3v/9lyJAh+Hw+tm7dyhlnnMFTTz1FXl4eBQUFrF+/nu7du3PPPffQr18/fv3110Mug1Lq2HPMnSnUx5gQoP7RQAcjMTGRQYMG0a1bN84//3yGDRtW4/nzzjuP1157jc6dO3PiiScyYMCARtnvu+++y0033URRURHt27fn7bffxuv1cvXVV5OXl4eIcPvttxMXF8eDDz7I7NmzcTgcdO3alfPPP79RyqCUOrYYEQnMho1pA0wEmgMCvC4i/9xrHQP8E7gAKALGiMiy+rbbt29f2fsiO2vXrqVz5877LVNBwSqcznDCwzscyFsJGg39HJVSRx9jzFIR6bu/9QJ5puAB7hSRZcaYaGCpMeYbEVlTbZ3zgRMqlpOBf1XcBoQxzoA0Hyml1LEiYH0KIrLDf9QvIvnAWqDVXqtdDEwUayEQZ4wJWC+wBgWllKrfYeloNsakAL2BRXs91QrYWu1+OvsGDowxNxpjlhhjlmRkZBxCOZyABgWllKpLwIOCMSYKmAyMFZE9B7MNEXldRPqKSN/k5ORDKIueKSilVH0CGhSMnTo8GXhfRD6rZZVtQJtq91tXPBYgGhSUUqo+AQsKFSOL/gOsFZHn61jtC2C0sQYAeSKyI3BlcgI+RHyB2oVSSh3VAjn6aBBwDbDSGONPRnQf0BZARF4DvsQOR/0dOyT12gCWp1pSPF/FZLamExUVRUFBQYMfV0qpwyFgQUFEfmQ/CYfETpK4NVBl2Js/KNjRskE1b08ppRokaNJcWDYQNHa/wrhx43jllVcq7/svhFNQUMBZZ53FSSedRPfu3Zk6dWqDtyki3H333XTr1o3u3bszadIkAHbs2MHgwYPp1asX3bp144cffsDr9TJmzJjKdV944YVGfX9KqeBx7B0ujx0Ly2tLnQ0h4iXcV4TDEQGVZw0N0KsXvFh36uxRo0YxduxYbr3VnvR8/PHHzJw5E7fbzZQpU4iJiSEzM5MBAwYwfPjwBl0P+bPPPmP58uWsWLGCzMxM+vXrx+DBg/nggw8499xzuf/++/F6vRQVFbF8+XK2bdvGqlWrAA7oSm5KKVXdsRcU6mUrY0EaNZF279692b17N9u3bycjI4P4+HjatGlDeXk59913H3PnzsXhcLBt2zZ27dpFixYt9rvNH3/8kSuvvBKn00nz5s0ZMmQIixcvpl+/fvz5z3+mvLycSy65hF69etG+fXs2bNjAbbfdxrBhwxg6dGgjvjulVDA59oJCPUf04iuluHAlYWEphIYmNepuL7vsMj799FN27tzJqFGjAHj//ffJyMhg6dKluFwuUlJSak2ZfSAGDx7M3LlzmTFjBmPGjOGOO+5g9OjRrFixgpkzZ/Laa6/x8ccf89ZbbzXG21JKBZkg61PwNxk1/lyFUaNG8dFHH/Hpp59y2WWXATZldrNmzXC5XMyePZvNmzc3eHunnXYakyZNwuv1kpGRwdy5c+nfvz+bN2+mefPm3HDDDVx//fUsW7aMzMxMfD4fI0eO5B//+AfLltWbU1Appep07J0p1COQ12nu2rUr+fn5tGrVipYtbfqmq666iosuuoju3bvTt2/fA7qozYgRI1iwYAE9e/bEGMPTTz9NixYtePfdd3nmmWdwuVxERUUxceJEtm3bxrXXXovPZ+dfPPHEE43+/pRSwSFgqbMD5VBSZwPk5y/D5UrG7W6z/5WDjKbOVurY1dDU2UHWfKT5j5RSqj5BGRQ0U6pSStUu6IKCJsVTSqm6BV1Q0OYjpZSqmwYFpZRSlYIwKIRgE+IppZTaW9AFhUD0KeTm5vLqq68e1GsvuOACzVWklDpiBF1QsKOPpFEvtFNfUPB46j8r+fLLL4mLi2u0siil1KEI0qDQuLOax40bx/r16+nVqxd33303c+bM4bTTTmP48OF06dIFgEsuuYQ+ffrQtWtXXn/99crXpqSkkJmZyaZNm+jcuTM33HADXbt2ZejQoRQXF++zr2nTpnHyySfTu3dvzj77bHbt2gVAQUEB1157Ld27d6dHjx5MnjwZgK+++oqTTjqJnj17ctZZZzXae1ZKHZuOuTQX9WTOBkAkHp8vHIfDSQMyWAP7zZzNk08+yapVq1heseM5c+awbNkyVq1aRWpqKgBvvfUWCQkJFBcX069fP0aOHEliYmKN7aSlpfHhhx/yxhtvcPnllzN58mSuvvrqGuuceuqpLFy4EGMMb775Jk8//TTPPfccjz76KLGxsaxcuRKAnJwcMjIyuOGGG5g7dy6pqalkZ2c37A0rpYLWMRcU9s8fCQKb3qN///6VAQFgwoQJTJkyBYCtW7eSlpa2T1BITU2lV69eAPTp04dNmzbts9309HRGjRrFjh07KCsrq9zHrFmz+OijjyrXi4+PZ9q0aQwePLhynYSEhEZ9j0qpY88xFxTqO6IH8HiKKS7+jfDwjoSExASsHJGRkZV/z5kzh1mzZrFgwQIiIiI4/fTTa02hHRYWVvm30+mstfnotttu44477mD48OHMmTOH8ePHB6T8SqngpH0KjSA6Opr8/Pw6n8/LyyM+Pp6IiAh+/fVXFi5ceND7ysvLo1WrVgC8++67lY+fc845NS4JmpOTw4ABA5g7dy4bN24E0OYjpdR+BXFQaLy5ComJiQwaNIhu3bpx99137/P8eeedh8fjoXPnzowbN44BAwYc9L7Gjx/PZZddRp8+fUhKqrpQ0AMPPEBOTg7dunWjZ8+ezJ49m+TkZF5//XX+8Ic/0LNnz8qL/yilVF2CLnW2iIeCguWEhbUmNHT/l8UMJpo6W6ljl6bOrlPgLrSjlFJHu6ALCsYYNFOqUkrVLuiCAmhSPKWUqkvQBgW90I5SSu0raIOCnikopdS+gjIoaJ+CUkrVLiiDwpFwphAVFdWk+1dKqdpoUFBKKVUpaIMCeGmsiXvjxo2rkWJi/PjxPPvssxQUFHDWWWdx0kkn0b17d6ZOnbrfbdWVYru2FNh1pctWSqmDdcwlxBv71ViW76wndzbg85UhUorTGUVV1tS69WrRixfPqzvT3qhRoxg7diy33norAB9//DEzZ87E7XYzZcoUYmJiyMzMZMCAAQwfPrxirkTtakux7fP5ak2BXVu6bKWUOhTHXFBoCGMMjZndo3fv3uzevZvt27eTkZFBfHw8bdq0oby8nPvuu4+5c+ficDjYtm0bu3btokWLutNr1JZiOyMjo9YU2LWly1ZKqUNxzAWF+o7o/crLsykp2UBERFeczvBG2e9ll13Gp59+ys6dOysTz73//vtkZGSwdOlSXC4XKSkptabM9mtoim2llAqUIO5TaNz8R6NGjeKjjz7i008/5bLLLgNsmutmzZrhcrmYPXs2mzdvrncbdaXYrisFdm3pspVS6lAEZVDwJ8VrzFnNXbt2JT8/n1atWtGyZUsArrrqKpYsWUL37t2ZOHEinTp1qncbdaXYrisFdm3pspVS6lAEXepsAK+3mKKi1bjd7XG59BKVfpo6W6ljl6bOrkcgmo+UUupYoEFBKaVUpYAFBWPMW8aY3caYVXU8f7oxJs8Ys7xieehQ9ndgzWD+t61Bwe9oa0ZUSgVGIM8U3gHO2886P4hIr4rl7we7I7fbTVZWVoMrNr3QTk0iQlZWFm63u6mLopRqYgGbpyAic40xKYHafnWtW7cmPT2djIyMulcqKoLsbGjRAkJCKC3NwuEowOUqPBxFPOK53W5at27d1MVQSjWxpp68NtAYswLYDtwlIqtrW8kYcyNwI0Dbtm33ed7lclXO9q3TrFkwdCjMng2nn86SJX8kLKwNnTt/cajvQSmljhlN2dG8DGgnIj2Bl4DP61pRRF4Xkb4i0jc5Ofng9uYPJhUTyJzOWDyevIPbllJKHaOaLCiIyB4RKaj4+0vAZYxJCtgO9woKISEaFJRSam9NFhSMMS1MRbpQY0z/irJkBWyHbjc0bw5btgA2KHi9GhSUUqq6gPUpGGM+BE4Hkowx6cDDgAtARF4DLgVuNsZ4gGLgCgn0uMh27aqdKcTh8eQGdHdKKXW0CeTooyv38/zLwMuB2n+t2rWD5fZaC7b5aA8iUu/1DZRSKpgE14zmdu1s85HPh9MZC/jweguaulRKKXXECL6gUFoKu3cTEhILoJ3NSilVTXAFBf8IpC1bCAmJA9B+BaWUqia4gkK7dvZ28+bKMwUdgaSUUlWCPiho85FSSlUJrqAQFwcxMbB5c0VHswYFpZSqLriCAlTOVdA+BaWU2lcQBwU9U1BKqb0FX1Bo2xa2bMHhcGOMSzualVKqmuALCu3aQW4uJj9fk+IppdRegjMoQGVnswYFpZSqEtRBQZPiKaVUTUEeFPRMQSmlqgu+oNC8OYSGVqS60GsqKKVUdcEXFBwOOwJJzxSUUmofwRcUoDIohIYeR1nZDrzekqYukVJKHRGCMyhUTGCLju6HiIeCguVNXSKllDoiBG9Q2LGDmLAeAOTn/9TEBVJKqSND8AYFICxDCA09jvz8xU1cIKWUOjIEdVBg82ZiYvqzZ4+eKSilFGhQIDq6H8XF6ygv10lsSikVnEGhdWswpiIo9AcgP39JExdKKaWaXnAGhdBQaNmyIij0BdB+BaWUIliDAlQOS3W54ggP76gjkJRSimAPClu2ABAd3U87m5VSimAPClu3gs9HTEx/ysq2U1q6ralLpZRSTSq4g0JZGezcWdnZvGeP9iuoBlq4ENLTm7oUSjW6BgUFY8zfjDExxvqPMWaZMWZooAsXUNWGpUZF9cKYEO1sVg3j9cLQofDAA01dEqUaXUPPFP4sInuAoUA8cA3wZMBKdTi0bWtvN2/G6XQTGdlDO5tVw6xeDfn5sGJFU5dEqUbX0KBgKm4vAP4rIqurPXZ08p8pVOtszs9fgoivCQuljgqLFtnbtWvB42nasijVyBoaFJYaY77GBoWZxpho4OiuPaOjIT4eNm8GICamPx5PLsXFv9dcr7gYTjsNvv66CQqpjkj+oFBaCmlpTVsWpRpZQ4PCdcA4oJ+IFAEu4NqAlepwqZirAFTrbN6rCWnWLPjxR3juucNdOnWkWrQIWrWyf69c2bRlUaqRNTQoDAR+E5FcY8zVwAPA0X/JsmpBITKyMw5H5L6dzVOn2ttZs2CbDlkNevn5tk/hmmvA6YRVq5q6REo1qoYGhX8BRcaYnsCdwHpgYsBKdbh07GhP//PyMMZJdHSfmp3NPh9Mmwb9+tm/P/ig6cqqjgxLloAIDB4MJ5ygZwrqmNPQoOAREQEuBl4WkVeA6MAV6zC59FLbLvzpp4C/s/lnfL5y+/yiRbB7N4wdCwMHwsSJtkJQwcvfn9C/P3TvrkFBHXMaGhTyjTH3YoeizjDGOLD9Cke3fv3gxBNtZY/tbBYppbCw4of+xRcQEgLnnw+jR9umAh2GGNwWLYLjj4fEROjWDTZsgMLCpi7VoduyBebNa+pSqCNAQ4PCKKAUO19hJ9AaeCZgpTpcjLGV/dy5sGkT0dH9gGoZU6dOtc0E8fFw+eU2u+rEo7/VTB0kERsUTj7Z3u/e3T62Zk3Tlqsx/O1vMGyYngmrhgWFikDwPhBrjLkQKBGRY6N2vOoqe/vee7jdKbhcSezZs9D2NaxdCxdfbJ9PSIALL4T339ex6cEqPR127KgZFODob0IqKYFvvoG8PJsPTAW1hqa5uBz4CbgMuBxYZIy5NJAFO2zatYPTT4eJEzFAbOwQsrO/QfyjjoYPr1p39Gjbx6BzFoKTvz/BHxTat4fw8KM/KHz/fVUTmI6mCnoNbT66HztH4U8iMhroDzxY3wuMMW8ZY3YbY2r9llXkUZpgjPndGPOLMeakAyt6Ixo92p4ZLFpEYuKFlJVtw/v5h9CjB6SkVK13/vm2LVmbkILTokW2CbFnT3vf4YCuXY/+oDBjBoSF2b81KAS9hgYFh4jsrnY/qwGvfQc4r57nzwdOqFhuxA57bRojR4LbDRMnkph4Pq48cC74ueZZAtgK4cor4fPPIVev6Rx0Fi2C3r2rKlA4+kcgicD06XDOOXZC3urVTV2iw2vhQli6tKlLcURpaFD4yhgz0xgzxhgzBpgBfFnfC0RkLpBdzyoXAxPFWgjEGWNaNrA8jSsmBkaMgEmTCJU4jlveHuOTqv6E6kaPrjGMVQUJj8dWHv6mI7/u3W2T4u7dtb/uSLd2LWzcaPvLunULrjMFnw8uuwyuu66pS3JEaWhH893A60CPiuV1EbnnEPfdCqjeq5Ve8dg+jDE3GmOWGGOWZGRkHOJu6zB6NGRnw5df0myhm9IkKOvWet/1+vaFTp20CSnYrFoFRUW1BwX/80ej6dPt7bBhtilszRqbGjwYLF5sBw+sWAFZWY2+eRHbh3+0jUsJaeiKIjIZmBzAstS379exQYm+ffsGZszc2WdDixbw+utE/LCR7WeBM3cmLVr8qeZ6/mGs991nx6i3bx+Q4qgjzN6dzH7VRyCdeebhLdMh8PnsV9lMnw69ekHr1ni7dKegJJT8+VvIT0plzx4bHxwOm9HDv7hctiXVvzidNl4WFNj+6sJCmw0kL8+2subl2cXrtdN+qi/+bTocdjEGysvtUlZmFxHbuhsebhe321a0e/bYJS/P7s/nq9qW02nfZ1FRVZkKC+123e6K7W2Owc2nOPHiHVaCt4Uto9db9T79i9Npy1JcbCv64mLbYFBaWlVO//2SEruUlVV93qGhtuwREfbW5dr3c/B67XvwL/6y+B/3euHmm+GeQz0c3496g4IxJh+orRI2gIhIzCHsexvQptr91hWPNY2QEPjjH+H55zFA3uAEJGv6vkEB4Oqr4f774b334KGHDntR1eHl9YJn/lKcic1xpravmTO+WTNISqrsVygpgYwMu+zebU8+PZ6ai79C9leCDofdR/UKpqzMVmAeT9VtWZmt2PyVb0GB3V/1ysPrtQmAW7SwS/PmtniZmbBpk031tWkT7NzpfwNzATAOEBkDjIHBjf8Zut22IvR/BuXl+3+NMbYyNca+z9qEhkJsrH3PDkdV5enz2WASHg5RURAZaacbuVz2cy4uFvK3lVMceRK+ohKcaQZHcVWQ8nqrglN5ub0fFlYtoITb/YaF2TL4b/2Vv3+9sDD7fouKai4eT8X3ylP1tz8wVg+S1YOxw3F4jkGNBHCyijEmBZguIt1qeW4Y8FdsOu6TgQki0n9/2+zbt68sWbKkkUtaYcUKe9QUFcW6+ZezK/cTBg3KxOEI3Xfds86yv7C0NPutrc3NN8Nxx8GD9Q7UajwFBfDUU3DjjdCmzf7XP0aI2B969R9P9ef8lVBZGezaZSfv+pf0dHs06z/q3LPHHnX6j/ZKS/c9/Tdmr6PbshIc+BB3BEVFjfOejKl5NOk/Yo2MtJWcf3G79z3azs+3lb5/8Xjsa9u1s0tKiv1aOtesRCZPxnft9Uir1ji9ZcQ8MY7oi88i+ophREfb1+0ddPY+kvd47BGwv2z+27g4W3HGxtrKcu//WfUKvPriPxPxH+1X/x/7j9KdzqpK+aAsX24HDbz+OnzyiZ1/cjQPGGgAY8xSEem7v/Ua3Hx0EAX4EDgdSDLGpAMPU5EaQ0Rew3ZUXwD8DhRxJKTi7tnT5jg68UQSWl7M9qy3yMv7gfj4s/Zdd/RoGDMG5s+HQYP2fX7tWnjtNTuE9b77an7DA2XKFPjHP+DNN22Kjn79Ar/PQyRij5yysuxRdVZW1ZF2ZqZdfL6qo7CwMFvx7dhh51lt2WJvCwqqtmmMrUih/iNSY+zRdGKiHWuQkGArzKioqqO9sDBwmxKcjz2K94yz8Q4+o8YpvQj4Zs1DflkFN91OfIKhWTNITrYnEQkJttzVmwqMsa/zV4w+n328+tGm01n3scaB8PlsoIuJqRksAbjqSUj+Bt54EJwAofDR5+DeDlcM23djeXl2mPYLL8CoPxxSufyBtaE/C2Oqjr7j4g5p19bkyfYDueQS+yW77z77pUtOboSNH90CFhRE5Mr9PC/ArYHa/0H7/ntwOIinBGPCyMqaXntQGDkSbrnFdjjXFhQmTLC3WVmwYAGcempgyw224ywiwv5yhgyxzVt/OLQfb0P5O9Wqt9/6l6IiWzFt21Z1hL51q72fnV2z7bU6Y+wpf0hIVZttWZmtkJs3tydDnTvbyyU3a1Z1VuA/HRexFWz1tuHkZHsl1nbt7AhMV0MyeH07Dx57HO4ZDOfW8vybG+GGsXDrRUdcH5PDUUcl6vHA//5nh11Xr5m7dat7WOq339p/3tNPH7bvVcBMnmxT2CQnwxln2MfmzLGjkYJcwILCUauilnASSXz8GWRlTef441/Yd72oKBsYJk2Cf/7TVsR+2dnw7rs2X9KUKfao/XAEhZ9+smcHH39sj4BGjoQnn4T/9/8O+rCzuNi+nexsyMmxzREbN9o+dv+ye7et+H0NuBZfVJStlNu2tS11iYl2SUiwS2Ki/Z0mJ9v7tR1J+ny1HPUGUvXMqLXpVtE6unLlERcU6rRwof2HDtvrjKBrVxssysr2bfPxz+RftMg2tfon8TUWnw9++cV+MQJp7Vq73HKLvd+nj/1izp6tQQENCvVKTLyQtLS/UlS0joiIjvuuMHo0/Pe/ttK//PKqx994w9amDz5oG6y/+MIeXQVSWRn8/LNNbNasGXz3HVx7LYwbZ/s+Xn11n5f4fLaCX77c/sbT020Fv2tX1dD7ujr4kpNt/Xfyyfao3d+WXH3xtzP725hbtbLtwIfaLHJYA4KIvaZG5872tKU2Xbva25Ura5/bciSaPt2egg0dWvPxbt3sWURaWtX78vvmG3tp2sWL4d//rvU7dUiee84ewHz3XdXReyBMrhhEOWKEvXW57EHbnDmB2+dRRINCPRISbF94VtZ0IiLu2HeFM86wNd3EiVVBobwcXn7ZDnHt1s2env/1r/DbbzZNd6CsXGkDg78fwe2GDz5AYuPI+tfHbLzwETbkJ7Nxow0Ea9bYQJCfb1d3OKBlSxtPmje39UGzZvbIPT6+6kg+KQlSU+1oj6Awa5Y9qq6vAoyOth/K0dRROX26bT6Jja35uP+sZ9WqmkFh/Xp7WnjHHfa9vveePdCJimqc8mzdCuPH27/ffjvwQWHgwKpLqoLd3z332FPhFi0Ct++jgYgcVUufPn3kcPrpp27y889n1L3CuHEiTqfIzp32/kcf2T7EadPs/c2b7f1nnglsQV99VTJJkJkTd8rzz4vccIPIoEEi8THlUtWtaZekJJFTThG59VaRN94QWbxYpKgosMU7Kvl89kNs3VqkpKT+dYcPF+nS5fCU61Bt2GC/CM8/v+9zxcUiDofIgw/WfPzVV+1r1q0T+fFH+/cbbzRemf7wB5HwcJFhw0QiIkT27Gm8bVe3fr0t+7PP1nz8p5/s4x9+GJj9HgGAJdKAOrbJK/kDXQ53UPj993tkzpwQKSvLqX2F1atr/sAGDBA5/ngRr7dqnV69RE47rVHLlZsrMn++yD//KfLHP4p0iN65T8U/eLDIX27wyvNh4+Tzc1+VFSsO8bc2ZYrIlVeKlJc32vs4on3zjf0wX3ll/+ved589ONhf8GhqPp/INdeIGCOSllb7OicyPiboAAAgAElEQVSeKDJiRM3HLrlEJCXFvt7nE+naVaRv38Yp04wZ9nN+/HH7pQaRt95qnG3v7Zln7PY3bKj5eHm5SEyMyI03Hvy2vd7A/TYyM21gnjfvoDehQaGR5OT8ILNnIzt3flD3Sn372op/wQL7kb70Us3nH3rIHn1lZBxUGfLyRN5/X+S220TOPlvkuONqHvkfd5zIiOhv5MlOb8l334ns3r3XBs4/X6RTp4Pad6XffhOJjLQ7/OKLQ9vWgdiyxR69Hm4+n8ipp4q0atWwiv7DD+1ns3x54Mvmt3ChyOTJB/aat96y5Rw/vu51Ro4UOeGEqvtlZftWmBMm2O0sXXpg+99bUZFI+/b2+1laaj/3jh3tEU0gDBggctJJtT934YV23wejpMSefp91ln0PB2LGDPuZP/CAyNSpItu328eLikQmTRK56CKRkBD7ed9xx8GVTzQoNBqfzyPz57eTpUtPqXull16yH2WfPiKxsSL5+TWfX7LEPv/uuw3eb26uyMSJ9vsQGmpfHhkp0q+fyOjRIk8+ab8/6eliD/+NqfuH/uSTdgP+Jq4DVVpq31t8vEizZrZQgVReLvLZZzYCgv0Rb9sW2H3ubdYsu++XX27Y+r/+ate/777Alsvvww9FXC77f58/v2GvWbXKNtGceaaIx1P3eg8/bLfrb1P0Nxd98knVOjk5dlu1HVlv3Sry888NK9NDD9ltf/tt1WOPPWYfW7++YdtoqK1b7XYfe6z255991j5/MN+1226rOkr7+OOGv+6zz2yFn5hoDxz922jVSiQ6uuqo76677AHHgQacajQoNKKtWyfI7NlIbu6Pta+QkVEVye+8c9/nfT77jx05ss59+Hwiv/xiz27POacqELRpI/J//2fPGqu3SNUwZ45d+csva39+4cID/7JWd9dd9vWffSZy7732y5uefnDbqk9mpsg//mHfdPU3Hxlp//7ll8bfZ218Ptvc16rVgZ2l/OlP9rNZsCBgRROfr6oJ5LTTbH9Hly77P5spKBDp3FmkeXORHTvqX/fjj+32ly2z9x9+2L6vrKya640ZIxIVVdUm6fOJvPaafSwsbP8HIevW2fWuvLLm41u22KD08MP1v15EvD6vFJYVSnZRtpSU1/4ZlHnKZHPuZpk3/s+yvDk2gNdm6VIRkKKJb0mZp6zOfZZ6SmX17tWyZNsS8fq89mwNbGDo3t2e+ZSUiMfrkflb5suSbUskryRv3w1NnmzrjYEDbXNAYaENwC+8YNuEr7/eBsv6AvgBaGhQCGiai0AIaJqLOni9hSxY0I7Y2FPo3v2L2le65BI7dHH9+poX5vG7+WY7fDUzs3JOw9atMOebcr6blMHXq1qyfbsdq9m1K5x3Hlx6qR0av98hmM88Y4fyZWTY4UF7Ky+3Q4jGjLEjow7EzJm2MDfdBP/6l31/xx8Pjz4KDzxwYNuqT2mpHTm1cqXN7X/LLTadc0iIHTM7bJgdKvXJJ3BubTPIDsKWLTaJ3ZAh9vPzjw777jubxuSll+zIsYbas8eO3Xc6bZkba2SOn9drR/9MmGBHu737rp1QduGF8PDDrLx5JJNWTyLCFcGFHS+ke7PuGP/43zFj7Ci5b76x760+a9dCly6898pfKOzfm5F3vklSqdOOwqpu4ULKBw1k00t/p/V5owi/6a92+6eeCvPmwf33k3v/nczdPJfZG2dT6i2lfXx7UuNSaR+XSvsb7ib2h8V2ZF7Lmlnztw8bzGeyhk8v78byncsBMMZgMDiMA4/PQ7GnmDJvzZmPYc4w4txxxLpjCQ8JZ1fhLnYV7EKqpXD7U88/8dzQ50iMSKzx2pLSQp4dnsTjA8opdQqtoluRGp9KalwqzSKbsSFnA2sy1pCWnYbHZ3OftI5oyai5WVxRcjx9pi+D2bNZfP35fHjrYCaFprGjYEfl9ptFNqNjYkdOSDiB9tuLSf33JNof143U/0wmNqk1m3I3sT5nPRtyNrA+ez3GGLokd6lcEsIT9vcNqVdD01xoUGigjRvHs3nzI/Trt5rIyC77rrBpk/0xnX9+7Rv43/8ouOAyvvh/8/g2sydz5tgRfgAJZHF2j92c+7fODB0KrWvJ2F2vyy+HJUuqNlibc8+F7dsPbNjkrl22kktKsmPTw8Pt42efbYPD+vWNN2nggQfgscfsBYxqG+ufnm4rv1Wr4JVXbH6nOiY8+MTHlrwtpGWlsT5nPeuz17MhdwObcjfRu0Vv7hh4B12Su8DDD9vg5nbbCRkjRth5HXfdBb//bt9f9UmJDfHDDzbIXH+9zasD7CrYxZLtS1i6YylLti9hZ8FOwl3hRLgiiHBFEB4STlJEEsdFH0er6FYcRxQtpn6LlJVREhlGcXgIJeEuvPN+JGHWPJKu+DPJj71IRFgUOwp28MG9F/Ke72dWNAenceIVm/q6bWxbLup4EcM2h9Lu4ReIumUsUfc8SFRoFKHOWvJ5VZCyMh4aFs4/TrWzEZ0+ONt04IpLHuDiEy9m656tfLvhW77d+C3fr/kfBS4fRiA1z9C5WVc69zkXx+dfMFs2sLSl4BMf7hA34SHh5JTk1NhXgomgQ8uudEjoQIf4DkS6IpmRNoN5W+cB0DUihSFdLiDEEYJgj2R94sPldFVu0x3iJiwkjOLyYnJLcsktySWvNI/C8kJaRLag1cZMWk/8nFYd+/DjtWfx9KLniXfH8+J5L3JltysxxjBj3Qz+9tXfWJ+znpGbI+l6zZ1szN3IptxNbMzdyK6CXaTGp9oKOslW0r7yMj75zx18lZRLuRM6xHdAEDbkbCDUAxd0HMao3lcT6gwlLSuNtOyKZdtKdnhy9vncq4t0RSIIReVVybSaRzbnrlPu4q5T7jqw72QFDQqNrKwsk4UL29Ks2Sg6dXq7wa/zeOzB03vvevl8UglFRBIfD0P6FHD68hc5PW8q3Xs5cSxfVnVlrwOVkgIDBsBHH9W9zuOP28yuGRms9O6g3FfOSS0rroBaUmJfn59vzwJOOMHeTp9uj/gWL64avw52FvcVV8DXX7NzYHdW7lpJXmkee0r3kFdib73irTyqM8YgIuSV5pFdnF25hLvCuf+0+zk9I9KOGx89Gt56q+73kJ9v9/vll9CmDd6zz2T9kB6s6BjLqpIt/Jr1K79l/sa6rHUUe4orXxbmDCM1PpXWMa2Zt2UexZ5ihp1wAXf/cwmDY3tg3nvfnhW8/HLVFfUmTIDbbmvwv2B34W5u/9/t7CjYQdnG3ynbuZ2y9u3IdpWzPX87AAZDp6ROtI1tS4mnhKLyIoo9xRSWFZJZlEl+WX6D9wfgDnFT5i3DJz5O3hnC1VmtGPXmQjxGmJE2g2m/TuWbtJkUs28CqHh3PA8NeYi/9v8rIY6q6Uo+8TH2q7G89NNLXJ/RhlsG3s7Hb9/NpHNasrFkR41tdEzsyFlFzenz0Q9s7ZnC2iFdWVu8hXVZ6/D6PAzY5OWsTudz5qhxnNzqZMJCwsgtyWVjZhob/jyCDeGlrL9uBOvzNrI+ez1b8rbgFS89mvfgsuMvZuT1z9H5zFH1fyf2Z9Ikm/34jDPsmXx4OL/s+oUbpt3AT9t+4vzjz8fpcDJ93XQ6JXXipeLTOfvu1+yEz7Zt69/2HXfACy+QM+kdPj/Bx8drPgbg8piBjLj8YeJuvROefbZqfZ/P5o265x6KT+nHpokvsaFsFxtyNpBXmkdKXAod4jvQIaEDyRHJCMKWvC2syVhTuZzb4VxGdRt1UB+FBoUASEu7je3b/83JJ2/A7a7/cH79etva8t//2pnB8fFwedxMrsr/N4M+GYtj5Ah7lD11qm226N7drrR0ae1Hp1lZtlmiIpHNzoKdvLjwRUa2OIN+3c+zX74776y7QPPmwamnsvC/T3D2ln9QWF7IX/r8hSfOeoL4Dz6zR7bDhtmzg7Q0m/wM7FG5Px2AX2kpGzq34OmLEng7OX2fU/i6RIdGkxCeQHx4PAnhCazLWkf6nnRGpEfzzLxIOsxfWyNRT2FZIQvSF7A9f3tlsMkrziHrl0WsylzNKlcORRW5ixwCqZGtObFVDzoldqJTUidOSDyBDvEdaBXTCoexZzSZRZm8uvhVXp73AhnlufQKa0dyq44UlBVQULKHgqwdSFkpXU48jZ7H9aZn8570aN6DjokdcTpqz95WWFbIGe+ewcrdKxnQegChJoTQ+T8RWlRK1PkX0ytlAH0SutJ7fSHR3y+0we3UU+0ZhX8CVXo6+bf9he0/fMn2Ph3ZedsYHCmphHsduMt8uMt8OMLcZEeHkFmUSUZhBplFmUSHRXNFtyvo+NVim9L9xRftrPZvv4W//pXi339l/qhTyLxlDAVuh32fZQXM3TKXr9d/Tc/mPfnXsH8xsM1APD4PN0y7gXeWv8P/ZZ/Ic1OLMeedbydBZmayOGM5X6Z9SUpcCmelnkWb2Da2olu82Db9VZw1en1eyn3luAefab9P69bVzFfy73/b5sgpU2yza4Vybzl5pXkkRVQ0gV53nU3ZsnOnnRZfm19/tRPppkyxMy379rWLvyny8svhlFNs6o5q2/D6vLz808vc/939GGN4eMjD3H7y7YSuWmvTbPTqZdf357ouLbVpP/x5s10u+PFHuPXW2ptk//xneP99W77UVFsJjBljyzFihG36O8wzQDUoBEBx8UYWLTqB1q3Hcvzxz+7zvM9n08O8/LI9mHU6bUvINdfYZvmwj961XwyHwx6Nz5gBHTrYF/vb7seOtUcT1U2dao+iU1Io+2kBE5a+yt+//zv5ZfnEOiP5/uVCen4y16YgqEtZGSuPj2HItZCQ2JphJwzj5cUvkxyRzAuzQrhiVxJm2c9gDFmFmSxc8zUrty4msV0X2sS2oU1MG9rEtmFr3laenPckH654H6dX+HPPP3Flvz8T744nJiyGWHcs0aHRlRWo/3Qf2KdSLS4v5rlHzuVJ+YGysBBuH/A3hnYYatugN83mp20/Vbbd+vnbjDsnd6Znsx70LI6hx5osur7+Oe4dGTa1yL337jfTXfHVo5i4ZRoTR/dEjCEqNKpy8fg8rNq9irWZayv3f0LCCXxy2Sf0bFEz34/H5+Hijy7mq9+/4vNRn3PRiRfZJ1avtjl1eva0FcmCBbZvx+Wy9/1TyTt0sB1H06fb08pHH7WVesgBJhsQsUH9++/t9+izz2wekgkT9s1vhP2/fLb2M/721d/Ylr+N63tfT05JDpPXTmb8kPE8NC8E88ADtq2/Xz/7HTxQn31m82998ontIIOqs9ETT7RlrS/nyQ8/2FnXEyfaHxHYz2jzZvvb+e9/bbOpwwGnn25Ty/z8c83cLAMG2B9lHRXwroJdOIyD5MiK7Kg+nz0bTU+3eVr8S1jYvlfZad/eHvnVdhC3bZv9jV98Mdxwgw3Y2dn2t33TTY2TAvcANTQoNPloogNdmmL0UXWrV18pc+dG1ZjM5vGIvPmmHdoNIi1a2IET/uHGft7du+TDPmHy9YgeUp6592QCEc9fb5EpnZDTnu8usU/EyrkTh8pj9w+WH9sgJalt5asOyImPNhfGI8PeHyaz1s+SVg9HS/O7kHVb6x8fn5aVJi3uDZVW97hkY85GERFZtn2Z9HumozAeOeepbnLt59fKiS+dKIyn3iXysUi584Mxsi2aQ5upvXChiMMh2268Uq79/Fox440wHnE+4pST3zhZ7vnmHvkq7Sv5Pet32V2wu87RJSJiR8b88Y/2H9C3r8iaNXWvm5Mj4naL3HxzvcUrKS+Rn3f8LG8ufVOOe+44cf/DLW///Hbl8z6fT66bep0wHnlt8Wv7buCll+wompNOErn7bpGvvrKjgDweO9Ll+edFLr5YJDlZ5NxzRX7/fT8f2H5s3mxH/rjdIo880qCRU3tK9shdM+8S5yNOYTzy/PyKSZiffy6VwyMbOix3bx6PncjZv3/VUMoHHrDbXLRo/6/3+exInvbt7ZC8Dh2qRvmBnRv03HM1f2hlZXbo5htv2KGnOXVMOj0cHnzQltMYOw9jxYqmK4s0fPRRk1fyB7o0dVDYs+dnmT0b2bTpcRGxv/Nu3ewn2a+fyAcf2GH9eyv3lss1n11TWbEmPZ0kf5n2F/l2w7eSW5wrLy16STq82F4Yj6Tc6ZQ/vztSut4dWbl+6KOhwnjk+LEOmb7so8rtrhlxmiTd65R2L7ST9Lzah4mm56VLyospkjg+QtYkI5KdXfmcZ+g58tLZMRLzRIwkPpUoF31wkTw+93GZs3GO5BbnyubczfLj5h/lw5UfyjPznpFn5z0rmYWZ9sWDBtnZrwczdrqoyP5Q2rSxw/FEZOWulfK/tP/VPnyvoT75xI75drtFXn+99nX+9S/7D1u8uMGb3Zm/U85890xhPHL91OuluLxYxs8eL4xH7v/2/rpfWNuXIZDWrLHB4QCt3r1aZq2fVfXA779XVb7r1h18efzpMX74wc4TCA/fdwhqfV55xc6N6ddPZNQoOw/kP/+xcy6OdHv2iPTsaYeWFhQ0dWk0KATS8uVD5Z13TpdzzvEI2AOZDyaVSnpe7ZNeSspL5JKPLhHGI3+f83eZsnaKXPHpFRL5mK30/UfIA98cKJ9Oe0rKXU47Ltztloz/vCSfrflM/u+r/5PnP7lDSkKomgvh84kkJsqSmy6W6MejpfPLnSWjsGrWdEFpgfyU/pN0frmzRD8eLYu/eM3+y6dOtSusXCn+9ALl3nLxHWjl/s479vXff39gr1uyxB49gk0l0dh27BAZOtQeoX311b7P9+9vI/kBvl+P1yP3zbpPGI+kvpgqjEf+NOVPB/65HQ28XluB+1NbHKzCQhukL77YzmsIDRXZuLHRiqkaToNCgOzZI3LjjVvF4fBIbGyRPP+8yK7cXDnlP6eIGW/kD5P+IAu3Lqxcv6C0QM6ZeI4wHpmwcEKNbRWWFconqz+Ru2beJfO3VJuV+s9/2hnEtaVMuO46O5N13bqq5F6vvSazN86WsEfDpNur3eSC9y+QlBdTKs8y3P9wy+yNs21zQlhY1VT5a6+1ycf2npTUUAUFNv3BGWfsf0KUiD2Vv/VWW1k3bx7Y5GOFhSI9eogkJNSshFatkjqTwTXQ9N+mS/yT8XL+e+fXO8npqHfjjSJPPXXo23noIfs/N8Y2o6kmoUEhAKZMsZNcjRG5/PKpMmNGquzK2yD93+gvIX8PkeunXi9xT8YJ45HBbw+WyWsmyyn/OUUcjzhqtEUfkh07bLvx8OFVGVkrZp5+8esX0uq5VtLjXz3kik+vkL/P+bt8uvpT2Zq3ter1gwfbgLNjhz1qu+WWQyvPyy/bIBUZKfLoo7Yy3ltBgcjbb9tmAIfDzvw8HG29aWk27UifPlXt63feadul90kQdWCKyorsbFa1f7t22ea8hISmbeMPchoUGlF6uk0aCXYW+4IFtm/h86+RrhOaS+ijofLFrzZJ3J6SPfLCghek7QtthfGI6+8u+XT1p41boCeekMpODLfbdq411IMP2or59tttdPvtt0MvT1qaTX0MNu3CO+/Y1OF33y1y8slVnYMnn1yVOuFw+eILu+/rr7efU7Nm+2YAVYH30UciM2c2dSmCmgaFRrJokUhcnK17n3iiqv7dXbBbTnwhXlyPIJ+v2jfRXZmnTCatmiQ/bq4jX9KhKC4WSU21/76BAw/stf5Eb8bYs43GNHeuHfnj76AMDbX5ee6/X+Trr+tJ3hRg999vy3PZZXLYs7wqdYRoaFDQK69VU+Yt48ZpN1JUXsTNfW8mYvfpDB1qSEqCr76yw469Pi+zNsxm7Fdj2VJYzBPdQzjROQcYXWNbLqeLy7teXut+DpnbbfMdXXpp1ZXWGmrgQDtWvry8/sluB+O00+ys7K+/thN8+vevSo3RlB55xF6/+pNP7FW16kpFopTSyWt+Xp+XKydfySdrPiE2LJa80jwcmV2IX38Lc1+6hpKI33nvl/f4aNVH7CjYQbw7ns9GfUZrz3TS01+gb98VREV12/+OGouInSV3wQVVE+Aa6pxz7CSiBQuaZBJNk8jMtMnvRo+2uY2UCjI6o/kA+MTHdV9cxzvL3+G5oc/Rl5s5785J+Pq8QmnSEkIcIXh8HlwOF8M6DuPq7lczrOMw3CFuysuzWbSoAzExg+jRY3qjlitg9uyxtzExTVuOw00keIKgUntpaFAI+uYjEeH2/93OO8vfYfyQ8ZwWcgfnnAMtE8cw++4x7HT+xIcrP6RLchcu7XIp8eHxNV7vciXQtu29bNhwD7m53xMXN6SJ3skBCLZg4KcBQan9CuozBRHh3m/v5al5T3HXwLsY2/Vpevc2REXBnDn7T5Lo5/UW89NPHQkNbUHv3gtwOII+1iqljjANPVNopGT4R6c3lr3BU/Oe4qY+N/HEmU8zerShsNDm2mpoQABwOsPp0OFZ8vOXsGXL44ErsFJKBVjQB4WTWp7EK8Ne4amnDN99Z9Pqd+584Ntq1mwUzZpdxaZNfycvb37jF1YppQ6DoA0K2/O3s2T7EkZ2Hsn8eQ4efhiuvBKuvfbgt9mx4yu43W1Yu/YqPJ68xiusUkodJkEbFGasmwHA4OYX8cc/Qrt28Nprh9YXGRISS+fOH1BSspV1625tpJIqpdThE7RBYdq6abSLbcez93Rj50571b7GGJQTGzuQlJSH2L37fXbufO/QN6iUUodRUAaF4vJiZm2YRQfvRUz93PDUU/YKfo2lbdv7iIkZRFraLRQXb2i8DSulVIAFZVD4duO3FHuKyV10EZ072ytgNiaHI4TOnd8DDGvWXInP17BrGCulVFMLyqAw7bdpRIVGse6bIQwZEpg5TeHhKXTq9Bb5+T+xfv3djb8DpZQKgKALCiLC9LTpDEg6l4LcMAYNCty+kpNH0qrV39i2bQK7d38SuB0ppVQjCbqgsGzHMrbnb6dl/kUAnHJKYPfXocPTxMQM4LffrqOoaF1gd6aUUoco6ILCtHXTMBiKVlxAixaQmhrY/TkcoXTp8jHGhLJ69aV4vUWB3aFSSh2CoAwKA9sMZOncZAYNOjw50tzuNnTu/B6FhatIS9P5C0qpI1dQBYVte7axbMcyhrS4iE2bCGh/wt4SE8+jXbsH2LnzHTZvfoKjLRGhUio4BDQoGGPOM8b8Zoz53RgzrpbnxxhjMowxyyuW6wNZnunr7PUOkrIOT3/C3lJSHqZZsyvYuPE+1q27CZ+v/PAWQCml9iNgOZ6NMU7gFeAcIB1YbIz5QkTW7LXqJBH5a6DKUd20ddNIjUtly7IuuN3Qu/fh2GsVY5x07vw+bncqW7Y8QUnJJrp2/ZiQkNjDWxCllKpDIM8U+gO/i8gGESkDPgIuDuD+6lVUXsS3G7/loo4XMX+eoX9/CA09/OUwxkH79o9z4olvkpv7HT//fColJVsOf0GUUqoWgQwKrYCt1e6nVzy2t5HGmF+MMZ8aY9oEqjCzNsyixFPCOe0u4uefD29/Qm1atryO7t3/R0nJFpYtO5mCghVNWyCllKLpO5qnASki0gP4Bni3tpWMMTcaY5YYY5ZkZGQc1I66JHfhocEPEbZzMB5P0wcFgISEsznppPkYE8Ly5afrdRiUUk0ukEFhG1D9yL91xWOVRCRLREor7r4J9KltQyLyuoj0FZG+ycnJB1WY4xOO55EzHmHxQttmNHDgQW2m0UVGdqV37x9xuZJYseIcsrO/aeoiKaWCWCCDwmLgBGNMqjEmFLgC+KL6CsaYltXuDgfWBrA8AMybZ6+slpAQ6D01nNvdjl69fiA8/HhWrryQjIwpTV0kpVSQClhQEBEP8FdgJray/1hEVhtj/m6MGV6x2u3GmNXGmBXA7cCYQJUHwOeD+fOPjKajvYWFtaBXrzlER/dh9epL2bmz1pY0pZQKqIANSQUQkS+BL/d67KFqf98L3BvIMlS3di3k5h6ZQQHA5YqnR4+vWb16BL/+ei0uVzKJiRc0dbGUUkGkqTuaD6v5Ff24R2pQAAgJiaJbt8+JiurFmjVXUFi497QOpZQKnKAKCvPmQXIyHH98U5ekfk5nJN26TcXhiGDlyosoK8ts6iIppYJE0AWFU045PEnwDpXb3Ybu3adSWrqN1asv1au3KaUOi6AJCrt2we+/H9lNR3uLiTmZTp3+Q17e96Sl3aZJ9JRSARfQjuYjyYIF9vZoCgoAzZtfRWHharZseYKQkHjatv1/uFxH0HhapdQxJWjOFLp3hyeegD61To87sqWm/oPmza9m69anWLCgNb/9dpN2QCulAsIcbU0Sffv2lSVLljR1MZpEQcEvpKdPYPfu9/H5SoiPP5v27Z8mOvowp3tVSh11jDFLRaTv/tYLmjOFY0FUVA86dXqTAQO2kpr6OAUFK/n550Hs2vVhUxdNKXWM0KBwFAoNTaJdu3vp1+8XoqP7snbtH1m//h5EvE1dNKXUUU6DwlEsNLQZPXvO4rjjbmbr1qdZufJCystzm7pYSqmjmAaFo5zDEUrHjq/SseO/ycn5lmXL+rNnT3D2uSilDp0GhWPEccfdSM+e3+H1FrJs2QA2bLgfn690/y9USqlqNCgcQ+LiTqVfv9W0aDGaLVseZ8mSPnrWoJQ6IBoUjjEuVxydOr1F9+5f4vHksmzZANLSxpKXNx+fz9PUxVNKHeGCZkZzsElMPJ/+/Vfz++93sm3bS2zb9k+czlji488iIWEoiYkXExbWoqmLqZQ6wujktSBQXp5NTs635OR8TXb2TEpLt+JwRNC27T20aXMXTmdEUxdRKRVgDZ28pkEhyIgIhYWr2bz5ETIyPiUsrA3t2z9Fs2ZXYI6G9LFKqYOiM5pVrYwxREV1o2vXT+jV63tcriTWrv1jxczo9ykt3d7URVRKNSHtUwhicXGD6dNnMTt3TmTjxvtZu/ZqAMLDTyAu7gzi4k4nLm4IYWHHNXFJlVKHizYfKQBEvBQUrCA3dw65ubPJzZ2L17sHgPDw44mNHUJc3MGNjsQAAA4CSURBVBDi488kLKxVE5dWKXWgtE9BHRIRL/n5P5OXN5fc3O/Jy/sBjycHcJCUNII2be4iNnZAUxdTKdVAGhRUoxLxUVi4it27P2T79tfweHKJiTmFNm3uIilpOMY4m7qISql6aFBQAePxFLBz59ukp79ASclGXK4kYmJOITb2FGJiBhEd3Ren093UxVRKVdPQoKAdzeqAhYRE0br1bRx33M1kZn5OdvYM8vLmk5X1BQDGuAgLa0toaIvKJSzsOGJjTyUmZiAOh6uJ34FSqi4aFNRBczhCaNbsUpo1uxSAsrIM9uxZwJ49Cygp2UxZ2U6KitaQm/tdRX8EOJ1RxMWdSULCUOLjzyE8/ASdH6HUEUSDgmo0oaHJJCUNJylp+D7PlZfnkps7u3JWtf+sIiysNXFxZxIffyZxcWfidrc53MVWSlWjQUEdFi5XHMnJI0hOHgFAcfF6srO/ITf3O7Kzv2TXrokAhIW1Izq6N1FRVUtYWCs9m1DqMNGgoJpEeHgHWrXqQKtWN1WObMrJ+Y78/EXk5y8jM3Mq4B8E4cTlSsDlSiQkJIHQ0GbExAwgLu4soqN768gnpRqRBgXV5IxxEBXVg6ioHpWPeTwFFBauoKBgOaWl2ykvz8LjyaK8PJvCwrVkZn4OQEhIPHFxZxAT0x8RQaQUn6+s4gJDPowJxeFwYYwLY0KJju5LfPwZGkiUqoMGBXVECgmJIjZ2ELGxg2p9vrR0J7m535GTM4ucnG/JzPys2rNOHI4wwCBSjkg5VWcdEBp6HM2b/5Hmza+pEYiUUjpPQR0DRASvNx9jXDgcobWeBYh48XqLyM7+il27/kt29v8Q8RAZ2Y2oqF6EhbXD7bZLWFhb3O42OJ2RTfBulAoMnaeggoYxhpCQmP2s4yQkJJpmzS6jWbPLKCvLJCNjEhkZU8jN/YHS0g8Bb43XhITEERbWhrCw1jgcERXNV7YJy+PJxucrwxgH4MAYgzEuoqJOIj7+LOLjzyQ6ur/OyVBHHT1TUArw+TyUlW2jpGQzJSVbKC1Nr7H4fEUVHd2JuFwJhIQk4HCEAoKIDxC83iL27FlAQcHPgOBwRBIT0x+XK4mQkFiczhhCQmJxONyIeCsWD+DFmDBCQmJwOqOr3cYSEhJHSEgcTmfsAc8S9/lKyc2dS17ej8TEnExCwrnalxLE9ExBqQPgcIRUNh8dqvLybHJzvyc39zvy85dQWLgSjycPj2cPPl9hLa8wVO/zqLuMEYSHH09EREfCw08kIqJjxXBdF8aEYEwI4KCgYBlZWV+SkzOrxv5CQ1vRsuWfadHiWsLDUw/5fapjk54pKHUY+XweREorKnAnxjgxxuDzleH15uPx7Kl2m1cRTHLxePIoL99NUVEaxcXrKC7ewN7NXdWFhbUjMXEYiYkXEBt7Kjk537Jjx5tkZ88EfMTEDMDhiEDEjtTy+cpwOqOIiupFdPRJREX1JjKyK8a48Hr3UFq6g7KynZSX78LrLarxOvARGtqc0NCWhIa2JCzsOEJCEnRuyRFGE+IpdQzz+copKdlAWdnOymYo/2LPJjrXWimXlGxl5853yM7+CmMcFUN2QzEmFI8nm4KC5Xi9BQCVZyA+X/EBl8/hcON2p+J2pxIe3h63uz0ORzheb0HlYs9iHBX7D6ssh9MZgdMZicMRidMZiTHOakFyDx5PHiEhMUREdCIiohPh4cdXNOVVfTYeTzZebwEuV/J++5v2x9aRUtF/dPTSoKCUOmAiPoqLf6eg4Gfy85ch4q1IaNiyMrmhwxHJ/2/v7mLkKus4jn9/O7OzL93aXcqCpSXQFwJiAoUSBEGDEE0lRryAoCIhhoQbTCAxQRoVI1d6I3JBFAIoKFECUiWEiFAICRcCCxQolJdaamiBboWWl3Z3Z3fn78V59jBdWnaydDpzur9PcjJznjmd/mb37PnPeXuejo6ufGMOMD6+Pe1NvE21+hajo28yOvoGo6ObGRnZnA/YNKWjozdd3RX5HkfEWEMZpXI6FzOlRE/PUiJqjI+/y+Tk+3stXyr1UakspqtrMeVyP7XanlSYdjM5uZuIKh9vByNlqqZ7Xsao1UaZOkdULvfT2TmQHgfp7l5GT88KenqW09OzgkplER0dXZ8oyLXaWPr5bGNs7C2q1XeoVrfne18TEx/Q0dFDqdSbP2bnlQbSOawByuUBenuPp6dnWeO/0L1+bi4KZtYGIiK/WqtU6qNU6t3PZcNBxDiTk3uo1XbXbbQnKZc/l07WL6Cjo4vJyY8YGXmNPXteSdNrSJ10di7Mp1Kpj2p1mLGxbWljvI2JiV0pwzxKpb5U4Cpk53WyDXl2JVklFb7utJEvMzHxIRMTO9PhvJ1Uq9sZHd2cika9Uv7+pVIvExO7GB//3z5+MiUqlUEqlc9TKi2gVhulVttDrTaSPvuH+V7blKOPvobly389q9+DTzSbWVuQRGfnwoaWmzqcBf2fumy5PJ/581cxf/6qA5RydiJqjI29xcjIJkZGNjE+viNt0D8+RFYqLaCrazFdXUelPZajqFQW0dm5cMZDUrVaNRWV95iY2EmlckTTP1NTi4Kk1cCNQAm4NSJ+Ne31LuBOYBXwLnBxRGxpZiYzswNF6qC7ewnd3UsYGDjngL9/R0eFSuWIg1IM8v+zWW+sbP/wJuCbwInA9ySdOG2xy4GdEbECuAGY3X6RmZkdEM08nX46sCkiNkdEFfgrcMG0ZS4A7kjP7wXOk69jMzNrmWYWhcXAm3XzW1PbPpeJ7HKC94GZDz6amVlTFOLCW0lXSBqSNLRjx45WxzEzO2Q1syhsA+rHVlyS2va5jLJbPBeQnXDeS0TcEhGnRcRpg4ODTYprZmbNLApPA8dJWiqpAnwXuH/aMvcDl6XnFwKPRtFunDAzO4Q07ZLUiJiQ9CPgIbJLUm+PiJckXQ8MRcT9wG3AnyRtAt4jKxxmZtYiTb1PISIeBB6c1nZd3fNR4KJmZjAzs8YVrpsLSTuA/87ynx8O7Ot+86Jw/tYpcnYodv4iZ4f2yX9MRMx4UrZwReGzkDTUSN8f7cr5W6fI2aHY+YucHYqXvxCXpJqZ2cHhomBmZrm5VhRuaXWAz8j5W6fI2aHY+YucHQqWf06dUzAzs0831/YUzMzsU8yZoiBptaRXJW2SdG2r88xE0u2ShiVtqGs7TNLDkl5PjwOtzLg/ko6W9JiklyW9JOmq1F6U/N2SnpL0fMr/y9S+VNKTaR26O92p35YklSQ9J+mBNF+k7FskvShpvaSh1FaUdadf0r2SXpG0UdKZRck+ZU4UhQbHdmg3fwRWT2u7FlgXEccB69J8O5oAfhwRJwJnAFemn3dR8o8B50bEycBKYLWkM8jG+7ghjf+xk2w8kHZ1FbCxbr5I2QG+FhEr6y7lLMq6cyPwz4g4ATiZ7HdQlOyZbFzUQ3sCzgQeqptfA6xpda4Gch8LbKibfxVYlJ4vAl5tdcYGP8c/gK8XMT/QCzwLfInsBqTyvtapdprIOp9cB5wLPEA2+HAhsqd8W4DDp7W1/bpD1qHnG6RztUXKXj/NiT0FGhvboQiOjIi30/N3gCNbGaYRko4FTgGepED50+GX9cAw8DDwH2BXZON+QHuvQ78FrgFqaX4hxckOEMC/JD0j6YrUVoR1ZymwA/hDOnR3q6R5FCN7bq4UhUNOZF872vrSMUl9wN+AqyPig/rX2j1/RExGxEqyb92nAye0OFJDJH0LGI6IZ1qd5TM4OyJOJTvce6Wkr9a/2MbrThk4FfhdRJwC7GbaoaI2zp6bK0WhkbEdimC7pEUA6XG4xXn2S1InWUG4KyLuS82FyT8lInYBj5EdculP435A+65DZwHflrSFbAjcc8mOcxchOwARsS09DgNryYpyEdadrcDWiHgyzd9LViSKkD03V4pCI2M7FEH9+BOXkR2rbztpnO3bgI0R8Zu6l4qSf1BSf3reQ3Y+ZCNZcbgwLdaW+SNiTUQsiYhjydbzRyPiEgqQHUDSPEnzp54D3wA2UIB1JyLeAd6UdHxqOg94mQJk30urT2ocrAk4H3iN7NjwT1udp4G8fwHeBsbJvoFcTnZseB3wOvAIcFirc+4n+9lku8gvAOvTdH6B8p8EPJfybwCuS+3LgKeATcA9QFers87wOc4BHihS9pTz+TS9NPW3WqB1ZyUwlNadvwMDRck+NfmOZjMzy82Vw0dmZtYAFwUzM8u5KJiZWc5FwczMci4KZmaWc1EwO4gknTPVc6lZO3JRMDOznIuC2T5I+kEaU2G9pJtTB3kfSbohjbGwTtJgWnalpH9LekHS2qn+8iWtkPRIGpfhWUnL09v31fW5f1e6A9ysLbgomE0j6QvAxcBZkXWKNwlcAswDhiLii8DjwC/SP7kT+ElEnAS8WNd+F3BTZOMyfJnsDnXIeo29mmxsj2Vk/RWZtYXyzIuYzTnnAauAp9OX+B6yTsxqwN1pmT8D90laAPRHxOOp/Q7gntR/z+KIWAsQEaMA6f2eioitaX492bgZTzT/Y5nNzEXB7JME3BERa/ZqlH4+bbnZ9hEzVvd8Ev8dWhvx4SOzT1oHXCjpCMjHBz6G7O9lqqfR7wNPRMT7wE5JX0ntlwKPR8SHwFZJ30nv0SWp96B+CrNZ8DcUs2ki4mVJPyMb/auDrKfaK8kGTTk9vTZMdt4Bsu6Qf582+puBH6b2S4GbJV2f3uOig/gxzGbFvaSaNUjSRxHR1+ocZs3kw0dmZpbznoKZmeW8p2BmZjkXBTMzy7komJlZzkXBzMxyLgpmZpZzUTAzs9z/AUQlMvtiJ3LsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 663us/sample - loss: 0.8304 - acc: 0.7780\n",
      "Loss: 0.830403923295121 Accuracy: 0.77798545\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0138 - acc: 0.4040\n",
      "Epoch 00001: val_loss improved from inf to 1.38634, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/001-1.3863.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 2.0137 - acc: 0.4040 - val_loss: 1.3863 - val_acc: 0.5516\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1840 - acc: 0.6370\n",
      "Epoch 00002: val_loss improved from 1.38634 to 0.88000, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/002-0.8800.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.1842 - acc: 0.6370 - val_loss: 0.8800 - val_acc: 0.7419\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9045 - acc: 0.7256\n",
      "Epoch 00003: val_loss improved from 0.88000 to 0.75024, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/003-0.7502.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.9046 - acc: 0.7255 - val_loss: 0.7502 - val_acc: 0.7736\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7453 - acc: 0.7766\n",
      "Epoch 00004: val_loss improved from 0.75024 to 0.64420, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/004-0.6442.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.7454 - acc: 0.7765 - val_loss: 0.6442 - val_acc: 0.8069\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6351 - acc: 0.8085\n",
      "Epoch 00005: val_loss improved from 0.64420 to 0.57109, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/005-0.5711.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.6352 - acc: 0.8085 - val_loss: 0.5711 - val_acc: 0.8309\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5487 - acc: 0.8340\n",
      "Epoch 00006: val_loss improved from 0.57109 to 0.51913, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/006-0.5191.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5488 - acc: 0.8339 - val_loss: 0.5191 - val_acc: 0.8493\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4910 - acc: 0.8518\n",
      "Epoch 00007: val_loss improved from 0.51913 to 0.48328, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/007-0.4833.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4909 - acc: 0.8518 - val_loss: 0.4833 - val_acc: 0.8607\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4414 - acc: 0.8673\n",
      "Epoch 00008: val_loss did not improve from 0.48328\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.4414 - acc: 0.8672 - val_loss: 0.4878 - val_acc: 0.8612\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8778\n",
      "Epoch 00009: val_loss improved from 0.48328 to 0.47370, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/009-0.4737.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4044 - acc: 0.8778 - val_loss: 0.4737 - val_acc: 0.8672\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8872\n",
      "Epoch 00010: val_loss improved from 0.47370 to 0.43579, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/010-0.4358.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3696 - acc: 0.8872 - val_loss: 0.4358 - val_acc: 0.8735\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8975\n",
      "Epoch 00011: val_loss improved from 0.43579 to 0.41899, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/011-0.4190.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3347 - acc: 0.8974 - val_loss: 0.4190 - val_acc: 0.8856\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.9023\n",
      "Epoch 00012: val_loss did not improve from 0.41899\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3168 - acc: 0.9023 - val_loss: 0.4232 - val_acc: 0.8873\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9112\n",
      "Epoch 00013: val_loss improved from 0.41899 to 0.40439, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/013-0.4044.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2919 - acc: 0.9111 - val_loss: 0.4044 - val_acc: 0.8861\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9169\n",
      "Epoch 00014: val_loss improved from 0.40439 to 0.35528, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/014-0.3553.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2709 - acc: 0.9169 - val_loss: 0.3553 - val_acc: 0.9031\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9200\n",
      "Epoch 00015: val_loss did not improve from 0.35528\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2545 - acc: 0.9200 - val_loss: 0.3764 - val_acc: 0.8982\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9243\n",
      "Epoch 00016: val_loss did not improve from 0.35528\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.2375 - acc: 0.9242 - val_loss: 0.3739 - val_acc: 0.8975\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9297\n",
      "Epoch 00017: val_loss improved from 0.35528 to 0.31319, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/017-0.3132.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2246 - acc: 0.9297 - val_loss: 0.3132 - val_acc: 0.9159\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9335\n",
      "Epoch 00018: val_loss did not improve from 0.31319\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2083 - acc: 0.9335 - val_loss: 0.3257 - val_acc: 0.9103\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9381\n",
      "Epoch 00019: val_loss did not improve from 0.31319\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1945 - acc: 0.9381 - val_loss: 0.3728 - val_acc: 0.9061\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9396\n",
      "Epoch 00020: val_loss improved from 0.31319 to 0.30893, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/020-0.3089.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1885 - acc: 0.9396 - val_loss: 0.3089 - val_acc: 0.9187\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9420\n",
      "Epoch 00021: val_loss did not improve from 0.30893\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1784 - acc: 0.9419 - val_loss: 0.3235 - val_acc: 0.9161\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9453\n",
      "Epoch 00022: val_loss did not improve from 0.30893\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1709 - acc: 0.9453 - val_loss: 0.3704 - val_acc: 0.9040\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9483\n",
      "Epoch 00023: val_loss did not improve from 0.30893\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1578 - acc: 0.9483 - val_loss: 0.3090 - val_acc: 0.9262\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9522\n",
      "Epoch 00024: val_loss did not improve from 0.30893\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1476 - acc: 0.9522 - val_loss: 0.3359 - val_acc: 0.9201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9536\n",
      "Epoch 00025: val_loss did not improve from 0.30893\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1434 - acc: 0.9536 - val_loss: 0.3468 - val_acc: 0.9078\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9552\n",
      "Epoch 00026: val_loss did not improve from 0.30893\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1356 - acc: 0.9551 - val_loss: 0.3355 - val_acc: 0.9110\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9567\n",
      "Epoch 00027: val_loss did not improve from 0.30893\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1336 - acc: 0.9567 - val_loss: 0.4043 - val_acc: 0.9019\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9570\n",
      "Epoch 00028: val_loss did not improve from 0.30893\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1297 - acc: 0.9570 - val_loss: 0.3595 - val_acc: 0.9103\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9611\n",
      "Epoch 00029: val_loss did not improve from 0.30893\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1208 - acc: 0.9611 - val_loss: 0.3538 - val_acc: 0.9096\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9630\n",
      "Epoch 00030: val_loss did not improve from 0.30893\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1117 - acc: 0.9630 - val_loss: 0.3369 - val_acc: 0.9208\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9634\n",
      "Epoch 00031: val_loss improved from 0.30893 to 0.30144, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/031-0.3014.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1076 - acc: 0.9634 - val_loss: 0.3014 - val_acc: 0.9299\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9645\n",
      "Epoch 00032: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1070 - acc: 0.9645 - val_loss: 0.3275 - val_acc: 0.9201\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9688\n",
      "Epoch 00033: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0980 - acc: 0.9688 - val_loss: 0.3441 - val_acc: 0.9217\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9690\n",
      "Epoch 00034: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0948 - acc: 0.9691 - val_loss: 0.4003 - val_acc: 0.8998\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9683\n",
      "Epoch 00035: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0953 - acc: 0.9683 - val_loss: 0.3237 - val_acc: 0.9231\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9714\n",
      "Epoch 00036: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0884 - acc: 0.9714 - val_loss: 0.3262 - val_acc: 0.9234\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9723\n",
      "Epoch 00037: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0838 - acc: 0.9723 - val_loss: 0.3650 - val_acc: 0.9159\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9709\n",
      "Epoch 00038: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0872 - acc: 0.9709 - val_loss: 0.3576 - val_acc: 0.9152\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9729\n",
      "Epoch 00039: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0832 - acc: 0.9729 - val_loss: 0.3487 - val_acc: 0.9203\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9749\n",
      "Epoch 00040: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0761 - acc: 0.9748 - val_loss: 0.3421 - val_acc: 0.9159\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9725\n",
      "Epoch 00041: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0833 - acc: 0.9725 - val_loss: 0.3210 - val_acc: 0.9241\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9752\n",
      "Epoch 00042: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0762 - acc: 0.9751 - val_loss: 0.3949 - val_acc: 0.9078\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9765\n",
      "Epoch 00043: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0736 - acc: 0.9765 - val_loss: 0.3347 - val_acc: 0.9164\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9801\n",
      "Epoch 00044: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0617 - acc: 0.9801 - val_loss: 0.3040 - val_acc: 0.9304\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9790\n",
      "Epoch 00045: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0658 - acc: 0.9790 - val_loss: 0.3094 - val_acc: 0.9292\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9804\n",
      "Epoch 00046: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0613 - acc: 0.9804 - val_loss: 0.4003 - val_acc: 0.9103\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9729\n",
      "Epoch 00047: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0839 - acc: 0.9729 - val_loss: 0.3358 - val_acc: 0.9266\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9815\n",
      "Epoch 00048: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0591 - acc: 0.9815 - val_loss: 0.3393 - val_acc: 0.9215\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9795\n",
      "Epoch 00049: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0613 - acc: 0.9795 - val_loss: 0.3182 - val_acc: 0.9278\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9781\n",
      "Epoch 00050: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0652 - acc: 0.9781 - val_loss: 0.3279 - val_acc: 0.9257\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9810\n",
      "Epoch 00051: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0558 - acc: 0.9810 - val_loss: 0.3222 - val_acc: 0.9287\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9810\n",
      "Epoch 00052: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0582 - acc: 0.9810 - val_loss: 0.3337 - val_acc: 0.9220\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9808\n",
      "Epoch 00053: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0573 - acc: 0.9808 - val_loss: 0.3543 - val_acc: 0.9173\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9825\n",
      "Epoch 00054: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0547 - acc: 0.9825 - val_loss: 0.3053 - val_acc: 0.9350\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9824\n",
      "Epoch 00055: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0562 - acc: 0.9824 - val_loss: 0.3756 - val_acc: 0.9166\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9829\n",
      "Epoch 00056: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0509 - acc: 0.9828 - val_loss: 0.3059 - val_acc: 0.9311\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9811\n",
      "Epoch 00057: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0589 - acc: 0.9811 - val_loss: 0.3188 - val_acc: 0.9322\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9804\n",
      "Epoch 00058: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0597 - acc: 0.9804 - val_loss: 0.3258 - val_acc: 0.9290\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9861\n",
      "Epoch 00059: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0420 - acc: 0.9861 - val_loss: 0.3507 - val_acc: 0.9276\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9837\n",
      "Epoch 00060: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0496 - acc: 0.9837 - val_loss: 0.3667 - val_acc: 0.9185\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9802\n",
      "Epoch 00061: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0591 - acc: 0.9802 - val_loss: 0.3268 - val_acc: 0.9290\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9875\n",
      "Epoch 00062: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0391 - acc: 0.9875 - val_loss: 0.3709 - val_acc: 0.9189\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9874\n",
      "Epoch 00063: val_loss did not improve from 0.30144\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0403 - acc: 0.9874 - val_loss: 0.3408 - val_acc: 0.9301\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9852\n",
      "Epoch 00064: val_loss improved from 0.30144 to 0.29554, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_6_conv_checkpoint/064-0.2955.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0445 - acc: 0.9852 - val_loss: 0.2955 - val_acc: 0.9350\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9851\n",
      "Epoch 00065: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0461 - acc: 0.9851 - val_loss: 0.3458 - val_acc: 0.9208\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9875\n",
      "Epoch 00066: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0397 - acc: 0.9875 - val_loss: 0.3409 - val_acc: 0.9345\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9877\n",
      "Epoch 00067: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0384 - acc: 0.9877 - val_loss: 0.3378 - val_acc: 0.9283\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9840\n",
      "Epoch 00068: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0484 - acc: 0.9840 - val_loss: 0.3308 - val_acc: 0.9278\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9881\n",
      "Epoch 00069: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0383 - acc: 0.9881 - val_loss: 0.3369 - val_acc: 0.9264\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9867\n",
      "Epoch 00070: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0412 - acc: 0.9867 - val_loss: 0.4560 - val_acc: 0.9103\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9875\n",
      "Epoch 00071: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0395 - acc: 0.9875 - val_loss: 0.3210 - val_acc: 0.9324\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9878\n",
      "Epoch 00072: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0393 - acc: 0.9878 - val_loss: 0.3381 - val_acc: 0.9255\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9893\n",
      "Epoch 00073: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0359 - acc: 0.9893 - val_loss: 0.3798 - val_acc: 0.9154\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9877\n",
      "Epoch 00074: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0390 - acc: 0.9877 - val_loss: 0.3893 - val_acc: 0.9236\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9869\n",
      "Epoch 00075: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0396 - acc: 0.9869 - val_loss: 0.5018 - val_acc: 0.9064\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9883\n",
      "Epoch 00076: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0376 - acc: 0.9883 - val_loss: 0.3322 - val_acc: 0.9352\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9886\n",
      "Epoch 00077: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0354 - acc: 0.9885 - val_loss: 0.3478 - val_acc: 0.9264\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9884\n",
      "Epoch 00078: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0367 - acc: 0.9884 - val_loss: 0.3273 - val_acc: 0.9334\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9901\n",
      "Epoch 00079: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0310 - acc: 0.9901 - val_loss: 0.4298 - val_acc: 0.9196\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9873\n",
      "Epoch 00080: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0404 - acc: 0.9872 - val_loss: 0.3496 - val_acc: 0.9294\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9889\n",
      "Epoch 00081: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0354 - acc: 0.9889 - val_loss: 0.3343 - val_acc: 0.9373\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9899\n",
      "Epoch 00082: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0297 - acc: 0.9899 - val_loss: 0.3389 - val_acc: 0.9320\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9896\n",
      "Epoch 00083: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0313 - acc: 0.9896 - val_loss: 0.3167 - val_acc: 0.9378\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9886\n",
      "Epoch 00084: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0354 - acc: 0.9886 - val_loss: 0.3309 - val_acc: 0.9322\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9902\n",
      "Epoch 00085: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0313 - acc: 0.9902 - val_loss: 0.3124 - val_acc: 0.9352\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9900\n",
      "Epoch 00086: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0302 - acc: 0.9900 - val_loss: 0.3251 - val_acc: 0.9373\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9904\n",
      "Epoch 00087: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0309 - acc: 0.9904 - val_loss: 0.3786 - val_acc: 0.9266\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9886\n",
      "Epoch 00088: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0351 - acc: 0.9886 - val_loss: 0.3275 - val_acc: 0.9338\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9910\n",
      "Epoch 00089: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0294 - acc: 0.9910 - val_loss: 0.3090 - val_acc: 0.9364\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9912\n",
      "Epoch 00090: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0290 - acc: 0.9912 - val_loss: 0.4264 - val_acc: 0.9196\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9882\n",
      "Epoch 00091: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0397 - acc: 0.9882 - val_loss: 0.3243 - val_acc: 0.9399\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9926\n",
      "Epoch 00092: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0244 - acc: 0.9926 - val_loss: 0.3399 - val_acc: 0.9336\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9887\n",
      "Epoch 00093: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0348 - acc: 0.9887 - val_loss: 0.3572 - val_acc: 0.9352\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9920\n",
      "Epoch 00094: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0245 - acc: 0.9920 - val_loss: 0.3492 - val_acc: 0.9324\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9921\n",
      "Epoch 00095: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0247 - acc: 0.9921 - val_loss: 0.3610 - val_acc: 0.9280\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9899\n",
      "Epoch 00096: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0324 - acc: 0.9899 - val_loss: 0.3504 - val_acc: 0.9287\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9920\n",
      "Epoch 00097: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0256 - acc: 0.9920 - val_loss: 0.4211 - val_acc: 0.9199\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9892\n",
      "Epoch 00098: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0340 - acc: 0.9892 - val_loss: 0.3357 - val_acc: 0.9324\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9929\n",
      "Epoch 00099: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0223 - acc: 0.9929 - val_loss: 0.3639 - val_acc: 0.9313\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9919\n",
      "Epoch 00100: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0262 - acc: 0.9919 - val_loss: 0.3444 - val_acc: 0.9308\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9902\n",
      "Epoch 00101: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0323 - acc: 0.9902 - val_loss: 0.3428 - val_acc: 0.9357\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9925\n",
      "Epoch 00102: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0237 - acc: 0.9924 - val_loss: 0.3663 - val_acc: 0.9331\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9908\n",
      "Epoch 00103: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0296 - acc: 0.9908 - val_loss: 0.3131 - val_acc: 0.9390\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9925\n",
      "Epoch 00104: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0245 - acc: 0.9925 - val_loss: 0.3534 - val_acc: 0.9362\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9924\n",
      "Epoch 00105: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0245 - acc: 0.9924 - val_loss: 0.3227 - val_acc: 0.9373\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9929\n",
      "Epoch 00106: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0232 - acc: 0.9929 - val_loss: 0.3228 - val_acc: 0.9397\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9915\n",
      "Epoch 00107: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0263 - acc: 0.9916 - val_loss: 0.4453 - val_acc: 0.9210\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9923\n",
      "Epoch 00108: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0247 - acc: 0.9923 - val_loss: 0.4197 - val_acc: 0.9189\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9909\n",
      "Epoch 00109: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0279 - acc: 0.9909 - val_loss: 0.3545 - val_acc: 0.9341\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9938\n",
      "Epoch 00110: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0212 - acc: 0.9938 - val_loss: 0.3523 - val_acc: 0.9362\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9920\n",
      "Epoch 00111: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0263 - acc: 0.9920 - val_loss: 0.3575 - val_acc: 0.9350\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9915\n",
      "Epoch 00112: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0255 - acc: 0.9915 - val_loss: 0.3965 - val_acc: 0.9324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9885\n",
      "Epoch 00113: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0377 - acc: 0.9885 - val_loss: 0.3364 - val_acc: 0.9364\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9919\n",
      "Epoch 00114: val_loss did not improve from 0.29554\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0263 - acc: 0.9919 - val_loss: 0.3401 - val_acc: 0.9371\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 723us/sample - loss: 0.3692 - acc: 0.9161\n",
      "Loss: 0.36920918753817444 Accuracy: 0.91609555\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0097 - acc: 0.3930\n",
      "Epoch 00001: val_loss improved from inf to 1.30182, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/001-1.3018.hdf5\n",
      "36805/36805 [==============================] - 69s 2ms/sample - loss: 2.0095 - acc: 0.3930 - val_loss: 1.3018 - val_acc: 0.5742\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0709 - acc: 0.6651\n",
      "Epoch 00002: val_loss improved from 1.30182 to 0.74630, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/002-0.7463.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.0709 - acc: 0.6651 - val_loss: 0.7463 - val_acc: 0.7789\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7421 - acc: 0.7708\n",
      "Epoch 00003: val_loss improved from 0.74630 to 0.51607, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/003-0.5161.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.7422 - acc: 0.7708 - val_loss: 0.5161 - val_acc: 0.8442\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5674 - acc: 0.8270\n",
      "Epoch 00004: val_loss improved from 0.51607 to 0.42179, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/004-0.4218.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5675 - acc: 0.8269 - val_loss: 0.4218 - val_acc: 0.8784\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.8580\n",
      "Epoch 00005: val_loss improved from 0.42179 to 0.34995, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/005-0.3499.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4662 - acc: 0.8580 - val_loss: 0.3499 - val_acc: 0.8968\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3932 - acc: 0.8798\n",
      "Epoch 00006: val_loss improved from 0.34995 to 0.33976, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/006-0.3398.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3933 - acc: 0.8798 - val_loss: 0.3398 - val_acc: 0.8989\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8945\n",
      "Epoch 00007: val_loss improved from 0.33976 to 0.33838, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/007-0.3384.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3443 - acc: 0.8945 - val_loss: 0.3384 - val_acc: 0.9003\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9069\n",
      "Epoch 00008: val_loss improved from 0.33838 to 0.31978, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/008-0.3198.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3056 - acc: 0.9069 - val_loss: 0.3198 - val_acc: 0.9003\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9167\n",
      "Epoch 00009: val_loss improved from 0.31978 to 0.27606, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/009-0.2761.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2719 - acc: 0.9166 - val_loss: 0.2761 - val_acc: 0.9192\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9237\n",
      "Epoch 00010: val_loss improved from 0.27606 to 0.23335, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/010-0.2333.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2474 - acc: 0.9237 - val_loss: 0.2333 - val_acc: 0.9317\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9326\n",
      "Epoch 00011: val_loss did not improve from 0.23335\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2202 - acc: 0.9326 - val_loss: 0.2360 - val_acc: 0.9329\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9349\n",
      "Epoch 00012: val_loss did not improve from 0.23335\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2068 - acc: 0.9349 - val_loss: 0.3284 - val_acc: 0.9005\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9390\n",
      "Epoch 00013: val_loss did not improve from 0.23335\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1905 - acc: 0.9390 - val_loss: 0.2356 - val_acc: 0.9294\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9440\n",
      "Epoch 00014: val_loss improved from 0.23335 to 0.19406, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/014-0.1941.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1784 - acc: 0.9439 - val_loss: 0.1941 - val_acc: 0.9443\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9475\n",
      "Epoch 00015: val_loss did not improve from 0.19406\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1647 - acc: 0.9475 - val_loss: 0.2089 - val_acc: 0.9397\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9516\n",
      "Epoch 00016: val_loss did not improve from 0.19406\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1521 - acc: 0.9516 - val_loss: 0.2128 - val_acc: 0.9392\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9530\n",
      "Epoch 00017: val_loss did not improve from 0.19406\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1452 - acc: 0.9530 - val_loss: 0.2012 - val_acc: 0.9464\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9582\n",
      "Epoch 00018: val_loss did not improve from 0.19406\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1294 - acc: 0.9582 - val_loss: 0.2208 - val_acc: 0.9373\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9608\n",
      "Epoch 00019: val_loss did not improve from 0.19406\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1219 - acc: 0.9608 - val_loss: 0.2133 - val_acc: 0.9432\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9635\n",
      "Epoch 00020: val_loss did not improve from 0.19406\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1144 - acc: 0.9635 - val_loss: 0.2094 - val_acc: 0.9415\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9609\n",
      "Epoch 00021: val_loss improved from 0.19406 to 0.18510, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/021-0.1851.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1189 - acc: 0.9609 - val_loss: 0.1851 - val_acc: 0.9497\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9678\n",
      "Epoch 00022: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0991 - acc: 0.9678 - val_loss: 0.2031 - val_acc: 0.9462\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9690\n",
      "Epoch 00023: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0962 - acc: 0.9690 - val_loss: 0.2068 - val_acc: 0.9434\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9719\n",
      "Epoch 00024: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0877 - acc: 0.9719 - val_loss: 0.2767 - val_acc: 0.9266\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9720\n",
      "Epoch 00025: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0866 - acc: 0.9720 - val_loss: 0.2027 - val_acc: 0.9474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9719\n",
      "Epoch 00026: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0861 - acc: 0.9719 - val_loss: 0.1991 - val_acc: 0.9488\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9744\n",
      "Epoch 00027: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0780 - acc: 0.9743 - val_loss: 0.2062 - val_acc: 0.9415\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9743\n",
      "Epoch 00028: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0787 - acc: 0.9742 - val_loss: 0.2303 - val_acc: 0.9429\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9724\n",
      "Epoch 00029: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0836 - acc: 0.9724 - val_loss: 0.2052 - val_acc: 0.9450\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9798\n",
      "Epoch 00030: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0625 - acc: 0.9798 - val_loss: 0.2011 - val_acc: 0.9478\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9804\n",
      "Epoch 00031: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0614 - acc: 0.9804 - val_loss: 0.2820 - val_acc: 0.9280\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9820\n",
      "Epoch 00032: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0559 - acc: 0.9820 - val_loss: 0.1874 - val_acc: 0.9529\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9811\n",
      "Epoch 00033: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0575 - acc: 0.9811 - val_loss: 0.2049 - val_acc: 0.9448\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9802\n",
      "Epoch 00034: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0596 - acc: 0.9802 - val_loss: 0.1981 - val_acc: 0.9464\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9833\n",
      "Epoch 00035: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0514 - acc: 0.9833 - val_loss: 0.2459 - val_acc: 0.9352\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9833\n",
      "Epoch 00036: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0510 - acc: 0.9833 - val_loss: 0.2055 - val_acc: 0.9474\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9844\n",
      "Epoch 00037: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0500 - acc: 0.9844 - val_loss: 0.2123 - val_acc: 0.9502\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9845\n",
      "Epoch 00038: val_loss did not improve from 0.18510\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0462 - acc: 0.9845 - val_loss: 0.2493 - val_acc: 0.9401\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9859\n",
      "Epoch 00039: val_loss improved from 0.18510 to 0.18115, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_7_conv_checkpoint/039-0.1812.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0448 - acc: 0.9858 - val_loss: 0.1812 - val_acc: 0.9553\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9811\n",
      "Epoch 00040: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0576 - acc: 0.9811 - val_loss: 0.1905 - val_acc: 0.9504\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9880\n",
      "Epoch 00041: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0364 - acc: 0.9880 - val_loss: 0.1895 - val_acc: 0.9548\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9876\n",
      "Epoch 00042: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0380 - acc: 0.9876 - val_loss: 0.2904 - val_acc: 0.9327\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9877\n",
      "Epoch 00043: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0383 - acc: 0.9876 - val_loss: 0.2026 - val_acc: 0.9495\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9847\n",
      "Epoch 00044: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0450 - acc: 0.9847 - val_loss: 0.1893 - val_acc: 0.9567\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9891\n",
      "Epoch 00045: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0333 - acc: 0.9891 - val_loss: 0.1870 - val_acc: 0.9562\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9881\n",
      "Epoch 00046: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0358 - acc: 0.9881 - val_loss: 0.1943 - val_acc: 0.9525\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9882\n",
      "Epoch 00047: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0350 - acc: 0.9882 - val_loss: 0.2268 - val_acc: 0.9476\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9906\n",
      "Epoch 00048: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0300 - acc: 0.9906 - val_loss: 0.1875 - val_acc: 0.9555\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9905\n",
      "Epoch 00049: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0308 - acc: 0.9905 - val_loss: 0.1941 - val_acc: 0.9518\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9889\n",
      "Epoch 00050: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0346 - acc: 0.9888 - val_loss: 0.2103 - val_acc: 0.9546\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9892\n",
      "Epoch 00051: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0339 - acc: 0.9892 - val_loss: 0.2102 - val_acc: 0.9511\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9910\n",
      "Epoch 00052: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0277 - acc: 0.9910 - val_loss: 0.1887 - val_acc: 0.9574\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9914\n",
      "Epoch 00053: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0266 - acc: 0.9914 - val_loss: 0.2199 - val_acc: 0.9506\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9901\n",
      "Epoch 00054: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0294 - acc: 0.9901 - val_loss: 0.1941 - val_acc: 0.9543\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9897\n",
      "Epoch 00055: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0313 - acc: 0.9897 - val_loss: 0.2234 - val_acc: 0.9513\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9907\n",
      "Epoch 00056: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0280 - acc: 0.9907 - val_loss: 0.2166 - val_acc: 0.9534\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9910\n",
      "Epoch 00057: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0292 - acc: 0.9910 - val_loss: 0.2321 - val_acc: 0.9481\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9895\n",
      "Epoch 00058: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0323 - acc: 0.9895 - val_loss: 0.1832 - val_acc: 0.9571\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9936\n",
      "Epoch 00059: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0199 - acc: 0.9936 - val_loss: 0.2359 - val_acc: 0.9464\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9933\n",
      "Epoch 00060: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0211 - acc: 0.9933 - val_loss: 0.2289 - val_acc: 0.9483\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9910\n",
      "Epoch 00061: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0268 - acc: 0.9910 - val_loss: 0.1882 - val_acc: 0.9606\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9943\n",
      "Epoch 00062: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0185 - acc: 0.9943 - val_loss: 0.2235 - val_acc: 0.9518\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9889\n",
      "Epoch 00063: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0344 - acc: 0.9889 - val_loss: 0.2315 - val_acc: 0.9464\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9936\n",
      "Epoch 00064: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0209 - acc: 0.9936 - val_loss: 0.2047 - val_acc: 0.9541\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9938\n",
      "Epoch 00065: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0192 - acc: 0.9938 - val_loss: 0.2867 - val_acc: 0.9378\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9920\n",
      "Epoch 00066: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0249 - acc: 0.9920 - val_loss: 0.2197 - val_acc: 0.9529\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9929\n",
      "Epoch 00067: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0214 - acc: 0.9929 - val_loss: 0.2078 - val_acc: 0.9550\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9925\n",
      "Epoch 00068: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0230 - acc: 0.9925 - val_loss: 0.2184 - val_acc: 0.9555\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9944\n",
      "Epoch 00069: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0187 - acc: 0.9944 - val_loss: 0.2466 - val_acc: 0.9460\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9927\n",
      "Epoch 00070: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0215 - acc: 0.9927 - val_loss: 0.1977 - val_acc: 0.9555\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9913\n",
      "Epoch 00071: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0262 - acc: 0.9913 - val_loss: 0.2363 - val_acc: 0.9541\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9946\n",
      "Epoch 00072: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0168 - acc: 0.9946 - val_loss: 0.2086 - val_acc: 0.9553\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9935\n",
      "Epoch 00073: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0191 - acc: 0.9935 - val_loss: 0.2240 - val_acc: 0.9527\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9953\n",
      "Epoch 00074: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0150 - acc: 0.9953 - val_loss: 0.2554 - val_acc: 0.9455\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9945\n",
      "Epoch 00075: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0179 - acc: 0.9944 - val_loss: 0.2596 - val_acc: 0.9495\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9906\n",
      "Epoch 00076: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0291 - acc: 0.9906 - val_loss: 0.1871 - val_acc: 0.9592\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9950\n",
      "Epoch 00077: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0159 - acc: 0.9950 - val_loss: 0.2066 - val_acc: 0.9578\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9920\n",
      "Epoch 00078: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0263 - acc: 0.9920 - val_loss: 0.2409 - val_acc: 0.9464\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9901\n",
      "Epoch 00079: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0327 - acc: 0.9901 - val_loss: 0.2118 - val_acc: 0.9520\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9959\n",
      "Epoch 00080: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0127 - acc: 0.9959 - val_loss: 0.2093 - val_acc: 0.9555\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9967\n",
      "Epoch 00081: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0112 - acc: 0.9967 - val_loss: 0.2003 - val_acc: 0.9606\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9924\n",
      "Epoch 00082: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0238 - acc: 0.9924 - val_loss: 0.2236 - val_acc: 0.9564\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9951\n",
      "Epoch 00083: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0155 - acc: 0.9951 - val_loss: 0.2501 - val_acc: 0.9511\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9958\n",
      "Epoch 00084: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0140 - acc: 0.9958 - val_loss: 0.2016 - val_acc: 0.9618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9950\n",
      "Epoch 00085: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0151 - acc: 0.9950 - val_loss: 0.2297 - val_acc: 0.9567\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9942\n",
      "Epoch 00086: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0184 - acc: 0.9942 - val_loss: 0.2366 - val_acc: 0.9541\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9916\n",
      "Epoch 00087: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0254 - acc: 0.9916 - val_loss: 0.2140 - val_acc: 0.9562\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9943\n",
      "Epoch 00088: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0176 - acc: 0.9943 - val_loss: 0.2245 - val_acc: 0.9576\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9959\n",
      "Epoch 00089: val_loss did not improve from 0.18115\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0126 - acc: 0.9959 - val_loss: 0.2248 - val_acc: 0.9550\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 751us/sample - loss: 0.2417 - acc: 0.9356\n",
      "Loss: 0.24168295059449824 Accuracy: 0.93561786\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9848 - acc: 0.3995\n",
      "Epoch 00001: val_loss improved from inf to 1.28477, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/001-1.2848.hdf5\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 1.9849 - acc: 0.3995 - val_loss: 1.2848 - val_acc: 0.5966\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9442 - acc: 0.7020\n",
      "Epoch 00002: val_loss improved from 1.28477 to 0.54935, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/002-0.5494.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.9443 - acc: 0.7019 - val_loss: 0.5494 - val_acc: 0.8470\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6280 - acc: 0.8069\n",
      "Epoch 00003: val_loss improved from 0.54935 to 0.38729, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/003-0.3873.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.6280 - acc: 0.8068 - val_loss: 0.3873 - val_acc: 0.8819\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4793 - acc: 0.8524\n",
      "Epoch 00004: val_loss did not improve from 0.38729\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4793 - acc: 0.8524 - val_loss: 0.3983 - val_acc: 0.8852\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.8782\n",
      "Epoch 00005: val_loss improved from 0.38729 to 0.30442, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/005-0.3044.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3927 - acc: 0.8782 - val_loss: 0.3044 - val_acc: 0.9113\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3280 - acc: 0.8987\n",
      "Epoch 00006: val_loss improved from 0.30442 to 0.22870, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/006-0.2287.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3280 - acc: 0.8987 - val_loss: 0.2287 - val_acc: 0.9329\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.9125\n",
      "Epoch 00007: val_loss did not improve from 0.22870\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2857 - acc: 0.9125 - val_loss: 0.2340 - val_acc: 0.9336\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9223\n",
      "Epoch 00008: val_loss improved from 0.22870 to 0.22587, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/008-0.2259.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2501 - acc: 0.9223 - val_loss: 0.2259 - val_acc: 0.9355\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9305\n",
      "Epoch 00009: val_loss did not improve from 0.22587\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2269 - acc: 0.9305 - val_loss: 0.2378 - val_acc: 0.9252\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9352\n",
      "Epoch 00010: val_loss improved from 0.22587 to 0.17203, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/010-0.1720.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2049 - acc: 0.9353 - val_loss: 0.1720 - val_acc: 0.9497\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9419\n",
      "Epoch 00011: val_loss did not improve from 0.17203\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1824 - acc: 0.9419 - val_loss: 0.2196 - val_acc: 0.9348\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9471\n",
      "Epoch 00012: val_loss improved from 0.17203 to 0.15567, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/012-0.1557.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1684 - acc: 0.9471 - val_loss: 0.1557 - val_acc: 0.9550\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9517\n",
      "Epoch 00013: val_loss did not improve from 0.15567\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1524 - acc: 0.9516 - val_loss: 0.1673 - val_acc: 0.9511\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9541\n",
      "Epoch 00014: val_loss did not improve from 0.15567\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1438 - acc: 0.9541 - val_loss: 0.3101 - val_acc: 0.9080\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9594\n",
      "Epoch 00015: val_loss improved from 0.15567 to 0.15159, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/015-0.1516.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1290 - acc: 0.9594 - val_loss: 0.1516 - val_acc: 0.9548\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9614\n",
      "Epoch 00016: val_loss did not improve from 0.15159\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1217 - acc: 0.9614 - val_loss: 0.1956 - val_acc: 0.9411\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9650\n",
      "Epoch 00017: val_loss improved from 0.15159 to 0.13433, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/017-0.1343.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1081 - acc: 0.9650 - val_loss: 0.1343 - val_acc: 0.9592\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9671\n",
      "Epoch 00018: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1040 - acc: 0.9671 - val_loss: 0.1503 - val_acc: 0.9569\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9699\n",
      "Epoch 00019: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0955 - acc: 0.9699 - val_loss: 0.1600 - val_acc: 0.9502\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9701\n",
      "Epoch 00020: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0935 - acc: 0.9701 - val_loss: 0.1491 - val_acc: 0.9564\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9725\n",
      "Epoch 00021: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0864 - acc: 0.9725 - val_loss: 0.1512 - val_acc: 0.9592\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9758\n",
      "Epoch 00022: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0762 - acc: 0.9757 - val_loss: 0.2193 - val_acc: 0.9404\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9747\n",
      "Epoch 00023: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0806 - acc: 0.9747 - val_loss: 0.1505 - val_acc: 0.9567\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9789\n",
      "Epoch 00024: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0675 - acc: 0.9789 - val_loss: 0.1507 - val_acc: 0.9567\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9795\n",
      "Epoch 00025: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0625 - acc: 0.9795 - val_loss: 0.1460 - val_acc: 0.9599\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9774\n",
      "Epoch 00026: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0697 - acc: 0.9774 - val_loss: 0.1556 - val_acc: 0.9550\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9815\n",
      "Epoch 00027: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0579 - acc: 0.9815 - val_loss: 0.1671 - val_acc: 0.9574\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9748\n",
      "Epoch 00028: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0793 - acc: 0.9748 - val_loss: 0.1358 - val_acc: 0.9602\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9849\n",
      "Epoch 00029: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0484 - acc: 0.9849 - val_loss: 0.1377 - val_acc: 0.9609\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9868\n",
      "Epoch 00030: val_loss did not improve from 0.13433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0432 - acc: 0.9868 - val_loss: 0.1598 - val_acc: 0.9588\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9859\n",
      "Epoch 00031: val_loss improved from 0.13433 to 0.13371, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_DO_BN_8_conv_checkpoint/031-0.1337.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0444 - acc: 0.9859 - val_loss: 0.1337 - val_acc: 0.9613\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9853\n",
      "Epoch 00032: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0475 - acc: 0.9853 - val_loss: 0.1704 - val_acc: 0.9543\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9870\n",
      "Epoch 00033: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0419 - acc: 0.9870 - val_loss: 0.1567 - val_acc: 0.9590\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9871\n",
      "Epoch 00034: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0403 - acc: 0.9871 - val_loss: 0.1504 - val_acc: 0.9592\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9872\n",
      "Epoch 00035: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0402 - acc: 0.9872 - val_loss: 0.1763 - val_acc: 0.9536\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9882\n",
      "Epoch 00036: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0383 - acc: 0.9882 - val_loss: 0.1390 - val_acc: 0.9648\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9886\n",
      "Epoch 00037: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0355 - acc: 0.9886 - val_loss: 0.1567 - val_acc: 0.9597\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9899\n",
      "Epoch 00038: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0319 - acc: 0.9899 - val_loss: 0.1698 - val_acc: 0.9555\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9890\n",
      "Epoch 00039: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0346 - acc: 0.9890 - val_loss: 0.1923 - val_acc: 0.9504\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9898\n",
      "Epoch 00040: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0323 - acc: 0.9898 - val_loss: 0.2028 - val_acc: 0.9485\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9915\n",
      "Epoch 00041: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0272 - acc: 0.9915 - val_loss: 0.1511 - val_acc: 0.9590\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9913\n",
      "Epoch 00042: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0293 - acc: 0.9913 - val_loss: 0.1473 - val_acc: 0.9595\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9902\n",
      "Epoch 00043: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0301 - acc: 0.9902 - val_loss: 0.1570 - val_acc: 0.9590\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9918\n",
      "Epoch 00044: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0262 - acc: 0.9918 - val_loss: 0.1993 - val_acc: 0.9534\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9907\n",
      "Epoch 00045: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0285 - acc: 0.9907 - val_loss: 0.1747 - val_acc: 0.9550\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9880\n",
      "Epoch 00046: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0371 - acc: 0.9880 - val_loss: 0.1560 - val_acc: 0.9595\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9898\n",
      "Epoch 00047: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0307 - acc: 0.9898 - val_loss: 0.2119 - val_acc: 0.9529\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9944\n",
      "Epoch 00048: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0194 - acc: 0.9944 - val_loss: 0.1930 - val_acc: 0.9511\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9939\n",
      "Epoch 00049: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0200 - acc: 0.9939 - val_loss: 0.1808 - val_acc: 0.9585\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9929\n",
      "Epoch 00050: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0220 - acc: 0.9929 - val_loss: 0.1652 - val_acc: 0.9595\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9935\n",
      "Epoch 00051: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0219 - acc: 0.9935 - val_loss: 0.2583 - val_acc: 0.9387\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9909\n",
      "Epoch 00052: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0281 - acc: 0.9909 - val_loss: 0.2100 - val_acc: 0.9490\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9903\n",
      "Epoch 00053: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0308 - acc: 0.9902 - val_loss: 0.1696 - val_acc: 0.9592\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9923\n",
      "Epoch 00054: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0235 - acc: 0.9923 - val_loss: 0.1720 - val_acc: 0.9574\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9954\n",
      "Epoch 00055: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0158 - acc: 0.9954 - val_loss: 0.1869 - val_acc: 0.9560\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9945\n",
      "Epoch 00056: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0177 - acc: 0.9945 - val_loss: 0.1704 - val_acc: 0.9609\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9938\n",
      "Epoch 00057: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0208 - acc: 0.9938 - val_loss: 0.2077 - val_acc: 0.9553\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9930\n",
      "Epoch 00058: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0212 - acc: 0.9930 - val_loss: 0.1806 - val_acc: 0.9597\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9957\n",
      "Epoch 00059: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0150 - acc: 0.9956 - val_loss: 0.2053 - val_acc: 0.9548\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9886\n",
      "Epoch 00060: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0352 - acc: 0.9886 - val_loss: 0.1590 - val_acc: 0.9611\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9939\n",
      "Epoch 00061: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0195 - acc: 0.9939 - val_loss: 0.1693 - val_acc: 0.9574\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9940\n",
      "Epoch 00062: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0195 - acc: 0.9940 - val_loss: 0.1570 - val_acc: 0.9641\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9952\n",
      "Epoch 00063: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0157 - acc: 0.9952 - val_loss: 0.1714 - val_acc: 0.9604\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9957\n",
      "Epoch 00064: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0144 - acc: 0.9957 - val_loss: 0.1567 - val_acc: 0.9618\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9952\n",
      "Epoch 00065: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0151 - acc: 0.9952 - val_loss: 0.1926 - val_acc: 0.9581\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9963\n",
      "Epoch 00066: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0122 - acc: 0.9963 - val_loss: 0.1871 - val_acc: 0.9611\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9911\n",
      "Epoch 00067: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0284 - acc: 0.9911 - val_loss: 0.1615 - val_acc: 0.9644\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9929\n",
      "Epoch 00068: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0255 - acc: 0.9929 - val_loss: 0.1473 - val_acc: 0.9632\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9957\n",
      "Epoch 00069: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0159 - acc: 0.9957 - val_loss: 0.1650 - val_acc: 0.9627\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9964\n",
      "Epoch 00070: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0117 - acc: 0.9964 - val_loss: 0.1664 - val_acc: 0.9632\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9970\n",
      "Epoch 00071: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0105 - acc: 0.9969 - val_loss: 0.1519 - val_acc: 0.9655\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9931\n",
      "Epoch 00072: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0211 - acc: 0.9931 - val_loss: 0.1776 - val_acc: 0.9578\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9965\n",
      "Epoch 00073: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0119 - acc: 0.9965 - val_loss: 0.1945 - val_acc: 0.9567\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9956\n",
      "Epoch 00074: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0138 - acc: 0.9956 - val_loss: 0.1847 - val_acc: 0.9562\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 00075: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0150 - acc: 0.9955 - val_loss: 0.1984 - val_acc: 0.9532\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9969\n",
      "Epoch 00076: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0100 - acc: 0.9969 - val_loss: 0.1925 - val_acc: 0.9555\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9964\n",
      "Epoch 00077: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0125 - acc: 0.9963 - val_loss: 0.2466 - val_acc: 0.9467\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9925\n",
      "Epoch 00078: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0250 - acc: 0.9925 - val_loss: 0.1575 - val_acc: 0.9634\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9944\n",
      "Epoch 00079: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0163 - acc: 0.9944 - val_loss: 0.2294 - val_acc: 0.9536\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9973\n",
      "Epoch 00080: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0097 - acc: 0.9973 - val_loss: 0.1649 - val_acc: 0.9632\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9951\n",
      "Epoch 00081: val_loss did not improve from 0.13371\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0160 - acc: 0.9951 - val_loss: 0.2765 - val_acc: 0.9504\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 806us/sample - loss: 0.1895 - acc: 0.9439\n",
      "Loss: 0.18951877616384566 Accuracy: 0.94392526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_concat_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 341312)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 492928)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 492928)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           7886864     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,929,104\n",
      "Trainable params: 7,928,720\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 787us/sample - loss: 2.2415 - acc: 0.4447\n",
      "Loss: 2.241532568433946 Accuracy: 0.44465214\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 113728)       0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 164224)       0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 164224)       0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2627600     dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,690,640\n",
      "Trainable params: 2,690,128\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 816us/sample - loss: 1.3796 - acc: 0.6681\n",
      "Loss: 1.3796283983862412 Accuracy: 0.66812044\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 37888)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 58816)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 58816)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           941072      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,045,712\n",
      "Trainable params: 1,044,944\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 817us/sample - loss: 0.8304 - acc: 0.7780\n",
      "Loss: 0.830403923295121 Accuracy: 0.77798545\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 12608)        0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 23616)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 23616)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           377872      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 565,072\n",
      "Trainable params: 564,048\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 840us/sample - loss: 0.3692 - acc: 0.9161\n",
      "Loss: 0.36920918753817444 Accuracy: 0.91609555\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 8320)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 11904)        0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 11904)        0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           190480      dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 460,240\n",
      "Trainable params: 458,960\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 879us/sample - loss: 0.2417 - acc: 0.9356\n",
      "Loss: 0.24168295059449824 Accuracy: 0.93561786\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 2688)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3840)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 3840)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           61456       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 413,776\n",
      "Trainable params: 412,240\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 912us/sample - loss: 0.1895 - acc: 0.9439\n",
      "Loss: 0.18951877616384566 Accuracy: 0.94392526\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_concat_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 341312)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 492928)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 492928)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           7886864     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,929,104\n",
      "Trainable params: 7,928,720\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 911us/sample - loss: 3.5884 - acc: 0.5963\n",
      "Loss: 3.588405165815898 Accuracy: 0.5962617\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 113728)       0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 164224)       0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 164224)       0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2627600     dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,690,640\n",
      "Trainable params: 2,690,128\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 921us/sample - loss: 2.9999 - acc: 0.6075\n",
      "Loss: 2.9999288387140135 Accuracy: 0.60747665\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 37888)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 58816)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 58816)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           941072      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,045,712\n",
      "Trainable params: 1,044,944\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 901us/sample - loss: 0.9546 - acc: 0.8183\n",
      "Loss: 0.9546497759417952 Accuracy: 0.8182762\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 12608)        0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 23616)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 23616)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           377872      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 565,072\n",
      "Trainable params: 564,048\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 958us/sample - loss: 0.3997 - acc: 0.9227\n",
      "Loss: 0.39971530357547885 Accuracy: 0.9227414\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 8320)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 11904)        0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 11904)        0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           190480      dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 460,240\n",
      "Trainable params: 458,960\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2983 - acc: 0.9391\n",
      "Loss: 0.2983007252920392 Accuracy: 0.9391485\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 2688)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3840)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 3840)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           61456       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 413,776\n",
      "Trainable params: 412,240\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3266 - acc: 0.9321\n",
      "Loss: 0.3265712559037491 Accuracy: 0.93208724\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
