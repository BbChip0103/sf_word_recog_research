{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    init_channel = 128\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/3))), \n",
    "                          strides=1, padding='same', activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                32768016  \n",
      "=================================================================\n",
      "Total params: 32,768,784\n",
      "Trainable params: 32,768,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                10922000  \n",
      "=================================================================\n",
      "Total params: 11,004,816\n",
      "Trainable params: 11,004,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 3,804,176\n",
      "Trainable params: 3,804,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 812,112\n",
      "Trainable params: 812,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 428,176\n",
      "Trainable params: 428,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 313,552\n",
      "Trainable params: 313,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 268,016\n",
      "Trainable params: 268,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 266,000\n",
      "Trainable params: 266,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 32)             5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 268,592\n",
      "Trainable params: 268,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0244 - acc: 0.3435\n",
      "Epoch 00001: val_loss improved from inf to 1.56559, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_4_conv_checkpoint/001-1.5656.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 2.0243 - acc: 0.3435 - val_loss: 1.5656 - val_acc: 0.4945\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4330 - acc: 0.5528\n",
      "Epoch 00002: val_loss improved from 1.56559 to 1.27141, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_4_conv_checkpoint/002-1.2714.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.4331 - acc: 0.5528 - val_loss: 1.2714 - val_acc: 0.6075\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2061 - acc: 0.6291\n",
      "Epoch 00003: val_loss improved from 1.27141 to 1.16199, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_4_conv_checkpoint/003-1.1620.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.2059 - acc: 0.6292 - val_loss: 1.1620 - val_acc: 0.6427\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0506 - acc: 0.6797\n",
      "Epoch 00004: val_loss improved from 1.16199 to 1.05261, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_4_conv_checkpoint/004-1.0526.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.0506 - acc: 0.6797 - val_loss: 1.0526 - val_acc: 0.6806\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9317 - acc: 0.7135\n",
      "Epoch 00005: val_loss improved from 1.05261 to 0.98458, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_4_conv_checkpoint/005-0.9846.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.9317 - acc: 0.7135 - val_loss: 0.9846 - val_acc: 0.7037\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8263 - acc: 0.7486\n",
      "Epoch 00006: val_loss improved from 0.98458 to 0.95933, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_4_conv_checkpoint/006-0.9593.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.8263 - acc: 0.7487 - val_loss: 0.9593 - val_acc: 0.7091\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7354 - acc: 0.7772\n",
      "Epoch 00007: val_loss improved from 0.95933 to 0.93246, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_4_conv_checkpoint/007-0.9325.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7354 - acc: 0.7772 - val_loss: 0.9325 - val_acc: 0.7128\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.7996\n",
      "Epoch 00008: val_loss improved from 0.93246 to 0.92722, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_4_conv_checkpoint/008-0.9272.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6572 - acc: 0.7996 - val_loss: 0.9272 - val_acc: 0.7186\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5918 - acc: 0.8192\n",
      "Epoch 00009: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5918 - acc: 0.8192 - val_loss: 0.9313 - val_acc: 0.7200\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.8410\n",
      "Epoch 00010: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.5178 - acc: 0.8411 - val_loss: 0.9320 - val_acc: 0.7298\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4678 - acc: 0.8541\n",
      "Epoch 00011: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4678 - acc: 0.8541 - val_loss: 0.9519 - val_acc: 0.7277\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4226 - acc: 0.8673\n",
      "Epoch 00012: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4225 - acc: 0.8674 - val_loss: 0.9448 - val_acc: 0.7384\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8792\n",
      "Epoch 00013: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3792 - acc: 0.8792 - val_loss: 0.9504 - val_acc: 0.7386\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8915\n",
      "Epoch 00014: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3408 - acc: 0.8914 - val_loss: 1.0072 - val_acc: 0.7372\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.9011\n",
      "Epoch 00015: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3149 - acc: 0.9011 - val_loss: 1.0293 - val_acc: 0.7314\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9092\n",
      "Epoch 00016: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2836 - acc: 0.9092 - val_loss: 1.0015 - val_acc: 0.7447\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9170\n",
      "Epoch 00017: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2654 - acc: 0.9170 - val_loss: 1.0248 - val_acc: 0.7508\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9203\n",
      "Epoch 00018: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2445 - acc: 0.9203 - val_loss: 1.0581 - val_acc: 0.7424\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9267\n",
      "Epoch 00019: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2275 - acc: 0.9267 - val_loss: 1.0840 - val_acc: 0.7412\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9321\n",
      "Epoch 00020: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2110 - acc: 0.9322 - val_loss: 1.0659 - val_acc: 0.7510\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9364\n",
      "Epoch 00021: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1972 - acc: 0.9363 - val_loss: 1.1005 - val_acc: 0.7482\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9405\n",
      "Epoch 00022: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1871 - acc: 0.9405 - val_loss: 1.1151 - val_acc: 0.7473\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9429\n",
      "Epoch 00023: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1810 - acc: 0.9429 - val_loss: 1.1436 - val_acc: 0.7575\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9462\n",
      "Epoch 00024: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1682 - acc: 0.9462 - val_loss: 1.1686 - val_acc: 0.7547\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.9484\n",
      "Epoch 00025: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1621 - acc: 0.9484 - val_loss: 1.1697 - val_acc: 0.7517\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9511\n",
      "Epoch 00026: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1530 - acc: 0.9511 - val_loss: 1.2441 - val_acc: 0.7475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9552\n",
      "Epoch 00027: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1424 - acc: 0.9552 - val_loss: 1.1769 - val_acc: 0.7561\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9563\n",
      "Epoch 00028: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1384 - acc: 0.9563 - val_loss: 1.2095 - val_acc: 0.7498\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9576\n",
      "Epoch 00029: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1346 - acc: 0.9576 - val_loss: 1.2000 - val_acc: 0.7554\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9583\n",
      "Epoch 00030: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1308 - acc: 0.9583 - val_loss: 1.1677 - val_acc: 0.7610\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9586\n",
      "Epoch 00031: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1290 - acc: 0.9586 - val_loss: 1.2410 - val_acc: 0.7612\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9601\n",
      "Epoch 00032: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1243 - acc: 0.9601 - val_loss: 1.1639 - val_acc: 0.7605\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9630\n",
      "Epoch 00033: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1158 - acc: 0.9630 - val_loss: 1.2243 - val_acc: 0.7668\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9630\n",
      "Epoch 00034: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1164 - acc: 0.9630 - val_loss: 1.2943 - val_acc: 0.7636\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9667\n",
      "Epoch 00035: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1071 - acc: 0.9667 - val_loss: 1.2740 - val_acc: 0.7552\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9668\n",
      "Epoch 00036: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1055 - acc: 0.9668 - val_loss: 1.2760 - val_acc: 0.7659\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9643\n",
      "Epoch 00037: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1119 - acc: 0.9643 - val_loss: 1.2561 - val_acc: 0.7582\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9682\n",
      "Epoch 00038: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1023 - acc: 0.9682 - val_loss: 1.2861 - val_acc: 0.7594\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9677\n",
      "Epoch 00039: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1046 - acc: 0.9677 - val_loss: 1.2349 - val_acc: 0.7685\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9682\n",
      "Epoch 00040: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1004 - acc: 0.9682 - val_loss: 1.2732 - val_acc: 0.7652\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9701\n",
      "Epoch 00041: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0974 - acc: 0.9701 - val_loss: 1.2432 - val_acc: 0.7594\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9715\n",
      "Epoch 00042: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0942 - acc: 0.9715 - val_loss: 1.2806 - val_acc: 0.7624\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9707\n",
      "Epoch 00043: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0957 - acc: 0.9707 - val_loss: 1.2808 - val_acc: 0.7605\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9723\n",
      "Epoch 00044: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0879 - acc: 0.9723 - val_loss: 1.3185 - val_acc: 0.7654\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9720\n",
      "Epoch 00045: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0899 - acc: 0.9720 - val_loss: 1.2664 - val_acc: 0.7675\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9750\n",
      "Epoch 00046: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0875 - acc: 0.9750 - val_loss: 1.2669 - val_acc: 0.7713\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9742\n",
      "Epoch 00047: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0878 - acc: 0.9742 - val_loss: 1.2816 - val_acc: 0.7778\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9747\n",
      "Epoch 00048: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0820 - acc: 0.9747 - val_loss: 1.2642 - val_acc: 0.7752\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9742\n",
      "Epoch 00049: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0841 - acc: 0.9742 - val_loss: 1.2898 - val_acc: 0.7647\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9748\n",
      "Epoch 00050: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0811 - acc: 0.9748 - val_loss: 1.3315 - val_acc: 0.7659\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9767\n",
      "Epoch 00051: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0792 - acc: 0.9767 - val_loss: 1.2405 - val_acc: 0.7778\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9761\n",
      "Epoch 00052: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0791 - acc: 0.9761 - val_loss: 1.2906 - val_acc: 0.7706\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9755\n",
      "Epoch 00053: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0797 - acc: 0.9755 - val_loss: 1.3161 - val_acc: 0.7689\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9773\n",
      "Epoch 00054: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0758 - acc: 0.9773 - val_loss: 1.3496 - val_acc: 0.7636\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9775\n",
      "Epoch 00055: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0758 - acc: 0.9775 - val_loss: 1.3697 - val_acc: 0.7640\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9785\n",
      "Epoch 00056: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0723 - acc: 0.9785 - val_loss: 1.3637 - val_acc: 0.7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9788\n",
      "Epoch 00057: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0728 - acc: 0.9788 - val_loss: 1.3670 - val_acc: 0.7727\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9793\n",
      "Epoch 00058: val_loss did not improve from 0.92722\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0713 - acc: 0.9793 - val_loss: 1.2955 - val_acc: 0.7750\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmWSSyZ6QBAKEEER2SMIqCoLUiqgV9euCW7Xaqq3W6k9rRa17tdaltbZaRaVqa0W/WL9qtVptQayAEhCQfRNIAglZyL7OzPP740wWQhISyGSS8Lxfr/u6M3fu8swQ7nPPOfeeY0QEpZRS6kgcgQ5AKaVUz6AJQymlVLtowlBKKdUumjCUUkq1iyYMpZRS7aIJQymlVLtowlBKKdUumjCUUkq1iyYMpZRS7RIc6AA6U0JCgqSmpgY6DKWU6jFWr15dICKJ7Vm3VyWM1NRUMjMzAx2GUkr1GMaYPe1dV6uklFJKtYsmDKWUUu2iCUMppVS79Ko2jJbU1dWRnZ1NdXV1oEPpkVwuF8nJyTidzkCHopQKsF6fMLKzs4mKiiI1NRVjTKDD6VFEhMLCQrKzsxkyZEigw1FKBVivr5Kqrq4mPj5ek8VRMMYQHx+vpTOlFHAcJAxAk8Ux0N9OKVXPbwnDGDPIGLPEGLPJGLPRGHNLC+sYY8wzxpgdxpj1xpgJTT672hiz3Tdd7a84RYSamn243SX+OoRSSvUK/ixhuIHbRWQ0MBW4yRgzutk6ZwHDfNP1wJ8AjDF9gPuBk4ApwP3GmDh/BGmMobY2z28Jo7i4mOeee+6otj377LMpLi5u9/oPPPAATz755FEdSymljsRvCUNE9ovIGt/rMmAzMLDZaucBr4m1Eog1xvQHzgQ+EZEiETkIfALM8VesxgQjUueXfbeVMNxud5vbfvjhh8TGxvojLKWU6rAuacMwxqQC44Evm300EMhq8j7bt6y15X7hcDgRafvkfbTmz5/Pzp07ycjI4I477mDp0qWceuqpzJ07l9GjbYHr/PPPZ+LEiYwZM4YFCxY0bJuamkpBQQG7d+9m1KhRXHfddYwZM4bZs2dTVVXV5nHXrl3L1KlTSUtL44ILLuDgwYMAPPPMM4wePZq0tDQuvfRSAD777DMyMjLIyMhg/PjxlJWV+eW3UEr1bH6/rdYYEwm8DdwqIqV+2P/12OosUlJS2lx3+/ZbKS9fe9hyr7cK8OJwRHT4+JGRGQwb9nSrnz/22GNs2LCBtWvtcZcuXcqaNWvYsGFDw62qCxcupE+fPlRVVTF58mQuvPBC4uPjm8W+nTfeeIMXX3yRSy65hLfffpsrr7yy1eNeddVV/OEPf2DmzJncd999PPjggzz99NM89thjfPvtt4SGhjZUdz355JM8++yzTJs2jfLyclwuV4d/B6VU7+fXEoYxxolNFq+LyN9bWCUHGNTkfbJvWWvLDyMiC0RkkohMSkxsV4eLLUWKiBzlth03ZcqUQ55reOaZZ0hPT2fq1KlkZWWxffv2w7YZMmQIGRkZAEycOJHdu3e3uv+SkhKKi4uZOXMmAFdffTXLli0DIC0tjSuuuIK//vWvBAfb64Vp06Zx22238cwzz1BcXNywXCmlmvLbmcHY+zFfBjaLyG9bWe094KfGmEXYBu4SEdlvjPkYeLRJQ/ds4K5jjam1kkBNzT5qa/cRGTkBY/xfSxcR0ViSWbp0KZ9++ikrVqwgPDyc0047rcXnHkJDQxteBwUFHbFKqjUffPABy5Yt4/333+eRRx7hm2++Yf78+Zxzzjl8+OGHTJs2jY8//piRI0ce1f6VUr2XPy8lpwHfB74xxtTXA90NpACIyPPAh8DZwA6gErjG91mRMeZhYJVvu4dEpMhfgdqCEIi4MSakU/cdFRXVZptASUkJcXFxhIeHs2XLFlauXHnMx4yJiSEuLo7PP/+cU089lb/85S/MnDkTr9dLVlYWs2bNYvr06SxatIjy8nIKCwsZN24c48aNY9WqVWzZskUThlLqMH5LGCLyX6DNp77E1gPd1MpnC4GFfgjtMMYE+47pBjo3YcTHxzNt2jTGjh3LWWedxTnnnHPI53PmzOH5559n1KhRjBgxgqlTp3bKcV999VV+/OMfU1lZyQknnMCf//xnPB4PV155JSUlJYgIP/vZz4iNjeXee+9lyZIlOBwOxowZw1lnndUpMSilehfTlXX3/jZp0iRpPoDS5s2bGTVqVJvbud3lVFVtISxsGMHBMf4MsUdqz2+olOqZjDGrRWRSe9Y9LroGORKHo76E4Z9nMZRSqjfQhEFjG4bX659nMZRSqjfQhAEYEwQ4tIShlFJt0IThY4xTE4ZSSrVBE4aP7U9Kq6SUUqo1mjB8tIShlFJt04Th43B0nxJGZGRkh5YrpVRX0IThU1/C6E3PpSilVGfShOHTtHuQzjR//nyeffbZhvf1gxyVl5dz+umnM2HCBMaNG8e7777b7n2KCHfccQdjx45l3LhxvPnmmwDs37+fGTNmkJGRwdixY/n888/xeDz84Ac/aFj3d7/7Xad+P6XU8eP46pb01lth7eHdmwMEixuHtwrjiICOdECYkQFPt969+bx587j11lu56SbbA8pbb73Fxx9/jMvl4p133iE6OpqCggKmTp3K3Llz2zWG9t///nfWrl3LunXrKCgoYPLkycyYMYO//e1vnHnmmdxzzz14PB4qKytZu3YtOTk5bNiwAaBDI/gppVRTx1fCaINp6Paqc6ukxo8fz4EDB9i3bx/5+fnExcUxaNAg6urquPvuu1m2bBkOh4OcnBzy8vJISko64j7/+9//ctlllxEUFES/fv2YOXMmq1atYvLkyVx77bXU1dVx/vnnk5GRwQknnMCuXbu4+eabOeecc5g9e3anfj+l1PHj+EoYbZQEvJ4qqio34nINwemMb3W9o3HxxRezePFicnNzmTdvHgCvv/46+fn5rF69GqfTSWpqaovdmnfEjBkzWLZsGR988AE/+MEPuO2227jqqqtYt24dH3/8Mc8//zxvvfUWCxd2SZ+OSqleRtswfPzVhgG2WmrRokUsXryYiy++GLDdmvft2xen08mSJUvYs2dPu/d36qmn8uabb+LxeMjPz2fZsmVMmTKFPXv20K9fP6677jp+9KMfsWbNGgoKCvB6vVx44YX86le/Ys2aNZ3+/ZRSx4fjq4TRBts9iMHr7fxnMcaMGUNZWRkDBw6kf//+AFxxxRWce+65jBs3jkmTJnVo/IkLLriAFStWkJ6ejjGGxx9/nKSkJF599VWeeOIJnE4nkZGRvPbaa+Tk5HDNNdfg9XoB+PWvf93p308pdXzQ7s2bKC9fT1BQFGFhQ4688nFEuzdXqvfqSPfm/hyidSHwPeCAiIxt4fM7gCuaxDEKSPSNtrcbKAM8gLu9X+bYY+4+D+8ppVR34882jFeAOa19KCJPiEiGiGRgx+v+rNkwrLN8n3dJsgDtHkQppdrit4QhIsuA9o7DfRnwhr9iaS8tYSilVOsCfpeUMSYcWxJ5u8liAf5ljFltjLm+62LR7kGUUqo13eEuqXOBL5pVR00XkRxjTF/gE2PMFl+J5TC+hHI9QEpKyjEF4nA4AUHEgzHd4adRSqnuI+AlDOBSmlVHiUiOb34AeAeY0trGIrJARCaJyKTExMRjCqQ+SWg7hlJKHS6gCcMYEwPMBN5tsizCGBNV/xqYDWzomng6/+G94uJinnvuuaPa9uyzz9a+n5RS3YbfEoYx5g1gBTDCGJNtjPmhMebHxpgfN1ntAuBfIlLRZFk/4L/GmHXAV8AHIvKRv+I8NOb6hNF5JYy2Eobb3XZi+vDDD4mNje20WJRS6lj48y6py0Skv4g4RSRZRF4WkedF5Pkm67wiIpc2226XiKT7pjEi8oi/YmyusUqq80oY8+fPZ+fOnWRkZHDHHXewdOlSTj31VObOncvo0aMBOP/885k4cSJjxoxhwYIFDdumpqZSUFDA7t27GTVqFNdddx1jxoxh9uzZVFVVHXas999/n5NOOonx48fz3e9+l7y8PADKy8u55pprGDduHGlpabz9tr2/4KOPPmLChAmkp6dz+umnd9p3Vkr1TsdVy24bvZv7BOPxjMCYEBztTKVH6N2cxx57jA0bNrDWd+ClS5eyZs0aNmzYwJAh9onyhQsX0qdPH6qqqpg8eTIXXngh8fGHdoC4fft23njjDV588UUuueQS3n77ba688spD1pk+fTorV67EGMNLL73E448/zlNPPcXDDz9MTEwM33zzDQAHDx4kPz+f6667jmXLljFkyBCKitp7B7RS6nh1XCWMIzO+yb+31U6ZMqUhWQA888wzvPPOOwBkZWWxffv2wxLGkCFDyMjIAGDixIns3r37sP1mZ2czb9489u/fT21tbcMxPv30UxYtWtSwXlxcHO+//z4zZsxoWKdPnz6d+h2VUr3PcZUw2ioJ1Kuo2IMxoYSHn+i3OCIiIhpeL126lE8//ZQVK1YQHh7Oaaed1mI356GhoQ2vg4KCWqySuvnmm7ntttuYO3cuS5cu5YEHHvBL/Eqp41N3uK22W+ns7kGioqIoKytr9fOSkhLi4uIIDw9ny5YtrFy58qiPVVJSwsCBAwF49dVXG5afccYZhwwTe/DgQaZOncqyZcv49ttvAbRKSil1RJowmrHdg3RewoiPj2fatGmMHTuWO+6447DP58yZg9vtZtSoUcyfP5+pU6ce9bEeeOABLr74YiZOnEhCQkLD8l/+8pccPHiQsWPHkp6ezpIlS0hMTGTBggX8z//8D+np6Q0DOymlVGu0e/NmqquzqKvLJypqQmeH12Np9+ZK9V4d6d5cSxjN2GcxvIh4Ah2KUkp1K5owmnE4tHsQpZRqiSaMZuqf9vZ6tZtzpZRqShNGM/7oHkQppXoDTRjNaI+1SinVMk0YzfijPymllOoNNGEAeDzg6znWGAfQuc9idFRkZGTAjq2UUq3RhOH12h4JfT27gr1TSksYSil1KE0YDge4XFBZ2bCoM7sHmT9//iHdcjzwwAM8+eSTlJeXc/rppzNhwgTGjRvHu+++28ZerNa6QW+pm/LWujRXSqmjdVx1PnjrR7eyNreF/s2rq2211Be2U0CvtwoRL0FBEYev20xGUgZPz2m9V8N58+Zx6623ctNNNwHw1ltv8fHHH+NyuXjnnXeIjo6moKCAqVOnMnfuXIwxre6rpW7QvV5vi92Ut9SluVJKHYvjKmG0yuGAujoQAWOwBa/OedJ7/PjxHDhwgH379pGfn09cXByDBg2irq6Ou+++m2XLluFwOMjJySEvL4+kpKRW99VSN+j5+fktdlPeUpfmSil1LPyWMIwxC4HvAQdEZGwLn5+GHcv7W9+iv4vIQ77P5gC/B4KAl0Tksc6IqdWSQEkJbN8OI0ZAVBQ1Nfuprc0hMnKCrxH82Fx88cUsXryY3Nzchk7+Xn/9dfLz81m9ejVOp5PU1NQWuzWv195u0JVSyl/82YbxCjDnCOt8LiIZvqk+WQQBzwJnAaOBy4wxo/0YJ4SF2blvjInOfhZj3rx5LFq0iMWLF3PxxRcDtivyvn374nQ6WbJkCXv27GlzH611g95aN+UtdWmulFLHwp9jei8DjmaQhSnADt/Y3rXAIuC8Tg2uOacTgoObJIz6p707506pMWPGUFZWxsCBA+nfvz8AV1xxBZmZmYwbN47XXnuNkSNHtrmP1rpBb62b8pa6NFdKqWMR6DaMk40x64B9wM9FZCMwEMhqsk42cFJrOzDGXA9cD5CSknJ0URhjSxm+O6Ucjvr+pOoICjq6XTZX3/hcLyEhgRUrVrS4bnl5+WHLQkND+ec//9ni+meddRZnnXXWIcsiIyMPGURJKaWOVSBvq10DDBaRdOAPwP8dzU5EZIGITBKRSYmJiUcfTViYLWGIaPcgSinVgoAlDBEpFZFy3+sPAacxJgHIAQY1WTXZt8y/wsLsQ3w1NZ1eJaWUUr1BwBKGMSbJ+B46MMZM8cVSCKwChhljhhhjQoBLgfeO5VjtGlUwPNzOq6p8d0YFaQmDdv52Sqnjgj9vq30DOA1IMMZkA/cDTgAReR64CPiJMcYNVAGXij07uY0xPwU+xt5Wu9DXtnFUXC4XhYWFxMfHt/lQHC6XnVdVQVxcp4/t3ROJCIWFhbjqfxul1HGt14/pXVdXR3Z2dvueWcjJgZAQSEyktjYXMISE9PNPsD2Ey+UiOTkZp9MZ6FCUUn7QkTG9A32XlN85nc6Gp6CP6N57Yf162LaNDRt+SWXlFtLTj7pwo5RSvYp2PthUWhrs2AEVFYSFDaWqagcejz5NrZRSoAnjUGlptj+pjRuJjj4FkVrKy1cHOiqllOoWNGE0lZZm5+vXExMzDYCSki8CGJBSSnUfmjCaSk2FyEhYv56QkETCwoZTUvLfQEellFLdgiaMphwOGDfONnwDMTHTKSlZjog3wIEppVTgacJoLi3NJgwRYmKm4XYXUlm5NdBRKaV6q4ICWLUK9u2zA7l1Y73+ttoOS0uDF16AnBxi+kwHbDtGRMSoAAemlOo13G746CNYuBD+8Q87gBvYXrMHDoTkZEhKgooKO15PcbGdysrgjjvgvvsCErYmjObGjbPzb74hbM4cnM5ESkr+y4ABPwpsXEqpnm/bNnj5ZXjtNcjNhb594Wc/g+nTYf9+yMpqnDZutG2qsbE2icTGwp49cP/9MHEinHNOl4evCaO5+oSxfj3mrLOIjj6F0lK9U0opdQwqK+HBB+Gpp+z7c86Ba6+Fs8+24/G0V3U1TJ0KV18N69bZRNKFtA2judhYSEk5pOG7qmoHtbV5AQ5MKdVlKivhk09sD9bH6pNP7IXo44/bE312Nrz7Lpx3XseSBdg+79580yaOK67o8jYPTRgtqW/4Bn0eQ6lACVQD8K5dcPLJMHs2/PnPR7+fggK46iq7n6Ag+M9/bHVUUtKxxTdiBDz3HHz2GTz88LHtq4M0YbQkLQ22bIGaGqKiJuBwuPR5DKW6SkUF3HADREfbK/Gu9NFHMGkS7N0Lo0fD3XfbRueOKC+3VU+jRsEbb8Avf2kvQGfN6rw4r7rKTg8/DEuXdt5+j0ATRkvS0uxdDFu24HCEEhU1WUsYSnWFr7+2DbovvmgbhC+6CN55x//H9XrhkUdsm8KgQZCZaRum8/PhV79q3z6Ki+0+UlPh5z+H9HT7fR5+uHH4hM707LMwbBhcfrmNswtowmhJky5CwLZjlJevweOpDGBQSrXizTfh17+2d9b0VF4vPPkknHSSvXX0009h7Vp7tX/JJbB4sf+OXVoKF15oSwKXXQbLl8PQoTZxXXMN/P739u6m1hw8aHu6HjzY7mPqVLuPTz+FsWP9F3dkpP23LyqybSOd0d5yJCLSa6aJEydKp6irEwkNFfn5z0VEpKDgH7JkCVJUtKRz9q9UZ/n8c5GgIBEQMUZkzhyRt94Sqa4++n3W1or84Q8i554rsmdP58UqIrJ1q8hnn9lp6VKRJUtEPv1U5PTT7Xe44AKRgoLG9UtKRKZNs99x0aL2H2fvXpGPPrLbt6akROTRR0USEuz+n35axOs9dJ39+0WiokTOOaflfezZIzJsmI39wgtF1qxpf4yd5dlnRc47T6S8/Kg2BzKlnedYv528gYXAAWBDK59fAawHvgGWA+lNPtvtW762I1+m0xKGiMiECSKzZ4uISG1tkSxZguze/avO279Sx+rAAZEBA0ROPFFk7VqRe+8VSU62/63j40V+8AORa68Vuewye0I54wyRmTNF7rlHZPv2lvf5z3+KjBpl9xEUJJKaKrJrV9txLFki8s47h59sm6quthdgtj/ow6fwcJEXX2x5H2VlIjNmiDgcIq+/3vL+q6pEPv5Y5LbbREaPbtxvaKjI3Lkir70mUlxs1y0sFLnvPpHYWLvO2WeLfPVV67E/8YRd78MPD12+fbtISopITIzIsmVt/kR+5fW2/dsfQXdJGDOACW0kjFOAON/rs4Avm3y2G0jo6DE7NWH86Eci0dENVyhffjlG1q2b03n7V+pYeDwiZ55pT4hff9243O22J/2LLxZJTBQZONBeAaeliUydKnLSSfbECyLTp4u8/LJIaanIxo22dAI2Af3f/4msWiUSF2eT0LZth8dQUyNy++2NJ+dp01o+8W7aJJKRYde54QZbovj0U5F//1vkP/+xCSc7u+3vW14uMmuWjT0jw36fsWNtchgxQiQszO4/JETku98VefJJ+zvcemtjEnU6Rb7zHZHISPv+f/5HZPXqI//WNTX2Nxwxwpa+RES++UYkKcmWTtqzj26sWyQMGweprSWMZuvFATlN3gc+YWRm2p/n178WEZEtW26QZctixOt1d94xlDpav/qV/ft8/vmOb5udbf+uR4xovLoPCrJXyk89ZU+Q9dautSfFpCSbVOpt2yYycaLd/sYbRRYsEOnb176/6iqRnBx71fvccyIul93Hu+8e23euqBC56SZbVXbeefaEf9FFIpdcInLLLSIffNBytYzHI7JihU1uo0aJXH65PeF3xPvv2+/229/ac0OfPiL9+x/6m/RQPTFh/Bx4qcn7b4E1wGrg+vYer1MThoi94kpMFKmokP37X5MlS5CysnWdewzVPe3ZY6++j6Go324lJbbapb2WLrVX2pdddmzxeb32RPqTn9iT6YEDLa+3caNNGImJIuvWibz6qkhEhD1pvvPOod9j/nx7lR8eLnLKKfYUM3u2yL59Rx9nd+D12hJddLSdBg8W2bEj0FF1ih6VMIBZwGYgvsmygb55X2AdMKON7a8HMoHMlJSUzv0l//tf+xM9/bRUVu6SJUuQ7OznOvcYqvuprhZJT7f/9r/5jX+PVVwscsIJIiNH2ivoI8nNtVe2w4fbqqSusnWrrd4KDbW/y8yZIllZLa+7c6e98g8Lsw3JHk/XxelPmzaJBAfbkllr370H6jEJA0gDdgLD21jnAeDn7Tlep5cwROx/jAEDxFtVJV980V82bry884+hupf6evlJk+yV/Kef+u9YV1zR2KZwyy1tr1tXZ+vnXS57pd/Vdu60pYaHH7ZtJUdSV+f/mLrahg0iBw8GOopO1SMSBpAC7ABOabY8Aohq8no5MKc9x/NLwvjXv+zP9MILsmHDRbJ8+eDOP4bqPj75RBrq5cvKbKNqQkLbt5ce7V0qr71mj/XQQ7ZuHmwjcGvH+MlP7Dovv9zxYynVio4kDGPX73zGmDeA04AEIA+4H3ACiMjzxpiXgAuBPb5N3CIyyRhzAlD/aGcw8DcReaQ9x5w0aZJkZmZ23pewwdoHcfLzyf7PT9mx+3amTt2LyzWoc4+jAq+w0D60GR0Nq1dDeDhs3QqTJ8PIkfD55xAaeug2//433Hyz7VBu3Di7fVqafZ2eDlFRLR9r507IyIDx42HJEtuZXEaGHRdh/XobQ1NPPAG/+IWdfvMb/3x/dVwyxqwWkUntWrm9maUnTH4pYYjYuztAqhY8ou0YvZXXa++6cToPf/jqnXfslf311zcuy821d9uAyNChtkQyY0bjvf1g6/AfffTQu45E7K2ZU6bYdZuWXJYvt9VTP/zhoeu/+abd37x5vac9QHUbdJcqqa6e/JYwPB6RtDTxjhwpX64YIWvWzPTPcVTgvPyy/e/w+OMtf37XXfbzF18U+dOf7C2oISH2AbCqqsb1vF7bIPrBB/bJX7C3ci5Z0rjO3Xfb5f/7v60f5x//sO8//9weZ/r0Q4+jVCfRhOEPixaJgOT+8WJZssRIdXUPv01QNdq2zd4m+p3vtH4F73bbBuf60sN3viOyZcuR9/3hhyJDhthtrrzSlhaMObwUUa+62j6UlpQk8sUX9tbV4cMP7S5DqU7UkYThtzaMQPBLG0Y9jwdGj8bjcvD501s4cdgzJCff7J9jqc5XVgYLFtheUIuKbFtBba2d3G6Ii7NtB8nJre+joABuugnOPdcOXmNM+45dVQWPPmrbHurqYPhw20YSGdny+uvX20733G5ISICVK+GEEzr+nZVqh460YWjC6IhXXoFrrmHbbwdTPjOZCRN0jIxur7AQnnkG/vAH26vozJl2nIKQEDs5nXY6/3zbAO1PW7facRJuvrlxKODW/O538NBDdnyGk07yb1zquKYJw1/q6mDYMGr6eFnxVBZTT9a7pbqt/fvtkJgLFtjhNs8/H+66C6ZMCXRk7ed2Q3BwoKNQvVxHEoaOh9ERTifceSehX2cR+zXk5/9voCNSzZWU2DEJhg61pYqLLoKNG+0gPD0pWYAmC9XtaMLoqGuugf79OeGNKA4ceDPQ0ah6NTV2oJuhQ+2oZ+edZ6uAXn3VDrWplDpmmjA6yuWCn/+c6MwyzMqvqKr6NtARHd/27oU//ck+WHfrrfbht8xMO5by0KGBjk6pXqVdCcMYc4sxJtpYLxtj1hhjZvs7uG7rhhuQ+DgG/xXy898KdDTHl4oK+PBDuOUWmyQGD4Ybb4TYWPj4Yzss5sSJgY5SqV6pvSWMa0WkFJiNHbvi+8Bjfouqu4uIwPy/24lfCWWf/znQ0RwfPB5751DfvnDOObYxOzUVfvtb2LAB1qyB2cfvNYxSXaG9CaP+hvOzgb+IyMYmy45PN92EN8pF4otbqazcHuhoerb1623DdGs2b4bp0+G22+A734F//cveIvvRR/D//h+MGdP+ZyKUUketvQljtTHmX9iE8bExJgrw+i+sHiA2Fu+N15G4DA4u/0Ogo+mZPv8czjjDdtI3dqw98T/8MGzbZj+vq4Nf/9q2S2zbBq+/Du+9Z7dxuQIbu1LHoXY9h2GMcQAZwC4RKTbG9AGSRWS9vwPsCL8/h9FcQQGeQf04OCuahA8Pdt1xezIR2zvrQw/BZ5/ZKqaf/xwiImDRIptEwD5E5/XCunVw8cX2Ftl+/QIbu1K9kD+ewzgZ2OpLFlcCvwRKjjbAXiMhgcrvzyT+42IqN3wc6Gi6v6+/hlNPhdNPh+3b4emn4dtv4Y47bMP1smWQlWXbJZxOKC2FxYvhrbc0WSjVDbS3hLEeSMeOkPcK8BJwiYjM9Gt0HdTlJQyg5tuvcY6YQNXMYUT8a6vWpbekogLuv98miIQEuO/otd/EAAAgAElEQVQ+uPZarVZSqhvwRwnD7evV8DzgjyLyLNDKyDDHl9Ah48m9dTQRn27H+5tHAx1O9/PPf9q2iaeegh/+0DZg33ijJguleqD2JowyY8xd2NtpP/C1aTiPtJExZqEx5oAxZkMrnxtjzDPGmB3GmPXGmAlNPrvaGLPdN13dzjgDImz+Mxw4Dcw999r6+d6otNS2I5x9tu3x1e1ue/3sbLj8crt+eLhtm3jhBdsrrFKqR2pvwpgH1GCfx8gFkoEn2rHdK8CcNj4/Cxjmm64H/gTga1S/HzgJmALcb4zptmea2LjvkHX/GKpTnMill0JOTqBD6jwbN9oSwcCB8LOf2XaI66+33W0sWmQbppv66iubKIYMgbffhgcftNtMnx6Y+JVSnaZdCcOXJF4HYowx3wOqReS1dmy3DChqY5XzgNd843isBGKNMf2BM4FPRKRIRA4Cn9B24gkoYwwDR97BNw/UQkWpvauntjbQYR2b1ath1ix7u+vChXDhhTYZ7NsH775rx7a+7DKYMAH+8Q/bOD1tmu2K+4MPbHLZutW2VzQfB1sp1SO1t2uQS4CvgIuBS4AvjTEXdcLxBwJZTd5n+5a1trzb6tv3UtzDkth733BYscLe+dNTffMNfPe79oT/m9/Y6qVXXoHJk22j/ty5sHYt/PWvdmCic8+1STI313YAmJ1t2yxSUwP9TZRSnai9/SffA0wWkQMAxphE4FNgsb8Cay9jzPXY6ixSUlICFofDEcqAATfx7ZR7GXDjlTifecZ2p33FFQGL6ajs2mW72AgPhy++aP2kHxRkv9sll9jbXqOibJcdQUFdGq5Squu0N2E46pOFTyGd09NtDtB0BKJk37Ic4LRmy5e2tAMRWQAsAHtbbSfEdNQGDPgxe/c+wq6fhDBi3TS48kr7dPI999jqmu5u/377FHVtrW2kbk8JwenseUlRdXsi9kF/Yxonh++MU1lphz1pOtXW2j/F4OBDJ4/HNrN5PI2vjbHXNQ6Hnddf49SvUz+53XZ03aoqe8z61yJ23/XbNp3q9+lw2ONUVzdOVVV27nbXDwzf+F09Hvt9m05ut43X621cv37d5t8pNtbWCvtbexPGR8aYj4E3fO/nAR92wvHfA35qjFmEbeAuEZH9vmM92qShezZwVyccz69CQhLo1+8qcnNfZci7mwl54Q3bYd706TBjhk0cZ5zRPZ/VKCqyJYu8PPjPf3QMiW6ottbWAHo8hy4XsSejoiLbxVZRkZ1KSw890dQLC7PDiddPERF2n0VFdkTbwsLG7etP1E1PhPUnq6bz2trGE2rTqbbWDlXSdG5M4wm3fi5iT6Y1NXbe05sAW1OfyOqTIDQmsPrRguun4ODG375p4mwpOUVEdE387R6i1RhzIVB/mfy5iLzTjm3ewJYUEoA87J1PTgARed4YY4A/Yhu0K4FrRCTTt+21wN2+XT0iIkfsFjYQD+41V1GxmVWrRpOa+iCpqffZh9ZeegmeeMLePTVpkn2I7Zxzuk/iqKiwbRZr1tiuw08/PdARdRsi9sRZUGCnwkJ7Qqursye1+qvBpifE+tf1V4n1V6tNrwibnsjrr6brT5ZNr0jLyuzxy8rs510lPByio+3r+qvc+gTR9Mq8/rXTaRNR08nlsvc71E/1w6iD/T2a/jbGNK5fP69ft+nv5fXak2NMjJ2io+08NLRxn263/T2bxtp0Xr+f5v8ezUsLwcGN3yU8vPG1MYf/mzZPoPWTy2WnsDAbY3ccRFHH9A6w9evPoawsk6lT9xAU5HtAraYG/vIX25nerl22feOhh+xVfWcmjrIyWzZ95RV7S2xqqh1I6IQT7Dw52V4+5uQ0TuvXw44ddrsLLui8WAKo/mq8uNhWWRQXN74uL7dVDBUVdqqstOvWT/Un6JISmyCO9MhJS5peJTavvmh+xWiMXbfpCbN+io62U1RU47ylk47LBfHx0KdP4xQd3ViNA/Y49aWR8vLGqazM7rPp9vpc5fGj0xKGMaYMaGkFA4iIRB9diP7RXRJGUdGnrF9/BiNGLKR//2sO/bCuDl57zfbKumePbdt46CHbbffR8nptP0x//rM96VdWwvDhthosKwt27oTduw8/80VG2gQycCD8+Md2/OtuxOu1J+2CAsjPb7zKb/q+fl6fCMrLbRKoq2vfMcLD7RVrZOThJ+boaNuTSf0UH2+nsDB7gg8JaUwMTa+gQ0K6T+FRqSPREkaAiQiZmemIeJk8eT32wfhmamvt8w2/+pW9yo+Nhf797TRggJ2Hh9vSQH1dSEGBraSurW0sE7vdtvRSVWXPcJdeCj/4AUydeuhZy+22ySM72579Bg5srHMIALfbhrJnj52ys+3PsG9fY8EnL+/w+vp6oaGQmGin+Hj7AHn9ib/pFBvbONVXY9TX29dXLyh1PNOE0Q3k5b3O5s1XMmbMYhITL2x9xepqW1W1fr29S2nfvsZ5ba0908XHN17ixsU1VobWV7QGB9thSS+4wCaZAKqosB3Rbttmh9tuWhVUXGzzX1aWTQjNHxKPi7N5bODAxpyZmHjoVX5Cgl0WHq4ne6U6gyaMbkDEw1dfjcbhCGXSpLUtlzLa3oG9vO6GrWQejy0VbN3aOG3bZqfs7EPXdTgOvcKPjYVBg+xQ3E2n5GR7xa+U6lodSRjd72zUSxgTRGrqfWzefCUFBe+0XcpoeQfdIlnk5toxjOqn9ettCaLpHTuxsTBihG2GGT68cRoyxLYFaElAqd4h8GekXqxv30vZvfshdu9+kISECzpeyuhi1dWwapV9wPuLL2zXUQeaPK45aBCkpcGcOTZBjBgBI0faaiJNCkr1fpow/OiYSxl+lptru71avtwmiNWrGx+YGjHC9kyekWGH3E5Ls7dbKqWOX5ow/MyWMh4OeCnD64VNm+ww2suX20Tx7bf2s5AQ22Z+yy32Lt9TTrENy0op1ZQmDD9rLGVc0aWlDBE7uN2SJbB0qZ0KCuxnAwbAySfDT39q5xMmaA/kSqkj04TRBfr2nef3tgyvFzZssCWIzz6zz/Hl59vPBg2y1UuzZsHMmfbhb21zUEp1lCaMLuCvUoaITQ4vvAD/+pd9xgEgJcU2TM+caZPEkCGaIJRSx04TRhfpzFJGaal91u+552y7RFwcnHcenHZaYwlCKaU6myaMLtK0lJGX9zeSkq7s0PYikJlpexP5y1/sE9WTJtnuo+bN04felFL+pwmjC/XteynZ2b9j1647SUg4n+DgyCNuk5NjR0J99VXbiO1y2e6ibrzRjpiqlFJdpXs/SdbLGOPgxBOfobZ2H3v3PtbqenV1ttPZM8+07RHz59tnIBYssM9O/PnPmiyUUl1PSxhdLCbmZPr1u5KsrCfp3/9awsJOaPjswAF48UX4059sySIlBe6+G666CoYNC2DQSimFJoyAOOGEx8jPf4edO3/O2LF/Z/Vq+P3v4c037ZPWZ5xhk8bZZzeON6yUOjKP18P+8v3sLt5NXnkeYc4wokKiiAyJJDIkkqjQKPpG9MVxFDedFFQWkLkvE1ewi6FxQxkYPfCo9tOZRIQaTw1VdVXEhcUdeYNj5NeEYYyZA/weCAJeEpHHmn3+O2CW72040FdEYn2feYBvfJ/tFZG5/oy1K4WGDmTw4Lv5/PM/c889ebz3Xj8iI+H66+Gmm2z/TEo15RUvueW55JTmEBIUQqwrllhXLFGhUe0+aYkIeRV5xITGEObs2F0SlXWVbM7fzOaCzTiMo+H49VN8WDyhwZ339Gedp46iqiIq6iqoqK2gsq6SyrpKKuoqKK4upqiq6JCpPklklWRR52179KyokCjG9x/PxP4T7TRgIqmxqdR56qj11FLntfODVQf5KucrlmcvZ3nWcrYVbjtkP6FBoQyJG8LQuKEMiBpAWHAYrmAXrmAXocGhhDvDSYpMYkDUgIYp3Nnx4QcOVBxgeZaNYUX2CnLLc6moraC8tpyKugq84qV/ZH/23b6vw/vuKL91b26MCQK2AWcA2cAq4DIR2dTK+jcD40XkWt/7chE5cqtwE92pe/O25ObCgw+6efFFCAmp5Y47XNx+uyOQ4xn1Kl7xsr1wO3tL9hIREkF0aDRRIVF2HhpFsOPorpM8Xg/7yvY1nKiPtG5eRR7ZpdnklOaQU5ZDTmkO+8v3U+upRRBEpGFujCHIBBHsCCbIEUSwCabWW0tWSRZ7Sva0eiI0GGJcMQyIGsDIhJGMShjFqIRRjEwYSUJ4Auvy1pG5L7Nhyq+0T3MOih7EsPhhDO8znGHxw4gPi6fWU3vICbO4uphN+ZvYcGADuw7uQlocfLNR34i+JEcnMyh6EMnRySSEJ1BRW0FJTQklNSWU1pRSWlNKkAkizBlGWHBYw7zWU0teRR655bnkludSWFl4xOMBxLpi6RPWh34R/UiNTWVwzGA7jx1M/8j+VLurKa8tp6y2jPLackqqS9iUv4nV+1ezLm8d1e7qIx4jITyBUwadwinJp3BS8km4vW52Fu1k58Gd7Cjawc6DO8krz6PGU0O1u7rNfUaHRuMKduEVLx6vB6948YqXYEcwcWFxxLniiAuLI9YVi9PhZNW+Vewo2gFASFAIE/vb5BbhjCAiJILIkEginBH0CevDDZNuOOJ3aUm3GA/DGHMy8ICInOl7fxeAiPy6lfWXA/eLyCe+970uYVRWwm9+A089ZbsH//73d/G9753MlCn3kpz800CHd9REhOLqYvaW7CWrNIuskiz2luylyl3F8PjhDSewpMgkTBtPEHrFy+7i3azNXcu63HXklOXYaoQmJ/uokChCgkJwBjkJdgTjdNh5VmkWa/avYc3+NazNXUtFXUWrxwl3hhMdGk1MaAwxrhhiQmPoE9aHhPAEEsITSAxPJCE8gSBHEFsKtrApfxOb8jextXBrw8kgzhVHSkwKg2MHkxKdAmCTgi8x5Jbn4pFDhwsMdgSTFJmEK9iFwWCMaZiLCB7x4PF6cHvdeMRDkAkiOTqZwbGDGRwzmJSYFJKjk6nz1FFSU0JxdTHF1cUcrDpIVmkWmws2s7No52HHdRgHYxLHMGnAJNL7pVNSU8L2ou1sL9zO9qLtFFUVtfg7BZkghscPZ2zfsQ3TqIRRGGMoqW5y/OqD5Ffkk1WaRXZpdsN0sPogYcFhxLhiGn7v6NBoPOKhqq6KKndVw9zpcJIUmUS/yH4kRSSRFJlEYkQikSGRhDvDD5nqSzSxrliCHEdfZ1vnqWNzwWZW71tNbnkuIUEhDX9bIUEhRDgjmDhgIkPjhrb5d9uciFDnraO8tpzc8lz2le07ZKrz1OEwDhzGQZAjCIdx2BJN9UEOVh1smFe5q8hIymDaoGlMGzSNiQMm4gru/MHWu0vCuAiYIyI/8r3/PnCSiBx2ZjTGDAZWAski9q/dGOMG1gJu4DER+b9WjnM9cD1ASkrKxD179vjj6xyzL76Aa66xY0lcfDE88giceKKwbt13KS//mpNO2o7TGe+343u8HrYWbmXDgQ3UuGsOu8INcgQRGhRKaHBowzwqJIqB0QPpF9HvkP+YIsLmgs0s3b2UJbuX8NnuzxquXOs5HfY/XdMTd0xoDCMSRhAZEmn/Y/rWcQY5ySrJYn3eespqywB7kusX0Y/KukpKa0rbdbUZGRJJRlIGE5ImMKH/BE6IO6Fh+7LaMkprSimpLqGstoyS6sar3pKaEoqqiiioLGjx5Jkam8roxNGMThjNiX1OpKSmhD3Fe9hbupc9xXvYU2L/5pKjkxkYNZCB0QPt3Pe6fnliRKLf67xrPbXsKNrBloItHKg4QFq/NDKSMtqsCimsLKSkpoTQoNCGk6XT4SQ0OPSoS2Ng/+aO5YSuukZPHEDpUmBxfbLwGSwiOcaYE4D/GGO+EZGdzTcUkQXAArAljK4Jt/0qK+Hee+F3v7N3Pf3733agIctw4om/JzMznV275jNixIsd2ndxdTHrctexPm891e7qw67ESmtKWb1/NZn7Mlmzf02bV91tCXYEMyBqAMnRycS6Ysncl8mBCjtQxqDoQZw17CzS+6WTEpPCoOhBpMSk0C+yHwbDvrJ9bC7YzOb8zWwp2ML2ou1U1lVSVlPWUPVR66mlX0Q/rkq/ivR+6WQkZTCm75iGk5yIHHLir/XU4va6qfPU2bm3jn4R/RgWP+yYT8hur7shedR6ahnWZxgRIRHHtM+uFBIUYpNb4uh2bxMfHk98eOdfrGiy6H38mTBygEFN3if7lrXkUuCmpgtEJMc332WMWQqMBw5LGN1Z01LFT35iq6OimlV9R0aOZdCg28nKeoKEhAsJipjaUKQvqCygxl1DjaemYV5WU8bG/I2szV3bcGXbFlewi/FJ47l2/LUNVRIRIRENVSEO48BgcHvdhxyn1lNLSXUJOWU5ZJVkkV1mY8oqyWL20NmcNvg0Zg2ZxZDYIW0W1wdG26vs757w3aP+HY0xRITYOtv+9D/q/bRHsCOYvhF96RvR16/HUaon8mfCWAUMM8YMwSaKS4HLm69kjBkJxAErmiyLAypFpMYYkwBMAx73Y6ydqrzclip+/3sYlOJl8T8LGJqxj89z97F/+34KqwoPucOjsLKA3QUh5P33bKo8bReSHMbBiPgRnDzoZH4y6SekJ6WT3i+d6NDoQ+4kqayrJDQolFGJo46pWkEpper57UwiIm5jzE+Bj7G31S4UkY3GmIeATBF5z7fqpcAiObQxZRTwgjHGi30a/bHW7q7qbj76CK69+2v2D/49kb/8D/uc+7noSzd8eeh6ToeT+PB4+oT1Ic4VR/qA6Tgrl3BCwiQmDruD5OhkEsMTG27RCwkKaWhbaK3apSdVnSileh6/NXoHQiDvkso74OGS+95jWc3TkLqMsKAIzh81l9TY1EPuw+4f2Z+E8ATCneGHVeXs2vVL9u59hLFj3yMh4dyAfA+l1PGlJzZ691he8XLdC8/zyvYn8PbfTYwM5q5ZT3LDlB8S64rt0L5SU++jsPB9tm27npiYDX69a0oppTpKOx88BvvK9jHykdkszLuJcM8Afjt1MQX37eDOGbd3OFkAOBwhjBz5KnV1BWzffrMfIlZKqaOnJYyj9O6W97j8zWuprK1iYv6LLP/jDwkJOfZh7aKiMhg8+F52776fxMQLu2wMcKWUOhItYXRQVV0VN35wE+e/eR6V+wdx/oHVrHzuR52SLOqlpNxFZOREtm69gaqq3Z22X6WUOhaaMDpgbe5aJr84mT9lPgfLb+d6x0refmEkwZ1cTnM4nIwe/TdE3GzceAEeT2XnHkAppY6CJox2qPXUcv+S+5n84mR25RbCXz7i9nFP8vyzoTj89AuGhw9n9Oi/UV6+ji1brqU33c2mlOqZNGEcwdrctUx5cQoPLXuIsXIZVU9u5N7Lz+SJJ6AD/ZEdlfj4sxky5FHy898kK+sJ/x5MKaWOQBNGK5qWKvIq8vjdlPfY8PBrXHJuHx580P/Jol5Kyp0kJs5j1675FBZ+1DUHVUqpFmjCaMWNH9zIQ8se4rKxl7H62o28dMe5JCbakfC6KlmA7Udp5MiXiYhIY9OmS6ms3N51B1dKqSY0YbRgb8leXl33Kj+d/FNeu+A1fvtIHzZuhIULoU+fro8nKCiCsWP/D2OC2bDhPNzusq4PQil13NOE0YLfrvgtAL+Y9gs++wx++1vb2+ycOYGLKSwslTFj/pfKyq36UJ9SKiA0YTRTWFnIi2te5IpxVxBjBnH11XDiifBEN2hzjoubxeDB95CX9yoHDrwZ6HCUUscZTRjN/PGrP1JZV8kvpv2CW26BrCx47TWI6CYdwQ4efB/R0VPZtu3HVFfvDXQ4SqnjiCaMJipqK3jmq2eYO2IuO1aM5pVX4K67YOrUQEfWyOEIZtSovyLiZvPm7yPNxm9WSil/0YTRxEtrXqKoqog7p93Js8/C0KFw332BjupwYWFDGTbsj5SULGPv3h4zrpRSqofThOFT56njqRVPcWrKqUzqdwr//S+cdRaEhAQ6spb163cViYnz2L37PkpLVwU6HKXUccCvCcMYM8cYs9UYs8MYM7+Fz39gjMk3xqz1TT9q8tnVxpjtvulqf8YJ8MaGN8gqzeLOaXeSmQmVlTBrlr+PevSMMQwf/idCQvqzefPluN3lgQ5JKdXL+S1hGGOCgGeBs4DRwGXGmNEtrPqmiGT4ppd82/YB7gdOAqYA9/vG+fYLr3h5/IvHGdt3LGcPO5slS+zyGTP8dcTO4XTGMWrUX6mq2snmzVfg9dYEOiSlVC/mzxLGFGCHiOwSkVpgEXBeO7c9E/hERIpE5CDwCeC3pyA+2PYBG/M3cue0OzHGsHQppKVBQoK/jth5YmNnMGzYHygsfI9vvpmrPdsqpfzGnwljIJDV5H22b1lzFxpj1htjFhtjBnVw207xmy9+w+CYwcwbM4+aGvjiCzjtNH8drfMNHHgTI0Ys5ODBT1m//kzc7pJAh6SU6oUC3ej9PpAqImnYUsSrHd2BMeZ6Y0ymMSYzPz+/wwGU1pTiFS+3n3w7ziAnX30FVVXdu/2iJf37X8Po0YsoLV3J2rWnU1dXGOiQlFK9jD8TRg4wqMn7ZN+yBiJSKCL1Fe8vARPbu22TfSwQkUkiMikxMbHDQUaHRvPFtV9w05SbAFi61HYu2N3bL1rSt+/FjB37LpWVG/n665nU1OwPdEhKqV7EnwljFTDMGDPEGBMCXAq813QFY0z/Jm/nApt9rz8GZhtj4nyN3bN9y/zCGIPD2J9iyRJITw9MJ4OdIT7+bMaN+yc1NXv4+uvpVFZuDXRISqlewm8JQ0TcwE+xJ/rNwFsistEY85AxZq5vtZ8ZYzYaY9YBPwN+4Nu2CHgYm3RWAQ/5lvlVdTWsWNHzqqOai4s7jfT0f+PxlLFmzckUF38W6JCUUr2A6U1Df06aNEkyMzOPevvPPrON3e++C3PnHnH1bq+q6lu++eZsqqp2MmLESyQlXRXokJRS3YwxZrWITGrPuoFu9O5WenL7RUvCwoYwfvwKYmJOZcuWq/n22/t1bHCl1FHThNHEkiUwfjzExgY6ks7jdMaSlvZPkpKuZc+eh9i8+Uo8nupAh6WU6oE0YfhUV8PKlT2//aIlDkcII0a8xJAhj3LgwN9Yt26W3kGllOowTRg+K1ZATU3PemCvI4wxDB58F2PGLKa8fD2rV0+mtPTo23uUUscfTRg+S5eCwwGnnhroSPwrMfFCJkxYjjFBrF17Knl5iwIdklKqh9CE4bNkCUycCDExgY7E/yIj05k4cRVRUZPYvPkydu26BxFvoMNSSnVzmjCwXZl/+WXvrY5qSUhIX9LT/03//j9i795H+eab71FbeyDQYSmlujFNGNj2i9ra3tng3RaHI4ThwxcwbNhzHDz4HzIz0ykq+iTQYSmluilNGNj2i6AgmD490JF0PWMMAwf+hIkTvyI4uA/r189m58478XprAx2aUqqb0YSBbb+YNAmiogIdSeBERqYxceIq+ve/gaysx/n66+lUVe0MdFhKqW7kuE8Y1dWQmXl8tV+0JigonBEjnmfMmMVUVW3nq6/GsH37z6iuzg50aEqpbiA40AEEmssF+/ZBXV2gI+k+EhMvJCpqCrt3P8C+fX9i374X6N//h6SkzMflSgl0eEqpADnuSxhguzLv1y/QUXQvLtcgRo58mSlTtpOUdA3797/El1+eyNat11NTsy/Q4SmlAkAThmpTWFgqI0Y8z0kn7aR//+vJzX2Vr74awd69T2jDuFLHGU0Yql1crkEMH/5HpkzZRGzsLHbt+oXehqvUcUYThuqQsLChjBv3HuPG/QOvt47162ezYcNFVFRsCXRoSik/82vCMMbMMcZsNcbsMMbMb+Hz24wxm4wx640x/zbGDG7ymccYs9Y3vdd8WxVY8fHnMHnyBoYM+RVFRR+yatUoMjPHs3fvb6iq2h3o8JRSfuC3EfeMMUHANuAMIBs71OplIrKpyTqzgC9FpNIY8xPgNBGZ5/usXEQiO3LMYx1xTx2dmppc8vPf5MCBRZSWrgQgOnoqffteRr9+V+J09tAB0pU6DnSXEfemADtEZJeI1AKLgPOariAiS0Sk0vd2JZDsx3iUn4SGJpGcfAsTJqzgpJN2ccIJj+H1VrNjxy0sXz6ATZuupLj4Mx3tT6kezp8JYyCQ1eR9tm9Za34I/LPJe5cxJtMYs9IYc74/AlSdLyxsCCkpdzJp0tdMmrSOAQOuo7DwH6xdexpffTWSvXufoLo668g7Ukp1O92i0dsYcyUwCXiiyeLBvmLS5cDTxpihrWx7vS+xZObn53dBtKq9IiPTGDbsD5xyyj5GjnyVkJC+7Nr1C1auTGH16pPYu/dxKit3BDpMpVQ7+bMN42TgARE50/f+LgAR+XWz9b4L/AGYKSIt9q9tjHkF+IeILG7rmNqG0f1VVm4nP/9tCgrepqzM/ltFRKSRkHABiYkXEBGRhjEmwFEqdfzoSBuGPxNGMLbR+3QgB9vofbmIbGyyznhgMTBHRLY3WR4HVIpIjTEmAVgBnNe0wbwlmjB6lurqPeTnv0NBwduUlHwBCC5XKgkJ55OQcD7R0dNwOI773muU8qtukTB8gZwNPA0EAQtF5BFjzENApoi8Z4z5FBgH7PdtsldE5hpjTgFeALzYarOnReTlIx1PE0bPVVubR0HB+xQU/B8HD36CSC3BwXHExs4iLu504uJOJyxsuJY+lOpk3SZhdDVNGL2D211GUdHHFBV9wMGD/6amxjaSh4QMJC7uO0RFTSEqajwREekEB3fozmulVDMdSRha3lfdTnBwFH37XkTfvhchIlRV7aS4+N8cPPgfioo+Ii/vL741DWFhw4mKGk94+ChCQ1NwuVIIDR1EaOgggoJcAf0eSvU2mjBUt2aMITz8RMLDT2TAgBsQEWpqcigv/5ry8jWUlX1NSclyDhxYdNi2ISEDiYmZTmzsDGJiZhARMY65JGAAAAuTSURBVBpjusWNgUr1SJowVI9ijMHlSsblSiYh4dyG5V5vDTU12VRXZ1FTs5fq6r1UVm6iuHgZ+flvAhAc3IeYmGmEh4/wlUJScLns3OlM0PYRpY5AE4bqFRyOUMLChhIWdujjOiJCdfVuSkqWUVy8jNLS5RQV/QuRmkPWCwqKJiJirG8aR0TEWCIjx+F0xnfl11CqW9NGb3XcERHq6gp8JZEsamr2UFm5jYqKDVRUfIPbfbBh3dDQwURFTSQqahJRUROJjJyA0xmvpRHVa2ijt1JtMMYQEpJISEgiUVETD/lMRKitzaWiYgPl5esoL19NWVkmBQV/b7J9KE5nfMMUHBzf7H2fw14HB/fRZ0pUj6d/wUo1YYwhNLQ/oaH96dPnjIbldXXFlJevobz8a2pr86irK6SurhC3u5DKyk2+10WIuFvdd1BQNE5nHxyOCIKCwnA46icXwcGxhIT0xensR0hIX0JC+hES0p+wsGEEBYV1xVdX6og0YSjVDk5nLHFx3yEu7jutriMieDxlhySTuroi3O4i3zL72uOpwOutwuutxuMppbY2D7d7HXV1eXi91c32anC5hhARMZrw8FGEh4/E4XAh4kHEA9h5cHAfIiPTCAsbih1ZQKnOpwlDqU5ijCE4OJrg4GjCwoZ0eHubcMp9JZgD1NRkUVm5hYqKzVRWbqKo6JPDGuubczjCiIgYQ0TEOMLDR+BwROBwuHA4QnE4XBjjpK4uj+rqPQ1TTc0ejHESEZFGZGQakZHpRETY5CNSh8dT0TB5vZUEBUXidCYSHByrbTnHGU0YSnUTNuFEERwcBZx42OciHqqr9yJS5ytFBGGMnWprcykvX09FxTdUVKynsPBDcnP/3MaxggkNHYTLNZjY2NPxequpqFhHYeH72B55AAzQ+k0xxgQTHBxPSEgiwcF9CA6OJigousk8huDg2GZTDCKC11uNSA1ebzVebw1BQRGEhQ0nJCRJk1A3pglDqR7CmKBWSy6hoQOIippwyDK3u9xX9VXT5ARdg9PZl9DQ/i1WXXk8VVRWbqK8fB3V1d/icLgICor0tbtE4HCE4fGUU1eX75sKfPOD1NTk4Hb///buL0ausozj+Pc3s90/7RYKtZKGFihSoyXBNhCCggmWiFWJcIGKAiHGhBtMINEoGI2RhAtvrF5ghAixahUqUmy8USykyIWUFqpAC7YihBKkpfSPpcuyM/N48b67DMuGPbvDdPbM/j7JZuacOXP2fdoz+8x533Oedxf1+mFqtcNEjEw5xmp1kIGB5QwMfJj+/tPyGc4QjcYx6vVjNBpDVCq9TW0azO3qI6JBSnANIgKpSm/vYvr6ltDXt4T+/qVUqycgKSet4bzfNwByopvvmzvfgxOGWZdKdbamVmurWh3IlxGfO/nGk6jX38zJ4xC12ujjIaCSu8jSj9RHvX6EoaHdHDv2L4aGducr0x6gUumlUpmbLxKYm8dvRqjXj+ZusqM0GscmbcuoSmUeUiUnicYEW4hqdT49PSdSrZ5AtTqXSmVg7LFSGSAV4h6vQUSjaVwp7bun56R3XVFXqfS9+7dqDj09C5gz56Sxs7GZOBblhGFmbVGt9lOt9tPbe0rBd1w6rd+T/lCPAJV8diBARNR4661XGB5+ieHhvWM/oJyE5uVEMA8gJ7cj1GqHx543GkPU68cYGXk9n40MAfUJWqH8B74y1k0Y0aBWO8TIyIFJx54mkpKbcvJpjD1KPUh970i6vb2LWbXqkWn9+02FE4aZlZpUQZr4W3t/fypI2UnpYoY38lVzBybsqms0hvMZ2MGcZA5Srx8hJaIKzckwopa704bHuhmr1XnHJRYnDDOzNkoXMwzS0zNIf//pnW5OSzy6Y2ZmhbQ1YUhaI+k5SXsk3TzB632S7s2vPybpjKbXbsnrn5P0mXa208zMJte2hKE0AnQ78FlgBfAVSSvGbfZ14GBEnAWsBX6U37sCuAo4G1gD/Ewz8ZIBM7NZpJ1nGOcDeyLi+Yh4C7gHuHzcNpcD6/Lz+4BLlO7auRy4JyKGI+I/wJ68PzMz65B2JoxTgZealvfmdRNuE6lq22FgYcH3mpnZcVT6QW9J10vaJmnb/v37O90cM7Ou1c6E8TKwtGl5SV434TZKt0+eCBwo+F4AIuLOiDgvIs5btGjR+9R0MzMbr50J43FguaRlknpJg9ibxm2zCbguP78SeCjSFICbgKvyVVTLgOXA1ja21czMJtG2G/cioibpG8CfgSpwd0Q8I+lWYFtEbALuAn4taQ/wOimpkLfbAOwEasANkYq0vKft27e/JunFaTb5A8Br03zvTNWNMUF3xuWYyqPb4ip8N2FXzendCknbis5rWxbdGBN0Z1yOqTy6Na4iSj/obWZmx4cThpmZFeKE8bY7O92ANujGmKA743JM5dGtcU3KYxhmZlaIzzDMzKyQWZ8wJquoWxaS7pa0T9LTTetOlvSgpN358aROtnGqJC2V9LCknZKekXRjXl/auCT1S9oq6R85ph/m9ctyxeY9uYJzb6fbOh2SqpKelPSnvFzquCS9IOkpSTskbcvrSnv8tWpWJ4yCFXXL4pekyr7NbgY2R8RyYHNeLpMa8M2IWAFcANyQ/3/KHNcwsDoiPgasBNZIuoBUqXltrtx8kFTJuYxuBHY1LXdDXJ+KiJVNl9KW+fhryaxOGBSrqFsKEfEI6ebHZs3VgNcBVxzXRrUoIl6JiCfy8/+R/hCdSonjiuRoXpyTfwJYTarYDCWLaZSkJcDngV/kZdEFcU2gtMdfq2Z7wuj2qrinRMQr+fl/gVM62ZhW5Mm1VgGPUfK4crfNDmAf8CDwb+BQrtgM5T0OfwJ8G2jk5YWUP64A/iJpu6Tr87pSH3+t8Jzes0REhKRSXhInaRD4A3BTRBxJX1yTMsaVy9yslLQA2Ah8pMNNapmky4B9EbFd0sWdbs/76KKIeFnSB4EHJT3b/GIZj79WzPYzjMJVcUvqVUmLAfLjvg63Z8okzSEli/URcX9eXfq4ACLiEPAw8HFgQa7YDOU8Di8EviDpBVLX7mrgp5Q8roh4OT/uIyX38+mS4286ZnvCKFJRt8yaqwFfB/yxg22ZstwHfhewKyJ+3PRSaeOStCifWSBpAPg0aWzmYVLFZihZTAARcUtELImIM0ifo4ci4mpKHJekeZLmjz4HLgWepsTHX6tm/Y17kj5H6nsdrah7W4ebNC2SfgdcTKqk+SrwA+ABYANwGvAi8KWIGD8wPmNJugj4G/AUb/eLf5c0jlHKuCSdQxoorZK+sG2IiFslnUn6Zn4y8CRwTUQMd66l05e7pL4VEZeVOa7c9o15sQf4bUTcJmkhJT3+WjXrE4aZmRUz27ukzMysICcMMzMrxAnDzMwKccIwM7NCnDDMzKwQJwyzGUDSxaMVXs1mKicMMzMrxAnDbAokXZPns9gh6Y5cSPCopLV5fovNkhblbVdK+rukf0raODpvgqSzJP01z4nxhKQP5d0PSrpP0rOS1qu5aJbZDOCEYVaQpI8CXwYujIiVQB24GpgHbIuIs4EtpLvsAX4FfCciziHdrT66fj1we54T4xPAaOXTVcBNpLlZziTVZzKbMVyt1qy4S4Bzgcfzl/8BUuG5BnBv3uY3wP2STgQWRMSWvH4d8Ptcm+jUiNgIEBFvAuT9bY2IvXl5B3AG8Gj7wzIrxgnDrDgB6yLilneslL4/brvp1ttprrFUx59Pm2HcJWVW3Gbgyjw3wujczqeTPkejFVm/CjwaEYeBg5I+mddfC2zJMwfulXRF3kefpLnHNQqzafI3GLOCImKnpO+RZmCrACPADcAbwPn5tX2kcQ5Ipa9/nhPC88DX8vprgTsk3Zr38cXjGIbZtLlarVmLJB2NiMFOt8Os3dwlZWZmhfgMw8zMCvEZhpmZFeKEYWZmhThhmJlZIU4YZmZWiBOGmZkV4oRhZmaF/B9FT+9PEE6cBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.0248 - acc: 0.6814\n",
      "Loss: 1.0247672792659495 Accuracy: 0.6814123\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1326 - acc: 0.3054\n",
      "Epoch 00001: val_loss improved from inf to 1.60658, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/001-1.6066.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 2.1325 - acc: 0.3054 - val_loss: 1.6066 - val_acc: 0.4778\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4911 - acc: 0.5205\n",
      "Epoch 00002: val_loss improved from 1.60658 to 1.25247, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/002-1.2525.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.4911 - acc: 0.5205 - val_loss: 1.2525 - val_acc: 0.6212\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2595 - acc: 0.6020\n",
      "Epoch 00003: val_loss improved from 1.25247 to 1.11027, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/003-1.1103.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.2596 - acc: 0.6020 - val_loss: 1.1103 - val_acc: 0.6643\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1100 - acc: 0.6542\n",
      "Epoch 00004: val_loss improved from 1.11027 to 1.03515, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/004-1.0352.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.1101 - acc: 0.6542 - val_loss: 1.0352 - val_acc: 0.6949\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9991 - acc: 0.6928\n",
      "Epoch 00005: val_loss improved from 1.03515 to 0.93435, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/005-0.9344.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.9991 - acc: 0.6928 - val_loss: 0.9344 - val_acc: 0.7219\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9172 - acc: 0.7164\n",
      "Epoch 00006: val_loss improved from 0.93435 to 0.85093, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/006-0.8509.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.9171 - acc: 0.7165 - val_loss: 0.8509 - val_acc: 0.7428\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8462 - acc: 0.7389\n",
      "Epoch 00007: val_loss improved from 0.85093 to 0.80898, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/007-0.8090.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.8463 - acc: 0.7388 - val_loss: 0.8090 - val_acc: 0.7633\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7851 - acc: 0.7601\n",
      "Epoch 00008: val_loss improved from 0.80898 to 0.79247, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/008-0.7925.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7851 - acc: 0.7601 - val_loss: 0.7925 - val_acc: 0.7612\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7343 - acc: 0.7758\n",
      "Epoch 00009: val_loss improved from 0.79247 to 0.74990, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/009-0.7499.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7342 - acc: 0.7758 - val_loss: 0.7499 - val_acc: 0.7736\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6960 - acc: 0.7870\n",
      "Epoch 00010: val_loss improved from 0.74990 to 0.71980, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/010-0.7198.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6961 - acc: 0.7870 - val_loss: 0.7198 - val_acc: 0.7869\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6543 - acc: 0.8007\n",
      "Epoch 00011: val_loss improved from 0.71980 to 0.69824, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/011-0.6982.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6542 - acc: 0.8007 - val_loss: 0.6982 - val_acc: 0.7987\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6127 - acc: 0.8151\n",
      "Epoch 00012: val_loss improved from 0.69824 to 0.68975, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/012-0.6898.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6127 - acc: 0.8151 - val_loss: 0.6898 - val_acc: 0.7962\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5823 - acc: 0.8229\n",
      "Epoch 00013: val_loss did not improve from 0.68975\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5822 - acc: 0.8229 - val_loss: 0.7025 - val_acc: 0.8022\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.8312\n",
      "Epoch 00014: val_loss improved from 0.68975 to 0.65155, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/014-0.6516.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5507 - acc: 0.8312 - val_loss: 0.6516 - val_acc: 0.8206\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5194 - acc: 0.8410\n",
      "Epoch 00015: val_loss improved from 0.65155 to 0.65045, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/015-0.6505.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5196 - acc: 0.8410 - val_loss: 0.6505 - val_acc: 0.8157\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5070 - acc: 0.8452\n",
      "Epoch 00016: val_loss improved from 0.65045 to 0.64058, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/016-0.6406.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5070 - acc: 0.8452 - val_loss: 0.6406 - val_acc: 0.8150\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.8590\n",
      "Epoch 00017: val_loss did not improve from 0.64058\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4649 - acc: 0.8591 - val_loss: 0.6465 - val_acc: 0.8255\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4527 - acc: 0.8604\n",
      "Epoch 00018: val_loss did not improve from 0.64058\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4526 - acc: 0.8604 - val_loss: 0.6597 - val_acc: 0.8188\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.8695\n",
      "Epoch 00019: val_loss did not improve from 0.64058\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4228 - acc: 0.8695 - val_loss: 0.6592 - val_acc: 0.8181\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8751\n",
      "Epoch 00020: val_loss did not improve from 0.64058\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4095 - acc: 0.8751 - val_loss: 0.6579 - val_acc: 0.8216\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8804\n",
      "Epoch 00021: val_loss improved from 0.64058 to 0.63271, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/021-0.6327.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3902 - acc: 0.8804 - val_loss: 0.6327 - val_acc: 0.8267\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3671 - acc: 0.8864\n",
      "Epoch 00022: val_loss did not improve from 0.63271\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3671 - acc: 0.8864 - val_loss: 0.6762 - val_acc: 0.8209\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8897\n",
      "Epoch 00023: val_loss did not improve from 0.63271\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3522 - acc: 0.8897 - val_loss: 0.6336 - val_acc: 0.8281\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.8952\n",
      "Epoch 00024: val_loss did not improve from 0.63271\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3344 - acc: 0.8952 - val_loss: 0.6698 - val_acc: 0.8293\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.8949\n",
      "Epoch 00025: val_loss did not improve from 0.63271\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3289 - acc: 0.8949 - val_loss: 0.6410 - val_acc: 0.8300\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.9010\n",
      "Epoch 00026: val_loss did not improve from 0.63271\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3127 - acc: 0.9010 - val_loss: 0.6596 - val_acc: 0.8260\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.9035\n",
      "Epoch 00027: val_loss did not improve from 0.63271\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3044 - acc: 0.9034 - val_loss: 0.6578 - val_acc: 0.8272\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9071\n",
      "Epoch 00028: val_loss improved from 0.63271 to 0.63209, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_5_conv_checkpoint/028-0.6321.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2905 - acc: 0.9071 - val_loss: 0.6321 - val_acc: 0.8397\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9105\n",
      "Epoch 00029: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2809 - acc: 0.9106 - val_loss: 0.6677 - val_acc: 0.8262\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2677 - acc: 0.9155\n",
      "Epoch 00030: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2677 - acc: 0.9155 - val_loss: 0.6519 - val_acc: 0.8337\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9185\n",
      "Epoch 00031: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2586 - acc: 0.9185 - val_loss: 0.6767 - val_acc: 0.8230\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9190\n",
      "Epoch 00032: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2543 - acc: 0.9190 - val_loss: 0.6732 - val_acc: 0.8227\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9240\n",
      "Epoch 00033: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2355 - acc: 0.9240 - val_loss: 0.6741 - val_acc: 0.8355\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9248\n",
      "Epoch 00034: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2355 - acc: 0.9248 - val_loss: 0.6958 - val_acc: 0.8281\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9265\n",
      "Epoch 00035: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2306 - acc: 0.9265 - val_loss: 0.6733 - val_acc: 0.8300\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9293\n",
      "Epoch 00036: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2230 - acc: 0.9294 - val_loss: 0.7283 - val_acc: 0.8307\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9332\n",
      "Epoch 00037: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2102 - acc: 0.9332 - val_loss: 0.7020 - val_acc: 0.8334\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9311\n",
      "Epoch 00038: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2146 - acc: 0.9311 - val_loss: 0.7098 - val_acc: 0.8369\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9345\n",
      "Epoch 00039: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2034 - acc: 0.9345 - val_loss: 0.6771 - val_acc: 0.8407\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9363\n",
      "Epoch 00040: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1982 - acc: 0.9363 - val_loss: 0.6991 - val_acc: 0.8316\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9342\n",
      "Epoch 00041: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2027 - acc: 0.9342 - val_loss: 0.7141 - val_acc: 0.8297\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9387\n",
      "Epoch 00042: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1923 - acc: 0.9387 - val_loss: 0.7261 - val_acc: 0.8318\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9410\n",
      "Epoch 00043: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1827 - acc: 0.9410 - val_loss: 0.7119 - val_acc: 0.8334\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9392\n",
      "Epoch 00044: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1874 - acc: 0.9392 - val_loss: 0.6975 - val_acc: 0.8407\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9442\n",
      "Epoch 00045: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1759 - acc: 0.9442 - val_loss: 0.7182 - val_acc: 0.8346\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9430\n",
      "Epoch 00046: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1766 - acc: 0.9430 - val_loss: 0.7171 - val_acc: 0.8348\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9451\n",
      "Epoch 00047: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1692 - acc: 0.9451 - val_loss: 0.7385 - val_acc: 0.8353\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9468\n",
      "Epoch 00048: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1674 - acc: 0.9467 - val_loss: 0.7197 - val_acc: 0.8365\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9476\n",
      "Epoch 00049: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1638 - acc: 0.9476 - val_loss: 0.7477 - val_acc: 0.8376\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9486\n",
      "Epoch 00050: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1586 - acc: 0.9486 - val_loss: 0.7275 - val_acc: 0.8376\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9488\n",
      "Epoch 00051: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1588 - acc: 0.9488 - val_loss: 0.7116 - val_acc: 0.8456\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9496\n",
      "Epoch 00052: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1549 - acc: 0.9496 - val_loss: 0.7810 - val_acc: 0.8381\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9520\n",
      "Epoch 00053: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1505 - acc: 0.9520 - val_loss: 0.7520 - val_acc: 0.8388\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9512\n",
      "Epoch 00054: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1515 - acc: 0.9512 - val_loss: 0.7609 - val_acc: 0.8369\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9535\n",
      "Epoch 00055: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1472 - acc: 0.9534 - val_loss: 0.7470 - val_acc: 0.8493\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9529\n",
      "Epoch 00056: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1446 - acc: 0.9529 - val_loss: 0.7510 - val_acc: 0.8432\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9538\n",
      "Epoch 00057: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1436 - acc: 0.9538 - val_loss: 0.7268 - val_acc: 0.8465\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9531\n",
      "Epoch 00058: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1457 - acc: 0.9531 - val_loss: 0.7657 - val_acc: 0.8407\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9549\n",
      "Epoch 00059: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1397 - acc: 0.9549 - val_loss: 0.7289 - val_acc: 0.8425\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9583\n",
      "Epoch 00060: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1335 - acc: 0.9583 - val_loss: 0.7380 - val_acc: 0.8470\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9566\n",
      "Epoch 00061: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1362 - acc: 0.9566 - val_loss: 0.7356 - val_acc: 0.8437\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9577\n",
      "Epoch 00062: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1337 - acc: 0.9577 - val_loss: 0.7520 - val_acc: 0.8409\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9566\n",
      "Epoch 00063: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1353 - acc: 0.9566 - val_loss: 0.7537 - val_acc: 0.8395\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9586\n",
      "Epoch 00064: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1309 - acc: 0.9586 - val_loss: 0.7703 - val_acc: 0.8432\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9596\n",
      "Epoch 00065: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1281 - acc: 0.9597 - val_loss: 0.7765 - val_acc: 0.8432\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9596\n",
      "Epoch 00066: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1232 - acc: 0.9596 - val_loss: 0.8187 - val_acc: 0.8474\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9570\n",
      "Epoch 00067: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1320 - acc: 0.9570 - val_loss: 0.7268 - val_acc: 0.8472\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9606\n",
      "Epoch 00068: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1205 - acc: 0.9606 - val_loss: 0.7979 - val_acc: 0.8353\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9609\n",
      "Epoch 00069: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1239 - acc: 0.9609 - val_loss: 0.7529 - val_acc: 0.8523\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9630\n",
      "Epoch 00070: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1185 - acc: 0.9630 - val_loss: 0.7945 - val_acc: 0.8486\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9593\n",
      "Epoch 00071: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1273 - acc: 0.9594 - val_loss: 0.7671 - val_acc: 0.8442\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9607\n",
      "Epoch 00072: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1175 - acc: 0.9607 - val_loss: 0.7496 - val_acc: 0.8477\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9639\n",
      "Epoch 00073: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1124 - acc: 0.9639 - val_loss: 0.7825 - val_acc: 0.8428\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9613\n",
      "Epoch 00074: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1219 - acc: 0.9613 - val_loss: 0.7845 - val_acc: 0.8470\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9626\n",
      "Epoch 00075: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1151 - acc: 0.9626 - val_loss: 0.7667 - val_acc: 0.8444\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9630\n",
      "Epoch 00076: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1147 - acc: 0.9630 - val_loss: 0.7822 - val_acc: 0.8509\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9629\n",
      "Epoch 00077: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1134 - acc: 0.9629 - val_loss: 0.7685 - val_acc: 0.8502\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9640\n",
      "Epoch 00078: val_loss did not improve from 0.63209\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1131 - acc: 0.9640 - val_loss: 0.7814 - val_acc: 0.8493\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.7509 - acc: 0.8012\n",
      "Loss: 0.7508955137751927 Accuracy: 0.8012461\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3216 - acc: 0.2381\n",
      "Epoch 00001: val_loss improved from inf to 1.70972, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/001-1.7097.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 2.3215 - acc: 0.2381 - val_loss: 1.7097 - val_acc: 0.4484\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6335 - acc: 0.4657\n",
      "Epoch 00002: val_loss improved from 1.70972 to 1.35060, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/002-1.3506.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.6334 - acc: 0.4657 - val_loss: 1.3506 - val_acc: 0.5751\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4094 - acc: 0.5442\n",
      "Epoch 00003: val_loss improved from 1.35060 to 1.28644, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/003-1.2864.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.4093 - acc: 0.5442 - val_loss: 1.2864 - val_acc: 0.6070\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2481 - acc: 0.6053\n",
      "Epoch 00004: val_loss improved from 1.28644 to 1.04256, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/004-1.0426.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.2480 - acc: 0.6054 - val_loss: 1.0426 - val_acc: 0.6797\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1199 - acc: 0.6479\n",
      "Epoch 00005: val_loss improved from 1.04256 to 0.93610, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/005-0.9361.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.1199 - acc: 0.6479 - val_loss: 0.9361 - val_acc: 0.7207\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0069 - acc: 0.6854\n",
      "Epoch 00006: val_loss improved from 0.93610 to 0.82857, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/006-0.8286.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.0068 - acc: 0.6854 - val_loss: 0.8286 - val_acc: 0.7559\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9210 - acc: 0.7167\n",
      "Epoch 00007: val_loss improved from 0.82857 to 0.80319, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/007-0.8032.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.9210 - acc: 0.7167 - val_loss: 0.8032 - val_acc: 0.7650\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8377 - acc: 0.7432\n",
      "Epoch 00008: val_loss did not improve from 0.80319\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.8378 - acc: 0.7432 - val_loss: 0.8253 - val_acc: 0.7524\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7789 - acc: 0.7630\n",
      "Epoch 00009: val_loss improved from 0.80319 to 0.64998, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/009-0.6500.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7791 - acc: 0.7630 - val_loss: 0.6500 - val_acc: 0.8053\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7243 - acc: 0.7821\n",
      "Epoch 00010: val_loss improved from 0.64998 to 0.62705, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/010-0.6271.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7244 - acc: 0.7820 - val_loss: 0.6271 - val_acc: 0.8160\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.7933\n",
      "Epoch 00011: val_loss did not improve from 0.62705\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6801 - acc: 0.7933 - val_loss: 0.6459 - val_acc: 0.8171\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6435 - acc: 0.8060\n",
      "Epoch 00012: val_loss did not improve from 0.62705\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6436 - acc: 0.8060 - val_loss: 0.6303 - val_acc: 0.8178\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6068 - acc: 0.8185\n",
      "Epoch 00013: val_loss improved from 0.62705 to 0.51158, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/013-0.5116.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6068 - acc: 0.8184 - val_loss: 0.5116 - val_acc: 0.8584\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5686 - acc: 0.8297\n",
      "Epoch 00014: val_loss did not improve from 0.51158\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5686 - acc: 0.8297 - val_loss: 0.5547 - val_acc: 0.8416\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.8389\n",
      "Epoch 00015: val_loss improved from 0.51158 to 0.47762, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/015-0.4776.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5403 - acc: 0.8389 - val_loss: 0.4776 - val_acc: 0.8677\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8474\n",
      "Epoch 00016: val_loss did not improve from 0.47762\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5118 - acc: 0.8474 - val_loss: 0.5207 - val_acc: 0.8458\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.8480\n",
      "Epoch 00017: val_loss improved from 0.47762 to 0.45980, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/017-0.4598.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5084 - acc: 0.8480 - val_loss: 0.4598 - val_acc: 0.8744\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.8589\n",
      "Epoch 00018: val_loss improved from 0.45980 to 0.44443, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/018-0.4444.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4684 - acc: 0.8590 - val_loss: 0.4444 - val_acc: 0.8779\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8636\n",
      "Epoch 00019: val_loss did not improve from 0.44443\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4572 - acc: 0.8636 - val_loss: 0.4481 - val_acc: 0.8763\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4358 - acc: 0.8696\n",
      "Epoch 00020: val_loss did not improve from 0.44443\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4358 - acc: 0.8696 - val_loss: 0.4464 - val_acc: 0.8744\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4258 - acc: 0.8715\n",
      "Epoch 00021: val_loss improved from 0.44443 to 0.42970, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/021-0.4297.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4258 - acc: 0.8715 - val_loss: 0.4297 - val_acc: 0.8847\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8780\n",
      "Epoch 00022: val_loss improved from 0.42970 to 0.39402, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/022-0.3940.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4043 - acc: 0.8780 - val_loss: 0.3940 - val_acc: 0.8931\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8792\n",
      "Epoch 00023: val_loss did not improve from 0.39402\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3937 - acc: 0.8791 - val_loss: 0.4410 - val_acc: 0.8819\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3857 - acc: 0.8850\n",
      "Epoch 00024: val_loss did not improve from 0.39402\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3856 - acc: 0.8850 - val_loss: 0.4091 - val_acc: 0.8866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8872\n",
      "Epoch 00025: val_loss improved from 0.39402 to 0.38857, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/025-0.3886.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3710 - acc: 0.8872 - val_loss: 0.3886 - val_acc: 0.8931\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3532 - acc: 0.8920\n",
      "Epoch 00026: val_loss improved from 0.38857 to 0.38560, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/026-0.3856.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3531 - acc: 0.8920 - val_loss: 0.3856 - val_acc: 0.8982\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8944\n",
      "Epoch 00027: val_loss improved from 0.38560 to 0.37131, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/027-0.3713.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3467 - acc: 0.8944 - val_loss: 0.3713 - val_acc: 0.9005\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8966\n",
      "Epoch 00028: val_loss did not improve from 0.37131\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3379 - acc: 0.8966 - val_loss: 0.3717 - val_acc: 0.8961\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.9006\n",
      "Epoch 00029: val_loss did not improve from 0.37131\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3241 - acc: 0.9006 - val_loss: 0.3836 - val_acc: 0.8994\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.9026\n",
      "Epoch 00030: val_loss did not improve from 0.37131\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3193 - acc: 0.9026 - val_loss: 0.4013 - val_acc: 0.8952\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.9042\n",
      "Epoch 00031: val_loss improved from 0.37131 to 0.36683, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/031-0.3668.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3118 - acc: 0.9042 - val_loss: 0.3668 - val_acc: 0.9008\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9089\n",
      "Epoch 00032: val_loss did not improve from 0.36683\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2988 - acc: 0.9089 - val_loss: 0.3929 - val_acc: 0.8956\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9072\n",
      "Epoch 00033: val_loss improved from 0.36683 to 0.35933, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/033-0.3593.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3029 - acc: 0.9072 - val_loss: 0.3593 - val_acc: 0.9064\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9096\n",
      "Epoch 00034: val_loss did not improve from 0.35933\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2892 - acc: 0.9096 - val_loss: 0.3712 - val_acc: 0.8996\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.9139\n",
      "Epoch 00035: val_loss did not improve from 0.35933\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2798 - acc: 0.9139 - val_loss: 0.3673 - val_acc: 0.9033\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9134\n",
      "Epoch 00036: val_loss did not improve from 0.35933\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2767 - acc: 0.9134 - val_loss: 0.3663 - val_acc: 0.9043\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9139\n",
      "Epoch 00037: val_loss did not improve from 0.35933\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2731 - acc: 0.9140 - val_loss: 0.3844 - val_acc: 0.9052\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9186\n",
      "Epoch 00038: val_loss improved from 0.35933 to 0.35448, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/038-0.3545.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2638 - acc: 0.9185 - val_loss: 0.3545 - val_acc: 0.9066\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9174\n",
      "Epoch 00039: val_loss did not improve from 0.35448\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2631 - acc: 0.9174 - val_loss: 0.3741 - val_acc: 0.9017\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.9200\n",
      "Epoch 00040: val_loss did not improve from 0.35448\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2538 - acc: 0.9200 - val_loss: 0.3697 - val_acc: 0.8945\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9202\n",
      "Epoch 00041: val_loss did not improve from 0.35448\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2509 - acc: 0.9202 - val_loss: 0.3545 - val_acc: 0.9089\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9233\n",
      "Epoch 00042: val_loss improved from 0.35448 to 0.34215, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/042-0.3421.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2449 - acc: 0.9233 - val_loss: 0.3421 - val_acc: 0.9059\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9230\n",
      "Epoch 00043: val_loss did not improve from 0.34215\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2413 - acc: 0.9230 - val_loss: 0.3493 - val_acc: 0.9085\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9239\n",
      "Epoch 00044: val_loss did not improve from 0.34215\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2369 - acc: 0.9239 - val_loss: 0.3441 - val_acc: 0.9092\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9253\n",
      "Epoch 00045: val_loss did not improve from 0.34215\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2319 - acc: 0.9253 - val_loss: 0.3652 - val_acc: 0.9115\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9281\n",
      "Epoch 00046: val_loss did not improve from 0.34215\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2258 - acc: 0.9281 - val_loss: 0.3573 - val_acc: 0.9115\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9312\n",
      "Epoch 00047: val_loss did not improve from 0.34215\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2170 - acc: 0.9312 - val_loss: 0.3639 - val_acc: 0.9113\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9292\n",
      "Epoch 00048: val_loss did not improve from 0.34215\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2179 - acc: 0.9292 - val_loss: 0.3642 - val_acc: 0.9087\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9308\n",
      "Epoch 00049: val_loss did not improve from 0.34215\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2158 - acc: 0.9307 - val_loss: 0.3515 - val_acc: 0.9124\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9336\n",
      "Epoch 00050: val_loss did not improve from 0.34215\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2072 - acc: 0.9336 - val_loss: 0.3587 - val_acc: 0.9126\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9335\n",
      "Epoch 00051: val_loss improved from 0.34215 to 0.32396, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_6_conv_checkpoint/051-0.3240.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2075 - acc: 0.9335 - val_loss: 0.3240 - val_acc: 0.9161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9349\n",
      "Epoch 00052: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1976 - acc: 0.9349 - val_loss: 0.4051 - val_acc: 0.9068\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9347\n",
      "Epoch 00053: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2011 - acc: 0.9347 - val_loss: 0.3488 - val_acc: 0.9152\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9337\n",
      "Epoch 00054: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2010 - acc: 0.9337 - val_loss: 0.3488 - val_acc: 0.9154\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9362\n",
      "Epoch 00055: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1956 - acc: 0.9362 - val_loss: 0.3650 - val_acc: 0.9124\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9399\n",
      "Epoch 00056: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1892 - acc: 0.9399 - val_loss: 0.3516 - val_acc: 0.9096\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9393\n",
      "Epoch 00057: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1860 - acc: 0.9393 - val_loss: 0.3787 - val_acc: 0.9157\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9397\n",
      "Epoch 00058: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1847 - acc: 0.9397 - val_loss: 0.3562 - val_acc: 0.9143\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9389\n",
      "Epoch 00059: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1838 - acc: 0.9389 - val_loss: 0.3531 - val_acc: 0.9178\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9440\n",
      "Epoch 00060: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1756 - acc: 0.9440 - val_loss: 0.3613 - val_acc: 0.9168\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9432\n",
      "Epoch 00061: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1734 - acc: 0.9431 - val_loss: 0.3460 - val_acc: 0.9166\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9417\n",
      "Epoch 00062: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1734 - acc: 0.9417 - val_loss: 0.3669 - val_acc: 0.9178\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9449\n",
      "Epoch 00063: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1664 - acc: 0.9449 - val_loss: 0.3620 - val_acc: 0.9131\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9445\n",
      "Epoch 00064: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1692 - acc: 0.9445 - val_loss: 0.3823 - val_acc: 0.9182\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9451\n",
      "Epoch 00065: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1663 - acc: 0.9451 - val_loss: 0.3661 - val_acc: 0.9157\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9458\n",
      "Epoch 00066: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1659 - acc: 0.9458 - val_loss: 0.3625 - val_acc: 0.9187\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9457\n",
      "Epoch 00067: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1622 - acc: 0.9457 - val_loss: 0.3645 - val_acc: 0.9203\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9471\n",
      "Epoch 00068: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1606 - acc: 0.9471 - val_loss: 0.3507 - val_acc: 0.9175\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9485\n",
      "Epoch 00069: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1535 - acc: 0.9485 - val_loss: 0.3635 - val_acc: 0.9180\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9476\n",
      "Epoch 00070: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1556 - acc: 0.9476 - val_loss: 0.4027 - val_acc: 0.9136\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9499\n",
      "Epoch 00071: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1505 - acc: 0.9500 - val_loss: 0.3916 - val_acc: 0.9164\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9512\n",
      "Epoch 00072: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1504 - acc: 0.9512 - val_loss: 0.3564 - val_acc: 0.9187\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9513\n",
      "Epoch 00073: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1514 - acc: 0.9513 - val_loss: 0.3583 - val_acc: 0.9199\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9521\n",
      "Epoch 00074: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1450 - acc: 0.9520 - val_loss: 0.3687 - val_acc: 0.9196\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9519\n",
      "Epoch 00075: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1439 - acc: 0.9519 - val_loss: 0.3525 - val_acc: 0.9222\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9515\n",
      "Epoch 00076: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1443 - acc: 0.9515 - val_loss: 0.3636 - val_acc: 0.9164\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9527\n",
      "Epoch 00077: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1407 - acc: 0.9527 - val_loss: 0.3839 - val_acc: 0.9129\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9526\n",
      "Epoch 00078: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1420 - acc: 0.9526 - val_loss: 0.3550 - val_acc: 0.9182\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9536\n",
      "Epoch 00079: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1399 - acc: 0.9536 - val_loss: 0.3646 - val_acc: 0.9187\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9540\n",
      "Epoch 00080: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1412 - acc: 0.9539 - val_loss: 0.3758 - val_acc: 0.9147\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9544\n",
      "Epoch 00081: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1353 - acc: 0.9544 - val_loss: 0.3720 - val_acc: 0.9180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9547\n",
      "Epoch 00082: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1334 - acc: 0.9547 - val_loss: 0.3834 - val_acc: 0.9182\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9576\n",
      "Epoch 00083: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1282 - acc: 0.9576 - val_loss: 0.3735 - val_acc: 0.9201\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9549\n",
      "Epoch 00084: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1327 - acc: 0.9549 - val_loss: 0.3908 - val_acc: 0.9138\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9588\n",
      "Epoch 00085: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1248 - acc: 0.9588 - val_loss: 0.3893 - val_acc: 0.9187\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9588\n",
      "Epoch 00086: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1257 - acc: 0.9587 - val_loss: 0.3868 - val_acc: 0.9185\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9563\n",
      "Epoch 00087: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1317 - acc: 0.9563 - val_loss: 0.3572 - val_acc: 0.9206\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9579\n",
      "Epoch 00088: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1264 - acc: 0.9579 - val_loss: 0.3744 - val_acc: 0.9208\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9570\n",
      "Epoch 00089: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1292 - acc: 0.9570 - val_loss: 0.3709 - val_acc: 0.9213\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9586\n",
      "Epoch 00090: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1241 - acc: 0.9586 - val_loss: 0.3826 - val_acc: 0.9220\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9603\n",
      "Epoch 00091: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1182 - acc: 0.9603 - val_loss: 0.3884 - val_acc: 0.9154\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9588\n",
      "Epoch 00092: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1205 - acc: 0.9588 - val_loss: 0.4083 - val_acc: 0.9166\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9598\n",
      "Epoch 00093: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1191 - acc: 0.9598 - val_loss: 0.3866 - val_acc: 0.9203\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9604\n",
      "Epoch 00094: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1206 - acc: 0.9604 - val_loss: 0.3630 - val_acc: 0.9220\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9627\n",
      "Epoch 00095: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1105 - acc: 0.9627 - val_loss: 0.3926 - val_acc: 0.9208\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9635\n",
      "Epoch 00096: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1111 - acc: 0.9635 - val_loss: 0.3829 - val_acc: 0.9227\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9636\n",
      "Epoch 00097: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1107 - acc: 0.9636 - val_loss: 0.3969 - val_acc: 0.9147\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9623\n",
      "Epoch 00098: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1161 - acc: 0.9623 - val_loss: 0.3827 - val_acc: 0.9171\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9648\n",
      "Epoch 00099: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1101 - acc: 0.9648 - val_loss: 0.4066 - val_acc: 0.9189\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9620\n",
      "Epoch 00100: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1116 - acc: 0.9620 - val_loss: 0.3759 - val_acc: 0.9236\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9628\n",
      "Epoch 00101: val_loss did not improve from 0.32396\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1093 - acc: 0.9628 - val_loss: 0.3769 - val_acc: 0.9206\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3874 - acc: 0.8887\n",
      "Loss: 0.38741967244806813 Accuracy: 0.88868123\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4703 - acc: 0.1862\n",
      "Epoch 00001: val_loss improved from inf to 1.91755, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/001-1.9175.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 2.4702 - acc: 0.1862 - val_loss: 1.9175 - val_acc: 0.4184\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8399 - acc: 0.3835\n",
      "Epoch 00002: val_loss improved from 1.91755 to 1.44620, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/002-1.4462.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.8400 - acc: 0.3835 - val_loss: 1.4462 - val_acc: 0.5628\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5665 - acc: 0.4769\n",
      "Epoch 00003: val_loss improved from 1.44620 to 1.21697, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/003-1.2170.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.5664 - acc: 0.4769 - val_loss: 1.2170 - val_acc: 0.6247\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3853 - acc: 0.5459\n",
      "Epoch 00004: val_loss improved from 1.21697 to 1.06215, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/004-1.0622.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.3853 - acc: 0.5459 - val_loss: 1.0622 - val_acc: 0.6909\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2419 - acc: 0.6011\n",
      "Epoch 00005: val_loss improved from 1.06215 to 0.97845, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/005-0.9784.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.2418 - acc: 0.6011 - val_loss: 0.9784 - val_acc: 0.7154\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1194 - acc: 0.6405\n",
      "Epoch 00006: val_loss improved from 0.97845 to 0.83352, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/006-0.8335.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.1194 - acc: 0.6405 - val_loss: 0.8335 - val_acc: 0.7554\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0211 - acc: 0.6767\n",
      "Epoch 00007: val_loss improved from 0.83352 to 0.74326, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/007-0.7433.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.0211 - acc: 0.6767 - val_loss: 0.7433 - val_acc: 0.7943\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9382 - acc: 0.7080\n",
      "Epoch 00008: val_loss did not improve from 0.74326\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.9382 - acc: 0.7080 - val_loss: 0.7782 - val_acc: 0.7689\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8826 - acc: 0.7282\n",
      "Epoch 00009: val_loss improved from 0.74326 to 0.62775, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/009-0.6278.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.8826 - acc: 0.7282 - val_loss: 0.6278 - val_acc: 0.8181\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8113 - acc: 0.7540\n",
      "Epoch 00010: val_loss improved from 0.62775 to 0.57070, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/010-0.5707.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.8113 - acc: 0.7540 - val_loss: 0.5707 - val_acc: 0.8367\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7618 - acc: 0.7667\n",
      "Epoch 00011: val_loss did not improve from 0.57070\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7618 - acc: 0.7667 - val_loss: 0.5849 - val_acc: 0.8351\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7227 - acc: 0.7792\n",
      "Epoch 00012: val_loss improved from 0.57070 to 0.51645, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/012-0.5165.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7227 - acc: 0.7793 - val_loss: 0.5165 - val_acc: 0.8477\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6825 - acc: 0.7913\n",
      "Epoch 00013: val_loss improved from 0.51645 to 0.46667, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/013-0.4667.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6825 - acc: 0.7913 - val_loss: 0.4667 - val_acc: 0.8728\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6564 - acc: 0.7995\n",
      "Epoch 00014: val_loss improved from 0.46667 to 0.46022, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/014-0.4602.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6564 - acc: 0.7995 - val_loss: 0.4602 - val_acc: 0.8705\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6183 - acc: 0.8127\n",
      "Epoch 00015: val_loss improved from 0.46022 to 0.43943, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/015-0.4394.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6183 - acc: 0.8127 - val_loss: 0.4394 - val_acc: 0.8782\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5949 - acc: 0.8180\n",
      "Epoch 00016: val_loss improved from 0.43943 to 0.42571, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/016-0.4257.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5948 - acc: 0.8180 - val_loss: 0.4257 - val_acc: 0.8796\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5696 - acc: 0.8271\n",
      "Epoch 00017: val_loss improved from 0.42571 to 0.41133, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/017-0.4113.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5696 - acc: 0.8271 - val_loss: 0.4113 - val_acc: 0.8903\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.8334\n",
      "Epoch 00018: val_loss improved from 0.41133 to 0.37574, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/018-0.3757.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5481 - acc: 0.8334 - val_loss: 0.3757 - val_acc: 0.9005\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.8384\n",
      "Epoch 00019: val_loss improved from 0.37574 to 0.36872, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/019-0.3687.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5309 - acc: 0.8384 - val_loss: 0.3687 - val_acc: 0.8982\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.8425\n",
      "Epoch 00020: val_loss improved from 0.36872 to 0.36413, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/020-0.3641.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5161 - acc: 0.8425 - val_loss: 0.3641 - val_acc: 0.8996\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4922 - acc: 0.8504\n",
      "Epoch 00021: val_loss improved from 0.36413 to 0.34602, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/021-0.3460.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4922 - acc: 0.8504 - val_loss: 0.3460 - val_acc: 0.9045\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4814 - acc: 0.8531\n",
      "Epoch 00022: val_loss improved from 0.34602 to 0.33660, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/022-0.3366.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4814 - acc: 0.8531 - val_loss: 0.3366 - val_acc: 0.9108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8587\n",
      "Epoch 00023: val_loss did not improve from 0.33660\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4604 - acc: 0.8587 - val_loss: 0.3380 - val_acc: 0.9124\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.8611\n",
      "Epoch 00024: val_loss improved from 0.33660 to 0.32986, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/024-0.3299.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4493 - acc: 0.8611 - val_loss: 0.3299 - val_acc: 0.9133\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.8646\n",
      "Epoch 00025: val_loss improved from 0.32986 to 0.32893, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/025-0.3289.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4405 - acc: 0.8646 - val_loss: 0.3289 - val_acc: 0.9145\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8654\n",
      "Epoch 00026: val_loss improved from 0.32893 to 0.31151, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/026-0.3115.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4345 - acc: 0.8654 - val_loss: 0.3115 - val_acc: 0.9115\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.8707\n",
      "Epoch 00027: val_loss improved from 0.31151 to 0.31022, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/027-0.3102.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4199 - acc: 0.8708 - val_loss: 0.3102 - val_acc: 0.9210\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8737\n",
      "Epoch 00028: val_loss improved from 0.31022 to 0.28091, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/028-0.2809.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4092 - acc: 0.8737 - val_loss: 0.2809 - val_acc: 0.9250\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3959 - acc: 0.8783\n",
      "Epoch 00029: val_loss did not improve from 0.28091\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3960 - acc: 0.8782 - val_loss: 0.2912 - val_acc: 0.9248\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8802\n",
      "Epoch 00030: val_loss improved from 0.28091 to 0.27618, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/030-0.2762.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3880 - acc: 0.8802 - val_loss: 0.2762 - val_acc: 0.9290\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8801\n",
      "Epoch 00031: val_loss did not improve from 0.27618\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3848 - acc: 0.8802 - val_loss: 0.3285 - val_acc: 0.9089\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8862\n",
      "Epoch 00032: val_loss did not improve from 0.27618\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3731 - acc: 0.8862 - val_loss: 0.2838 - val_acc: 0.9294\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8847\n",
      "Epoch 00033: val_loss improved from 0.27618 to 0.26181, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/033-0.2618.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3680 - acc: 0.8847 - val_loss: 0.2618 - val_acc: 0.9283\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8904\n",
      "Epoch 00034: val_loss did not improve from 0.26181\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3565 - acc: 0.8904 - val_loss: 0.2639 - val_acc: 0.9287\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8871\n",
      "Epoch 00035: val_loss did not improve from 0.26181\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3590 - acc: 0.8871 - val_loss: 0.2645 - val_acc: 0.9241\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.8932\n",
      "Epoch 00036: val_loss improved from 0.26181 to 0.24518, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/036-0.2452.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3452 - acc: 0.8932 - val_loss: 0.2452 - val_acc: 0.9338\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8947\n",
      "Epoch 00037: val_loss did not improve from 0.24518\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3394 - acc: 0.8947 - val_loss: 0.2591 - val_acc: 0.9304\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8945\n",
      "Epoch 00038: val_loss did not improve from 0.24518\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3389 - acc: 0.8946 - val_loss: 0.2637 - val_acc: 0.9294\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8988\n",
      "Epoch 00039: val_loss did not improve from 0.24518\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3232 - acc: 0.8987 - val_loss: 0.2468 - val_acc: 0.9343\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.8997\n",
      "Epoch 00040: val_loss did not improve from 0.24518\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3192 - acc: 0.8997 - val_loss: 0.2452 - val_acc: 0.9311\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3186 - acc: 0.9006\n",
      "Epoch 00041: val_loss improved from 0.24518 to 0.23906, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/041-0.2391.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3186 - acc: 0.9006 - val_loss: 0.2391 - val_acc: 0.9329\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9026\n",
      "Epoch 00042: val_loss did not improve from 0.23906\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3094 - acc: 0.9026 - val_loss: 0.2430 - val_acc: 0.9301\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9025\n",
      "Epoch 00043: val_loss did not improve from 0.23906\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3061 - acc: 0.9025 - val_loss: 0.2604 - val_acc: 0.9294\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.9045\n",
      "Epoch 00044: val_loss improved from 0.23906 to 0.22500, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/044-0.2250.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3054 - acc: 0.9045 - val_loss: 0.2250 - val_acc: 0.9415\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9082\n",
      "Epoch 00045: val_loss did not improve from 0.22500\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2950 - acc: 0.9082 - val_loss: 0.2377 - val_acc: 0.9352\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9078\n",
      "Epoch 00046: val_loss improved from 0.22500 to 0.21698, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/046-0.2170.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2956 - acc: 0.9078 - val_loss: 0.2170 - val_acc: 0.9392\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9076\n",
      "Epoch 00047: val_loss did not improve from 0.21698\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2904 - acc: 0.9076 - val_loss: 0.2333 - val_acc: 0.9341\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9110\n",
      "Epoch 00048: val_loss did not improve from 0.21698\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2842 - acc: 0.9110 - val_loss: 0.2393 - val_acc: 0.9355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9102\n",
      "Epoch 00049: val_loss improved from 0.21698 to 0.21517, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/049-0.2152.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2816 - acc: 0.9102 - val_loss: 0.2152 - val_acc: 0.9448\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9120\n",
      "Epoch 00050: val_loss did not improve from 0.21517\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2783 - acc: 0.9121 - val_loss: 0.2234 - val_acc: 0.9385\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.9130\n",
      "Epoch 00051: val_loss did not improve from 0.21517\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2758 - acc: 0.9130 - val_loss: 0.2296 - val_acc: 0.9315\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9131\n",
      "Epoch 00052: val_loss did not improve from 0.21517\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2692 - acc: 0.9131 - val_loss: 0.2186 - val_acc: 0.9352\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9132\n",
      "Epoch 00053: val_loss did not improve from 0.21517\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2708 - acc: 0.9132 - val_loss: 0.2254 - val_acc: 0.9411\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9181\n",
      "Epoch 00054: val_loss did not improve from 0.21517\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2606 - acc: 0.9181 - val_loss: 0.2202 - val_acc: 0.9392\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9187\n",
      "Epoch 00055: val_loss did not improve from 0.21517\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2564 - acc: 0.9187 - val_loss: 0.2232 - val_acc: 0.9380\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9167\n",
      "Epoch 00056: val_loss improved from 0.21517 to 0.21429, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/056-0.2143.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2603 - acc: 0.9166 - val_loss: 0.2143 - val_acc: 0.9462\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9190\n",
      "Epoch 00057: val_loss improved from 0.21429 to 0.21429, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/057-0.2143.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2534 - acc: 0.9190 - val_loss: 0.2143 - val_acc: 0.9432\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9167\n",
      "Epoch 00058: val_loss improved from 0.21429 to 0.20322, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/058-0.2032.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2588 - acc: 0.9167 - val_loss: 0.2032 - val_acc: 0.9411\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9206\n",
      "Epoch 00059: val_loss did not improve from 0.20322\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2453 - acc: 0.9206 - val_loss: 0.2039 - val_acc: 0.9455\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9201\n",
      "Epoch 00060: val_loss improved from 0.20322 to 0.20148, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/060-0.2015.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2468 - acc: 0.9201 - val_loss: 0.2015 - val_acc: 0.9455\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9220\n",
      "Epoch 00061: val_loss did not improve from 0.20148\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2452 - acc: 0.9220 - val_loss: 0.2171 - val_acc: 0.9443\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9239\n",
      "Epoch 00062: val_loss did not improve from 0.20148\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2349 - acc: 0.9239 - val_loss: 0.2028 - val_acc: 0.9420\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9237\n",
      "Epoch 00063: val_loss improved from 0.20148 to 0.20141, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/063-0.2014.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2403 - acc: 0.9237 - val_loss: 0.2014 - val_acc: 0.9432\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9241\n",
      "Epoch 00064: val_loss improved from 0.20141 to 0.20056, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/064-0.2006.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2345 - acc: 0.9241 - val_loss: 0.2006 - val_acc: 0.9450\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9250\n",
      "Epoch 00065: val_loss did not improve from 0.20056\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2328 - acc: 0.9250 - val_loss: 0.2262 - val_acc: 0.9362\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9265\n",
      "Epoch 00066: val_loss did not improve from 0.20056\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2281 - acc: 0.9264 - val_loss: 0.2051 - val_acc: 0.9460\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9258\n",
      "Epoch 00067: val_loss improved from 0.20056 to 0.20038, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/067-0.2004.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2288 - acc: 0.9258 - val_loss: 0.2004 - val_acc: 0.9455\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9277\n",
      "Epoch 00068: val_loss did not improve from 0.20038\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2225 - acc: 0.9276 - val_loss: 0.2031 - val_acc: 0.9462\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9271\n",
      "Epoch 00069: val_loss did not improve from 0.20038\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2269 - acc: 0.9271 - val_loss: 0.2011 - val_acc: 0.9457\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9277\n",
      "Epoch 00070: val_loss did not improve from 0.20038\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2219 - acc: 0.9277 - val_loss: 0.2076 - val_acc: 0.9453\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9280\n",
      "Epoch 00071: val_loss did not improve from 0.20038\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2198 - acc: 0.9280 - val_loss: 0.2090 - val_acc: 0.9464\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9299\n",
      "Epoch 00072: val_loss improved from 0.20038 to 0.19862, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/072-0.1986.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2166 - acc: 0.9299 - val_loss: 0.1986 - val_acc: 0.9448\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9318\n",
      "Epoch 00073: val_loss improved from 0.19862 to 0.19808, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/073-0.1981.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2104 - acc: 0.9319 - val_loss: 0.1981 - val_acc: 0.9462\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9305\n",
      "Epoch 00074: val_loss did not improve from 0.19808\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2137 - acc: 0.9305 - val_loss: 0.2040 - val_acc: 0.9467\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9312\n",
      "Epoch 00075: val_loss did not improve from 0.19808\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2072 - acc: 0.9312 - val_loss: 0.2201 - val_acc: 0.9392\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9314\n",
      "Epoch 00076: val_loss did not improve from 0.19808\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2121 - acc: 0.9314 - val_loss: 0.2137 - val_acc: 0.9436\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9342\n",
      "Epoch 00077: val_loss did not improve from 0.19808\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2074 - acc: 0.9342 - val_loss: 0.2039 - val_acc: 0.9448\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9349\n",
      "Epoch 00078: val_loss did not improve from 0.19808\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2004 - acc: 0.9349 - val_loss: 0.2064 - val_acc: 0.9478\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9349\n",
      "Epoch 00079: val_loss did not improve from 0.19808\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2011 - acc: 0.9349 - val_loss: 0.2009 - val_acc: 0.9455\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9348\n",
      "Epoch 00080: val_loss improved from 0.19808 to 0.19781, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/080-0.1978.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2004 - acc: 0.9348 - val_loss: 0.1978 - val_acc: 0.9497\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9355\n",
      "Epoch 00081: val_loss improved from 0.19781 to 0.19691, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/081-0.1969.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1973 - acc: 0.9355 - val_loss: 0.1969 - val_acc: 0.9450\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9380\n",
      "Epoch 00082: val_loss did not improve from 0.19691\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1941 - acc: 0.9379 - val_loss: 0.2057 - val_acc: 0.9490\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9369\n",
      "Epoch 00083: val_loss did not improve from 0.19691\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1936 - acc: 0.9369 - val_loss: 0.2085 - val_acc: 0.9502\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9386\n",
      "Epoch 00084: val_loss did not improve from 0.19691\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1910 - acc: 0.9386 - val_loss: 0.2010 - val_acc: 0.9441\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9383\n",
      "Epoch 00085: val_loss did not improve from 0.19691\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1895 - acc: 0.9383 - val_loss: 0.2026 - val_acc: 0.9469\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9368\n",
      "Epoch 00086: val_loss did not improve from 0.19691\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1940 - acc: 0.9368 - val_loss: 0.2058 - val_acc: 0.9478\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9384\n",
      "Epoch 00087: val_loss did not improve from 0.19691\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1873 - acc: 0.9384 - val_loss: 0.2168 - val_acc: 0.9425\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9389\n",
      "Epoch 00088: val_loss did not improve from 0.19691\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1869 - acc: 0.9389 - val_loss: 0.2050 - val_acc: 0.9425\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9372\n",
      "Epoch 00089: val_loss did not improve from 0.19691\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1864 - acc: 0.9372 - val_loss: 0.2106 - val_acc: 0.9448\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9394\n",
      "Epoch 00090: val_loss did not improve from 0.19691\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1846 - acc: 0.9394 - val_loss: 0.2111 - val_acc: 0.9448\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9414\n",
      "Epoch 00091: val_loss did not improve from 0.19691\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1776 - acc: 0.9414 - val_loss: 0.1985 - val_acc: 0.9453\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9406\n",
      "Epoch 00092: val_loss improved from 0.19691 to 0.19484, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_7_conv_checkpoint/092-0.1948.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1783 - acc: 0.9406 - val_loss: 0.1948 - val_acc: 0.9485\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9417\n",
      "Epoch 00093: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1761 - acc: 0.9417 - val_loss: 0.2119 - val_acc: 0.9469\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9427\n",
      "Epoch 00094: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1752 - acc: 0.9427 - val_loss: 0.2017 - val_acc: 0.9499\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9409\n",
      "Epoch 00095: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1776 - acc: 0.9409 - val_loss: 0.2048 - val_acc: 0.9471\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9432\n",
      "Epoch 00096: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1707 - acc: 0.9432 - val_loss: 0.2020 - val_acc: 0.9443\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9413\n",
      "Epoch 00097: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1755 - acc: 0.9413 - val_loss: 0.1989 - val_acc: 0.9476\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9449\n",
      "Epoch 00098: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1689 - acc: 0.9449 - val_loss: 0.2115 - val_acc: 0.9464\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9442\n",
      "Epoch 00099: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1670 - acc: 0.9442 - val_loss: 0.2064 - val_acc: 0.9483\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9446\n",
      "Epoch 00100: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1696 - acc: 0.9446 - val_loss: 0.2178 - val_acc: 0.9446\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9464\n",
      "Epoch 00101: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1656 - acc: 0.9464 - val_loss: 0.2116 - val_acc: 0.9455\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9464\n",
      "Epoch 00102: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1630 - acc: 0.9464 - val_loss: 0.2001 - val_acc: 0.9509\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9454\n",
      "Epoch 00103: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1620 - acc: 0.9454 - val_loss: 0.1963 - val_acc: 0.9478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9460\n",
      "Epoch 00104: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1634 - acc: 0.9460 - val_loss: 0.1981 - val_acc: 0.9469\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9471\n",
      "Epoch 00105: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1615 - acc: 0.9471 - val_loss: 0.1988 - val_acc: 0.9453\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9449\n",
      "Epoch 00106: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1639 - acc: 0.9449 - val_loss: 0.2024 - val_acc: 0.9457\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9495\n",
      "Epoch 00107: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1535 - acc: 0.9495 - val_loss: 0.2047 - val_acc: 0.9504\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9476\n",
      "Epoch 00108: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1552 - acc: 0.9476 - val_loss: 0.2036 - val_acc: 0.9460\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9473\n",
      "Epoch 00109: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1565 - acc: 0.9472 - val_loss: 0.2040 - val_acc: 0.9481\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9471\n",
      "Epoch 00110: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1560 - acc: 0.9471 - val_loss: 0.2230 - val_acc: 0.9420\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9495\n",
      "Epoch 00111: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1545 - acc: 0.9494 - val_loss: 0.2043 - val_acc: 0.9464\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9477\n",
      "Epoch 00112: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1541 - acc: 0.9477 - val_loss: 0.2203 - val_acc: 0.9411\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9486\n",
      "Epoch 00113: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1510 - acc: 0.9485 - val_loss: 0.1985 - val_acc: 0.9515\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9495\n",
      "Epoch 00114: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1508 - acc: 0.9495 - val_loss: 0.2086 - val_acc: 0.9495\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9510\n",
      "Epoch 00115: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1461 - acc: 0.9510 - val_loss: 0.2070 - val_acc: 0.9490\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9515\n",
      "Epoch 00116: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1458 - acc: 0.9515 - val_loss: 0.2022 - val_acc: 0.9471\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9494\n",
      "Epoch 00117: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1499 - acc: 0.9494 - val_loss: 0.2048 - val_acc: 0.9488\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9511\n",
      "Epoch 00118: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1480 - acc: 0.9511 - val_loss: 0.2392 - val_acc: 0.9397\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9511\n",
      "Epoch 00119: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1489 - acc: 0.9511 - val_loss: 0.2155 - val_acc: 0.9481\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9515\n",
      "Epoch 00120: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1424 - acc: 0.9515 - val_loss: 0.2014 - val_acc: 0.9490\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9520\n",
      "Epoch 00121: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1430 - acc: 0.9520 - val_loss: 0.2067 - val_acc: 0.9490\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9526\n",
      "Epoch 00122: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1404 - acc: 0.9526 - val_loss: 0.2242 - val_acc: 0.9455\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9528\n",
      "Epoch 00123: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1422 - acc: 0.9528 - val_loss: 0.2197 - val_acc: 0.9439\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9516\n",
      "Epoch 00124: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1439 - acc: 0.9516 - val_loss: 0.2124 - val_acc: 0.9439\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9526\n",
      "Epoch 00125: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1406 - acc: 0.9526 - val_loss: 0.2123 - val_acc: 0.9464\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9511\n",
      "Epoch 00126: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1436 - acc: 0.9511 - val_loss: 0.2052 - val_acc: 0.9492\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9527\n",
      "Epoch 00127: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1364 - acc: 0.9527 - val_loss: 0.2041 - val_acc: 0.9529\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9547\n",
      "Epoch 00128: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1326 - acc: 0.9547 - val_loss: 0.2256 - val_acc: 0.9462\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9547\n",
      "Epoch 00129: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1336 - acc: 0.9547 - val_loss: 0.2170 - val_acc: 0.9439\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9536\n",
      "Epoch 00130: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1356 - acc: 0.9536 - val_loss: 0.2139 - val_acc: 0.9488\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9553\n",
      "Epoch 00131: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1336 - acc: 0.9553 - val_loss: 0.2187 - val_acc: 0.9495\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9563\n",
      "Epoch 00132: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1319 - acc: 0.9563 - val_loss: 0.2222 - val_acc: 0.9464\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9558\n",
      "Epoch 00133: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1335 - acc: 0.9558 - val_loss: 0.2043 - val_acc: 0.9490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9555\n",
      "Epoch 00134: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1307 - acc: 0.9555 - val_loss: 0.2165 - val_acc: 0.9455\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9565\n",
      "Epoch 00135: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1278 - acc: 0.9566 - val_loss: 0.2228 - val_acc: 0.9495\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9572\n",
      "Epoch 00136: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1290 - acc: 0.9572 - val_loss: 0.1975 - val_acc: 0.9504\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9579\n",
      "Epoch 00137: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1273 - acc: 0.9579 - val_loss: 0.2325 - val_acc: 0.9443\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9566\n",
      "Epoch 00138: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1278 - acc: 0.9566 - val_loss: 0.2219 - val_acc: 0.9499\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9551\n",
      "Epoch 00139: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1332 - acc: 0.9551 - val_loss: 0.2089 - val_acc: 0.9539\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9574\n",
      "Epoch 00140: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1246 - acc: 0.9574 - val_loss: 0.2031 - val_acc: 0.9476\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9597\n",
      "Epoch 00141: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1215 - acc: 0.9597 - val_loss: 0.2210 - val_acc: 0.9464\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9583\n",
      "Epoch 00142: val_loss did not improve from 0.19484\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1253 - acc: 0.9583 - val_loss: 0.2256 - val_acc: 0.9495\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2247 - acc: 0.9385\n",
      "Loss: 0.22465223592327888 Accuracy: 0.93852544\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7047 - acc: 0.1000\n",
      "Epoch 00001: val_loss improved from inf to 2.51716, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/001-2.5172.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 2.7047 - acc: 0.1000 - val_loss: 2.5172 - val_acc: 0.2278\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2310 - acc: 0.2805\n",
      "Epoch 00002: val_loss improved from 2.51716 to 1.70331, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/002-1.7033.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 2.2309 - acc: 0.2805 - val_loss: 1.7033 - val_acc: 0.4689\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7482 - acc: 0.4273\n",
      "Epoch 00003: val_loss improved from 1.70331 to 1.31656, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/003-1.3166.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.7481 - acc: 0.4273 - val_loss: 1.3166 - val_acc: 0.5816\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5052 - acc: 0.5031\n",
      "Epoch 00004: val_loss improved from 1.31656 to 1.15207, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/004-1.1521.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.5051 - acc: 0.5031 - val_loss: 1.1521 - val_acc: 0.6362\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3375 - acc: 0.5573\n",
      "Epoch 00005: val_loss improved from 1.15207 to 1.05503, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/005-1.0550.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.3375 - acc: 0.5572 - val_loss: 1.0550 - val_acc: 0.6669\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2196 - acc: 0.5986\n",
      "Epoch 00006: val_loss improved from 1.05503 to 0.90908, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/006-0.9091.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.2197 - acc: 0.5986 - val_loss: 0.9091 - val_acc: 0.7130\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1065 - acc: 0.6393\n",
      "Epoch 00007: val_loss improved from 0.90908 to 0.76771, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/007-0.7677.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.1065 - acc: 0.6394 - val_loss: 0.7677 - val_acc: 0.7715\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0044 - acc: 0.6769\n",
      "Epoch 00008: val_loss improved from 0.76771 to 0.72792, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/008-0.7279.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.0044 - acc: 0.6769 - val_loss: 0.7279 - val_acc: 0.7857\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9258 - acc: 0.7024\n",
      "Epoch 00009: val_loss improved from 0.72792 to 0.60538, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/009-0.6054.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.9258 - acc: 0.7024 - val_loss: 0.6054 - val_acc: 0.8204\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8470 - acc: 0.7344\n",
      "Epoch 00010: val_loss improved from 0.60538 to 0.53274, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/010-0.5327.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.8470 - acc: 0.7344 - val_loss: 0.5327 - val_acc: 0.8425\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7904 - acc: 0.7507\n",
      "Epoch 00011: val_loss improved from 0.53274 to 0.50843, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/011-0.5084.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.7903 - acc: 0.7507 - val_loss: 0.5084 - val_acc: 0.8479\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7356 - acc: 0.7710\n",
      "Epoch 00012: val_loss improved from 0.50843 to 0.44891, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/012-0.4489.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7355 - acc: 0.7710 - val_loss: 0.4489 - val_acc: 0.8677\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.7835\n",
      "Epoch 00013: val_loss did not improve from 0.44891\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6923 - acc: 0.7836 - val_loss: 0.4720 - val_acc: 0.8668\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6544 - acc: 0.7987\n",
      "Epoch 00014: val_loss improved from 0.44891 to 0.39677, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/014-0.3968.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6544 - acc: 0.7987 - val_loss: 0.3968 - val_acc: 0.8884\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6144 - acc: 0.8102\n",
      "Epoch 00015: val_loss improved from 0.39677 to 0.38943, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/015-0.3894.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6144 - acc: 0.8102 - val_loss: 0.3894 - val_acc: 0.8894\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.8138\n",
      "Epoch 00016: val_loss improved from 0.38943 to 0.34622, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/016-0.3462.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6017 - acc: 0.8138 - val_loss: 0.3462 - val_acc: 0.9033\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5644 - acc: 0.8248\n",
      "Epoch 00017: val_loss improved from 0.34622 to 0.33551, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/017-0.3355.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5644 - acc: 0.8248 - val_loss: 0.3355 - val_acc: 0.9094\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.8308\n",
      "Epoch 00018: val_loss did not improve from 0.33551\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5429 - acc: 0.8309 - val_loss: 0.3405 - val_acc: 0.9045\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.8377\n",
      "Epoch 00019: val_loss improved from 0.33551 to 0.30086, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/019-0.3009.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5267 - acc: 0.8377 - val_loss: 0.3009 - val_acc: 0.9133\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4978 - acc: 0.8480\n",
      "Epoch 00020: val_loss did not improve from 0.30086\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4978 - acc: 0.8481 - val_loss: 0.3076 - val_acc: 0.9133\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8476\n",
      "Epoch 00021: val_loss improved from 0.30086 to 0.28184, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/021-0.2818.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4938 - acc: 0.8476 - val_loss: 0.2818 - val_acc: 0.9189\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8553\n",
      "Epoch 00022: val_loss did not improve from 0.28184\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4714 - acc: 0.8553 - val_loss: 0.2915 - val_acc: 0.9229\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.8574\n",
      "Epoch 00023: val_loss improved from 0.28184 to 0.27829, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/023-0.2783.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4607 - acc: 0.8574 - val_loss: 0.2783 - val_acc: 0.9196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4466 - acc: 0.8619\n",
      "Epoch 00024: val_loss did not improve from 0.27829\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4466 - acc: 0.8619 - val_loss: 0.2785 - val_acc: 0.9213\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4361 - acc: 0.8650\n",
      "Epoch 00025: val_loss improved from 0.27829 to 0.26409, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/025-0.2641.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4363 - acc: 0.8649 - val_loss: 0.2641 - val_acc: 0.9259\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.8676\n",
      "Epoch 00026: val_loss improved from 0.26409 to 0.25800, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/026-0.2580.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4245 - acc: 0.8675 - val_loss: 0.2580 - val_acc: 0.9229\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8731\n",
      "Epoch 00027: val_loss improved from 0.25800 to 0.23967, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/027-0.2397.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4092 - acc: 0.8731 - val_loss: 0.2397 - val_acc: 0.9359\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3921 - acc: 0.8793\n",
      "Epoch 00028: val_loss did not improve from 0.23967\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3921 - acc: 0.8793 - val_loss: 0.2486 - val_acc: 0.9280\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8777\n",
      "Epoch 00029: val_loss did not improve from 0.23967\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3893 - acc: 0.8777 - val_loss: 0.2431 - val_acc: 0.9285\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8801\n",
      "Epoch 00030: val_loss improved from 0.23967 to 0.22804, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/030-0.2280.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3876 - acc: 0.8801 - val_loss: 0.2280 - val_acc: 0.9352\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8849\n",
      "Epoch 00031: val_loss improved from 0.22804 to 0.22294, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/031-0.2229.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3725 - acc: 0.8849 - val_loss: 0.2229 - val_acc: 0.9324\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8848\n",
      "Epoch 00032: val_loss improved from 0.22294 to 0.21834, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/032-0.2183.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3718 - acc: 0.8848 - val_loss: 0.2183 - val_acc: 0.9373\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3570 - acc: 0.8896\n",
      "Epoch 00033: val_loss did not improve from 0.21834\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3570 - acc: 0.8896 - val_loss: 0.2243 - val_acc: 0.9362\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8899\n",
      "Epoch 00034: val_loss did not improve from 0.21834\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3515 - acc: 0.8899 - val_loss: 0.2265 - val_acc: 0.9357\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.8907\n",
      "Epoch 00035: val_loss improved from 0.21834 to 0.21767, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/035-0.2177.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3452 - acc: 0.8907 - val_loss: 0.2177 - val_acc: 0.9399\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3386 - acc: 0.8952\n",
      "Epoch 00036: val_loss improved from 0.21767 to 0.20329, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/036-0.2033.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3386 - acc: 0.8952 - val_loss: 0.2033 - val_acc: 0.9397\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.8966\n",
      "Epoch 00037: val_loss did not improve from 0.20329\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3297 - acc: 0.8966 - val_loss: 0.2340 - val_acc: 0.9348\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8967\n",
      "Epoch 00038: val_loss improved from 0.20329 to 0.19696, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/038-0.1970.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3312 - acc: 0.8967 - val_loss: 0.1970 - val_acc: 0.9420\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.9018\n",
      "Epoch 00039: val_loss did not improve from 0.19696\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3137 - acc: 0.9018 - val_loss: 0.2110 - val_acc: 0.9359\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.9014\n",
      "Epoch 00040: val_loss did not improve from 0.19696\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3152 - acc: 0.9013 - val_loss: 0.2057 - val_acc: 0.9413\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.9014\n",
      "Epoch 00041: val_loss did not improve from 0.19696\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3128 - acc: 0.9014 - val_loss: 0.2055 - val_acc: 0.9401\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3033 - acc: 0.9039\n",
      "Epoch 00042: val_loss improved from 0.19696 to 0.19332, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/042-0.1933.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3033 - acc: 0.9039 - val_loss: 0.1933 - val_acc: 0.9420\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9028\n",
      "Epoch 00043: val_loss did not improve from 0.19332\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3041 - acc: 0.9027 - val_loss: 0.2141 - val_acc: 0.9394\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9074\n",
      "Epoch 00044: val_loss did not improve from 0.19332\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2982 - acc: 0.9074 - val_loss: 0.2049 - val_acc: 0.9418\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9095\n",
      "Epoch 00045: val_loss did not improve from 0.19332\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2865 - acc: 0.9094 - val_loss: 0.2018 - val_acc: 0.9439\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9114\n",
      "Epoch 00046: val_loss improved from 0.19332 to 0.19039, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/046-0.1904.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2844 - acc: 0.9114 - val_loss: 0.1904 - val_acc: 0.9469\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9090\n",
      "Epoch 00047: val_loss improved from 0.19039 to 0.18099, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/047-0.1810.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2828 - acc: 0.9090 - val_loss: 0.1810 - val_acc: 0.9462\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9123\n",
      "Epoch 00048: val_loss did not improve from 0.18099\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2789 - acc: 0.9123 - val_loss: 0.1851 - val_acc: 0.9462\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9140\n",
      "Epoch 00049: val_loss improved from 0.18099 to 0.17656, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/049-0.1766.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2704 - acc: 0.9141 - val_loss: 0.1766 - val_acc: 0.9490\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9154\n",
      "Epoch 00050: val_loss did not improve from 0.17656\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2697 - acc: 0.9154 - val_loss: 0.1864 - val_acc: 0.9476\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2678 - acc: 0.9160\n",
      "Epoch 00051: val_loss did not improve from 0.17656\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2678 - acc: 0.9160 - val_loss: 0.1966 - val_acc: 0.9429\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9150\n",
      "Epoch 00052: val_loss did not improve from 0.17656\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2655 - acc: 0.9150 - val_loss: 0.1795 - val_acc: 0.9481\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9174\n",
      "Epoch 00053: val_loss did not improve from 0.17656\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2610 - acc: 0.9175 - val_loss: 0.1789 - val_acc: 0.9488\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9194\n",
      "Epoch 00054: val_loss did not improve from 0.17656\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2534 - acc: 0.9194 - val_loss: 0.1836 - val_acc: 0.9441\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9188\n",
      "Epoch 00055: val_loss did not improve from 0.17656\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2523 - acc: 0.9188 - val_loss: 0.1791 - val_acc: 0.9474\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9202\n",
      "Epoch 00056: val_loss did not improve from 0.17656\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2489 - acc: 0.9201 - val_loss: 0.1917 - val_acc: 0.9467\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2530 - acc: 0.9196\n",
      "Epoch 00057: val_loss improved from 0.17656 to 0.17281, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/057-0.1728.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2530 - acc: 0.9196 - val_loss: 0.1728 - val_acc: 0.9467\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9235\n",
      "Epoch 00058: val_loss did not improve from 0.17281\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2439 - acc: 0.9234 - val_loss: 0.1818 - val_acc: 0.9460\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9231\n",
      "Epoch 00059: val_loss improved from 0.17281 to 0.16765, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/059-0.1677.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2430 - acc: 0.9231 - val_loss: 0.1677 - val_acc: 0.9495\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9239\n",
      "Epoch 00060: val_loss did not improve from 0.16765\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2354 - acc: 0.9239 - val_loss: 0.1916 - val_acc: 0.9453\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9258\n",
      "Epoch 00061: val_loss did not improve from 0.16765\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2299 - acc: 0.9258 - val_loss: 0.1822 - val_acc: 0.9511\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9266\n",
      "Epoch 00062: val_loss did not improve from 0.16765\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2295 - acc: 0.9266 - val_loss: 0.1746 - val_acc: 0.9497\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9251\n",
      "Epoch 00063: val_loss did not improve from 0.16765\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2287 - acc: 0.9251 - val_loss: 0.1722 - val_acc: 0.9520\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9274\n",
      "Epoch 00064: val_loss did not improve from 0.16765\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2253 - acc: 0.9274 - val_loss: 0.1752 - val_acc: 0.9483\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9270\n",
      "Epoch 00065: val_loss did not improve from 0.16765\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2228 - acc: 0.9270 - val_loss: 0.2184 - val_acc: 0.9355\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9286\n",
      "Epoch 00066: val_loss did not improve from 0.16765\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2212 - acc: 0.9286 - val_loss: 0.1706 - val_acc: 0.9502\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9302\n",
      "Epoch 00067: val_loss improved from 0.16765 to 0.16634, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/067-0.1663.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2154 - acc: 0.9302 - val_loss: 0.1663 - val_acc: 0.9478\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9301\n",
      "Epoch 00068: val_loss did not improve from 0.16634\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2146 - acc: 0.9301 - val_loss: 0.1698 - val_acc: 0.9506\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9323\n",
      "Epoch 00069: val_loss did not improve from 0.16634\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2128 - acc: 0.9322 - val_loss: 0.1729 - val_acc: 0.9481\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9345\n",
      "Epoch 00070: val_loss did not improve from 0.16634\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2052 - acc: 0.9345 - val_loss: 0.1883 - val_acc: 0.9450\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9309\n",
      "Epoch 00071: val_loss did not improve from 0.16634\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2100 - acc: 0.9309 - val_loss: 0.1750 - val_acc: 0.9467\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9354\n",
      "Epoch 00072: val_loss improved from 0.16634 to 0.16487, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/072-0.1649.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2017 - acc: 0.9354 - val_loss: 0.1649 - val_acc: 0.9513\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9343\n",
      "Epoch 00073: val_loss did not improve from 0.16487\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2008 - acc: 0.9343 - val_loss: 0.1932 - val_acc: 0.9460\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9343\n",
      "Epoch 00074: val_loss did not improve from 0.16487\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2017 - acc: 0.9343 - val_loss: 0.1717 - val_acc: 0.9490\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9367\n",
      "Epoch 00075: val_loss did not improve from 0.16487\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1952 - acc: 0.9366 - val_loss: 0.1662 - val_acc: 0.9522\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9360\n",
      "Epoch 00076: val_loss improved from 0.16487 to 0.16318, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/076-0.1632.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1955 - acc: 0.9360 - val_loss: 0.1632 - val_acc: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9363\n",
      "Epoch 00077: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1950 - acc: 0.9363 - val_loss: 0.1642 - val_acc: 0.9495\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9377\n",
      "Epoch 00078: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1888 - acc: 0.9378 - val_loss: 0.1785 - val_acc: 0.9492\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9385\n",
      "Epoch 00079: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1888 - acc: 0.9385 - val_loss: 0.1726 - val_acc: 0.9495\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9392\n",
      "Epoch 00080: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1875 - acc: 0.9391 - val_loss: 0.1821 - val_acc: 0.9490\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1919 - acc: 0.9377\n",
      "Epoch 00081: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1919 - acc: 0.9377 - val_loss: 0.1669 - val_acc: 0.9520\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9411\n",
      "Epoch 00082: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1806 - acc: 0.9411 - val_loss: 0.1740 - val_acc: 0.9509\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9401\n",
      "Epoch 00083: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1820 - acc: 0.9401 - val_loss: 0.1687 - val_acc: 0.9527\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9415\n",
      "Epoch 00084: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1785 - acc: 0.9415 - val_loss: 0.1745 - val_acc: 0.9506\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9424\n",
      "Epoch 00085: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1729 - acc: 0.9424 - val_loss: 0.1639 - val_acc: 0.9515\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9430\n",
      "Epoch 00086: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1704 - acc: 0.9430 - val_loss: 0.1688 - val_acc: 0.9534\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9438\n",
      "Epoch 00087: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1724 - acc: 0.9438 - val_loss: 0.1681 - val_acc: 0.9497\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9437\n",
      "Epoch 00088: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1716 - acc: 0.9437 - val_loss: 0.1714 - val_acc: 0.9513\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9442\n",
      "Epoch 00089: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1677 - acc: 0.9441 - val_loss: 0.1691 - val_acc: 0.9518\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9442\n",
      "Epoch 00090: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1640 - acc: 0.9442 - val_loss: 0.1845 - val_acc: 0.9504\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9458\n",
      "Epoch 00091: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1665 - acc: 0.9457 - val_loss: 0.1678 - val_acc: 0.9513\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9473\n",
      "Epoch 00092: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1632 - acc: 0.9473 - val_loss: 0.1697 - val_acc: 0.9520\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9472\n",
      "Epoch 00093: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1594 - acc: 0.9472 - val_loss: 0.1742 - val_acc: 0.9509\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9470\n",
      "Epoch 00094: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1579 - acc: 0.9470 - val_loss: 0.1759 - val_acc: 0.9495\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9477\n",
      "Epoch 00095: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1564 - acc: 0.9476 - val_loss: 0.1721 - val_acc: 0.9499\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9461\n",
      "Epoch 00096: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1612 - acc: 0.9461 - val_loss: 0.1712 - val_acc: 0.9474\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9481\n",
      "Epoch 00097: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1566 - acc: 0.9481 - val_loss: 0.1668 - val_acc: 0.9499\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9492\n",
      "Epoch 00098: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1541 - acc: 0.9492 - val_loss: 0.1639 - val_acc: 0.9511\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9493\n",
      "Epoch 00099: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1520 - acc: 0.9493 - val_loss: 0.1845 - val_acc: 0.9509\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9498\n",
      "Epoch 00100: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1506 - acc: 0.9498 - val_loss: 0.1768 - val_acc: 0.9497\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9475\n",
      "Epoch 00101: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1588 - acc: 0.9475 - val_loss: 0.1708 - val_acc: 0.9520\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9492\n",
      "Epoch 00102: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1481 - acc: 0.9492 - val_loss: 0.1802 - val_acc: 0.9497\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9498\n",
      "Epoch 00103: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1519 - acc: 0.9498 - val_loss: 0.1677 - val_acc: 0.9511\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9487\n",
      "Epoch 00104: val_loss did not improve from 0.16318\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1490 - acc: 0.9486 - val_loss: 0.2034 - val_acc: 0.9469\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9476\n",
      "Epoch 00105: val_loss improved from 0.16318 to 0.16068, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/105-0.1607.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1584 - acc: 0.9476 - val_loss: 0.1607 - val_acc: 0.9518\n",
      "Epoch 106/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9537\n",
      "Epoch 00106: val_loss did not improve from 0.16068\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1413 - acc: 0.9537 - val_loss: 0.1613 - val_acc: 0.9539\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9535\n",
      "Epoch 00107: val_loss improved from 0.16068 to 0.15937, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_8_conv_checkpoint/107-0.1594.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1386 - acc: 0.9535 - val_loss: 0.1594 - val_acc: 0.9527\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9535\n",
      "Epoch 00108: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1378 - acc: 0.9535 - val_loss: 0.1722 - val_acc: 0.9518\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9518\n",
      "Epoch 00109: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1395 - acc: 0.9518 - val_loss: 0.1714 - val_acc: 0.9546\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9549\n",
      "Epoch 00110: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1366 - acc: 0.9549 - val_loss: 0.1764 - val_acc: 0.9485\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9536\n",
      "Epoch 00111: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1350 - acc: 0.9536 - val_loss: 0.1730 - val_acc: 0.9536\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9534\n",
      "Epoch 00112: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1398 - acc: 0.9534 - val_loss: 0.1660 - val_acc: 0.9502\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9554\n",
      "Epoch 00113: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1309 - acc: 0.9554 - val_loss: 0.1752 - val_acc: 0.9513\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9534\n",
      "Epoch 00114: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1375 - acc: 0.9534 - val_loss: 0.1768 - val_acc: 0.9488\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9555\n",
      "Epoch 00115: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1308 - acc: 0.9555 - val_loss: 0.2037 - val_acc: 0.9469\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9542\n",
      "Epoch 00116: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1348 - acc: 0.9542 - val_loss: 0.1915 - val_acc: 0.9492\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9572\n",
      "Epoch 00117: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1260 - acc: 0.9572 - val_loss: 0.1767 - val_acc: 0.9509\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9555\n",
      "Epoch 00118: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1295 - acc: 0.9555 - val_loss: 0.1869 - val_acc: 0.9497\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9570\n",
      "Epoch 00119: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1258 - acc: 0.9570 - val_loss: 0.1736 - val_acc: 0.9518\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9562\n",
      "Epoch 00120: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1303 - acc: 0.9561 - val_loss: 0.1863 - val_acc: 0.9515\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9561\n",
      "Epoch 00121: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1279 - acc: 0.9561 - val_loss: 0.1797 - val_acc: 0.9515\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9567\n",
      "Epoch 00122: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1272 - acc: 0.9567 - val_loss: 0.1823 - val_acc: 0.9509\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9587\n",
      "Epoch 00123: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1214 - acc: 0.9587 - val_loss: 0.1800 - val_acc: 0.9511\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9583\n",
      "Epoch 00124: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1234 - acc: 0.9583 - val_loss: 0.2022 - val_acc: 0.9450\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9583\n",
      "Epoch 00125: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1218 - acc: 0.9583 - val_loss: 0.1878 - val_acc: 0.9518\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9614\n",
      "Epoch 00126: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1141 - acc: 0.9614 - val_loss: 0.2062 - val_acc: 0.9460\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9603\n",
      "Epoch 00127: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1197 - acc: 0.9603 - val_loss: 0.1864 - val_acc: 0.9490\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9598\n",
      "Epoch 00128: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1174 - acc: 0.9598 - val_loss: 0.1886 - val_acc: 0.9539\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9610\n",
      "Epoch 00129: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1163 - acc: 0.9610 - val_loss: 0.2007 - val_acc: 0.9506\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9612\n",
      "Epoch 00130: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1145 - acc: 0.9611 - val_loss: 0.1861 - val_acc: 0.9488\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9595\n",
      "Epoch 00131: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1186 - acc: 0.9595 - val_loss: 0.1805 - val_acc: 0.9529\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9610\n",
      "Epoch 00132: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1146 - acc: 0.9610 - val_loss: 0.1880 - val_acc: 0.9490\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9612\n",
      "Epoch 00133: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1134 - acc: 0.9612 - val_loss: 0.1975 - val_acc: 0.9502\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9624\n",
      "Epoch 00134: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1093 - acc: 0.9624 - val_loss: 0.1856 - val_acc: 0.9502\n",
      "Epoch 135/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9619\n",
      "Epoch 00135: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1127 - acc: 0.9619 - val_loss: 0.1880 - val_acc: 0.9522\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9633\n",
      "Epoch 00136: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1091 - acc: 0.9633 - val_loss: 0.1941 - val_acc: 0.9506\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9632\n",
      "Epoch 00137: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1058 - acc: 0.9632 - val_loss: 0.2016 - val_acc: 0.9520\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9630\n",
      "Epoch 00138: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1081 - acc: 0.9630 - val_loss: 0.2038 - val_acc: 0.9492\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9653\n",
      "Epoch 00139: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1042 - acc: 0.9653 - val_loss: 0.1915 - val_acc: 0.9509\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9632\n",
      "Epoch 00140: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1110 - acc: 0.9632 - val_loss: 0.1896 - val_acc: 0.9541\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9654\n",
      "Epoch 00141: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1008 - acc: 0.9654 - val_loss: 0.2012 - val_acc: 0.9509\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9641\n",
      "Epoch 00142: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1037 - acc: 0.9641 - val_loss: 0.1847 - val_acc: 0.9509\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9647\n",
      "Epoch 00143: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1048 - acc: 0.9647 - val_loss: 0.1840 - val_acc: 0.9499\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9651\n",
      "Epoch 00144: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1034 - acc: 0.9651 - val_loss: 0.1794 - val_acc: 0.9509\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9649\n",
      "Epoch 00145: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1044 - acc: 0.9649 - val_loss: 0.2092 - val_acc: 0.9492\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9639\n",
      "Epoch 00146: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1009 - acc: 0.9639 - val_loss: 0.1946 - val_acc: 0.9511\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9646\n",
      "Epoch 00147: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1009 - acc: 0.9646 - val_loss: 0.1886 - val_acc: 0.9536\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9667\n",
      "Epoch 00148: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0975 - acc: 0.9667 - val_loss: 0.1814 - val_acc: 0.9522\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9674\n",
      "Epoch 00149: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0956 - acc: 0.9674 - val_loss: 0.2418 - val_acc: 0.9481\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9669\n",
      "Epoch 00150: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0960 - acc: 0.9669 - val_loss: 0.2002 - val_acc: 0.9515\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9669\n",
      "Epoch 00151: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0946 - acc: 0.9669 - val_loss: 0.1988 - val_acc: 0.9522\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9666\n",
      "Epoch 00152: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0972 - acc: 0.9666 - val_loss: 0.1901 - val_acc: 0.9534\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9670\n",
      "Epoch 00153: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0951 - acc: 0.9670 - val_loss: 0.1954 - val_acc: 0.9525\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9671\n",
      "Epoch 00154: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0946 - acc: 0.9671 - val_loss: 0.2041 - val_acc: 0.9511\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9695\n",
      "Epoch 00155: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0886 - acc: 0.9694 - val_loss: 0.2045 - val_acc: 0.9525\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9660\n",
      "Epoch 00156: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1022 - acc: 0.9659 - val_loss: 0.2227 - val_acc: 0.9495\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9679\n",
      "Epoch 00157: val_loss did not improve from 0.15937\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0925 - acc: 0.9679 - val_loss: 0.2033 - val_acc: 0.9525\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2129 - acc: 0.9402\n",
      "Loss: 0.212888934351852 Accuracy: 0.9401869\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5957 - acc: 0.1545\n",
      "Epoch 00001: val_loss improved from inf to 1.98864, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/001-1.9886.hdf5\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 2.5957 - acc: 0.1545 - val_loss: 1.9886 - val_acc: 0.4435\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9199 - acc: 0.3824\n",
      "Epoch 00002: val_loss improved from 1.98864 to 1.36057, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/002-1.3606.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.9199 - acc: 0.3825 - val_loss: 1.3606 - val_acc: 0.5938\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5782 - acc: 0.4816\n",
      "Epoch 00003: val_loss improved from 1.36057 to 1.13384, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/003-1.1338.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.5781 - acc: 0.4817 - val_loss: 1.1338 - val_acc: 0.6587\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4118 - acc: 0.5335\n",
      "Epoch 00004: val_loss improved from 1.13384 to 1.00167, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/004-1.0017.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.4118 - acc: 0.5335 - val_loss: 1.0017 - val_acc: 0.6881\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3098 - acc: 0.5677\n",
      "Epoch 00005: val_loss improved from 1.00167 to 0.94141, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/005-0.9414.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.3099 - acc: 0.5677 - val_loss: 0.9414 - val_acc: 0.6900\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2123 - acc: 0.6007\n",
      "Epoch 00006: val_loss improved from 0.94141 to 0.83091, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/006-0.8309.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.2122 - acc: 0.6007 - val_loss: 0.8309 - val_acc: 0.7263\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1241 - acc: 0.6319\n",
      "Epoch 00007: val_loss improved from 0.83091 to 0.77696, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/007-0.7770.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.1242 - acc: 0.6319 - val_loss: 0.7770 - val_acc: 0.7612\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0512 - acc: 0.6584\n",
      "Epoch 00008: val_loss improved from 0.77696 to 0.67556, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/008-0.6756.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.0512 - acc: 0.6584 - val_loss: 0.6756 - val_acc: 0.7978\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9810 - acc: 0.6794\n",
      "Epoch 00009: val_loss improved from 0.67556 to 0.62983, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/009-0.6298.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.9810 - acc: 0.6794 - val_loss: 0.6298 - val_acc: 0.8113\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9043 - acc: 0.7091\n",
      "Epoch 00010: val_loss improved from 0.62983 to 0.57334, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/010-0.5733.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.9043 - acc: 0.7091 - val_loss: 0.5733 - val_acc: 0.8146\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8445 - acc: 0.7283\n",
      "Epoch 00011: val_loss improved from 0.57334 to 0.49262, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/011-0.4926.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.8446 - acc: 0.7283 - val_loss: 0.4926 - val_acc: 0.8633\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8032 - acc: 0.7436\n",
      "Epoch 00012: val_loss improved from 0.49262 to 0.46101, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/012-0.4610.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.8032 - acc: 0.7436 - val_loss: 0.4610 - val_acc: 0.8696\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7504 - acc: 0.7615\n",
      "Epoch 00013: val_loss improved from 0.46101 to 0.42764, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/013-0.4276.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.7503 - acc: 0.7616 - val_loss: 0.4276 - val_acc: 0.8735\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7034 - acc: 0.7744\n",
      "Epoch 00014: val_loss improved from 0.42764 to 0.39035, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/014-0.3903.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.7035 - acc: 0.7744 - val_loss: 0.3903 - val_acc: 0.8866\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6725 - acc: 0.7865\n",
      "Epoch 00015: val_loss improved from 0.39035 to 0.36382, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/015-0.3638.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6725 - acc: 0.7865 - val_loss: 0.3638 - val_acc: 0.8956\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6409 - acc: 0.7962\n",
      "Epoch 00016: val_loss improved from 0.36382 to 0.33778, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/016-0.3378.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6408 - acc: 0.7963 - val_loss: 0.3378 - val_acc: 0.8998\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.8084\n",
      "Epoch 00017: val_loss improved from 0.33778 to 0.32761, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/017-0.3276.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6033 - acc: 0.8084 - val_loss: 0.3276 - val_acc: 0.9050\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5801 - acc: 0.8154\n",
      "Epoch 00018: val_loss did not improve from 0.32761\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5801 - acc: 0.8154 - val_loss: 0.3288 - val_acc: 0.9033\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.8234\n",
      "Epoch 00019: val_loss improved from 0.32761 to 0.30512, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/019-0.3051.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5601 - acc: 0.8234 - val_loss: 0.3051 - val_acc: 0.9073\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.8309\n",
      "Epoch 00020: val_loss improved from 0.30512 to 0.28979, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/020-0.2898.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5393 - acc: 0.8309 - val_loss: 0.2898 - val_acc: 0.9136\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.8382\n",
      "Epoch 00021: val_loss improved from 0.28979 to 0.27523, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/021-0.2752.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5151 - acc: 0.8381 - val_loss: 0.2752 - val_acc: 0.9175\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4996 - acc: 0.8424\n",
      "Epoch 00022: val_loss improved from 0.27523 to 0.27060, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/022-0.2706.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4996 - acc: 0.8424 - val_loss: 0.2706 - val_acc: 0.9220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8485\n",
      "Epoch 00023: val_loss improved from 0.27060 to 0.25330, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/023-0.2533.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4786 - acc: 0.8484 - val_loss: 0.2533 - val_acc: 0.9241\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4710 - acc: 0.8499\n",
      "Epoch 00024: val_loss did not improve from 0.25330\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4710 - acc: 0.8499 - val_loss: 0.2612 - val_acc: 0.9196\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8531\n",
      "Epoch 00025: val_loss did not improve from 0.25330\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4605 - acc: 0.8531 - val_loss: 0.2775 - val_acc: 0.9138\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4356 - acc: 0.8606\n",
      "Epoch 00026: val_loss improved from 0.25330 to 0.24743, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/026-0.2474.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4356 - acc: 0.8606 - val_loss: 0.2474 - val_acc: 0.9276\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.8654\n",
      "Epoch 00027: val_loss improved from 0.24743 to 0.23907, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/027-0.2391.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4268 - acc: 0.8654 - val_loss: 0.2391 - val_acc: 0.9304\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8676\n",
      "Epoch 00028: val_loss did not improve from 0.23907\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4173 - acc: 0.8676 - val_loss: 0.2532 - val_acc: 0.9255\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8717\n",
      "Epoch 00029: val_loss improved from 0.23907 to 0.22880, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/029-0.2288.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4005 - acc: 0.8717 - val_loss: 0.2288 - val_acc: 0.9331\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8743\n",
      "Epoch 00030: val_loss improved from 0.22880 to 0.22597, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/030-0.2260.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3937 - acc: 0.8743 - val_loss: 0.2260 - val_acc: 0.9331\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3813 - acc: 0.8776\n",
      "Epoch 00031: val_loss did not improve from 0.22597\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3813 - acc: 0.8776 - val_loss: 0.2272 - val_acc: 0.9331\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3720 - acc: 0.8811\n",
      "Epoch 00032: val_loss did not improve from 0.22597\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3720 - acc: 0.8811 - val_loss: 0.2325 - val_acc: 0.9322\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8821\n",
      "Epoch 00033: val_loss improved from 0.22597 to 0.22017, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/033-0.2202.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3708 - acc: 0.8821 - val_loss: 0.2202 - val_acc: 0.9341\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8881\n",
      "Epoch 00034: val_loss did not improve from 0.22017\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3546 - acc: 0.8881 - val_loss: 0.2207 - val_acc: 0.9304\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3458 - acc: 0.8909\n",
      "Epoch 00035: val_loss did not improve from 0.22017\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3458 - acc: 0.8909 - val_loss: 0.2259 - val_acc: 0.9311\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8921\n",
      "Epoch 00036: val_loss improved from 0.22017 to 0.21382, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/036-0.2138.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3415 - acc: 0.8921 - val_loss: 0.2138 - val_acc: 0.9329\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.8930\n",
      "Epoch 00037: val_loss did not improve from 0.21382\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3340 - acc: 0.8929 - val_loss: 0.2438 - val_acc: 0.9287\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8945\n",
      "Epoch 00038: val_loss did not improve from 0.21382\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3322 - acc: 0.8945 - val_loss: 0.2151 - val_acc: 0.9366\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.9002\n",
      "Epoch 00039: val_loss improved from 0.21382 to 0.19984, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/039-0.1998.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3150 - acc: 0.9002 - val_loss: 0.1998 - val_acc: 0.9432\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.8993\n",
      "Epoch 00040: val_loss did not improve from 0.19984\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3105 - acc: 0.8993 - val_loss: 0.2048 - val_acc: 0.9390\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9020\n",
      "Epoch 00041: val_loss did not improve from 0.19984\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3065 - acc: 0.9021 - val_loss: 0.2327 - val_acc: 0.9292\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9048\n",
      "Epoch 00042: val_loss did not improve from 0.19984\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2944 - acc: 0.9048 - val_loss: 0.2011 - val_acc: 0.9420\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9051\n",
      "Epoch 00043: val_loss improved from 0.19984 to 0.19611, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/043-0.1961.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2974 - acc: 0.9051 - val_loss: 0.1961 - val_acc: 0.9406\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9062\n",
      "Epoch 00044: val_loss improved from 0.19611 to 0.19513, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/044-0.1951.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2897 - acc: 0.9063 - val_loss: 0.1951 - val_acc: 0.9432\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9117\n",
      "Epoch 00045: val_loss did not improve from 0.19513\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2800 - acc: 0.9117 - val_loss: 0.2050 - val_acc: 0.9399\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9111\n",
      "Epoch 00046: val_loss did not improve from 0.19513\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2756 - acc: 0.9111 - val_loss: 0.1999 - val_acc: 0.9408\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9115\n",
      "Epoch 00047: val_loss improved from 0.19513 to 0.19432, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/047-0.1943.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2711 - acc: 0.9115 - val_loss: 0.1943 - val_acc: 0.9425\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9130\n",
      "Epoch 00048: val_loss did not improve from 0.19432\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2713 - acc: 0.9130 - val_loss: 0.2046 - val_acc: 0.9355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.9141\n",
      "Epoch 00049: val_loss improved from 0.19432 to 0.19283, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/049-0.1928.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2641 - acc: 0.9141 - val_loss: 0.1928 - val_acc: 0.9483\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9188\n",
      "Epoch 00050: val_loss improved from 0.19283 to 0.18424, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/050-0.1842.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2550 - acc: 0.9188 - val_loss: 0.1842 - val_acc: 0.9432\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9174\n",
      "Epoch 00051: val_loss improved from 0.18424 to 0.17686, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/051-0.1769.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2537 - acc: 0.9174 - val_loss: 0.1769 - val_acc: 0.9497\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9205\n",
      "Epoch 00052: val_loss did not improve from 0.17686\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2507 - acc: 0.9205 - val_loss: 0.1928 - val_acc: 0.9474\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9204\n",
      "Epoch 00053: val_loss did not improve from 0.17686\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2452 - acc: 0.9204 - val_loss: 0.1820 - val_acc: 0.9499\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9221\n",
      "Epoch 00054: val_loss did not improve from 0.17686\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2384 - acc: 0.9221 - val_loss: 0.1848 - val_acc: 0.9471\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9220\n",
      "Epoch 00055: val_loss improved from 0.17686 to 0.16734, saving model to model/checkpoint/1D_CNN_custom_4_ch_128_DO_9_conv_checkpoint/055-0.1673.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2394 - acc: 0.9220 - val_loss: 0.1673 - val_acc: 0.9513\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9243\n",
      "Epoch 00056: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2315 - acc: 0.9243 - val_loss: 0.1956 - val_acc: 0.9457\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9259\n",
      "Epoch 00057: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2215 - acc: 0.9259 - val_loss: 0.1685 - val_acc: 0.9504\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9265\n",
      "Epoch 00058: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2240 - acc: 0.9265 - val_loss: 0.1859 - val_acc: 0.9474\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9289\n",
      "Epoch 00059: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2176 - acc: 0.9289 - val_loss: 0.2100 - val_acc: 0.9371\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9282\n",
      "Epoch 00060: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2152 - acc: 0.9282 - val_loss: 0.1723 - val_acc: 0.9497\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9308\n",
      "Epoch 00061: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2116 - acc: 0.9308 - val_loss: 0.1809 - val_acc: 0.9497\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9308\n",
      "Epoch 00062: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2103 - acc: 0.9309 - val_loss: 0.1725 - val_acc: 0.9504\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9325\n",
      "Epoch 00063: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2037 - acc: 0.9325 - val_loss: 0.1872 - val_acc: 0.9471\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9339\n",
      "Epoch 00064: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2011 - acc: 0.9339 - val_loss: 0.1766 - val_acc: 0.9522\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9332\n",
      "Epoch 00065: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1988 - acc: 0.9332 - val_loss: 0.1869 - val_acc: 0.9481\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9370\n",
      "Epoch 00066: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1935 - acc: 0.9370 - val_loss: 0.1866 - val_acc: 0.9474\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9378\n",
      "Epoch 00067: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1901 - acc: 0.9378 - val_loss: 0.1951 - val_acc: 0.9474\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9364\n",
      "Epoch 00068: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1921 - acc: 0.9364 - val_loss: 0.1853 - val_acc: 0.9483\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9373\n",
      "Epoch 00069: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1887 - acc: 0.9373 - val_loss: 0.1928 - val_acc: 0.9509\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9383\n",
      "Epoch 00070: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1848 - acc: 0.9383 - val_loss: 0.1862 - val_acc: 0.9492\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9403\n",
      "Epoch 00071: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1779 - acc: 0.9403 - val_loss: 0.1957 - val_acc: 0.9474\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9420\n",
      "Epoch 00072: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1773 - acc: 0.9420 - val_loss: 0.1846 - val_acc: 0.9518\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9414\n",
      "Epoch 00073: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1800 - acc: 0.9414 - val_loss: 0.1694 - val_acc: 0.9548\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9414\n",
      "Epoch 00074: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1747 - acc: 0.9414 - val_loss: 0.1976 - val_acc: 0.9483\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9422\n",
      "Epoch 00075: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1753 - acc: 0.9422 - val_loss: 0.1824 - val_acc: 0.9520\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9423\n",
      "Epoch 00076: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1722 - acc: 0.9423 - val_loss: 0.1834 - val_acc: 0.9518\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9440\n",
      "Epoch 00077: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1671 - acc: 0.9440 - val_loss: 0.1744 - val_acc: 0.9548\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9454\n",
      "Epoch 00078: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1662 - acc: 0.9453 - val_loss: 0.1800 - val_acc: 0.9529\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9438\n",
      "Epoch 00079: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1662 - acc: 0.9438 - val_loss: 0.1802 - val_acc: 0.9522\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9470\n",
      "Epoch 00080: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1559 - acc: 0.9470 - val_loss: 0.1890 - val_acc: 0.9502\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9442\n",
      "Epoch 00081: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1640 - acc: 0.9442 - val_loss: 0.1731 - val_acc: 0.9539\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9471\n",
      "Epoch 00082: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1579 - acc: 0.9471 - val_loss: 0.1851 - val_acc: 0.9504\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9496\n",
      "Epoch 00083: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1536 - acc: 0.9496 - val_loss: 0.1887 - val_acc: 0.9536\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9494\n",
      "Epoch 00084: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1522 - acc: 0.9494 - val_loss: 0.2037 - val_acc: 0.9518\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9475\n",
      "Epoch 00085: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1560 - acc: 0.9475 - val_loss: 0.1786 - val_acc: 0.9529\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9500\n",
      "Epoch 00086: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1465 - acc: 0.9500 - val_loss: 0.1888 - val_acc: 0.9522\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9515\n",
      "Epoch 00087: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1468 - acc: 0.9515 - val_loss: 0.1760 - val_acc: 0.9539\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9520\n",
      "Epoch 00088: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1435 - acc: 0.9520 - val_loss: 0.1944 - val_acc: 0.9511\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9523\n",
      "Epoch 00089: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1431 - acc: 0.9523 - val_loss: 0.1874 - val_acc: 0.9557\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9516\n",
      "Epoch 00090: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1415 - acc: 0.9516 - val_loss: 0.1948 - val_acc: 0.9511\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9510\n",
      "Epoch 00091: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1409 - acc: 0.9510 - val_loss: 0.1879 - val_acc: 0.9529\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9519\n",
      "Epoch 00092: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1429 - acc: 0.9519 - val_loss: 0.2032 - val_acc: 0.9509\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9519\n",
      "Epoch 00093: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1403 - acc: 0.9519 - val_loss: 0.1884 - val_acc: 0.9522\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9538\n",
      "Epoch 00094: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1344 - acc: 0.9538 - val_loss: 0.1925 - val_acc: 0.9541\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9548\n",
      "Epoch 00095: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1333 - acc: 0.9548 - val_loss: 0.1967 - val_acc: 0.9513\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9559\n",
      "Epoch 00096: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1301 - acc: 0.9559 - val_loss: 0.1862 - val_acc: 0.9534\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9568\n",
      "Epoch 00097: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1282 - acc: 0.9568 - val_loss: 0.1890 - val_acc: 0.9515\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9556\n",
      "Epoch 00098: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1284 - acc: 0.9556 - val_loss: 0.2048 - val_acc: 0.9483\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9567\n",
      "Epoch 00099: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1281 - acc: 0.9567 - val_loss: 0.2022 - val_acc: 0.9506\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9579\n",
      "Epoch 00100: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1247 - acc: 0.9578 - val_loss: 0.1838 - val_acc: 0.9522\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9571\n",
      "Epoch 00101: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1233 - acc: 0.9572 - val_loss: 0.1971 - val_acc: 0.9536\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9574\n",
      "Epoch 00102: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1231 - acc: 0.9574 - val_loss: 0.2002 - val_acc: 0.9539\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9574\n",
      "Epoch 00103: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1242 - acc: 0.9575 - val_loss: 0.1934 - val_acc: 0.9550\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9589\n",
      "Epoch 00104: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1178 - acc: 0.9589 - val_loss: 0.2064 - val_acc: 0.9550\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9595\n",
      "Epoch 00105: val_loss did not improve from 0.16734\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1192 - acc: 0.9595 - val_loss: 0.1952 - val_acc: 0.9564\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2131 - acc: 0.9381\n",
      "Loss: 0.21307669344473232 Accuracy: 0.93811005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 10):\n",
    "    base = '1D_CNN_custom_4_ch_128_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_4_ch_128_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 812,112\n",
      "Trainable params: 812,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.0248 - acc: 0.6814\n",
      "Loss: 1.0247672792659495 Accuracy: 0.6814123\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 428,176\n",
      "Trainable params: 428,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.7509 - acc: 0.8012\n",
      "Loss: 0.7508955137751927 Accuracy: 0.8012461\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 313,552\n",
      "Trainable params: 313,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3874 - acc: 0.8887\n",
      "Loss: 0.38741967244806813 Accuracy: 0.88868123\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 268,016\n",
      "Trainable params: 268,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2247 - acc: 0.9385\n",
      "Loss: 0.22465223592327888 Accuracy: 0.93852544\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 266,000\n",
      "Trainable params: 266,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2129 - acc: 0.9402\n",
      "Loss: 0.212888934351852 Accuracy: 0.9401869\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 32)             5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 268,592\n",
      "Trainable params: 268,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2131 - acc: 0.9381\n",
      "Loss: 0.21307669344473232 Accuracy: 0.93811005\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_4_ch_128_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_4_ch_128_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 812,112\n",
      "Trainable params: 812,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.4899 - acc: 0.7421\n",
      "Loss: 1.4898866236271764 Accuracy: 0.7420561\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 428,176\n",
      "Trainable params: 428,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.9458 - acc: 0.8064\n",
      "Loss: 0.9457982686821174 Accuracy: 0.8064382\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 313,552\n",
      "Trainable params: 313,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.4055 - acc: 0.8999\n",
      "Loss: 0.4055377007567499 Accuracy: 0.89989614\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 268,016\n",
      "Trainable params: 268,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2406 - acc: 0.9335\n",
      "Loss: 0.24061018266596154 Accuracy: 0.933541\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 266,000\n",
      "Trainable params: 266,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2340 - acc: 0.9437\n",
      "Loss: 0.23404569132978856 Accuracy: 0.94371754\n",
      "\n",
      "1D_CNN_custom_4_ch_128_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 32)            10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 32)             5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 268,592\n",
      "Trainable params: 268,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2512 - acc: 0.9421\n",
      "Loss: 0.2511632609169743 Accuracy: 0.94205606\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
