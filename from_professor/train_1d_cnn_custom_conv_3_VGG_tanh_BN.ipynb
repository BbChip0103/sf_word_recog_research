{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, \n",
    "                      padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 1024000)           4096000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 20,493,136\n",
      "Trainable params: 18,444,880\n",
      "Non-trainable params: 2,048,256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 341312)            1365248   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 6,864,592\n",
      "Trainable params: 6,181,456\n",
      "Non-trainable params: 683,136\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 113728)            454912    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 2,338,128\n",
      "Trainable params: 2,109,904\n",
      "Non-trainable params: 228,224\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 37888)             151552    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 846,544\n",
      "Trainable params: 769,744\n",
      "Non-trainable params: 76,800\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 25216)             100864    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 668,112\n",
      "Trainable params: 616,144\n",
      "Non-trainable params: 51,968\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 8320)              33280     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 429,776\n",
      "Trainable params: 411,088\n",
      "Non-trainable params: 18,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 2688)              10752     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 416,720\n",
      "Trainable params: 408,784\n",
      "Non-trainable params: 7,936\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 480,464\n",
      "Trainable params: 475,600\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 770,256\n",
      "Trainable params: 765,136\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.1015 - acc: 0.0989\n",
      "Epoch 00001: val_loss improved from inf to 11.40214, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_1_conv_checkpoint/001-11.4021.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 11.1018 - acc: 0.0989 - val_loss: 11.4021 - val_acc: 0.0974\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.5655 - acc: 0.2978\n",
      "Epoch 00002: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 8.5653 - acc: 0.2978 - val_loss: 12.4168 - val_acc: 0.1069\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.0220 - acc: 0.4354\n",
      "Epoch 00003: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 7.0222 - acc: 0.4354 - val_loss: 13.2364 - val_acc: 0.0976\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.0606 - acc: 0.5285\n",
      "Epoch 00004: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 6.0608 - acc: 0.5284 - val_loss: 13.6841 - val_acc: 0.0983\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.5070 - acc: 0.5815\n",
      "Epoch 00005: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 5.5069 - acc: 0.5815 - val_loss: 12.8659 - val_acc: 0.1190\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.1057 - acc: 0.6215\n",
      "Epoch 00006: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 5.1060 - acc: 0.6215 - val_loss: 13.1533 - val_acc: 0.1204\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.8825 - acc: 0.6423\n",
      "Epoch 00007: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 4.8836 - acc: 0.6422 - val_loss: 13.6049 - val_acc: 0.1186\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.6496 - acc: 0.6642\n",
      "Epoch 00008: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 4.6503 - acc: 0.6642 - val_loss: 13.0729 - val_acc: 0.1435\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.5314 - acc: 0.6775\n",
      "Epoch 00009: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 4.5317 - acc: 0.6774 - val_loss: 12.9396 - val_acc: 0.1593\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.4563 - acc: 0.6836\n",
      "Epoch 00010: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 4.4565 - acc: 0.6835 - val_loss: 13.0551 - val_acc: 0.1370\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.3409 - acc: 0.6950\n",
      "Epoch 00011: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 4.3416 - acc: 0.6949 - val_loss: 12.8647 - val_acc: 0.1635\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.2097 - acc: 0.7070\n",
      "Epoch 00012: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 4.2104 - acc: 0.7069 - val_loss: 12.7186 - val_acc: 0.1686\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.1349 - acc: 0.7132\n",
      "Epoch 00013: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 4.1348 - acc: 0.7132 - val_loss: 13.1115 - val_acc: 0.1638\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.0469 - acc: 0.7229\n",
      "Epoch 00014: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 4.0474 - acc: 0.7229 - val_loss: 13.6412 - val_acc: 0.1358\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.0772 - acc: 0.7204\n",
      "Epoch 00015: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 4.0775 - acc: 0.7204 - val_loss: 12.4389 - val_acc: 0.1824\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.9013 - acc: 0.7370\n",
      "Epoch 00016: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.9013 - acc: 0.7370 - val_loss: 13.9996 - val_acc: 0.1076\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.8170 - acc: 0.7428\n",
      "Epoch 00017: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.8173 - acc: 0.7428 - val_loss: 13.6939 - val_acc: 0.1253\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.8139 - acc: 0.7437\n",
      "Epoch 00018: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.8134 - acc: 0.7437 - val_loss: 13.5266 - val_acc: 0.1447\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.7519 - acc: 0.7478\n",
      "Epoch 00019: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.7518 - acc: 0.7478 - val_loss: 13.1105 - val_acc: 0.1582\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.6952 - acc: 0.7539\n",
      "Epoch 00020: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.6958 - acc: 0.7538 - val_loss: 13.1325 - val_acc: 0.1561\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.7726 - acc: 0.7472\n",
      "Epoch 00021: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.7730 - acc: 0.7472 - val_loss: 13.2240 - val_acc: 0.1528\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.6653 - acc: 0.7563\n",
      "Epoch 00022: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.6653 - acc: 0.7563 - val_loss: 12.5593 - val_acc: 0.1840\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.6251 - acc: 0.7587\n",
      "Epoch 00023: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.6253 - acc: 0.7587 - val_loss: 13.1169 - val_acc: 0.1661\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.5542 - acc: 0.7651\n",
      "Epoch 00024: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.5542 - acc: 0.7651 - val_loss: 12.4283 - val_acc: 0.1959\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.5261 - acc: 0.7673\n",
      "Epoch 00025: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.5261 - acc: 0.7673 - val_loss: 12.9969 - val_acc: 0.1649\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4732 - acc: 0.7711\n",
      "Epoch 00026: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.4732 - acc: 0.7711 - val_loss: 12.9403 - val_acc: 0.1672\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4582 - acc: 0.7725\n",
      "Epoch 00027: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.4579 - acc: 0.7725 - val_loss: 13.5728 - val_acc: 0.1440\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4126 - acc: 0.7760\n",
      "Epoch 00028: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.4126 - acc: 0.7760 - val_loss: 13.5801 - val_acc: 0.1363\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3557 - acc: 0.7798\n",
      "Epoch 00029: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.3562 - acc: 0.7797 - val_loss: 13.2965 - val_acc: 0.1512\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3469 - acc: 0.7809\n",
      "Epoch 00030: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.3477 - acc: 0.7809 - val_loss: 12.2936 - val_acc: 0.2092\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3207 - acc: 0.7829\n",
      "Epoch 00031: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.3206 - acc: 0.7829 - val_loss: 12.2062 - val_acc: 0.2148\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2866 - acc: 0.7859\n",
      "Epoch 00032: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.2862 - acc: 0.7859 - val_loss: 13.0268 - val_acc: 0.1756\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2504 - acc: 0.7882\n",
      "Epoch 00033: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.2508 - acc: 0.7881 - val_loss: 12.5965 - val_acc: 0.1973\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2122 - acc: 0.7906\n",
      "Epoch 00034: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.2123 - acc: 0.7906 - val_loss: 12.7035 - val_acc: 0.1898\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2260 - acc: 0.7902\n",
      "Epoch 00035: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.2256 - acc: 0.7902 - val_loss: 13.2338 - val_acc: 0.1624\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2079 - acc: 0.7911\n",
      "Epoch 00036: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.2078 - acc: 0.7910 - val_loss: 12.9655 - val_acc: 0.1796\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2027 - acc: 0.7914\n",
      "Epoch 00037: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.2033 - acc: 0.7913 - val_loss: 12.4711 - val_acc: 0.2008\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1677 - acc: 0.7946\n",
      "Epoch 00038: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.1679 - acc: 0.7946 - val_loss: 13.0473 - val_acc: 0.1700\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1587 - acc: 0.7953\n",
      "Epoch 00039: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.1586 - acc: 0.7953 - val_loss: 12.8371 - val_acc: 0.1822\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2084 - acc: 0.7916\n",
      "Epoch 00040: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.2088 - acc: 0.7916 - val_loss: 12.2628 - val_acc: 0.2138\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0601 - acc: 0.8029\n",
      "Epoch 00041: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.0597 - acc: 0.8029 - val_loss: 12.4715 - val_acc: 0.2015\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0830 - acc: 0.8011\n",
      "Epoch 00042: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.0830 - acc: 0.8011 - val_loss: 12.8539 - val_acc: 0.1826\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0856 - acc: 0.8003\n",
      "Epoch 00043: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.0856 - acc: 0.8003 - val_loss: 13.3063 - val_acc: 0.1598\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0417 - acc: 0.8040\n",
      "Epoch 00044: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.0426 - acc: 0.8039 - val_loss: 12.6337 - val_acc: 0.1940\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0105 - acc: 0.8068\n",
      "Epoch 00045: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.0108 - acc: 0.8067 - val_loss: 12.2119 - val_acc: 0.2206\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0124 - acc: 0.8061\n",
      "Epoch 00046: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.0124 - acc: 0.8061 - val_loss: 11.9231 - val_acc: 0.2367\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9734 - acc: 0.8089\n",
      "Epoch 00047: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.9730 - acc: 0.8089 - val_loss: 12.6437 - val_acc: 0.1973\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9947 - acc: 0.8076\n",
      "Epoch 00048: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.9956 - acc: 0.8075 - val_loss: 12.7225 - val_acc: 0.1989\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9608 - acc: 0.8101\n",
      "Epoch 00049: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.9608 - acc: 0.8101 - val_loss: 12.1691 - val_acc: 0.2257\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0237 - acc: 0.8038\n",
      "Epoch 00050: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 3.0246 - acc: 0.8038 - val_loss: 12.2602 - val_acc: 0.2169\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9738 - acc: 0.8083\n",
      "Epoch 00051: val_loss did not improve from 11.40214\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.9734 - acc: 0.8083 - val_loss: 11.9103 - val_acc: 0.2383\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 858us/sample - loss: 11.2598 - acc: 0.1047\n",
      "Loss: 11.25984339788324 Accuracy: 0.104672894\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.8382 - acc: 0.2294\n",
      "Epoch 00001: val_loss improved from inf to 10.68608, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_2_conv_checkpoint/001-10.6861.hdf5\n",
      "36805/36805 [==============================] - 140s 4ms/sample - loss: 4.8382 - acc: 0.2293 - val_loss: 10.6861 - val_acc: 0.0955\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1414 - acc: 0.4618\n",
      "Epoch 00002: val_loss improved from 10.68608 to 9.67387, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_2_conv_checkpoint/002-9.6739.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 3.1417 - acc: 0.4618 - val_loss: 9.6739 - val_acc: 0.1826\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8326 - acc: 0.6662\n",
      "Epoch 00003: val_loss did not improve from 9.67387\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 1.8324 - acc: 0.6662 - val_loss: 10.2706 - val_acc: 0.2213\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0088 - acc: 0.8114\n",
      "Epoch 00004: val_loss improved from 9.67387 to 7.81844, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_2_conv_checkpoint/004-7.8184.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 1.0092 - acc: 0.8114 - val_loss: 7.8184 - val_acc: 0.2779\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7053 - acc: 0.8760\n",
      "Epoch 00005: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.7055 - acc: 0.8760 - val_loss: 9.0760 - val_acc: 0.2159\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5861 - acc: 0.8980\n",
      "Epoch 00006: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.5863 - acc: 0.8980 - val_loss: 8.5707 - val_acc: 0.2602\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5771 - acc: 0.9032\n",
      "Epoch 00007: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.5776 - acc: 0.9032 - val_loss: 9.6237 - val_acc: 0.2332\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.9124\n",
      "Epoch 00008: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.5485 - acc: 0.9123 - val_loss: 9.2678 - val_acc: 0.2548\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.9133\n",
      "Epoch 00009: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.5531 - acc: 0.9133 - val_loss: 8.6880 - val_acc: 0.3010\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.9236\n",
      "Epoch 00010: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.5031 - acc: 0.9236 - val_loss: 8.8471 - val_acc: 0.2961\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4813 - acc: 0.9291\n",
      "Epoch 00011: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.4818 - acc: 0.9291 - val_loss: 10.3593 - val_acc: 0.2544\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.9381\n",
      "Epoch 00012: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.4543 - acc: 0.9381 - val_loss: 12.1609 - val_acc: 0.1749\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.9474\n",
      "Epoch 00013: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.3865 - acc: 0.9474 - val_loss: 12.1007 - val_acc: 0.1971\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.9464\n",
      "Epoch 00014: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.3928 - acc: 0.9464 - val_loss: 11.0377 - val_acc: 0.2078\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.9484\n",
      "Epoch 00015: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.3858 - acc: 0.9483 - val_loss: 12.4634 - val_acc: 0.1551\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.9508\n",
      "Epoch 00016: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3771 - acc: 0.9508 - val_loss: 11.1039 - val_acc: 0.2350\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.9489\n",
      "Epoch 00017: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.4051 - acc: 0.9489 - val_loss: 10.2491 - val_acc: 0.2711\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.9550\n",
      "Epoch 00018: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3603 - acc: 0.9550 - val_loss: 10.4706 - val_acc: 0.2621\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3412 - acc: 0.9585\n",
      "Epoch 00019: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3413 - acc: 0.9585 - val_loss: 10.0637 - val_acc: 0.2611\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.9575\n",
      "Epoch 00020: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.3542 - acc: 0.9575 - val_loss: 12.6756 - val_acc: 0.1808\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.9596\n",
      "Epoch 00021: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.3424 - acc: 0.9596 - val_loss: 10.4981 - val_acc: 0.2546\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.9621\n",
      "Epoch 00022: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3267 - acc: 0.9620 - val_loss: 9.7219 - val_acc: 0.2907\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3505 - acc: 0.9586\n",
      "Epoch 00023: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.3505 - acc: 0.9586 - val_loss: 9.0227 - val_acc: 0.3205\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9642\n",
      "Epoch 00024: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3129 - acc: 0.9642 - val_loss: 11.8121 - val_acc: 0.2010\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9674\n",
      "Epoch 00025: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2973 - acc: 0.9674 - val_loss: 10.5481 - val_acc: 0.2667\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9689\n",
      "Epoch 00026: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2808 - acc: 0.9689 - val_loss: 10.3016 - val_acc: 0.2879\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9707\n",
      "Epoch 00027: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2749 - acc: 0.9706 - val_loss: 9.9865 - val_acc: 0.2956\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.9635\n",
      "Epoch 00028: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3226 - acc: 0.9635 - val_loss: 8.5666 - val_acc: 0.3627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9704\n",
      "Epoch 00029: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2768 - acc: 0.9703 - val_loss: 9.7426 - val_acc: 0.3119\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9717\n",
      "Epoch 00030: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2645 - acc: 0.9717 - val_loss: 10.0306 - val_acc: 0.2900\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.9713\n",
      "Epoch 00031: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2697 - acc: 0.9713 - val_loss: 10.5573 - val_acc: 0.2690\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9765\n",
      "Epoch 00032: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2345 - acc: 0.9765 - val_loss: 8.7870 - val_acc: 0.3671\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9705\n",
      "Epoch 00033: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2747 - acc: 0.9705 - val_loss: 9.8384 - val_acc: 0.3019\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2683 - acc: 0.9710\n",
      "Epoch 00034: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2687 - acc: 0.9710 - val_loss: 9.0177 - val_acc: 0.3378\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2633 - acc: 0.9728\n",
      "Epoch 00035: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2639 - acc: 0.9727 - val_loss: 11.4567 - val_acc: 0.2290\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9682\n",
      "Epoch 00036: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3060 - acc: 0.9682 - val_loss: 10.3484 - val_acc: 0.2683\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9760\n",
      "Epoch 00037: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2467 - acc: 0.9760 - val_loss: 11.4868 - val_acc: 0.2266\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9725\n",
      "Epoch 00038: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2665 - acc: 0.9725 - val_loss: 9.1438 - val_acc: 0.3457\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9770\n",
      "Epoch 00039: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2312 - acc: 0.9770 - val_loss: 10.1903 - val_acc: 0.2965\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9801\n",
      "Epoch 00040: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2057 - acc: 0.9801 - val_loss: 9.5131 - val_acc: 0.3298\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9742\n",
      "Epoch 00041: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2606 - acc: 0.9741 - val_loss: 10.8949 - val_acc: 0.2674\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9770\n",
      "Epoch 00042: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2311 - acc: 0.9770 - val_loss: 8.4412 - val_acc: 0.3885\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9795\n",
      "Epoch 00043: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2151 - acc: 0.9795 - val_loss: 10.0385 - val_acc: 0.3084\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9730\n",
      "Epoch 00044: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2622 - acc: 0.9730 - val_loss: 11.2916 - val_acc: 0.2483\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9767\n",
      "Epoch 00045: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2394 - acc: 0.9767 - val_loss: 9.2725 - val_acc: 0.3583\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9757\n",
      "Epoch 00046: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2479 - acc: 0.9757 - val_loss: 10.3990 - val_acc: 0.2865\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9767\n",
      "Epoch 00047: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2428 - acc: 0.9766 - val_loss: 9.6495 - val_acc: 0.3284\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.9712\n",
      "Epoch 00048: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2900 - acc: 0.9712 - val_loss: 11.8018 - val_acc: 0.2197\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9782\n",
      "Epoch 00049: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2344 - acc: 0.9782 - val_loss: 11.5956 - val_acc: 0.2378\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9804\n",
      "Epoch 00050: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2075 - acc: 0.9804 - val_loss: 9.2163 - val_acc: 0.3517\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9799\n",
      "Epoch 00051: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2182 - acc: 0.9798 - val_loss: 10.1791 - val_acc: 0.3145\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9803\n",
      "Epoch 00052: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2139 - acc: 0.9803 - val_loss: 8.8468 - val_acc: 0.3715\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9807\n",
      "Epoch 00053: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2065 - acc: 0.9807 - val_loss: 9.6446 - val_acc: 0.3298\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9811\n",
      "Epoch 00054: val_loss did not improve from 7.81844\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.2109 - acc: 0.9810 - val_loss: 9.5134 - val_acc: 0.3315\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 8.1143 - acc: 0.2692\n",
      "Loss: 8.11433346610698 Accuracy: 0.26915887\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5398 - acc: 0.3605\n",
      "Epoch 00001: val_loss improved from inf to 5.41025, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_3_conv_checkpoint/001-5.4102.hdf5\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 2.5401 - acc: 0.3605 - val_loss: 5.4102 - val_acc: 0.2313\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0479 - acc: 0.7041\n",
      "Epoch 00002: val_loss did not improve from 5.41025\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 1.0481 - acc: 0.7041 - val_loss: 7.5936 - val_acc: 0.2229\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8995\n",
      "Epoch 00003: val_loss did not improve from 5.41025\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.3809 - acc: 0.8994 - val_loss: 5.9048 - val_acc: 0.2655\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9582\n",
      "Epoch 00004: val_loss improved from 5.41025 to 5.27931, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_3_conv_checkpoint/004-5.2793.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.1966 - acc: 0.9582 - val_loss: 5.2793 - val_acc: 0.2688\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9830\n",
      "Epoch 00005: val_loss improved from 5.27931 to 4.05167, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_3_conv_checkpoint/005-4.0517.hdf5\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.1041 - acc: 0.9830 - val_loss: 4.0517 - val_acc: 0.3510\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9907\n",
      "Epoch 00006: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0753 - acc: 0.9907 - val_loss: 4.3526 - val_acc: 0.3482\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9864\n",
      "Epoch 00007: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0861 - acc: 0.9863 - val_loss: 4.5517 - val_acc: 0.3282\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9868\n",
      "Epoch 00008: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0779 - acc: 0.9868 - val_loss: 5.8112 - val_acc: 0.3187\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9890\n",
      "Epoch 00009: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0637 - acc: 0.9890 - val_loss: 8.3102 - val_acc: 0.2194\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9845\n",
      "Epoch 00010: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0738 - acc: 0.9845 - val_loss: 6.0229 - val_acc: 0.3310\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9775\n",
      "Epoch 00011: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0959 - acc: 0.9774 - val_loss: 7.3187 - val_acc: 0.2853\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9802\n",
      "Epoch 00012: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0784 - acc: 0.9802 - val_loss: 7.4385 - val_acc: 0.2961\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9883\n",
      "Epoch 00013: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0580 - acc: 0.9883 - val_loss: 9.4596 - val_acc: 0.2259\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9873\n",
      "Epoch 00014: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0596 - acc: 0.9873 - val_loss: 6.5871 - val_acc: 0.2870\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9883\n",
      "Epoch 00015: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0543 - acc: 0.9882 - val_loss: 9.5080 - val_acc: 0.1880\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9808\n",
      "Epoch 00016: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0771 - acc: 0.9808 - val_loss: 10.1852 - val_acc: 0.2325\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9880\n",
      "Epoch 00017: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0528 - acc: 0.9880 - val_loss: 5.4014 - val_acc: 0.3853\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9908\n",
      "Epoch 00018: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0430 - acc: 0.9908 - val_loss: 5.6032 - val_acc: 0.3799\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9916\n",
      "Epoch 00019: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0448 - acc: 0.9916 - val_loss: 7.2892 - val_acc: 0.3229\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9907\n",
      "Epoch 00020: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0462 - acc: 0.9906 - val_loss: 7.5296 - val_acc: 0.3124\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9851\n",
      "Epoch 00021: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0577 - acc: 0.9851 - val_loss: 5.8143 - val_acc: 0.4104\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9943\n",
      "Epoch 00022: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0327 - acc: 0.9943 - val_loss: 5.2290 - val_acc: 0.4330\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9722\n",
      "Epoch 00023: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.1248 - acc: 0.9722 - val_loss: 8.6976 - val_acc: 0.2793\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9923\n",
      "Epoch 00024: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0415 - acc: 0.9923 - val_loss: 6.5642 - val_acc: 0.3625\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9957\n",
      "Epoch 00025: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0290 - acc: 0.9956 - val_loss: 10.5638 - val_acc: 0.2187\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9875\n",
      "Epoch 00026: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0566 - acc: 0.9874 - val_loss: 8.4640 - val_acc: 0.3056\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9864\n",
      "Epoch 00027: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0638 - acc: 0.9863 - val_loss: 6.4369 - val_acc: 0.3892\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9922\n",
      "Epoch 00028: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0375 - acc: 0.9922 - val_loss: 7.5732 - val_acc: 0.3443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9898\n",
      "Epoch 00029: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0472 - acc: 0.9898 - val_loss: 6.8349 - val_acc: 0.3447\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9911\n",
      "Epoch 00030: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0439 - acc: 0.9911 - val_loss: 5.6858 - val_acc: 0.4104\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9939\n",
      "Epoch 00031: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0353 - acc: 0.9939 - val_loss: 6.4654 - val_acc: 0.3965\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9872\n",
      "Epoch 00032: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0619 - acc: 0.9872 - val_loss: 5.0837 - val_acc: 0.4328\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9954\n",
      "Epoch 00033: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0295 - acc: 0.9954 - val_loss: 7.4182 - val_acc: 0.3175\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9906\n",
      "Epoch 00034: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0461 - acc: 0.9905 - val_loss: 6.1986 - val_acc: 0.4065\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9917\n",
      "Epoch 00035: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0406 - acc: 0.9917 - val_loss: 4.7597 - val_acc: 0.4715\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9917\n",
      "Epoch 00036: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0432 - acc: 0.9917 - val_loss: 5.9446 - val_acc: 0.4014\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9896\n",
      "Epoch 00037: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0485 - acc: 0.9895 - val_loss: 6.7283 - val_acc: 0.3627\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9914\n",
      "Epoch 00038: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0429 - acc: 0.9914 - val_loss: 4.5220 - val_acc: 0.4843\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9972\n",
      "Epoch 00039: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0251 - acc: 0.9972 - val_loss: 4.4264 - val_acc: 0.4945\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9914\n",
      "Epoch 00040: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0481 - acc: 0.9914 - val_loss: 5.4947 - val_acc: 0.4295\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9917\n",
      "Epoch 00041: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0439 - acc: 0.9916 - val_loss: 7.0714 - val_acc: 0.3564\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9905\n",
      "Epoch 00042: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0465 - acc: 0.9905 - val_loss: 5.6425 - val_acc: 0.4326\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9922\n",
      "Epoch 00043: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0408 - acc: 0.9922 - val_loss: 5.4870 - val_acc: 0.4393\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9944\n",
      "Epoch 00044: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0320 - acc: 0.9944 - val_loss: 7.0312 - val_acc: 0.3762\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9950\n",
      "Epoch 00045: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0314 - acc: 0.9950 - val_loss: 6.6910 - val_acc: 0.3832\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9906\n",
      "Epoch 00046: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0451 - acc: 0.9906 - val_loss: 6.6808 - val_acc: 0.3923\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9928\n",
      "Epoch 00047: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0390 - acc: 0.9928 - val_loss: 7.1726 - val_acc: 0.3769\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9934\n",
      "Epoch 00048: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0339 - acc: 0.9934 - val_loss: 7.3755 - val_acc: 0.3312\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9928\n",
      "Epoch 00049: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0360 - acc: 0.9928 - val_loss: 6.7403 - val_acc: 0.4062\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9921\n",
      "Epoch 00050: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0399 - acc: 0.9921 - val_loss: 5.2576 - val_acc: 0.4677\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9918\n",
      "Epoch 00051: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0417 - acc: 0.9917 - val_loss: 5.8014 - val_acc: 0.4559\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9924\n",
      "Epoch 00052: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0393 - acc: 0.9924 - val_loss: 5.5550 - val_acc: 0.4535\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9960\n",
      "Epoch 00053: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0266 - acc: 0.9960 - val_loss: 5.1275 - val_acc: 0.4717\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9969\n",
      "Epoch 00054: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0241 - acc: 0.9968 - val_loss: 5.4681 - val_acc: 0.4503\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9884\n",
      "Epoch 00055: val_loss did not improve from 4.05167\n",
      "36805/36805 [==============================] - 152s 4ms/sample - loss: 0.0536 - acc: 0.9884 - val_loss: 9.8190 - val_acc: 0.2798\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 4.3486 - acc: 0.3192\n",
      "Loss: 4.348554058114564 Accuracy: 0.3192108\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0537 - acc: 0.4039\n",
      "Epoch 00001: val_loss improved from inf to 2.55899, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_4_conv_checkpoint/001-2.5590.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 2.0536 - acc: 0.4039 - val_loss: 2.5590 - val_acc: 0.3410\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2404 - acc: 0.6282\n",
      "Epoch 00002: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 1.2405 - acc: 0.6281 - val_loss: 4.3767 - val_acc: 0.2830\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8394 - acc: 0.7475\n",
      "Epoch 00003: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.8395 - acc: 0.7475 - val_loss: 3.8109 - val_acc: 0.3093\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.8382\n",
      "Epoch 00004: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.5585 - acc: 0.8381 - val_loss: 7.1612 - val_acc: 0.2106\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.9025\n",
      "Epoch 00005: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.3773 - acc: 0.9024 - val_loss: 4.7399 - val_acc: 0.3170\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9476\n",
      "Epoch 00006: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2403 - acc: 0.9476 - val_loss: 6.8787 - val_acc: 0.2448\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9689\n",
      "Epoch 00007: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1685 - acc: 0.9689 - val_loss: 3.7881 - val_acc: 0.4174\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9785\n",
      "Epoch 00008: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1319 - acc: 0.9784 - val_loss: 4.5958 - val_acc: 0.3797\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9850\n",
      "Epoch 00009: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1006 - acc: 0.9850 - val_loss: 5.3545 - val_acc: 0.3692\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9904\n",
      "Epoch 00010: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0794 - acc: 0.9904 - val_loss: 4.7777 - val_acc: 0.3545\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9911\n",
      "Epoch 00011: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0682 - acc: 0.9911 - val_loss: 4.3848 - val_acc: 0.3829\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9893\n",
      "Epoch 00012: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0725 - acc: 0.9893 - val_loss: 3.0823 - val_acc: 0.4712\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9938\n",
      "Epoch 00013: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0516 - acc: 0.9938 - val_loss: 3.5078 - val_acc: 0.4379\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9912\n",
      "Epoch 00014: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0561 - acc: 0.9912 - val_loss: 5.9653 - val_acc: 0.3473\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9901\n",
      "Epoch 00015: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0576 - acc: 0.9901 - val_loss: 3.1177 - val_acc: 0.4752\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9965\n",
      "Epoch 00016: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0338 - acc: 0.9965 - val_loss: 5.5788 - val_acc: 0.3345\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9939\n",
      "Epoch 00017: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0410 - acc: 0.9939 - val_loss: 3.3996 - val_acc: 0.4416\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9944\n",
      "Epoch 00018: val_loss did not improve from 2.55899\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0381 - acc: 0.9944 - val_loss: 2.6928 - val_acc: 0.5250\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9946\n",
      "Epoch 00019: val_loss improved from 2.55899 to 2.40248, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_4_conv_checkpoint/019-2.4025.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0324 - acc: 0.9945 - val_loss: 2.4025 - val_acc: 0.5313\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9946\n",
      "Epoch 00020: val_loss did not improve from 2.40248\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0355 - acc: 0.9946 - val_loss: 2.6871 - val_acc: 0.5204\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9949\n",
      "Epoch 00021: val_loss did not improve from 2.40248\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0343 - acc: 0.9948 - val_loss: 5.3515 - val_acc: 0.3620\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9896\n",
      "Epoch 00022: val_loss did not improve from 2.40248\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0475 - acc: 0.9896 - val_loss: 2.8745 - val_acc: 0.5246\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9954\n",
      "Epoch 00023: val_loss improved from 2.40248 to 2.19420, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_4_conv_checkpoint/023-2.1942.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0270 - acc: 0.9954 - val_loss: 2.1942 - val_acc: 0.5875\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9979\n",
      "Epoch 00024: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0196 - acc: 0.9978 - val_loss: 2.4176 - val_acc: 0.5660\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9887\n",
      "Epoch 00025: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0488 - acc: 0.9887 - val_loss: 3.2305 - val_acc: 0.5253\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9950\n",
      "Epoch 00026: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0282 - acc: 0.9950 - val_loss: 3.1635 - val_acc: 0.5080\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9982\n",
      "Epoch 00027: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0150 - acc: 0.9981 - val_loss: 2.2556 - val_acc: 0.5879\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9939\n",
      "Epoch 00028: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0319 - acc: 0.9938 - val_loss: 2.7233 - val_acc: 0.5288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9925\n",
      "Epoch 00029: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0344 - acc: 0.9925 - val_loss: 3.2303 - val_acc: 0.5052\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9920\n",
      "Epoch 00030: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0349 - acc: 0.9920 - val_loss: 2.2645 - val_acc: 0.5905\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9957\n",
      "Epoch 00031: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0244 - acc: 0.9956 - val_loss: 2.7049 - val_acc: 0.5514\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9970\n",
      "Epoch 00032: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0192 - acc: 0.9970 - val_loss: 2.3685 - val_acc: 0.5840\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9971\n",
      "Epoch 00033: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0169 - acc: 0.9971 - val_loss: 2.7772 - val_acc: 0.5504\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9940\n",
      "Epoch 00034: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0286 - acc: 0.9940 - val_loss: 2.9254 - val_acc: 0.5215\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9974\n",
      "Epoch 00035: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0178 - acc: 0.9974 - val_loss: 2.6232 - val_acc: 0.5700\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9976\n",
      "Epoch 00036: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0156 - acc: 0.9975 - val_loss: 2.7363 - val_acc: 0.5556\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9895\n",
      "Epoch 00037: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0419 - acc: 0.9895 - val_loss: 3.0847 - val_acc: 0.5351\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9984\n",
      "Epoch 00038: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0121 - acc: 0.9984 - val_loss: 2.3556 - val_acc: 0.6000\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9994\n",
      "Epoch 00039: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0083 - acc: 0.9994 - val_loss: 2.9033 - val_acc: 0.5567\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9912\n",
      "Epoch 00040: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0381 - acc: 0.9912 - val_loss: 6.2444 - val_acc: 0.3792\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9919\n",
      "Epoch 00041: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0332 - acc: 0.9919 - val_loss: 3.7594 - val_acc: 0.5013\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9990\n",
      "Epoch 00042: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0094 - acc: 0.9990 - val_loss: 2.4959 - val_acc: 0.5961\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9944\n",
      "Epoch 00043: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0237 - acc: 0.9943 - val_loss: 3.7055 - val_acc: 0.4908\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9917\n",
      "Epoch 00044: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0333 - acc: 0.9917 - val_loss: 2.7473 - val_acc: 0.5698\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9986\n",
      "Epoch 00045: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0112 - acc: 0.9986 - val_loss: 2.5879 - val_acc: 0.5863\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9989\n",
      "Epoch 00046: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0101 - acc: 0.9989 - val_loss: 3.0562 - val_acc: 0.5439\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9964\n",
      "Epoch 00047: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0202 - acc: 0.9963 - val_loss: 3.3525 - val_acc: 0.5227\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9894\n",
      "Epoch 00048: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0393 - acc: 0.9894 - val_loss: 2.7119 - val_acc: 0.5795\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9985\n",
      "Epoch 00049: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0109 - acc: 0.9985 - val_loss: 2.5820 - val_acc: 0.5928\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9949\n",
      "Epoch 00050: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0226 - acc: 0.9949 - val_loss: 2.6956 - val_acc: 0.5847\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9985\n",
      "Epoch 00051: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0110 - acc: 0.9985 - val_loss: 2.7962 - val_acc: 0.5658\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9992\n",
      "Epoch 00052: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0082 - acc: 0.9992 - val_loss: 3.3296 - val_acc: 0.5351\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9926\n",
      "Epoch 00053: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0300 - acc: 0.9926 - val_loss: 2.7360 - val_acc: 0.5816\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9976\n",
      "Epoch 00054: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0131 - acc: 0.9975 - val_loss: 2.6552 - val_acc: 0.5959\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9932\n",
      "Epoch 00055: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0268 - acc: 0.9932 - val_loss: 2.8190 - val_acc: 0.5854\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9956\n",
      "Epoch 00056: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0188 - acc: 0.9955 - val_loss: 3.0005 - val_acc: 0.5623\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9893\n",
      "Epoch 00057: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0449 - acc: 0.9893 - val_loss: 2.6828 - val_acc: 0.5970\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9951\n",
      "Epoch 00058: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0224 - acc: 0.9950 - val_loss: 3.0555 - val_acc: 0.5609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9955\n",
      "Epoch 00059: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0193 - acc: 0.9955 - val_loss: 2.5573 - val_acc: 0.6031\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9995\n",
      "Epoch 00060: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0060 - acc: 0.9995 - val_loss: 2.5460 - val_acc: 0.6115\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9988\n",
      "Epoch 00061: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0077 - acc: 0.9988 - val_loss: 2.9768 - val_acc: 0.5728\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9983\n",
      "Epoch 00062: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0100 - acc: 0.9983 - val_loss: 3.5259 - val_acc: 0.5115\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9954\n",
      "Epoch 00063: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0214 - acc: 0.9954 - val_loss: 3.1921 - val_acc: 0.5563\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9978\n",
      "Epoch 00064: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0117 - acc: 0.9978 - val_loss: 3.0539 - val_acc: 0.5653\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9985\n",
      "Epoch 00065: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0100 - acc: 0.9985 - val_loss: 2.8925 - val_acc: 0.5837\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9927\n",
      "Epoch 00066: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0269 - acc: 0.9927 - val_loss: 2.8733 - val_acc: 0.5856\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9990\n",
      "Epoch 00067: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0073 - acc: 0.9990 - val_loss: 2.7424 - val_acc: 0.6040\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9965\n",
      "Epoch 00068: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0158 - acc: 0.9965 - val_loss: 3.0460 - val_acc: 0.5714\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9966\n",
      "Epoch 00069: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0162 - acc: 0.9965 - val_loss: 3.8919 - val_acc: 0.5076\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9924\n",
      "Epoch 00070: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0266 - acc: 0.9924 - val_loss: 2.6870 - val_acc: 0.6042\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9990\n",
      "Epoch 00071: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0085 - acc: 0.9990 - val_loss: 2.8263 - val_acc: 0.5926\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9996\n",
      "Epoch 00072: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0056 - acc: 0.9995 - val_loss: 2.6376 - val_acc: 0.6117\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9929\n",
      "Epoch 00073: val_loss did not improve from 2.19420\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0255 - acc: 0.9929 - val_loss: 3.0045 - val_acc: 0.5700\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXl4FEX6x781R+47QAIEEi4RQkiAABEElBtRPPFExYvV9dbVdXFVXHW9Vxd/rIqK4okKooIcooJ4gBAgHAoIgQBJCDlIJpmck5n398dLZyaTOXommWSS1Od5+umZ7uqu6p6eb7/1VtVbgoggkUgkko6Ppq0LIJFIJJLWQQq+RCKRdBKk4EskEkknQQq+RCKRdBKk4EskEkknQQq+RCKRdBKk4EskEkknQQq+RCKRdBKk4EskEkknQdfWBbClS5culJSU1NbFkEgkknbDjh07iomoq5q0fiX4SUlJyMzMbOtiSCQSSbtBCHFMbVrp0pFIJJJOghR8iUQi6SRIwZdIJJJOgl/58B1hMpmQm5uLmpqati5KuyQoKAgJCQnQ6/VtXRSJRNLG+L3g5+bmIjw8HElJSRBCtHVx2hVEhJKSEuTm5qJPnz5tXRyJRNLG+L1Lp6amBrGxsVLsvUAIgdjYWFk7kkgkANqB4AOQYt8M5L2TSCQK7ULwWxyLBSguBuT0jhKJpBPROQW/ogLIyQGMRrdJy8rK8L///c+rbC644AKUlZWpTr9gwQK89NJLXuUlkUgk7vCZ4AshBgohsmyWciHEfb7KzyPM5sZrF7gS/Pr6epfHrlmzBlFRUR4XTyKRSHyBzwSfiA4SURoRpQEYAaAKwEpf5ecRFguv3Qg2ADzyyCPIzs5GWloaHnroIWzatAnjxo3DrFmzMHjwYADAJZdcghEjRiA5ORmLFy9uODYpKQnFxcXIycnBoEGDcNtttyE5ORlTp05FdXW1y3yzsrKQkZGBoUOH4tJLL0VpaSkAYOHChRg8eDCGDh2Kq6++GgDw448/Ii0tDWlpaRg2bBgqKiq8uSsSiaSD01rdMicByCYi1TEfHHHo0H0wGrOaXxpTHVBTCxwJRFjpKAwY8KrTpM899xz27duHrCzOd9OmTdi5cyf27dvX0NVxyZIliImJQXV1NUaOHInLL78csbGxdmU/hE8++QRvvfUWrrzySqxYsQJz5sxxmu8NN9yA1157DRMmTMDjjz+OJ598Eq+++iqee+45HD16FIGBgQ3uopdeegmLFi3C2LFjYTQaERQU1Nw7JJFIOiCt5cO/GsAnrZSXe8hu7SGjRo1q1K994cKFSE1NRUZGBk6cOIFDhw41OaZPnz5IS0sDAIwYMQI5OTlOz28wGFBWVoYJEyYAAG688UZs3rwZADB06FBcd911+PDDD6HT8ft67NixeOCBB7Bw4UKUlZU1bJdIJBJbfK4MQogAALMA/MPJ/nkA5gFA7969XZ7LlSXuEXl5wMmTQFwc0KuXx4eHhoY2fN60aRO+++47bNmyBSEhITjvvPMc9nsPDAxs+KzVat26dJzxzTffYPPmzVi1ahWeeeYZ7N27F4888ghmzpyJNWvWYOzYsVi/fj3OPvtsr84vkUg6Lq1h4c8AsJOITjnaSUSLiSidiNK7dlUV0rn5KI21Knz44eHhLn3iBoMB0dHRCAkJwYEDB7B169ZmFy8yMhLR0dH46aefAAAffPABJkyYAIvFghMnTuD888/H888/D4PBAKPRiOzsbKSkpODvf/87Ro4ciQMHDjS7DBKJpOPRGnX/a+BP7hzA2mjrqpdOURGg1yM2NhZjx47FkCFDMGPGDMycObNRsunTp+ONN97AoEGDMHDgQGRkZLRIEZcuXYrbb78dVVVV6Nu3L959912YzWbMmTMHBoMBRIR77rkHUVFReOyxx7Bx40ZoNBokJydjxowZLVIGiUTSsRDkw8FHQohQAMcB9CUig7v06enpZD8Byv79+zFo0KCWLVh2NlBaCoSHAwMHOk6zdy8QFAQMGNCyebcBPrmHEonELxBC7CCidDVpfWrhE1ElgFi3CVsbNRZ+fb2qfvoSiUTSXuicI23d9cMnYrFX0kkkEkkHoHMKvruRth6MxJVIJJL2QucUfFuXjqM2DMXylxa+RCLpQHRuwQccW/GK4EsLXyKRdCA6p+CbzYBGY/3saD/ALwYZQlkikXQQOqfgWyxAQAB/dmXhK2k9JCwszKPtEolE0hp0PsFXrHZlUm9HPXVst0m3jkQi6SB0TsEHVFv4j8yfj0WLFjV8VyYpMRqNmDRpEoYPH46UlBR89dVXqotARHjooYcwZMgQpKSk4NNPPwUAnDx5EuPHj0daWhqGDBmCn376CWazGXPnzm1I+8orr3h+zRKJRILWC4/cMtx3H5DVzPDIRDzTVUAAUFcHpKcDb77ZOI3NS+Cqyy7Dff/4B+68804AwGeffYb169cjKCgIK1euREREBIqLi5GRkYFZs2apmkP2iy++QFZWFnbv3o3i4mKMHDkS48ePx8cff4xp06bh0UcfhdlsRlVVFbKyspCXl4d9+/YBgEczaEkkEokt7UvwWwKlEVZptHXVLRPAsJQUFBYWIj8/H0VFRYiOjkavXr1gMpkwf/58bN68GRqNBnl5eTh16hTi4+PdFuHnn3/GNddcA61Wi7i4OEyYMAHbt2/HyJEjcfPNN8NkMuGSSy5BWloa+vbtiyNHjuDuu+/GzJkzMXXq1Ja4CxKJpBPSvgT/1RYIj1xZCezfD/TvDxw+DHTv3jSNnQ9/9uzZWL58OQoKCnDVVVcBAD766CMUFRVhx44d0Ov1SEpKchgW2RPGjx+PzZs345tvvsHcuXPxwAMP4IYbbsDu3buxfv16vPHGG/jss8+wZMmSZuUjkUg6J53Xh6/V8uKsW6aNj/+qq67CsmXLsHz5csyePRsAh0Xu1q0b9Ho9Nm7ciGPH1E/mNW7cOHz66acwm80oKirC5s2bMWrUKBw7dgxxcXG47bbbcOutt2Lnzp0oLi6GxWLB5Zdfjqeffho7d+5s7h2QSCSdlPZl4bcEisBrNM4Fv77e6uO3WJCcnIyKigr07NkT3c/UCK677jpcdNFFSElJQXp6ukcTjlx66aXYsmULUlNTIYTACy+8gPj4eCxduhQvvvgi9Ho9wsLC8P777yMvLw833XQTLGdeVM8++2yzb4FEIumc+DQ8sqe0Snjk06eBI0eA5GReBwQ0DYG8axcQFQWUlAAJCYAKv7w/I8MjSyQdF0/CI3c+l45i0Wu1gE7X1MJXImUqLh0ZT0fiDaWlcpS2xO/ofIKvCLgzl47yXafjNHLglcRTDAauGa5Y0dYlkUgaIQXfXtCVHjqK4EsLX+IpRUVAVRXPrCaR+BGdT/DNZkAIFnOdrmloBVvBd9aoK5G4wmjktRwkJ/EzOp/gWyzWQVdabdOImLY+fmnhS7yhspLXUvAlfoZPBV8IESWEWC6EOCCE2C+EOMeX+anCYmExB6xrWyteWviS5qJY+AZD25ZDIrHD1xb+fwGsI6KzAaQC2O/j/NxjGwtfEXxbt46dD7+stBT/+9//vMrqggsukLFvOiPSpSPxU3wm+EKISADjAbwDAERUR0Rt/w+wdenozow7c2ThnxmJW2YwOBX8emeToJ9hzZo1iIqKam6JJe0NKfgSP8WXFn4fAEUA3hVC7BJCvC2ECLVPJISYJ4TIFEJkFhUV+bA4Z3Dn0jGbebsQgFaLR155BdnZ2UhLS8NDDz2ETZs2Ydy4cZg1axYGDx4MALjkkkswYsQIJCcnY/HixQ2nSkpKQnFxMXJycjBo0CDcdtttSE5OxtSpU1FdXd2kaKtWrcLo0aMxbNgwTJ48GadOnQIMBhhLS3HTTTchJSUFQ4cOxYoz3f3WrVuH4cOHIzU1FZMmTfLN/ZJ4jvThS/wUX4ZW0AEYDuBuIvpNCPFfAI8AeMw2EREtBrAY4JG2rk7YEtGRUZXAYh4MwBKCtJ698OoiO5eOYvlrNHju7ruxLzcXWWcy3rRpE3bu3Il9+/ahT58+AIAlS5YgJiYG1dXVGDlyJC6//HLExsY2yvbQoUP45JNP8NZbb+HKK6/EihUrMGfOnEZpzj33XGzduhVCCLz99tt44fnn8fK11+Kpd99FZGQk9u7dCwAoLS1FUVERbrvtNmzevBl9+vTB6dOnm3ljJC2G9OFL/BRfCn4ugFwi+u3M9+VgwW9bCCz4AIAza3uXjiL4ThptR40a1SD2ALBw4UKsXLkSAHDixAkcOnSoieD36dMHaWlpAIARI0YgJyenyXlzc3Nx1VVX4eTJk6irq0OfpCQAwHebN2PZF180pIuOjsaqVaswfvz4hnLExMSovAESnyNdOhI/xWeCT0QFQogTQoiBRHQQwCQAfzTnnC0RHRl7soGICCApCTATsOsEYE6w7ldcOoDV129HaKjVM7Vp0yZ899132LJlC0JCQnDeeec5DJMcGBjY8Fmr1Tp06dx999144IEHMGvWLGzatAkLHn/cu2uUtC2KS6eqigPwKWE6JJI2xte9dO4G8JEQYg+ANAD/9nF+7rHtpaOsXVj44SEhqKiocHo6g8GA6OhohISE4MCBA9i6davXRTMYDOjZsycAYOnSpQ3jA6aMGdNomsXS0lJkZGRg8+bNOHr0KABIl44/oVj4gHTrSPwKnwo+EWURUToRDSWiS4io1Jf5qcK20VaIpqNt7Xz4sVFRGJuRgSFDhuChhx5qcrrp06ejvr4egwYNwiOPPIKMjAyvi7ZgwQLMnj0bI0aMQJcuXRq2//P221FaWoohQ4YgNTUVGzduRNeuXbF48WJcdtllSE1NbZiYReIHSMGX+CmdKzyyxQLs3An07Gmd6WrvXiA0FOjbly3qHTuAHj14KS3leCiDBwMhIS1TBk9QQjmHhgLNuAcyPHIrc/HFwNdf8+ft23neZInER8jwyM6wDZymYNswaxtWwXbdVqNt7cslaR9UVlo7BsiGW4kf0bkE317Qlc/KdttRtoD1xdBW8XSUcsl4Pu0LoxGIi+PPUvAlfkTnEnxHFr6tD99e8KWFL/EGo5Hj4QNS8CV+hRR8Vy4dR714WhPbcvlRW4vEDbaCLxttJX5E5xJ8T106Srq2cqnY9h5yVobycuDEidYpj0QdlZU8D7JGIy18iV/RuQTfmYVvsfDizIff1ha+qzKUlgKnTskagD9hNALh4TzATwq+xI+Qgm8bMdM2UqaSTgiPLfywsLBmFhTWMik4K4P08/sXZjNQUwOEhQFRUVLwJX5F5xJ8Zy4dZZ9tpEzb/W1p4QsH8X7s07jaL2ldlLAKiuBLH77Ej+hcgu/MpQNYLXxd4/BCjyxciEXvvdfwfcGCBXjppZdgNBoxadIkDB8+HCkpKfjqq6/cZu8sjLKjMMdGoxE3zZ+PlKuvxtBrrsEKm+BpjbDvYSRpW5RRtqGh0sKX+B2+jJbZ4ty37j5kFTQjPnJdHVBbC+wLb9iUFpuMV7vfxILpQPCvuuAC3PfCC7jziScAAJ999hnWr1+PoKAgrFy5EhERESguLkZGRgZmzZoFYVs7sMNRGGWLxeIwzPFTTz2FyNBQ7F29GqioQKld9M0GpIXvXyiCHxYGREbySGmJxE9oV4LfbBw1bNq6TGwjZZ5hWHIyCktKkJ+fj6KiIkRHR6NXr14wmUyYP38+Nm/eDI1Gg7y8PJzaswfxZyZFcYSjMMpFRUUOwxx/9913WPboow2RFqPDwx2f1L6HUUtQWgqsXQtce23LnbOzYCv40sKX+BntSvBfnd7M+MjHj3N8mjNx6QGw1b9nj9WlYxPGGACg1WL2tGlYvnw5CgoKGoKUffTRRygqKsKOHTug1+uRlJiImmPHgG7dHGatNoxyA8rLSQmt667RtiUF/+OPgbvuAs4/3xpzSKIO6cOX+DGdz4dvH+PediJzBy4daDS4aupULFu2DMuXL8fs2bMBcCjjbt26Qa/XY+PGjTh2/DinLy93mLWzMMrOwhxPmTgRiz7/vEHwS0tKHF+P8iJoSZeOEmq5tO2Dm7Y77H345eXS3SbxGzqX4NvGwldQul4qLh17wddqkdy3LyoqKtCzZ090P2PxXnfddcjMzERKSgref/99nN2vH6e3DY1rg7Mwys7CHP/zoYdQWlGBIePHI/Xaa7Hx558dX49CS1r4ilUq3RGeY+/DB5waARJJa9OuXDrNxjYWvsKZycpRW8vf7fef6ZapzCer0KVLF2zZssW6Yd8+wGQCzGYY8/KaZB0YGIi1a9c6LNaMGTMwY8aMRtvCgoOxdMECYOBADtEcHd30QCn4/oe9Swfg++jo95NIWhlp4QONBd+BSwcWi+uRrMpgm65d+QXiYoYs1dgOAnM2FkDNSFxvUCxSKfieY+/SAaQfX+I3dC7Bd+TDB1jknQm+Vsti70rwq6p4HRbGS0tU4RUB1+msLx1naYSQFr6/YN9LB5D3UeI3+NSlI4TIAVABwAygXu2sLPYQkcv+7apx5NIBGlvQjix8wHntALAKfkgIx1DJz2f3jl7vfVltRwU7s/AVkQ8MdCr4Xs1opgi+tEw9x2jk5yQoSAq+xO9oDQv/fCJK81bsg4KCUFJS4p1w2ePKpePos+13V/F0KitZ3AMCOGAW0Hy3jiLwGo17l05goMP9RISSkhIEBQV5lrd06XhPZSVb90JYG23lfZT4CX7faJuQkIDc3FwUFRU1/2QFBWyBVVc33l5SYq2KZ2c3filUVQHFxcCBA9Y+8fbk53PNYP9+dv2UlLBP39noWDWcPs3iceAAUFTE4wXsazkGA4tJbS2/YByULygoCAlKbHa1SJeO9xiN7L8HpIUv8T+IyGcLgKMAdgLYAWCeu/QjRowgn2GxEGm1RI8+2nTfww+zl14IIrO58b5163jfL784Pm95OR/35JPWbRdfTNSnT/PKO2cOUd++/HnePKK4uKZp5s/na3ruOS6j0di8PBXCwvh8V17ZMufrTFx9NdGAAfzZZOL7uGBB25ZJ0qEBkEkqNdnXLp1ziWg4gBkA7hRCjLdPIISYJ4TIFEJktogV74zaWnZ7KNaXLYolFh3d1OWjhDRw5qLZtYut+nQbj9XkycDRo82Lo1JWZi1XRITjhmCDgdMoNQllwFRzMJuttR1pmXqO0cguHYBrfeHh8j5K/AafCj4R5Z1ZFwJYCWCUgzSLiSidiNK7du3qu8LY9o+2RxHWM3FsGqGkdyb4mZm8HjHCum3yZF5/953n5VSwF/zq6qYNs0oapdwtIfi21ymFynMqKxsbFZGR8j5K/AafCb4QIlQIEa58BjAVwD5f5ecW2+5y9iiDYhwJvmLhOxlBi8xMnr80Ls66beBAoGfPlhN8Z7UMXwi+bc8c2UvHc2wtfEAGUJP4Fb608OMA/CyE2A1gG4BviGidD/Nzje2AGHsUYXXUyOrOpbNjR2N3DsCNq5MnA99/7/18uPYWPtDUreNLwY+NlULlDY4EX744JX6CzwSfiI4QUeqZJZmInvFVXqrwhUvHYAD+/LOp4AMs+KdPA1lexu9vK8FX8khMlILvDUq3TAVp4Uv8iM4z0taVS8eV4AcGcuObI5fOrl28tvXfK5yZuQobNnhe1vp6zk/px90WFn7v3tzQ7SqEs6Qptt0yAenDl/gVUvAB1y4dIdit48jCd9Rgq9C9OzB8OPDpp56XVRFdtT784GB+MbW04Ct5SNQjffgSP6bzCL7i0nHkw+/aFbjwQp7wwxFhYc4FPzGRj3fEzTdzLWDnTs/KqgiEK5eOycTXFBnJL6WYmJZ36diWReIek4kHyDny4bfESHGJpJl0HsF3ZeFrtcCqVcD4JsMEmPBwxy6dHTscW/cK117LMVXeftuzsqoRfPtaQEsJvr2FLxsc1ePIqIiK4oZ7Z728JJJWRAq+Ghy5dEpLgcOHHTfYKkRHA1dcwVMGKgHW1KBG8O3TtKTga7VAjx6N85G4x9EzJsMrSPyIziP4rlw67nDk0lHcNK4sfAC45RYW0RUr1OdnL+aKgLSW4EdGSqHyBkeCLwOoSfyIziP4RqO1x42nOHLpKII/fLjrYydMAPr398ytYy/mWi2/qGxfOvYunejolvPhS8H3Dkddf+V9lPgRnUvwvXHnAI5dOllZQK9eQJcuro8Vgq38zZu5z74a7AUfaBpPx5cWfkSEFCpvcDS4T856JfEjOpfge+POARy7dHbtAtLS1B1/441spS9Zoi59WRkHcbN9QakR/Kqq5vebV1w6wcEc418KlXqkD1/i53QewbcfAekJ9i6dqirg4EFg2DB1x3fvDsycCbz3Hnfdc0dZGYuubeTO8HD3gg9wY3JzUFw6Qsg+5J4iffgSP6fzCH5zXTq1tVax3ruXu9qptfAB4NZbgVOngDVr3Ke1DaugEBHRNJKlbS2gpUbbKi4dQAq+pzjy4UvBl/gRHVPwN24E1tnFaWuuSwewCq4SH0ethQ8AM2awpa/GreNM8O0t/Kgo6yxYLSn4ikjJsACe4ciHHxjI7jHpGpP4AR1T8B9/nC1q29GNzXXpANY/9K5dLLbKaFQ16HTAlCnqRt16IvgKLSH4RFaXDiAtfE9xNtZD3keJn9AxBb+wEMjL4/lgFZrr0gEaW/hpaU3nmHVHYiLPf+vOj6/MZGWLI8FXhBloGcFXJlmRgu8dRiO/2O3nFpY1JYmf0DEFX5kq0TZSZXME39alYzYDe/Z45r9X6N2bff/5+a7TObLwla6hSq3FFxa+4naw9eFLV4R6lNmu7A0B+eKU+AkdT/BNJmtPFVvBt596zhNsXTp//smWsCf+ewUlPs3x467T2VvvAItwfb2126W94EdEcNfPlhB8aeF7hzOjQr44JX5CxxP84mJeh4Zy421dHVvFLeXSUWLge2vhA64Fv76e83Hk0gGsbh17t48QzR9tq5zbttG2qorvocQ9rgRfvjglfkDHE/zCQl5feilb9Vu3slVssbSMSycri320gwZ5fp5evXjtSvAV0XUn+I7cPs0dbevIpWO7XeIaZx0DpOBL/ISOK/hXXsn91DdsaF7gNKCxS2fXLmDIEB6F6imhoTzJiivBdxRWwbYM5eXOawEtJfi2Lh3bMklc46zrr9JoK2PiS9oYnwu+EEIrhNglhFjt67wAWBtsBwwARo1iwW9OaGSgsUsnK8s7/71C797AsWPO9zsTfMXqrqhwXgtoruDbu3Sk4HuGK5eOySSni5S0Oa1h4d8LYH8r5MMoFn63bsDUqcD27cCJE7zNW8EPDubawoED3Ebgjf9eoXdv7yx8W5eOszTSpdO2uBJ8QL44JW2OTwVfCJEAYCYAD6d8agZFRdwXOiqKBzpZLDybFeC9S0cI/iP/9BN/bwkL31n13p8EX4YF8AxnPcGk4Ev8BF9b+K8CeBiAxcf5WCks5JDFGg0wejS7Y1au5H3eWvgAnyc7m8V/6FDvz5OYyJagM6tZjQ/fleCXl6sL0OYIg4HvkVbb+PxSqNThzMKXL06Jn+AzwRdCXAigkIh2uEk3TwiRKYTILFL8782hqIjdOQA3rJ53Hk9FCDRP8JVj+/e3iq83uOuaqcaH70rwbc/hKbZhFWzPL4XKPa66/sr7KPETfGnhjwUwSwiRA2AZgIlCiA/tExHRYiJKJ6L0rl27Nj/XwkLA9jxTp1o/e+vSAawi3xz/PaBO8IVo+lIJDmbL252FD3jv1rGNlAmweGk0UqjUUFfHvadcCb5sC+l41Ne3zMRDrYTPBJ+I/kFECUSUBOBqAD8Q0Rxf5deArYUPsB9fobkuHaB5/ntAneDbx8IH+CWgxNNRBNh+NG5LCL7tOWVMfPW46vorLfyOyyuvAGedxcLfDuiY/fBtLfyzzrIOeGoJl05zLfy4OHY1uRJ8e8tdQZkExVktoLmCb+/SAfi7tEzd46rrr/Thd1x++QUoKeFgje0AVYIvhLhXCBEhmHeEEDuFEFPdH8kQ0SYiutD7YqqktpZFy9bCF4KtfCFaxqXTXAtfo+EXkDeCr0yColji9rWAlnbpANLCV4srwVemi5T3seOxezevjx5t23KoRK2FfzMRlQOYCiAawPUAnvNZqbxFafS1bwt4/HHggw+svU+8oU8fHswVH+/9ORRc9cV3J/iKhe8oTUu7dAAp+Gpx5dJRXGOyptSxMBiAnBz+rKz9HLWCr8R7vQDAB0T0u802/8F20JUtiYnAddc179xPPAHscNnhSD3uBN9edBXcCb6yrSVdOlLw1eFuNHeXLtbnU9Ix2LfP+rmDWfg7hBDfggV/vRAiHK3Zt14tioVvL/gtgV7fvO6YtvTuzT4/Rw09zbHwtVre7o3gm0wcGVMKvne4E3x3ITXWrAESEhrPWyzxbxR3TnBwhxP8WwA8AmAkEVUB0AO4yWel8hbFgmqJ7p2+xNVEKO4abZV++M7SeDvaVomjY+/Dl7M1qcOd4Ccmuhb8X35hI8DWapT4N3v28P8wPb3DuXTOAXCQiMqEEHMA/BOA/zkkfWnhtyRK10x7ATCbHUfBVHBn4QPeC759pEyFqCgWs3bS7azNcBeRNTGR4zAp6exRLMT9rRd2StJM9uwBUlO5fa+DWfivA6gSQqQCeBBANoD3fVYqbyksZNeLvZXqbzjri+8sCqZCRASLb0mJ7yx8R4Jvu1/iGDUWPuC87UaxEKXgtw8sFhb8oUOBpCSunbWDiYLUCn49ERGAiwH8HxEtAtBCDu0WRBl05enk4q2Ns4lQnI2gVVBeZJWVvrPwHXXLtC2bxDGK4Luy8AHnbh0p+M2nNWuhR4/y/3DoULbwidxPXeoHqBX8CiHEP8DdMb8RQmjAfnz/wn7Qlb8SFsbC7Kng2zYat6ZLx7ZsEsdUVnLtMiDA8X5Xgl9dDZw8yZ+l4HtHURFPLvR+Kzke9uzhtWLhA+3Cj69W8K8CUAvuj18AIAHAiz4rlbfYh1XwZxx1zdy+ndd9+jg+xtb6diX4paVc5fQEZy4dOUpUHe7mTO7Rg8N2OxJ85Tno148tx+pq35SxI7N+PT/DTz3FbWG+Zs8e9iQMGWL9v7YDP74qwT8j8h8BiDwTBbOGiPzTh99eBD8xsangf/ghz5XrLPyyreA766sfE8PVS0+Y+moTAAAgAElEQVQH+biz8OWgIde4E3ytlrtdOhJ8RShmzODf7s8/fVPGjsz69SzAhw8DX33l+/x27+aBmCEhQM+e/Pt2FAtfCHElgG0AZgO4EsBvQogrfFkwr2gvLh2gqYWfk8MTrMyZ47wNQq2FD3ju1pE+/ObhTvAB510zFaG44AJeS7eOZ1gswLff8jzWffsCL7zg+/mDlQZbgGtuvXt3HAsfwKPgPvg3EtENAEYBeMx3xfKCqir2o7YXC793b66CKkL78ce8vvZa58eo8eEnJPA6O9uz8hgM7H8OCmq8XQq+OpzNdmWLK8HX64EJE6xTaXpDdbX3x7ZnsrLY2LvgAuDBB4HffgN+/tl3+RmN/P9KTbVuS0rqOBY+AA0R2Y4LL/Hg2NbBWRwdf8W2ayYRx/oZN87aAOQINRZ+ejrXEH77zbPyOAqroOQphBR8d6i18PPzm85IdvQo7wsJYX+wtxb+nXdycD+lx1BLkJsL/P3v3s+i1hqsX8/rqVOBuXM5jMWLPmxi3LuX17au13bSF1+taK8TQqwXQswVQswF8A2ANb4rlhe0l0FXCraCv2sXW2Zz3EwXoEbwIyOBs8/2XPAdRcoE2OIMD5eC7w61gm+xsIjakpNjfdEPGuSd4B88CCxdCtTUAFu3en68M954g10kmze33DlbmvXrOWx5fDy/NO+8k+ex9pVrzLaHjkJSElBQ4PcN7mobbR8CsBjA0DPLYiL6uy8L5jHtJayCgq3gf/ghu1Nmz3Z9jOLSUSZDcUZGBv/pPfFjOoqUqSDj6bjHaFTn0gGaunVycqw9PQYN4kZbT3uaLFjAMV00mpYV5zVn7Loffmi5c7Yk5eUclmLaNOu2O+/ke/HSS77Jc88e/v8pvydg/f1chc/wA1S7ZYhoBRE9cGZZ6ctCeUV7s/Dj49lve+QI8MknwMyZQHS062P0evaxR0Q0jYVvy+jRPBr3yBH15XHm0gG8C+27ciVbXf/5j/c9fN58E7jmGu+ObW0qK9VZ+EBjUaisZGNFsfDPPpvndfDEPbBnD7BsGXDvvXzPf/rJo6I75eRJrn0C/iv4GzfygKvp063bunYFbrqJDSllfENLsns3W/e2nSvaSV98l4IvhKgQQpQ7WCqEEP411r69WfgaDTewfvwxVwXduXMUIiKcu3MURo/mtSdV+5a28JcvZ1/ngw/ydd57r3UyebUsXAh8+qnfV5MBqHPpKCOsbQVf+Wxr4QOeuSOeeIJ/u7/9DRg/nn/3lhjmv24dry+8kMeI+GN4jfXr+b6PGdN4+wMP8IvgH//wfEyKK4ga99BRaCd98V0KPhGFE1GEgyWciPwrYE1REVu/zZnGsLXp3Zsb8aKirF3y3KFG8IcMYV+mJ358Zz58wDvBz8piocjMBC69FHj9dSA5GTh0SN3x2dnAH3/wH0ztMW0FkTrBDwrimp2t4CsCYevDB9QLfmYm8OWXLPbR0dzwX1PTMnM3rF3LA8buu49dTC1Vc2gpiPildP75TUc49+vHYr90Kfd8q61tmTyPHeMAh7Y9dACge3cuQ3u28NsVyqArf4+jY4vix589u2l3SGdERDi3xBV0Ou6t44ngu3LpeBoiWekemJYGjBjBw91/+42tzl9+UXeOVausn/29X3ptLVuRaqbQtO+aqQiEIvhRUfxSUHvN//wnhxS4917+fu65vG6uONfXc9/2GTPYeg4M9D+3zuHD/MK0defY8vTT3Fvn00/ZZdoSNRQlBr69ha/R8G/bni385iCECBJCbBNC7BZC/C6EeNJXeQFgC7+9uHMUFMFX684B2JK77z736TIy2P9aU+M+LZF7H74ngr9vHwug7YTvQ4eyaPz+u7pzrFrFVpoQbd+3/P77gX/9y/l+d5EybXEk+Irlr6C2p85PP7FL45FHrA363boBAwc2v+F2yxau9c2YwQ2gY8b4VvDr6z0fLKV0x7RtsLXnb39jK3/TJuC889h92hyUHjpDhjTd1w764vvSwq8FMJGIUgGkAZguhMjwWW7tKayCwnXXAY89ZrXK1HDNNewiccfo0dx3OivLfVqjkQXalUunvFy9L1TJ01bwtVpukFQj+AYDC9YVV/CfqC0t/MOHgf/+F3jnHedpPBX848et91Lpg29bMx00iF9y7gTw2Wf5RfHXvzbePm4c16Sa47teu5ZripMn8/eJE/l3LSnx/pzOqKjgdp7XXvPsuPXr2Sjo1891uhtuYAPi4EG+N81pyN25k/Nz9Fu3g774PhN8YpQRIPozi+/GO7ensAoKgwax5eiqx423ZJx5t6ppuHUWR0chKorFR+30e7t28cvDfhBZcrI6wV+3ji2+iy6yil9bsXChNfTtqVOO07gLjWxLYiK7tpRz2XbJVBg0iH8TV9ZoSQm7XObO5fYaW8aN4xpZc2bPWrMGGDvW+kxMnMjrTZs8P9fhw8Dixc5fYB9+yPfjww/Vn7O2lmscztw59syYAWzYwGI/ZYp3L64jR/jFMXOm4/1JSTzJjacD3955B5g3r+XaGVzgUx++EEIrhMgCUAhgAxE1cSoLIeYJITKFEJlFStdKTyFqX5EyW4MePdhqUuPHdxYpU8HT8ApZWWzd27enJCezcLrzpa5axaMlMzJY/A4ebJ0IiPaUlQFLlnDNBLBGM7VHmcVKrYUPWN06toOuFJT8XNVsvv6a78nllzfdN24cr7314+flsa96xgzrtpEj+fo8deucOsW1hL/8hV9Q9hBxgz7A99fRtJ+O+PlnDqfiyp1jz5gxfN8OH+YXhac+/aef5q7Rf3cyBEl5cXvi1iECXn6Z/zOBgZ6Vxwt8KvhEZCaiNHA45VFCiCaOLyJaTETpRJTe1VsLvbKSGwrbm4Xva0aPVif4aix8QJ3gm83s5xw2rOm+5GRe//GH8+Pr69m6nDnT6gaqqWm5ySWOHwc+/1xd2nfe4Wfrrbe4FrZtm+N0nrp0AGtvj5KSpoKvpqfOihV8rhEjmu5LSuKXvbeCr3THtO05ptdzl09PBL+mht2PhYVAXBwLpj2//srdd5VGZ9vGelcsW8b3e9Ik9eUBuKayfLm1F1lVlbrjDh/mzgd/+QsbU45QfkdP3DqbNvHvfOed6o9pBq3SS4eIygBsBKCy/uUh7W3QVWuRkcEPX2Gh63TOImUqeBITPzubRdLWf6+gNHS5cuv88gvH87/oIv7uTb90Zxw8CJxzDkdV3LjRddr6evYpjx/PbSxDhji38BVLUa1LB2DBVyxBe5dOjx7cCOvsmg0GtpavuMJxrzQh2Mr/6SfvokauXcshf+0bJidOZPeaGiucCLjlFm78ff994NFH2Sq3b0x+/XV+7p55hiNdqgltXFvLon3ppU3dWWq48EJ2HykjdF9+GfjiC/bPO3vG3Vn3gHcW/qJFHOH2yivVH9MMfNlLp6sQIurM52AAUwD4xhnb3gZdtRbKACx3Vr47l44yAliNy81Rg61Cnz7c48OV4K9axf2Zp07l72rcG2rYt4+jUZpM3ND5+OOuxfCrr1iU77+fv48cyRa+o2N++40bOAcMcF8OZRzFsWNN++ArCOG6p86qVXwdjtw5CuPGsTB7Mtoa4PNu2MDuHPuXieLHd/eyBFggP/6YhfyKK4Bbb2WD7JlnrGmKiri2deON/LK8+GLg++/dtxWtWcPCfN11nl2bLVddxe66ffu4J8/ll3NtKSYGePLJxr/zoUMc3PCOO7i/vTO6dePnW62Fn5vLYyhuuYWPawV8aeF3B7BRCLEHwHawD3+1T3KSFr5jRoxgt4i7hlt3Lp2zz2ZB27nTfZ67drElNHhw030aDQuZO8E//3xrN8PYWH6RN6fhdtcu7pKn0QA//sg9o37+GfjuO+fHvPIKW5xKTWPUKJ5jwJGArl/PDZyu4hvZonTNdGbhA64Ff8UKtsCVF7ojvPXj//orGwCOBgKmpvLL351b59NP+YV6/fU8+AlgQXvwQa6ZKDWlJUu4Afv22/n7rFn83ZGv35aPPuL/uqfuHHtuvJF/09On+dn+4gvg6qs5LtENN1gbUZ96iv3rrqx7gF+Q9l0zi4o4H0eN3W+9xT2p7rijedfhCUTkN8uIESPIK955hwggysnx7viOzLBhRJMmuU7z4ot8/wwG52lGjiSaMMF9ftOnE6WmOt9//fVEPXs63nfgAJfj//6v8fZx44jGjnWftyMyM4miooh69SI6dIi31dTw93POIbJYmh6zbRuX49VXrdt27eJtn3zSOO3Jk7z93/9WX6ZZs4iGDCG6/36ikBDHZXjuOT5vWVnj7eXlRIGBRHff7ToPs5koOpro5pvVl6uoiGjaNCKdzvmzcNllRElJzs+xbh2RXk907rl8n+3LHh1NdPHFXL4+fRo/UyYTUUwMPyPOKCtTd/3eYrEQPf003/tx44h+/ZVIoyF68EF1x8+YQZSWxp+PHiU66yw+V2Qk0Z9/WtPV1hLFxxPNnNnsIgPIJJUa2zFG2ra3WPitSUYGuyJc9XIxGNg6cdXoeM45bJm5i4uu9NBxRnIy9wJx5CtVGuwuvLDx9uZ0zbz9dr6uzZuB/v15W2Ag+5S3bLEO3rHl1Ve5hnHTTY3LHRzctOFWsUbVdg8ErBb+0aNsETrywyttF/ZdK9esYcvzCjcTzmk03Pag1sL//HOulX3/PYdDdlZbmTiRLVhHNZ1ffwUuu4zPs2pV014n4eHAPfewu+zll/n6ba1bnY4b67/5httQHLFiBV9/c9w5rhCCn41PPuHf+txzeWDcww+rO75PH74/e/dyra+wkF1bej27rBT36cqV3O22lRprG1D7ZmiNxWsL/4EHiEJDvTu2o/Pee2xh7NvnPM0997AF4oply/g8mZnO0yjWrq1lbM+qVZzm55+b7hs/3nHt4JVX+JjCwsbbLRai1auJqqsd57V3Lx/3yitN99XWEiUmcs1FsbDr6ogefpiPeeCBpseMGdO0pnHNNUTdurHFqpaXXuI8EhOdW3gnTxKFhRENGtT4umfPJoqLI6qvd5/PCy9wPkePOk9TUEB0+eWcbsQIot27XZ/zzz/Z4u3Th+j9963lyMriZ2jAAD6nM0pK+LoAvo7a2sb7P/+c923a5Pj4SZOI+vVzXCtqaX75hah7d6J//Uv9MUptOSKCqEcPfgaJiDZuJNJqiS66iJ+VceOI+vb17LlxAjyw8Ntc5G0XrwX/+utdVzM7M4qbxJHoKVx1FVHv3q7Pc+wYn+e115ynWbfO9Z+ViOjIEU7z5puNtxcUEAlB9PjjTY9Zu5aP2by58faffuLtjo4hIvrb39g9ceqU4/1vv83Hr17NopiRwd//8heiqqqm6e+7jyg4mF0PRCx2sbGuXRCOUEQNILrzTufpNm0iCgril+Dp00SVlewCuv12dfkcOMDlTUy0Co8ta9cSdenCLpJnn7Velzs2bCAaPpzLn5xM9NZb/NJLSFDnVlVeqvPnN91XXk4UEMDuLnvy8pw/I77CU0FWftuBA5vei9de431XX83rF19skSJ2PsGfNo1o1Cjvju3oWCzsT+3enQXDniNH2Od6663uz9OzJ1u0znj2WX6kSkudpzGbWbTuuafx9tdf52MdCVNOjuOXxB138PbY2KbXZjKxBXnxxc7LUlfHVlbfvuznj4gg+uwz5+k//pjzy8ri74qv/8MPnR/jCOU4gK19V6xbxwI4apS1tvbdd+rz2r6df/vwcBZ4Ir7uv/+dzzVkCNHvv3tWfiL+HT/7zOqj7tKFaP9+dccWF/Pz5qwmMGMG/yb2VvzLL3NeBw96Xt7WwmAgeuKJprVRIr6em2/mawgK4tpOC9D5BH/YsBZp/OiwbN7MP/XzzzfdN2cOP3wnTrg/zxVXsLXojKuuUlfTSk9v2pA8eTJbRY6q6spLwtbqq6tjkTn7bL62RYsaH7N6NW9fudJ1Wd59l9ONHEmUne067aFDnHbxYv7+1FNscTr6c7vi1Cmr4C9f7j79V19xTUWr5ZebWktc4cQJbkjUaLhxecwY1zUZTzCZuCH7jz+adx5blJe/vRty+HB+dtozNTXcaP/YYy12ys4n+AkJRDfd5N2xnYUZM7iHhK31nZXFgvX3v6s7x3/+w49MXp7j/QMHEl1yifvzzJ3LPRQUiopYzBxV8RWGDeMeQAqKm+fLL4lGj2a/rq1f+4or+IVg7yO2x2Ih+vFH9+mUtNHRRLfdxt/HjmW/t6dYLOxqcdcmYsuyZSzY8+Z5nh8RUUUF+48BtvaXLfPuPK1BXh6X8/bbibZs4XaD334jt67JTkrnEnyLhau8Dz/s+bGdiZ07+ed+9FHrNuUlcPq0unNs3ercKjUa+eWxYIH78ygNW8XF/F3xpe/c6fyYa65pXHu44QZ2w9TUWP2mSrlKSviZuPdeddflCdOmsU+9tJRfUrb30xMGDuQye1Kt/+MPvs/eUl/PNZrDh70/R2sxdqy1FqQsGg1Rfn5bl8zv8ETwda3bJ8hHHDnSdMYbSWOGDePRha++Ctx9Nw/qWbuWu+C5m0vX9hyBgdyd0X6U5969/Ld0FEPHHiWmzu+/c+iC5ct5kJOr7pyDBnH8lKoq7jq3ciVPHBMYyEPs+/blyS4uu4zT1dVxJMmWZtQo4N//5m6HZrNnwbtsSUzkkbBq7z1g7arpLVqtb+6JL1izhp/RkhLr0quX65GuEre0f8EXgkcdStzz1FMsrk8/zX2MExKAu+5Sf3xAAM+k9euvTfcpk127Em0FW8FPSeG+3/ff73q2skGD+IVy8CC/4CsqrBOca7U8h+ldd3HZ3nuPR4WqKYunjBzJQv/889xXPcPLKR6uvJKvqT3N0NaaRES4Hkks8YqOMfBKoo4BA4Cbbwb+7/9Y8P/1L89jeJxzDs+Xah+7OyuLrVVlom5X9OrFg3B+/11dXBjAGlPnwAEeFBMXxyEYFG66icMw3HUXDxDzlSU7ciSvf/+dh/br9d6d55ZbuLYlkbQiUvA7G48/zm6QwYM5XoinjBnD7hLbuDp5ecBnn/GoRDUWqxCc/++/c42jVy+rkDpjwABriOLVq9lC1mqt+0NCeOanrCwesXnttZ5fmxri461TU3oyulYi8QOk4Hc2EhI4kNOaNY0FUy3nnMPrLVt4TWfC4NbVAf/5j/rzJCezOK9f7zzMry2Bgeynf+strl1cfXXTNHfdxcPgZ870bSC9UaN47a3/XiJpI6Tgd0YyMqxx2T0lPp7jhSh+/DffZNF+6SVrrBo1JCdzPJ26OvdxYRQGDeJY+4mJ1hePLd26ccwcZQYlX3HHHRxbxdt7KJG0EVLwJZ4zZgwL/uHDHPJ26lRriFu1KA23PXqob/hU/PhXX+28RjBypO97ckycyI22Ekk7Qwq+xHPOOYcng774Yu658847nvc2UWZTuvRS9ZO4Dx/O+fgqUqJE0sFp/90yJa3PmDG8/uMPniouIcHzc/TsyRNZeDKJxezZPKmLmpmlJBJJE6TgSzwnJYW7QE6c2LzeMJ4eq9VKsZdImoEUfInn6HQ8MUdsrBw4JJG0I3w5iXkvIcRGIcQfQojfhRD3+iovSRsQH+/9oCOJRNIm+LLRth7Ag0Q0GEAGgDuFEA5mtm4eFosJBQVLYTBsaelTSyQSSYfCZ4JPRCeJaOeZzxUA9gNo8aA3Quhw6NA9OHXqw5Y+tUQikXQoWqVbphAiCcAwAL/54NwIC0uD0birpU8tkUgkHQqfC74QIgzACgD3EVG5g/3zhBCZQojMoqIir/Jgwd8DInMzSyuRSCQdF58KvhBCDxb7j4joC0dpiGgxEaUTUXrXrl29yicsbBgslkpUV2c3o7QSiUTSsfFlLx0B4B0A+4nIg6hanhMWxnHPjcYsX2YjkUgk7RpfWvhjAVwPYKIQIuvMcoEvMgoNHQwh9FLwJRKJxAU+G3hFRD8DaJVRORpNAEJCBsuGW4lEInFBhwmexg230sKXSCQSZ3QYwQ8PH4a6ugLU1ha0dVEkEonEL+kwgq803FZW7m7jkkgkEol/0mEEPzQ0FYDsqSORSCTO6DCCr9dHISgoCRUVsuFWIpFIHNFhBB+QDbcSiUTiig4m+MNQXf0nzObKti6KRCKR+B0dTPDTABCMxr1tXRSJRCLxOzqg4MuGW4lEInFEhxL8wMBe0Omi5YhbiUQicUCHEnxrbHxp4UskEok9HUrwAW64razcA4ulvq2LIpFIJH5FBxT8NFgsNaiuPtTWRZFIJBK/okMKPiAbbiUSicSeDif4ISFnQ4gA2XArkUgkdnQ4wddo9AgNHYKKiu1tXRSJRCLxKzqc4ANATMwMlJVtRm1tXlsXRSKRSPyGDin48fFzAVhQUPBBWxdFIpFI/IYOKfghIf0RGTkeBQVLQERtXRyJRCLxC3wm+EKIJUKIQiHEPl/l4Yru3W9GdfUhGAy/tEX2EolE4nf40sJ/D8B0H57fJV27XgGtNgwFBUvaqggSiUTiV+h8dWIi2iyESPLV+d2h1Yaia9erUFi4DP37L4ROF9aq+RMBtbVAfT1/VhYuG6DT8RrgdLW1QE0NUFdn3a8sWi2g0ViXujqgqgqorOR1XV3j9Dpd0zzty+ZumxBAUBAQHMxLYCCXsbqa81Tyra+3LjodEBnJS1QUH19dbS1ndTUfU1cHmEy86HR87qAgXuv1ja9ViMYLYD2HspjNgMViXdsjBBAQwOdWzm8yNS6L/fUHBwMREUB4OK8tFsBgsC5VVbyNyJqnkjY8nBeNpvHvUFMDGI28VFZyvkFB1iUwkO+HXs9rjcZ6/yor+TizuXFZhbCm1+n4OsPCgNBQXgcFARUV1nKXl1vvh7IQWe+F8swq10bE6UNCrOcMCeGy19RYF42Gy68sWm3jZ4OIj1POExTEeSnPRXW19dqUxWy2/jeUZ025ZmeLcv+UBWhcDuUZsb224GAuV3CwtVzKM1tVxels/1vKc2n7G9heu17P16P8zlVVjXUA4DTK7xQays9LXFzTZ7el8Zngq0UIMQ/APADo3bt3i567e/ebUVDwDoqKPkf37jd5fDwRUFQEHD4MZGcDeXlAWZl1MRisP6qyKA9vTY1jYZVIJBJ7unYFCgt9n0+bCz4RLQawGADS09NbVCIjIs5BcPBAFBQsUSX4RMC+fcDq1cCaNUBWFgu6LQEBQHQ0W7AREfyW7tmT39KKBaNYxcHBbBHYW6hmMy+KxaJYuEFB/Oa3WKwWicnE35XFbOYyKJZSaCgfo5zPZOK1I8vYFnfbLBZ+aSkvsNpaLqftNQYENLZ8TKbGVnBNTWOrTjlGsbZ1Oi6rbQ1HsS5tF/vainIOxZrSahvXguyvzWKxWvTK/VTKoKw1Ns5NIr7m8nK2jhWrWKm9REby9djWRCwWflaU9BUVvM32NwgOtlrJYWF8/cp1K4vt715fz/fO1hJUrFblGu2fFcU6VWoENTV8vFLriojg42wteo2mscVva8UKwXkoNUplCQhoXDuxWKy/Y20tP4+2NQ/AajErhpFSg1SeJyWdrbWu/M5KuZTfx9mi3AdlUc5ja53bX5vynFdV8eegoMbPrEbTtJZg/3zZXrvJ1Pi3DglpqgN1dY2NRU0rdZ9pc8H3JUIIdO9+E44ceQRVVX8iJOQsh+n27QMWLwa++go4fpy3DR8OzJ0LDBgA9O8P9OsH9O7NP6REIpG0Rzq04ANAXNwNOHLkURQUvIe+ff/dsN1kAr78Eli0CPjxR36rT58OPPYYcMEFQI8ebVhoiUQi8QG+7Jb5CYAtAAYKIXKFELf4Ki9XBAZ2R0zMdBQUvAeLpRYWC7BkCdCnD3DllWzRv/ACkJsLrFwJ3HqrFHuJRNIx8ZngE9E1RNSdiPRElEBE7/gqL3ckJNyHurqTWLNmBc45B7jlFnbPrFoFHDoEPPQQEBvbVqWTSCSS1qHDu3QAgGgyFi78Bl9+OR1xcRa8/74Gc+Y4briUSCSSjkqHF/zMTODyy4H8/BmYPftlzJ9vRGrqgrYulkQikbQ6HTKWjsLbbwNjx/LnLVsEFizYDYPhedTUnGjbgkkkEkkb0CEFv7qa/fS33QZMmADs2AGkpwNJSU+ByIKcnCfauogSiUTS6nQ4l47FAlxyCfDtt8CjjwJPPmkNYRAcnISePe9Gbu5/kJBwP8LCUtq2sJJ2SWl1KXLLc5HcLRka4Z3NVGeuQ219LcIDw1u4dK6pMlVBr9FDr9W3yPlq62tRWFkIrUaLmOAYBOmC3B5Tb6nHCcMJ9IrsBZ2mqQSZzCZszd2KI6VHUF5b3rAE6gIxrvc4jO09FiH6kBYpvyMsZMEp4ykcMxzDCcMJdA/vjuHdh6vKs8pUhTWH1iBQG4guIV0QGxKL6KBoFFcVI6csBzllOThmOIauIV0xpd8UpHRLgWjFxsQOJ/iLFrHYL1oE/PWvTfcnJs5HQcE7OHLkEQwd+k3rF7CdU1pdivey3sN7u98DEaF/TP+GZWjcUKT3SHf4J7bFZDZh6e6leG3baxgQMwC3DLsFU/tNhVajdXpMfkU+HvvhMXx58EsMix+GiX0mYmKfiRjRfQTyK/KxLW8bfsv7DZn5mYgKikJGQgYyEjKQ3iMdYQHO4ygdKjmEH4/9iPCAcMSHxTcsEYERTf6Ie0/txWvbXsOHez5EdX01uoZ0xbT+0zC933RM6z8NXUK6qLqHaw+txZ1r7kRpTSkWXbAI1wy5pkleFbUV+PT3T5F9Oht5FXnILc9FfkU++kb3xbR+0zCt/zQMjB3oVCzKa8ux/I/lWPXnKuRX5KOwshCFlYWoMlUBAIJ0QYgIjEBEYAS6hXZDUlQSEiMTkRSVhAExAzCs+zBEBUU1Oufh04fx9cGv8W32tzhuOI4CYwFKa0obpQnSBSEmOAbdw7rjrNizMDB2IAZ2GYiwgDBsy9uGX0/8it/yfoOxzoiwgDCck3AOzu19Lkb3HI3s0mx8m/0tfjj6AyrqKpqc12Q2wUxm6DV6jGEAxSYAABliSURBVOo5CpP6TML1qdejf0x/h/eg3lKPkqoSGGoNMNQYYKg1oLCyEMfKjuGYgZdTxlMwWUwwW8wwkxl15jrkV+SjzlzX6FxaoUVKXApG9xyNKX2nYNbAWU1emhuyN+D2b27HkdIjTn55Rq/Rw2QxARuA+LB4TO47GVP7TsW1Kde6/A+0BMKf4sWnp6dTZmam18f/+SeQlgacdx7wzTfOe+EcP/4ijhx5GIMGfYy4uGu8zq+9cLT0KAqMBag0VaLKVIXKusqGP6ayxIXFuRTqHfk7sGj7Inyy7xPU1NcgIyEDXUO64vDpw8guzW74g0QERuD8pPMxpe8UnNv7XCRFJSEyKBIAW06f7vsUj296HIdPH0ZafBpyy3NRXFWMhIgEzE2di4sGXoSBsQMbjqmsq8SLv76IF399ESazCZcOuhQHig9gz6k9AACdRod6C8eoCNQGIi0+DaU1pfiz5E8AgEZokNw1GWnxaRgWPwxp8WkICwjD1we/xsoDK/F70e8OrzcyMBL9Yvqhb3Rf9Inqg2152/DjsR8RpAvCdSnXYUyvMfjh6A9Yn70exVXFCNQG4utrvsbUflOd3sP8inzcu+5eLP9jOQbGDkR0cDS25m7F7MGz8frM1xEbEotqUzX+t/1/eO6X51BcVQydRoee4T2REJGA+LB47C3c23BtvSN7Y0yvMegf3R/9Yvqhf0x/VJmq8P7u9/HF/i9QXV+NPlF9MCB2AOJC49AttBu6hHRBvaUe5bXlqKitgKHWgAJjAXLKcnCi/ETDvQSAftH9kN4jHfFh8fg2+1vsL94PAEjumoyzu5yN+LB4xIXGIS4sDhay4HT1aZRWl+J09WnkVuTiz5I/cazsGAisMVqhRWp8KsYkjEFyt2TsK9yHn47/hL2n9jakSYpKwrR+0zC131SkxqUiMigS4QHhCNQFwlhnxC/Hf8HGnI3YmLMRmfmZsJAFk/pMwrwR83DJ2ZegpKoEaw6twZrDa7Ahe0OTF4dCbHAsEqMS0T2sOwK0AdBqtNBpdNBpdOgR1gOJUYnoHdkbCREJOGE4gd/yfsO2vG3YlrcNhloDeoT3wLzh8zBvxDzoNDo8+O2D+GDPBxgQMwCvTn8VXUO6oriqGCXVJThdfRqxwbFIikpCUlQSuod3x8mKk9hwZAO+zf4WG45sQJAuCMfvO+6VtS+E2EFE6arSdhTBr68Hxo0DDh7kUAmuBk9ZLHXYvXsyysu3IS1tEyIjM7wssX9z+PRhzP9+Pj7/43O3ac+KPQs/3PADekb0bLLvrR1vYd7qeQjVh2LO0Dm4I/0OpManNuy3kAW55bnYmrsV3x35DhuObEBOWU7D/vCAcPSK7AWT2YRDpw9haNxQPDPxGcwcMBMmiwlfH/wa7+x6B+sPr2/448eHxWNg7ED8WfInThpP4orBV+C5Sc+hX0w/AEBRZRF+PPYjtuVtQ1JUEkb3HI2UuBQEaAMAACVVJdiWtw1bcrdgx8kdyCrIQn5FfkOZNEKDcb3H4dKzL8X0/tNRb6lHgbEABcYC5Ffk45jhGI6UHkF2aTZyynLQI7wH/pr+V9wy/BbEBMc0uvYd+Ttw66pbcbT0KH65+RekxDV2FRIR3tzxJh7e8DBMFhP+Oe6f+NuYv0Gn0eHFX1/E4xsfR2xILG4ddive2fUOThpPYmq/qXjyvCcxqueoJm6jnLIcfJv9LdZnr8euk7twzHAMFrKGCY0KisLVyVfjxrQbMbrnaNUiYraYkV+Rj/3F+7Ejfwd2nNyBzPxM5FfkY0LSBMw6axYuGngRkqKSVJ0PAKpN1Th8+jAMtYaGl609ZTVlyMzPRGJkIvrH9Fdd3vyKfCzZtQRv7XwLxw3HER4Q3iDwCREJmDlgJlK6pSAqKAqRQZGIDIxEl5Au6B3ZG6EBoaqvwRazxYy1h9di0fZFWHd4HXQaHUL1oagyVeGRcx/B/HHzVbm1bLGQBXnleegV2curMnVKwX/2WWD+fODjj4FrVBjtdXXF2LlzNMxmI0aM2IagoESv8vUWC1nw8d6P8W32txgaNxSje47GiB4jXPoJa+pr8Ldv/4YQfQienfSs0+pfUWURntr8FF7PfB0B2gA8kPFAg98zVB+KYH0w6sx1OF19GqerTyO/Ih///OGf6BHeAz/O/RFxYdY4rcv2LcO1K67F9P7TseyKZYgIjHB7bUSEI6VHsD1/O3LLc3HCcAInyk+gvLYctw2/DbOTZzv0feeV5yEzPxMHSw7iQPEBHCw5iBB9CJ4870mM6TVGxV11zSnjKew+tRslVSWY3HcyuoZ2VXWchSwQEC6FKLc8F6PfHg2t0GLrrVvRI5wtjmpTNf6y+i/4YM8HmNJ3Cl6f+XrDS0thd8FuXL/yeuwt3Itxvcfh6YlPY3zieNXXVWeuw7GyY8guzYbJbMKUflM8Fh1XWMjidVtFa2C2mPFt9rf4/I/PMSBmAGaeNbNVfOOHTx/G69tfx4nyE3hiwhNI7pbs0/yc4Yngg4j8ZhkxYgR5Q1YWkV5PNHs2kcWi/jij8Q/avDmStm1LIZOp3GXayrpKOlxymCyeZOCErSe20ui3RhMWgKKfiyYsAGEBSPukltIXp9OKP1Y0yedkxUnKeDujIe0Vn11BNaaaRmksFgu9mfkmhf87nLRPaukvq/5C+eX5qsr007GfKOSZEBryvyFUVFlERESrD64m3b90NP7d8VRZV9ns6+7o7MzfSaHPhNLwN4eTsdZIOaU5NOyNYSQWCHrqx6fIbDE7PbbGVEOHSg61yPMl6VwAyCSVGtvmIm+7eCP4tbVEQ4cSxcURFRV5fDiVlHxLGzdqaffumWSx1DtMU1lX2SC2cS/G0ZWfX0mLti2iXSd3UX55fhPhdYTJbKI9BXvohpU3EBaA4l+Kp3d3vUtmi5lOGU/R1we+pke/f5QGLxpMWACa8v4U2l+0n4iIsk5mUa//9KLgp4Np+e/L6aVfXiIsAE1+fzJV1FYQEdEp4ym68OMLCQtAk5ZOajjWE74/8j0FPR1Ew94YRiv3r6Sgp4MofXE6GWoMHp+rs7L64GrSPKmh8e+Op9jnYyny2UhafXB1WxdL0oHxRPDbvUvHaARuvP8QbrioHy6e5V21My/vDRw6dAe6dbsWZ5/9LjSagIZ99ZZ6XP7Z5Vh1cBXmj5uPnLIcbMrZhLyKvEbnCNGHcONnaFxDT4+40DgUGAuw+9Ru7Cvch1pzbYOLZf64+Q675NVb6vH69tfx+KbHYawzYs7QOfj8988RFRSFr6/5GsO7DwcALM1ailu+vgXDuw/HfRn34f7198NQY8Dzk5/H3aPv9roKvu7wOsz6ZBZMFhMGdx2MH+f+qLr3iYRZtG0R7lp7FwZ3HYwvr/oSA2IHtHWRJB2YTuXDr6mvQY+XeyA8MBw3pt6IuWlz0Te6b8N+IkJeRR7MFjMSoxz76c0WM9795SYEVHyA1J6TkZz8BXS6cBAR/vrNX/HGjjfw2ozXcNeouxrOeaT0CHac3NHgBy+tLkVJdQlOVZ7CyYqTOGk8icLKQsQGxyItPg2pcalIi0/D+MTxqhpnCisLMf/7+ViyawnSe6Tjy6u/bPALK6w6uApXLr8SNfU1SI1LxUeXfdQifsTVf67GmzvexBsz33DYiCtxz5YTW5ASl+KyS6hE0hJ0KsGvM9fhi/1f4N2sd7EhewMIhAmJEzAwdiB+L/od+wr3wVBrgIDAPaPvwdMTn270JzxaehQ3fHkDfj7+MwQExnUh3DJwIK4atwkvb1uCR394FA+PeRjPT3ne4+tR09jnjpyyHHQP645AXaDD/Vtzt+LXE7/izpF3Ok0jkUg6Lp1K8G05YTiBD/Z8gKW7l6KosghDug3BkG5DkNItBXsL9+L1zNeRGJmIxRctxpS+U7B091Lcs/YeCCHw4pQXcazsGP5v239RXleJwZGB+MNQi+tSrsP7l77v170UJBJJ56XTCr47fj7+M279+lYcLDnY8BKYkDgBSy9Z2uDuKa8tx6s/P4qF2/6HPqEWvHX+5eiXNB/h4cN9Vi6JRCLxFin4Lqipr8HTm5/G65mvY/6583H/Ofc7tN5rak8iN/e/OJn/OszmckRHT0XPnncjKmoCdLrWjX8ikUgkzpCCrwIiUuVbr683ID//DZw48QpMplMANAgLG4aoqHGIjDwX4eGjEBiY0KoBkCQSiUTBbwRfCDEdwH8BaAG8TUTPuUrfmoLvKWZzDQyGn2EwbIbB8BPKy7fCYqkBAOj1cQgPT0d4+AiEhJyN4OD+CA7uB70+xs1ZJRKJpHl4Ivg+i5YphNACWARgCoBcANuFEF8T0R++ytOXaLVBiImZjJiYyQA4Ho/RmIWKiu2oqMhEefl2nD69BoD1BarTRSEgoAf0+ljo9bHQ6WKh0QSBqBYWSw0slloAGgQH90FQUD8EB/c7E+KBGvZbLDUgMoPIDMACIjP0+hgEBw+ETuffXf6ILBCysVsi8Rt8GR55FIDDRHQEAIQQywBcDKBdCr49Gk0AIiJGISJiVMM2s7kK1dVHUFOTjerqbFRXH0Zd3SmYTCWors6GybQNFksNNJrAM0sQLBYTiou/AJHJ4zIEBPRESMjZCAzsDrO5EmazEWazERZLNQAtNBo9hNBDiABoNEHQakOg0QRDowmGEAIWiwlEdSAy2YizBkJoQFSPuroimEyFqKsrRH19KQICuiM4uO+ZF1Nf6PVdoNWGQasNh1YbBpOpCEbjrjNLFurqChEU1BtBQcoxfRAQEI+AgDjo9d2g13dBfX0pamtzGxazuRxE9WfKVg/AzEPCz7xIhdBAp4uCThcDnS4aen00NJrQM9enXFsAhNCdWbSwWGpRX18Ck6kYJlMJzOYq6HRR0OtjoNPFQK+PbnQMIFBTk4PKyn0Ni/j/9s42RqrqjOO/586dmZ2dBRbkReRFoCKKVJEmvlTaWE1bNI3pB5tqrTGNjV800aRJK+l7vzT9UuuHprVpbW1qrNFKa4ypVWpMbFIVERRBFAQEFRZlgX2ZOzP3ztMP58zs7IKwxV3nDvP8kpO598y9M/97zrnPec4zc8+RkClTLqanZxVTpqyiUFhG0DS7qKoSRbsYGNjI4OBGhoZeQ7VGEHQ1Ui43l+7u83xaRibTQxwfolLZ71MfcXy4kZJkkHx+vh8xLqVQOOeYTl61RhwfJY77ieN+qtWDRNHuRqpU+igUltLTs5KenosoFj8NKNWqK484/hAIyGZnNlImUxjzHUocH2J4+C1KpR2Uy3sIw17y+QXk8/PJ5+ejWqVSOdBIqmXfLqb6Z1pqDA9vbZRnqbSLfP4sCoVz6e4+l0JhKWE4zbfXLEGQJZOZ6tvKGTj/8eTUamV/bR8Sx4eAjG8vLmUyxVHhV9WaL7O9lMv7qFb7yGZneydsMWE4tVEGtVpEkhz1TthIm1StUK32+zo4RJIMkcvNpatrIfn8QsKw97gh3zgeIIp2EUW7SJJB5sy5aVzX+HGYtJCOiFwPrFHVb/v9m4FLVfWOjzonzSGdyUQ1IYr2EkU7iaK9iGQaHUIQ5L0RCnyjF6rVgwwPb2d4+A2Gh7dTrfaRyRS98e0hCAp+VFBFtUqtVvEjhhJJMuw7BPUdgbvB3OJnihtFOOOfzc4ml5tNNjuLMOylXH6PKHrbd159x70WkZDu7uX09Kwkn59HFO2hVNpJFL1NtXrwJCWR8Td9OMpgg4B/nkE19gbxCM2jqckiDKdTLK5Atcrg4GZfdiN6gyBHEORRjUmSQaBeBucTBHlvJErUaiUqlf3AyKyWIqHv1I7F1X2BOO4f805mVMfsRom1Y84XyZLPLySbPYNS6U3i+PC4r9mVe91ZCP21HR33+SciDHspFlfQ1bWESuU9hoffpFx+5yRnuQ4pDKc1jXYTvx37lPi2XjrJZ4FrSxlcpDn5yDpweqcDAUly5ITHnYj6fVkvT5EscXyIavWDUd+zevWhU/r8VIR0xouI3AbcBrBw4cIWq2kNIhkKhUUUCotaLWXcJMkQ1Wp/Y1SRJAOE4VS6uy8gkzn+TI1JMkSl0tcYNVSrB72n6LzEXG7OuD051YQ4PkIc9zc6sXqnVh8d1FMQZEd5sEHQTRwfbniBcdw/6njVhHx+AcXiCnK5MxveWa0WUyptZ2BgI1G0i1qtgmrFG10oFlcwZcoqisUVBMGxD8HVamVKpZ2+s95Gkgw2Rjy53Jlks7MJw+neE3VlGMeDlEo7KJWcd50kQ9Q7ZagRBPnGOW7EM5OurkXk82c1ylJVKZffYXBwsx+t1MvDhRqdl1sfAX3QGGU5A1pFJKCra3FjlNHVtYgkOdLwisvlfb6M5/hrmUMQ5InjAZJkoNFZdHefTy439xhvN0lKlEo7SZLBhpOiWiWOj/oRsmsvSXIE1+E1p3BUymSmNV3bDD8Cah41DTHSWSSIZMjn5zXaYDY7m0rlQMPzjqI9gBCGU/1oZZqv2xEnRCRsGnHOIAi6qVTeI4reoVzeQxTtpVYbbpSnapUw7PVluoSuLjea+CSYTA//cuAnqvplv78WQFV//lHndKqHbxiGcar8Px7+ZP6i9hKwVEQWi0gOuAF4fBK/zzAMwzgBkxbSUdVYRO4AnsIFy+5X1eOvJ2cYhmFMOpMaw1fVJ4EnJ/M7DMMwjPFhf5I2DMPoEMzgG4ZhdAhm8A3DMDoEM/iGYRgdghl8wzCMDiFV0yOLyEFgzymePhP44KRHtR7TOfG0i1bTObG0i06YXK1nq+qs8RyYKoP/cRCRDeN92qyVmM6Jp120ms6JpV10Qnq0WkjHMAyjQzCDbxiG0SGcTgb/d60WME5M58TTLlpN58TSLjohJVpPmxi+YRiGcWJOJw/fMAzDOAFtb/BFZI2IbBeRHSJyd6v1NCMi94tIn4hsacqbISJPi8hb/nV6KzV6TQtE5FkR2Soir4vInWnUKiJdIvKiiGz2On/q8xeLyAu+DTzsp+NuOSKSEZFXROQJv59WnbtF5DUR2SQiG3xequrea+oVkUdF5A0R2SYil6dNp4gs8+VYT0dF5K606Gxrg9+0UPo1wHLgRhFZ3lpVo/gTsGZM3t3AelVdCqz3+60mBr6jqsuBy4DbfTmmTWsZuEpVLwJWAmtE5DLgF8A9qnoO0A/c2kKNzdwJbGvaT6tOgC+o6sqmvw6mre4B7gX+qarnARfhyjZVOlV1uy/HlcBngGFgHWnRqaptm4DLgaea9tcCa1uta4zGRcCWpv3twFy/PRfY3mqNx9H8D+CLadYKdAMbgUtxD7SEx2sTLdQ3H3djXwU8gVsTL3U6vZbdwMwxeamqe2AasAv/u2NadY7R9iXgP2nS2dYePjAP2Nu0v8/npZk5qvq+394PzGmlmLGIyCLgYuAFUqjVh0k2AX3A08BO4LCOrDCdljbwK+C7jKwwfgbp1AluNfh/icjLfo1pSF/dLwYOAn/0YbLfi0iR9Ols5gbgIb+dCp3tbvDbGnXdfWr+JiUiPcDfgLtU9Wjze2nRqqqJuuHyfOAS4LwWSzoGEfkK0KeqL7dayzhZraqrcKHR20Xk881vpqTuQ2AV8BtVvRgYYkxYJCU6AfC/z1wHPDL2vVbqbHeD/y6woGl/vs9LMwdEZC6Af+1rsR4ARCSLM/YPqupjPjuVWgFU9TDwLC400isi9dXb0tAGrgCuE5HdwF9xYZ17SZ9OAFT1Xf/ah4s3X0L66n4fsE9VX/D7j+I6gLTprHMNsFFVD/j9VOhsd4PfjgulPw7c4rdvwcXLW4qICPAHYJuq/rLprVRpFZFZItLrtwu43xm24Qz/9f6wlutU1bWqOl9VF+Ha5L9V9SZSphNARIoiMqW+jYs7byFlda+q+4G9IrLMZ10NbCVlOpu4kZFwDqRFZ6t/2JiAH0auBd7ExXK/32o9Y7Q9BLwPVHEeyq24WO564C3gGWBGCnSuxg0xXwU2+XRt2rQCFwKveJ1bgB/5/CXAi8AO3BA63+oybdJ8JfBEWnV6TZt9er1+D6Wt7r2mlcAGX/9/B6anVGcR+BCY1pSXCp32pK1hGEaH0O4hHcMwDGOcmME3DMPoEMzgG4ZhdAhm8A3DMDoEM/iGYRgdghl8w5gAROTK+qyYhpFWzOAbhmF0CGbwjY5CRL7p59TfJCL3+cnYBkXkHj/H/noRmeWPXSki/xWRV0VkXX0OcxE5R0Se8fPybxSRT/mP72mar/1B/wSzYaQGM/hGxyAi5wNfB65QNwFbAtyEezJyg6peADwH/Nif8mfge6p6IfBaU/6DwK/Vzcv/WdzT1OBmGb0LtzbDEtycOoaRGsKTH2IYpw1X4xaleMk73wXcJFY14GF/zF+Ax0RkGtCrqs/5/AeAR/y8M/NUdR2AqkYA/vNeVNV9fn8Tbi2E5yf/sgxjfJjBNzoJAR5Q1bWjMkV+OOa4U51vpNy0nWD3l5EyLKRjdBLrgetFZDY01m09G3cf1Gex/AbwvKoeAfpF5HM+/2bgOVUdAPaJyFf9Z+RFpPsTvQrDOEXMAzE6BlXdKiI/wK3uFOBmMb0dt5jGJf69PlycH9w0tr/1Bv1t4Fs+/2bgPhH5mf+Mr32Cl2EYp4zNlml0PCIyqKo9rdZhGJONhXQMwzA6BPPwDcMwOgTz8A3DMDoEM/iGYRgdghl8wzCMDsEMvmEYRodgBt8wDKNDMINvGIbRIfwP5JLTFNfRS8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 2ms/sample - loss: 2.4855 - acc: 0.5333\n",
      "Loss: 2.4855427053734767 Accuracy: 0.53333336\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9385 - acc: 0.4226\n",
      "Epoch 00001: val_loss improved from inf to 2.16017, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_5_conv_checkpoint/001-2.1602.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 1.9386 - acc: 0.4226 - val_loss: 2.1602 - val_acc: 0.3757\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2819 - acc: 0.6118\n",
      "Epoch 00002: val_loss did not improve from 2.16017\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 1.2818 - acc: 0.6118 - val_loss: 4.2740 - val_acc: 0.2749\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0080 - acc: 0.6936\n",
      "Epoch 00003: val_loss improved from 2.16017 to 1.71760, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_5_conv_checkpoint/003-1.7176.hdf5\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 1.0080 - acc: 0.6935 - val_loss: 1.7176 - val_acc: 0.5346\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8027 - acc: 0.7595\n",
      "Epoch 00004: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.8031 - acc: 0.7594 - val_loss: 2.3024 - val_acc: 0.4973\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6467 - acc: 0.8075\n",
      "Epoch 00005: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.6467 - acc: 0.8075 - val_loss: 2.4943 - val_acc: 0.5024\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5030 - acc: 0.8517\n",
      "Epoch 00006: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.5032 - acc: 0.8517 - val_loss: 5.1081 - val_acc: 0.3163\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8944\n",
      "Epoch 00007: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.3839 - acc: 0.8943 - val_loss: 5.0148 - val_acc: 0.2919\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9210\n",
      "Epoch 00008: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.3025 - acc: 0.9210 - val_loss: 5.8699 - val_acc: 0.3138\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9402\n",
      "Epoch 00009: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.2420 - acc: 0.9402 - val_loss: 1.7494 - val_acc: 0.5660\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9603\n",
      "Epoch 00010: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1805 - acc: 0.9603 - val_loss: 5.0248 - val_acc: 0.3657\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9602\n",
      "Epoch 00011: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1800 - acc: 0.9602 - val_loss: 4.0318 - val_acc: 0.3948\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9787\n",
      "Epoch 00012: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1161 - acc: 0.9786 - val_loss: 4.2102 - val_acc: 0.3965\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9797\n",
      "Epoch 00013: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1099 - acc: 0.9796 - val_loss: 3.1308 - val_acc: 0.4910\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9808\n",
      "Epoch 00014: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1027 - acc: 0.9808 - val_loss: 2.6941 - val_acc: 0.5514\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9879\n",
      "Epoch 00015: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0765 - acc: 0.9879 - val_loss: 2.5862 - val_acc: 0.5399\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9879\n",
      "Epoch 00016: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0736 - acc: 0.9878 - val_loss: 3.4235 - val_acc: 0.4638\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9817\n",
      "Epoch 00017: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0888 - acc: 0.9816 - val_loss: 2.1758 - val_acc: 0.5754\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9822\n",
      "Epoch 00018: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0847 - acc: 0.9822 - val_loss: 2.4137 - val_acc: 0.5693\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9917\n",
      "Epoch 00019: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0530 - acc: 0.9917 - val_loss: 2.5354 - val_acc: 0.5404\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9852\n",
      "Epoch 00020: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0756 - acc: 0.9852 - val_loss: 2.8573 - val_acc: 0.5276\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9910\n",
      "Epoch 00021: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0521 - acc: 0.9909 - val_loss: 2.7202 - val_acc: 0.5253\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9910\n",
      "Epoch 00022: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0508 - acc: 0.9910 - val_loss: 1.9060 - val_acc: 0.6252\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9928\n",
      "Epoch 00023: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0429 - acc: 0.9928 - val_loss: 4.6898 - val_acc: 0.4032\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9932\n",
      "Epoch 00024: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0433 - acc: 0.9931 - val_loss: 2.4243 - val_acc: 0.5740\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9827\n",
      "Epoch 00025: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0753 - acc: 0.9827 - val_loss: 2.8560 - val_acc: 0.5572\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9943\n",
      "Epoch 00026: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0358 - acc: 0.9943 - val_loss: 2.4978 - val_acc: 0.5788\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9942\n",
      "Epoch 00027: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0365 - acc: 0.9942 - val_loss: 3.3097 - val_acc: 0.4817\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9909\n",
      "Epoch 00028: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0474 - acc: 0.9909 - val_loss: 4.1821 - val_acc: 0.4356\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9904\n",
      "Epoch 00029: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0460 - acc: 0.9903 - val_loss: 4.2271 - val_acc: 0.4684\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9916\n",
      "Epoch 00030: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0430 - acc: 0.9916 - val_loss: 2.5349 - val_acc: 0.5733\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9924\n",
      "Epoch 00031: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0379 - acc: 0.9924 - val_loss: 2.2198 - val_acc: 0.6201\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9877\n",
      "Epoch 00032: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0540 - acc: 0.9877 - val_loss: 2.1279 - val_acc: 0.6217\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9895\n",
      "Epoch 00033: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0486 - acc: 0.9895 - val_loss: 1.9312 - val_acc: 0.6399\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9949\n",
      "Epoch 00034: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0303 - acc: 0.9949 - val_loss: 1.9217 - val_acc: 0.6520\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9975\n",
      "Epoch 00035: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0216 - acc: 0.9975 - val_loss: 6.6293 - val_acc: 0.3270\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9863\n",
      "Epoch 00036: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0539 - acc: 0.9863 - val_loss: 2.2906 - val_acc: 0.6166\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9932\n",
      "Epoch 00037: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0347 - acc: 0.9932 - val_loss: 3.3533 - val_acc: 0.5043\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9971\n",
      "Epoch 00038: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0218 - acc: 0.9971 - val_loss: 3.1384 - val_acc: 0.5346\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9928\n",
      "Epoch 00039: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0358 - acc: 0.9928 - val_loss: 2.0519 - val_acc: 0.6317\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9865\n",
      "Epoch 00040: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0516 - acc: 0.9865 - val_loss: 2.2161 - val_acc: 0.6177\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9925\n",
      "Epoch 00041: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0353 - acc: 0.9924 - val_loss: 2.8295 - val_acc: 0.5602\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9922\n",
      "Epoch 00042: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0346 - acc: 0.9922 - val_loss: 2.2210 - val_acc: 0.6327\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9983\n",
      "Epoch 00043: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0161 - acc: 0.9982 - val_loss: 2.9646 - val_acc: 0.5616\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9874\n",
      "Epoch 00044: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0501 - acc: 0.9874 - val_loss: 2.1439 - val_acc: 0.6350\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9966\n",
      "Epoch 00045: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0205 - acc: 0.9966 - val_loss: 2.0239 - val_acc: 0.6599\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9948\n",
      "Epoch 00046: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0275 - acc: 0.9948 - val_loss: 2.3982 - val_acc: 0.5821\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9907\n",
      "Epoch 00047: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0397 - acc: 0.9907 - val_loss: 1.9920 - val_acc: 0.6520\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9964\n",
      "Epoch 00048: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0216 - acc: 0.9963 - val_loss: 2.3487 - val_acc: 0.6129\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9890\n",
      "Epoch 00049: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0449 - acc: 0.9890 - val_loss: 2.0014 - val_acc: 0.6578\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9960\n",
      "Epoch 00050: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0212 - acc: 0.9960 - val_loss: 2.2324 - val_acc: 0.6315\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9953\n",
      "Epoch 00051: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0247 - acc: 0.9953 - val_loss: 2.0254 - val_acc: 0.6606\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9948\n",
      "Epoch 00052: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0275 - acc: 0.9948 - val_loss: 2.2340 - val_acc: 0.6294\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9844\n",
      "Epoch 00053: val_loss did not improve from 1.71760\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0629 - acc: 0.9843 - val_loss: 1.9278 - val_acc: 0.6520\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.8931 - acc: 0.5034\n",
      "Loss: 1.893113769474802 Accuracy: 0.5034268\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9341 - acc: 0.4071\n",
      "Epoch 00001: val_loss improved from inf to 3.86839, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv_checkpoint/001-3.8684.hdf5\n",
      "36805/36805 [==============================] - 192s 5ms/sample - loss: 1.9342 - acc: 0.4071 - val_loss: 3.8684 - val_acc: 0.2360\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2978 - acc: 0.6021\n",
      "Epoch 00002: val_loss improved from 3.86839 to 1.46814, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv_checkpoint/002-1.4681.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 1.2979 - acc: 0.6021 - val_loss: 1.4681 - val_acc: 0.5570\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0603 - acc: 0.6761\n",
      "Epoch 00003: val_loss improved from 1.46814 to 1.43959, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv_checkpoint/003-1.4396.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 1.0604 - acc: 0.6760 - val_loss: 1.4396 - val_acc: 0.5672\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9171 - acc: 0.7232\n",
      "Epoch 00004: val_loss did not improve from 1.43959\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.9171 - acc: 0.7231 - val_loss: 1.5346 - val_acc: 0.5793\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8085 - acc: 0.7585\n",
      "Epoch 00005: val_loss did not improve from 1.43959\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.8087 - acc: 0.7585 - val_loss: 2.1913 - val_acc: 0.5243\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7432 - acc: 0.7808\n",
      "Epoch 00006: val_loss did not improve from 1.43959\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.7431 - acc: 0.7808 - val_loss: 3.2528 - val_acc: 0.4528\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6700 - acc: 0.8032\n",
      "Epoch 00007: val_loss improved from 1.43959 to 1.19942, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv_checkpoint/007-1.1994.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.6700 - acc: 0.8032 - val_loss: 1.1994 - val_acc: 0.6569\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6090 - acc: 0.8217\n",
      "Epoch 00008: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.6091 - acc: 0.8217 - val_loss: 2.5398 - val_acc: 0.5043\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5565 - acc: 0.8362\n",
      "Epoch 00009: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.5566 - acc: 0.8362 - val_loss: 1.2287 - val_acc: 0.6415\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8487\n",
      "Epoch 00010: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.5133 - acc: 0.8487 - val_loss: 1.5348 - val_acc: 0.6194\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4634 - acc: 0.8640\n",
      "Epoch 00011: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.4637 - acc: 0.8640 - val_loss: 2.9065 - val_acc: 0.4931\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.8748\n",
      "Epoch 00012: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.4288 - acc: 0.8747 - val_loss: 1.6444 - val_acc: 0.6238\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.8875\n",
      "Epoch 00013: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.3885 - acc: 0.8874 - val_loss: 2.2802 - val_acc: 0.5460\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8980\n",
      "Epoch 00014: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.3577 - acc: 0.8980 - val_loss: 1.6765 - val_acc: 0.6089\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.9113\n",
      "Epoch 00015: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.3179 - acc: 0.9113 - val_loss: 2.0606 - val_acc: 0.5437\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9189\n",
      "Epoch 00016: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.2878 - acc: 0.9188 - val_loss: 1.9592 - val_acc: 0.5735\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9301\n",
      "Epoch 00017: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.2576 - acc: 0.9301 - val_loss: 1.2558 - val_acc: 0.6918\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9428\n",
      "Epoch 00018: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.2176 - acc: 0.9428 - val_loss: 1.9872 - val_acc: 0.5882\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9449\n",
      "Epoch 00019: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.2121 - acc: 0.9448 - val_loss: 2.4260 - val_acc: 0.5623\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9502\n",
      "Epoch 00020: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1926 - acc: 0.9503 - val_loss: 2.5805 - val_acc: 0.5122\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9616\n",
      "Epoch 00021: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1609 - acc: 0.9615 - val_loss: 2.0327 - val_acc: 0.5719\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9585\n",
      "Epoch 00022: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1615 - acc: 0.9584 - val_loss: 1.2612 - val_acc: 0.7009\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9677\n",
      "Epoch 00023: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1348 - acc: 0.9677 - val_loss: 2.2810 - val_acc: 0.5917\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9655\n",
      "Epoch 00024: val_loss did not improve from 1.19942\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1413 - acc: 0.9655 - val_loss: 1.5355 - val_acc: 0.6611\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9756\n",
      "Epoch 00025: val_loss improved from 1.19942 to 1.17844, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv_checkpoint/025-1.1784.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1112 - acc: 0.9756 - val_loss: 1.1784 - val_acc: 0.7133\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9773\n",
      "Epoch 00026: val_loss did not improve from 1.17844\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1023 - acc: 0.9773 - val_loss: 1.1847 - val_acc: 0.7172\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9790\n",
      "Epoch 00027: val_loss did not improve from 1.17844\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0963 - acc: 0.9790 - val_loss: 1.7807 - val_acc: 0.6539\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9748\n",
      "Epoch 00028: val_loss did not improve from 1.17844\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1066 - acc: 0.9748 - val_loss: 1.6367 - val_acc: 0.6417\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9817\n",
      "Epoch 00029: val_loss did not improve from 1.17844\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0860 - acc: 0.9817 - val_loss: 2.5498 - val_acc: 0.5537\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9795\n",
      "Epoch 00030: val_loss did not improve from 1.17844\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0893 - acc: 0.9795 - val_loss: 1.3726 - val_acc: 0.7002\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9808\n",
      "Epoch 00031: val_loss did not improve from 1.17844\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0869 - acc: 0.9808 - val_loss: 2.0298 - val_acc: 0.6403\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9833\n",
      "Epoch 00032: val_loss did not improve from 1.17844\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0769 - acc: 0.9833 - val_loss: 1.5850 - val_acc: 0.6725\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9850\n",
      "Epoch 00033: val_loss did not improve from 1.17844\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0725 - acc: 0.9850 - val_loss: 1.3436 - val_acc: 0.7072\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9832\n",
      "Epoch 00034: val_loss did not improve from 1.17844\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0761 - acc: 0.9832 - val_loss: 1.2319 - val_acc: 0.7279\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9858\n",
      "Epoch 00035: val_loss did not improve from 1.17844\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0680 - acc: 0.9858 - val_loss: 1.4581 - val_acc: 0.7028\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9845\n",
      "Epoch 00036: val_loss improved from 1.17844 to 1.14078, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv_checkpoint/036-1.1408.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0713 - acc: 0.9845 - val_loss: 1.1408 - val_acc: 0.7454\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9918\n",
      "Epoch 00037: val_loss did not improve from 1.14078\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0472 - acc: 0.9917 - val_loss: 2.7484 - val_acc: 0.5628\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9783\n",
      "Epoch 00038: val_loss did not improve from 1.14078\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0873 - acc: 0.9783 - val_loss: 1.3687 - val_acc: 0.7140\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9883\n",
      "Epoch 00039: val_loss did not improve from 1.14078\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0581 - acc: 0.9883 - val_loss: 1.1787 - val_acc: 0.7340\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9936\n",
      "Epoch 00040: val_loss did not improve from 1.14078\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0393 - acc: 0.9935 - val_loss: 2.4847 - val_acc: 0.5782\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9883\n",
      "Epoch 00041: val_loss improved from 1.14078 to 1.08036, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv_checkpoint/041-1.0804.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0558 - acc: 0.9883 - val_loss: 1.0804 - val_acc: 0.7584\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9935\n",
      "Epoch 00042: val_loss did not improve from 1.08036\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0391 - acc: 0.9934 - val_loss: 1.2638 - val_acc: 0.7228\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9762\n",
      "Epoch 00043: val_loss did not improve from 1.08036\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0922 - acc: 0.9762 - val_loss: 1.5744 - val_acc: 0.6879\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9944\n",
      "Epoch 00044: val_loss did not improve from 1.08036\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0356 - acc: 0.9943 - val_loss: 2.1682 - val_acc: 0.6329\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9837\n",
      "Epoch 00045: val_loss improved from 1.08036 to 0.98788, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv_checkpoint/045-0.9879.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0669 - acc: 0.9836 - val_loss: 0.9879 - val_acc: 0.7848\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9921\n",
      "Epoch 00046: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0425 - acc: 0.9921 - val_loss: 1.3075 - val_acc: 0.7261\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9954\n",
      "Epoch 00047: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0309 - acc: 0.9954 - val_loss: 1.5273 - val_acc: 0.6990\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9840\n",
      "Epoch 00048: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0641 - acc: 0.9841 - val_loss: 1.2439 - val_acc: 0.7424\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9941\n",
      "Epoch 00049: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0323 - acc: 0.9940 - val_loss: 3.1200 - val_acc: 0.5469\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9887\n",
      "Epoch 00050: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0489 - acc: 0.9887 - val_loss: 1.2109 - val_acc: 0.7512\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9865\n",
      "Epoch 00051: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0549 - acc: 0.9864 - val_loss: 1.2647 - val_acc: 0.7338\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9889\n",
      "Epoch 00052: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0518 - acc: 0.9889 - val_loss: 1.2751 - val_acc: 0.7463\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9972\n",
      "Epoch 00053: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0238 - acc: 0.9971 - val_loss: 1.2045 - val_acc: 0.7536\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9842\n",
      "Epoch 00054: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0626 - acc: 0.9841 - val_loss: 1.2095 - val_acc: 0.7494\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9902\n",
      "Epoch 00055: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0436 - acc: 0.9901 - val_loss: 1.1314 - val_acc: 0.7608\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9921\n",
      "Epoch 00056: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0384 - acc: 0.9921 - val_loss: 1.9991 - val_acc: 0.6620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9938\n",
      "Epoch 00057: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0339 - acc: 0.9938 - val_loss: 1.6500 - val_acc: 0.6813\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9918\n",
      "Epoch 00058: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0394 - acc: 0.9918 - val_loss: 1.2836 - val_acc: 0.7417\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9921\n",
      "Epoch 00059: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0366 - acc: 0.9920 - val_loss: 1.6991 - val_acc: 0.6900\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9895\n",
      "Epoch 00060: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0457 - acc: 0.9894 - val_loss: 1.0939 - val_acc: 0.7754\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9877\n",
      "Epoch 00061: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0558 - acc: 0.9877 - val_loss: 1.1575 - val_acc: 0.7729\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9965\n",
      "Epoch 00062: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0229 - acc: 0.9965 - val_loss: 1.5629 - val_acc: 0.7042\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9892\n",
      "Epoch 00063: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0474 - acc: 0.9892 - val_loss: 1.2133 - val_acc: 0.7524\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9926\n",
      "Epoch 00064: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0350 - acc: 0.9926 - val_loss: 1.1382 - val_acc: 0.7692\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9946\n",
      "Epoch 00065: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0286 - acc: 0.9945 - val_loss: 1.0717 - val_acc: 0.7836\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9849\n",
      "Epoch 00066: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0562 - acc: 0.9849 - val_loss: 1.1713 - val_acc: 0.7738\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9969\n",
      "Epoch 00067: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0216 - acc: 0.9968 - val_loss: 1.9155 - val_acc: 0.6550\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9912\n",
      "Epoch 00068: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0407 - acc: 0.9912 - val_loss: 2.8545 - val_acc: 0.5714\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9873\n",
      "Epoch 00069: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0519 - acc: 0.9873 - val_loss: 1.3016 - val_acc: 0.7454\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9980\n",
      "Epoch 00070: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0171 - acc: 0.9980 - val_loss: 1.0650 - val_acc: 0.7787\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9893\n",
      "Epoch 00071: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0480 - acc: 0.9893 - val_loss: 1.1312 - val_acc: 0.7768\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9905\n",
      "Epoch 00072: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0410 - acc: 0.9905 - val_loss: 1.1217 - val_acc: 0.7724\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9967\n",
      "Epoch 00073: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0218 - acc: 0.9967 - val_loss: 2.4217 - val_acc: 0.6182\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9889\n",
      "Epoch 00074: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0454 - acc: 0.9889 - val_loss: 1.0009 - val_acc: 0.7929\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9976\n",
      "Epoch 00075: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0188 - acc: 0.9976 - val_loss: 1.2553 - val_acc: 0.7568\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9888\n",
      "Epoch 00076: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0439 - acc: 0.9888 - val_loss: 1.1788 - val_acc: 0.7619\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9960\n",
      "Epoch 00077: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0233 - acc: 0.9960 - val_loss: 1.1894 - val_acc: 0.7566\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9868\n",
      "Epoch 00078: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0539 - acc: 0.9867 - val_loss: 1.5157 - val_acc: 0.7226\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9860\n",
      "Epoch 00079: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0552 - acc: 0.9860 - val_loss: 1.0372 - val_acc: 0.7901\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9949\n",
      "Epoch 00080: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0269 - acc: 0.9949 - val_loss: 1.1409 - val_acc: 0.7715\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9904\n",
      "Epoch 00081: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0407 - acc: 0.9904 - val_loss: 1.0731 - val_acc: 0.7764\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9985\n",
      "Epoch 00082: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0150 - acc: 0.9985 - val_loss: 1.0606 - val_acc: 0.7908\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9910\n",
      "Epoch 00083: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0401 - acc: 0.9910 - val_loss: 1.1712 - val_acc: 0.7708\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9895\n",
      "Epoch 00084: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0417 - acc: 0.9895 - val_loss: 1.1465 - val_acc: 0.7801\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9974\n",
      "Epoch 00085: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0177 - acc: 0.9974 - val_loss: 1.2074 - val_acc: 0.7664\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9956\n",
      "Epoch 00086: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0238 - acc: 0.9956 - val_loss: 1.8073 - val_acc: 0.7009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9907\n",
      "Epoch 00087: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0402 - acc: 0.9907 - val_loss: 1.0817 - val_acc: 0.7831\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9950\n",
      "Epoch 00088: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0264 - acc: 0.9949 - val_loss: 1.3976 - val_acc: 0.7340\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9851\n",
      "Epoch 00089: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0574 - acc: 0.9851 - val_loss: 1.1269 - val_acc: 0.7694\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9915\n",
      "Epoch 00090: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0371 - acc: 0.9914 - val_loss: 1.2874 - val_acc: 0.7570\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9926\n",
      "Epoch 00091: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0326 - acc: 0.9926 - val_loss: 1.0673 - val_acc: 0.7897\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9986\n",
      "Epoch 00092: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0137 - acc: 0.9986 - val_loss: 1.1004 - val_acc: 0.7894\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9932\n",
      "Epoch 00093: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0313 - acc: 0.9932 - val_loss: 1.1237 - val_acc: 0.7780\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9929\n",
      "Epoch 00094: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0333 - acc: 0.9928 - val_loss: 1.2878 - val_acc: 0.7556\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9898\n",
      "Epoch 00095: val_loss did not improve from 0.98788\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0406 - acc: 0.9898 - val_loss: 1.1502 - val_acc: 0.7789\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.1385 - acc: 0.7570\n",
      "Loss: 1.1384778298569123 Accuracy: 0.7570093\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9517 - acc: 0.4023\n",
      "Epoch 00001: val_loss improved from inf to 2.33867, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/001-2.3387.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 1.9517 - acc: 0.4023 - val_loss: 2.3387 - val_acc: 0.3422\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3123 - acc: 0.5967\n",
      "Epoch 00002: val_loss improved from 2.33867 to 1.40056, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/002-1.4006.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 1.3125 - acc: 0.5966 - val_loss: 1.4006 - val_acc: 0.5740\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0595 - acc: 0.6835\n",
      "Epoch 00003: val_loss did not improve from 1.40056\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 1.0595 - acc: 0.6835 - val_loss: 1.8794 - val_acc: 0.4994\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8888 - acc: 0.7373\n",
      "Epoch 00004: val_loss did not improve from 1.40056\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.8887 - acc: 0.7373 - val_loss: 1.8037 - val_acc: 0.5367\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7807 - acc: 0.7721\n",
      "Epoch 00005: val_loss did not improve from 1.40056\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.7807 - acc: 0.7722 - val_loss: 1.4129 - val_acc: 0.6215\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.8002\n",
      "Epoch 00006: val_loss improved from 1.40056 to 0.79248, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/006-0.7925.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.6916 - acc: 0.8002 - val_loss: 0.7925 - val_acc: 0.7699\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6239 - acc: 0.8214\n",
      "Epoch 00007: val_loss did not improve from 0.79248\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.6239 - acc: 0.8213 - val_loss: 1.5688 - val_acc: 0.6094\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.8368\n",
      "Epoch 00008: val_loss did not improve from 0.79248\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.5702 - acc: 0.8368 - val_loss: 0.8217 - val_acc: 0.7519\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.8525\n",
      "Epoch 00009: val_loss did not improve from 0.79248\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.5166 - acc: 0.8524 - val_loss: 1.2685 - val_acc: 0.6739\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4709 - acc: 0.8662\n",
      "Epoch 00010: val_loss did not improve from 0.79248\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.4710 - acc: 0.8661 - val_loss: 1.0565 - val_acc: 0.6921\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.8770\n",
      "Epoch 00011: val_loss did not improve from 0.79248\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.4316 - acc: 0.8770 - val_loss: 0.8106 - val_acc: 0.7508\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8867\n",
      "Epoch 00012: val_loss did not improve from 0.79248\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3955 - acc: 0.8867 - val_loss: 1.8416 - val_acc: 0.5693\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3666 - acc: 0.8959\n",
      "Epoch 00013: val_loss did not improve from 0.79248\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3667 - acc: 0.8959 - val_loss: 0.8736 - val_acc: 0.7671\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.9042\n",
      "Epoch 00014: val_loss improved from 0.79248 to 0.59784, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/014-0.5978.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3399 - acc: 0.9042 - val_loss: 0.5978 - val_acc: 0.8169\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9136\n",
      "Epoch 00015: val_loss did not improve from 0.59784\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3086 - acc: 0.9136 - val_loss: 1.6156 - val_acc: 0.6215\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9204\n",
      "Epoch 00016: val_loss did not improve from 0.59784\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2841 - acc: 0.9203 - val_loss: 0.7877 - val_acc: 0.7741\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.9252\n",
      "Epoch 00017: val_loss did not improve from 0.59784\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2669 - acc: 0.9252 - val_loss: 1.2845 - val_acc: 0.6874\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9308\n",
      "Epoch 00018: val_loss did not improve from 0.59784\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2450 - acc: 0.9307 - val_loss: 1.2606 - val_acc: 0.6956\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9353\n",
      "Epoch 00019: val_loss did not improve from 0.59784\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2335 - acc: 0.9353 - val_loss: 0.8693 - val_acc: 0.7610\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9385\n",
      "Epoch 00020: val_loss improved from 0.59784 to 0.58933, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/020-0.5893.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2185 - acc: 0.9385 - val_loss: 0.5893 - val_acc: 0.8362\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9447\n",
      "Epoch 00021: val_loss did not improve from 0.58933\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2019 - acc: 0.9447 - val_loss: 0.9063 - val_acc: 0.7561\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9510\n",
      "Epoch 00022: val_loss did not improve from 0.58933\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1817 - acc: 0.9510 - val_loss: 0.9149 - val_acc: 0.7629\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9543\n",
      "Epoch 00023: val_loss did not improve from 0.58933\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1683 - acc: 0.9542 - val_loss: 1.0366 - val_acc: 0.7368\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9572\n",
      "Epoch 00024: val_loss improved from 0.58933 to 0.54268, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/024-0.5427.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1586 - acc: 0.9572 - val_loss: 0.5427 - val_acc: 0.8435\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9588\n",
      "Epoch 00025: val_loss did not improve from 0.54268\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1494 - acc: 0.9588 - val_loss: 0.6516 - val_acc: 0.8237\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9629\n",
      "Epoch 00026: val_loss did not improve from 0.54268\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1395 - acc: 0.9629 - val_loss: 1.1706 - val_acc: 0.7372\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9696\n",
      "Epoch 00027: val_loss did not improve from 0.54268\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1201 - acc: 0.9695 - val_loss: 2.0235 - val_acc: 0.6350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9680\n",
      "Epoch 00028: val_loss improved from 0.54268 to 0.53493, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/028-0.5349.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1207 - acc: 0.9680 - val_loss: 0.5349 - val_acc: 0.8574\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9719\n",
      "Epoch 00029: val_loss did not improve from 0.53493\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1062 - acc: 0.9719 - val_loss: 0.6469 - val_acc: 0.8258\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9665\n",
      "Epoch 00030: val_loss did not improve from 0.53493\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1243 - acc: 0.9665 - val_loss: 0.5489 - val_acc: 0.8530\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9774\n",
      "Epoch 00031: val_loss did not improve from 0.53493\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0891 - acc: 0.9774 - val_loss: 0.9164 - val_acc: 0.7857\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9773\n",
      "Epoch 00032: val_loss did not improve from 0.53493\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0896 - acc: 0.9773 - val_loss: 0.8305 - val_acc: 0.7911\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9765\n",
      "Epoch 00033: val_loss did not improve from 0.53493\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0926 - acc: 0.9766 - val_loss: 0.8017 - val_acc: 0.8074\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9820\n",
      "Epoch 00034: val_loss did not improve from 0.53493\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0738 - acc: 0.9820 - val_loss: 0.9496 - val_acc: 0.7554\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9815\n",
      "Epoch 00035: val_loss did not improve from 0.53493\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0763 - acc: 0.9816 - val_loss: 0.5817 - val_acc: 0.8488\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9823\n",
      "Epoch 00036: val_loss did not improve from 0.53493\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0721 - acc: 0.9823 - val_loss: 0.6569 - val_acc: 0.8414\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9812\n",
      "Epoch 00037: val_loss improved from 0.53493 to 0.50531, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/037-0.5053.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0773 - acc: 0.9812 - val_loss: 0.5053 - val_acc: 0.8686\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9792\n",
      "Epoch 00038: val_loss did not improve from 0.50531\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0791 - acc: 0.9791 - val_loss: 0.6624 - val_acc: 0.8362\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9814\n",
      "Epoch 00039: val_loss improved from 0.50531 to 0.44550, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/039-0.4455.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0732 - acc: 0.9814 - val_loss: 0.4455 - val_acc: 0.8814\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9868\n",
      "Epoch 00040: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0559 - acc: 0.9868 - val_loss: 0.5029 - val_acc: 0.8726\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9902\n",
      "Epoch 00041: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0476 - acc: 0.9902 - val_loss: 0.7715 - val_acc: 0.8160\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9834\n",
      "Epoch 00042: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0662 - acc: 0.9834 - val_loss: 0.4929 - val_acc: 0.8710\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9831\n",
      "Epoch 00043: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0646 - acc: 0.9831 - val_loss: 0.6002 - val_acc: 0.8477\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9847\n",
      "Epoch 00044: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0593 - acc: 0.9847 - val_loss: 0.6100 - val_acc: 0.8532\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9889\n",
      "Epoch 00045: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0509 - acc: 0.9889 - val_loss: 0.6809 - val_acc: 0.8372\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9919\n",
      "Epoch 00046: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0390 - acc: 0.9919 - val_loss: 0.6293 - val_acc: 0.8437\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9895\n",
      "Epoch 00047: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0461 - acc: 0.9895 - val_loss: 1.0158 - val_acc: 0.7706\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9851\n",
      "Epoch 00048: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0566 - acc: 0.9850 - val_loss: 0.6690 - val_acc: 0.8404\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9854\n",
      "Epoch 00049: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0570 - acc: 0.9854 - val_loss: 0.6379 - val_acc: 0.8467\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9857\n",
      "Epoch 00050: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0588 - acc: 0.9857 - val_loss: 0.7826 - val_acc: 0.8130\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9901\n",
      "Epoch 00051: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0435 - acc: 0.9901 - val_loss: 0.4565 - val_acc: 0.8847\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9932\n",
      "Epoch 00052: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0326 - acc: 0.9932 - val_loss: 0.5267 - val_acc: 0.8691\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9918\n",
      "Epoch 00053: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0374 - acc: 0.9918 - val_loss: 0.6355 - val_acc: 0.8446\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9871\n",
      "Epoch 00054: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0490 - acc: 0.9871 - val_loss: 0.6285 - val_acc: 0.8423\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9937\n",
      "Epoch 00055: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0314 - acc: 0.9937 - val_loss: 0.7376 - val_acc: 0.8267\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9875\n",
      "Epoch 00056: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0519 - acc: 0.9875 - val_loss: 0.8575 - val_acc: 0.8027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9971\n",
      "Epoch 00057: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0195 - acc: 0.9971 - val_loss: 0.7536 - val_acc: 0.8304\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9878\n",
      "Epoch 00058: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0475 - acc: 0.9878 - val_loss: 0.5073 - val_acc: 0.8763\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9851\n",
      "Epoch 00059: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0582 - acc: 0.9851 - val_loss: 0.8409 - val_acc: 0.8069\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9919\n",
      "Epoch 00060: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0367 - acc: 0.9918 - val_loss: 0.4989 - val_acc: 0.8763\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9908\n",
      "Epoch 00061: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0388 - acc: 0.9908 - val_loss: 0.7607 - val_acc: 0.8300\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9957\n",
      "Epoch 00062: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0222 - acc: 0.9957 - val_loss: 0.6100 - val_acc: 0.8539\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9930\n",
      "Epoch 00063: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0297 - acc: 0.9930 - val_loss: 0.9207 - val_acc: 0.8120\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9897\n",
      "Epoch 00064: val_loss did not improve from 0.44550\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0406 - acc: 0.9897 - val_loss: 1.6286 - val_acc: 0.7014\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9903\n",
      "Epoch 00065: val_loss improved from 0.44550 to 0.41632, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/065-0.4163.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0379 - acc: 0.9903 - val_loss: 0.4163 - val_acc: 0.8915\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9940\n",
      "Epoch 00066: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0268 - acc: 0.9940 - val_loss: 0.5641 - val_acc: 0.8644\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9932\n",
      "Epoch 00067: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0289 - acc: 0.9932 - val_loss: 1.2275 - val_acc: 0.7554\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9906\n",
      "Epoch 00068: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0364 - acc: 0.9906 - val_loss: 0.4881 - val_acc: 0.8821\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9948\n",
      "Epoch 00069: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0241 - acc: 0.9948 - val_loss: 0.5162 - val_acc: 0.8840\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9958\n",
      "Epoch 00070: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0215 - acc: 0.9958 - val_loss: 0.4804 - val_acc: 0.8882\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9914\n",
      "Epoch 00071: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0336 - acc: 0.9914 - val_loss: 0.6170 - val_acc: 0.8577\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9914\n",
      "Epoch 00072: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0339 - acc: 0.9914 - val_loss: 0.4863 - val_acc: 0.8905\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9933\n",
      "Epoch 00073: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0283 - acc: 0.9933 - val_loss: 0.6915 - val_acc: 0.8574\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9979\n",
      "Epoch 00074: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0132 - acc: 0.9979 - val_loss: 0.6089 - val_acc: 0.8614\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9879\n",
      "Epoch 00075: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0455 - acc: 0.9879 - val_loss: 0.4523 - val_acc: 0.8968\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9965\n",
      "Epoch 00076: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0184 - acc: 0.9965 - val_loss: 0.5484 - val_acc: 0.8726\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9917\n",
      "Epoch 00077: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0332 - acc: 0.9917 - val_loss: 0.5976 - val_acc: 0.8514\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9942\n",
      "Epoch 00078: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0254 - acc: 0.9942 - val_loss: 0.4704 - val_acc: 0.8912\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9967\n",
      "Epoch 00079: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0165 - acc: 0.9967 - val_loss: 1.6462 - val_acc: 0.7074\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9877\n",
      "Epoch 00080: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0468 - acc: 0.9877 - val_loss: 0.4437 - val_acc: 0.8945\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9984\n",
      "Epoch 00081: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0117 - acc: 0.9984 - val_loss: 0.5065 - val_acc: 0.8863\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9878\n",
      "Epoch 00082: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0459 - acc: 0.9878 - val_loss: 0.4518 - val_acc: 0.8908\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9987\n",
      "Epoch 00083: val_loss did not improve from 0.41632\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0108 - acc: 0.9987 - val_loss: 0.5514 - val_acc: 0.8742\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9889\n",
      "Epoch 00084: val_loss improved from 0.41632 to 0.40617, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/084-0.4062.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0430 - acc: 0.9889 - val_loss: 0.4062 - val_acc: 0.9043\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9966\n",
      "Epoch 00085: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0180 - acc: 0.9966 - val_loss: 0.4573 - val_acc: 0.8959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9938\n",
      "Epoch 00086: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0258 - acc: 0.9938 - val_loss: 0.5957 - val_acc: 0.8698\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9943\n",
      "Epoch 00087: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0251 - acc: 0.9943 - val_loss: 0.4481 - val_acc: 0.8915\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9940\n",
      "Epoch 00088: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0254 - acc: 0.9939 - val_loss: 0.5883 - val_acc: 0.8763\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9908\n",
      "Epoch 00089: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0363 - acc: 0.9908 - val_loss: 0.4814 - val_acc: 0.8882\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9941\n",
      "Epoch 00090: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0258 - acc: 0.9941 - val_loss: 0.4079 - val_acc: 0.9096\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9985\n",
      "Epoch 00091: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0107 - acc: 0.9985 - val_loss: 0.5101 - val_acc: 0.8870\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9870\n",
      "Epoch 00092: val_loss did not improve from 0.40617\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0493 - acc: 0.9870 - val_loss: 0.4459 - val_acc: 0.8956\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9986\n",
      "Epoch 00093: val_loss improved from 0.40617 to 0.40503, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/093-0.4050.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0106 - acc: 0.9986 - val_loss: 0.4050 - val_acc: 0.9075\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9964\n",
      "Epoch 00094: val_loss did not improve from 0.40503\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0165 - acc: 0.9964 - val_loss: 0.5237 - val_acc: 0.8854\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9942\n",
      "Epoch 00095: val_loss did not improve from 0.40503\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0229 - acc: 0.9942 - val_loss: 0.4312 - val_acc: 0.9036\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9953\n",
      "Epoch 00096: val_loss did not improve from 0.40503\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0204 - acc: 0.9952 - val_loss: 2.8200 - val_acc: 0.5574\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9868\n",
      "Epoch 00097: val_loss did not improve from 0.40503\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0479 - acc: 0.9868 - val_loss: 0.4244 - val_acc: 0.9071\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9980\n",
      "Epoch 00098: val_loss did not improve from 0.40503\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0117 - acc: 0.9980 - val_loss: 0.5415 - val_acc: 0.8805\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9965\n",
      "Epoch 00099: val_loss did not improve from 0.40503\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0155 - acc: 0.9965 - val_loss: 0.5011 - val_acc: 0.8868\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9954\n",
      "Epoch 00100: val_loss did not improve from 0.40503\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0207 - acc: 0.9954 - val_loss: 0.5238 - val_acc: 0.8793\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9912\n",
      "Epoch 00101: val_loss did not improve from 0.40503\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0307 - acc: 0.9911 - val_loss: 0.5320 - val_acc: 0.8859\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9895\n",
      "Epoch 00102: val_loss improved from 0.40503 to 0.39007, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv_checkpoint/102-0.3901.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0366 - acc: 0.9895 - val_loss: 0.3901 - val_acc: 0.9052\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9989\n",
      "Epoch 00103: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0086 - acc: 0.9989 - val_loss: 0.4317 - val_acc: 0.8973\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9935\n",
      "Epoch 00104: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0270 - acc: 0.9935 - val_loss: 0.4648 - val_acc: 0.8949\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9987\n",
      "Epoch 00105: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0096 - acc: 0.9987 - val_loss: 0.4441 - val_acc: 0.9019\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9952\n",
      "Epoch 00106: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0208 - acc: 0.9952 - val_loss: 0.4232 - val_acc: 0.9031\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9904\n",
      "Epoch 00107: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0359 - acc: 0.9904 - val_loss: 0.5526 - val_acc: 0.8793\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9970\n",
      "Epoch 00108: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0147 - acc: 0.9969 - val_loss: 0.6397 - val_acc: 0.8651\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9899\n",
      "Epoch 00109: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0397 - acc: 0.9898 - val_loss: 0.7095 - val_acc: 0.8581\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9937\n",
      "Epoch 00110: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0250 - acc: 0.9937 - val_loss: 0.4098 - val_acc: 0.9043\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9967\n",
      "Epoch 00111: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0156 - acc: 0.9967 - val_loss: 0.4960 - val_acc: 0.8917\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9949\n",
      "Epoch 00112: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0208 - acc: 0.9948 - val_loss: 0.4173 - val_acc: 0.9080\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9930\n",
      "Epoch 00113: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0278 - acc: 0.9930 - val_loss: 0.4426 - val_acc: 0.9005\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9935\n",
      "Epoch 00114: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0268 - acc: 0.9935 - val_loss: 0.4608 - val_acc: 0.8931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9952\n",
      "Epoch 00115: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0210 - acc: 0.9952 - val_loss: 0.5476 - val_acc: 0.8819\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9971\n",
      "Epoch 00116: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0135 - acc: 0.9971 - val_loss: 0.8212 - val_acc: 0.8318\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9951\n",
      "Epoch 00117: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0213 - acc: 0.9950 - val_loss: 0.5160 - val_acc: 0.8847\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9909\n",
      "Epoch 00118: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0340 - acc: 0.9909 - val_loss: 0.4192 - val_acc: 0.9033\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9990\n",
      "Epoch 00119: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0076 - acc: 0.9990 - val_loss: 0.4424 - val_acc: 0.9015\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9964\n",
      "Epoch 00120: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0159 - acc: 0.9964 - val_loss: 0.6019 - val_acc: 0.8682\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9967\n",
      "Epoch 00121: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0154 - acc: 0.9967 - val_loss: 0.9114 - val_acc: 0.8123\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9930\n",
      "Epoch 00122: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0277 - acc: 0.9930 - val_loss: 0.4036 - val_acc: 0.9094\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9988\n",
      "Epoch 00123: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0088 - acc: 0.9988 - val_loss: 0.4662 - val_acc: 0.9040\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9948\n",
      "Epoch 00124: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0203 - acc: 0.9948 - val_loss: 0.6061 - val_acc: 0.8658\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9959\n",
      "Epoch 00125: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0183 - acc: 0.9958 - val_loss: 0.5702 - val_acc: 0.8772\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9908\n",
      "Epoch 00126: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0355 - acc: 0.9908 - val_loss: 0.4680 - val_acc: 0.8910\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9984\n",
      "Epoch 00127: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0095 - acc: 0.9983 - val_loss: 0.4204 - val_acc: 0.9092\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9932\n",
      "Epoch 00128: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0269 - acc: 0.9932 - val_loss: 0.5608 - val_acc: 0.8756\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9943\n",
      "Epoch 00129: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0215 - acc: 0.9943 - val_loss: 0.4767 - val_acc: 0.8968\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9992\n",
      "Epoch 00130: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0066 - acc: 0.9992 - val_loss: 0.3965 - val_acc: 0.9175\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9965\n",
      "Epoch 00131: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0155 - acc: 0.9964 - val_loss: 0.8594 - val_acc: 0.8204\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9891\n",
      "Epoch 00132: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0408 - acc: 0.9891 - val_loss: 0.5148 - val_acc: 0.8921\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9952\n",
      "Epoch 00133: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0210 - acc: 0.9951 - val_loss: 0.4318 - val_acc: 0.9092\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9915\n",
      "Epoch 00134: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0323 - acc: 0.9914 - val_loss: 0.4285 - val_acc: 0.9080\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9956\n",
      "Epoch 00135: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0201 - acc: 0.9956 - val_loss: 0.4671 - val_acc: 0.9001\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9934\n",
      "Epoch 00136: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0273 - acc: 0.9934 - val_loss: 0.4193 - val_acc: 0.9119\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9993\n",
      "Epoch 00137: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0063 - acc: 0.9993 - val_loss: 0.4980 - val_acc: 0.8868\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9939\n",
      "Epoch 00138: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0247 - acc: 0.9939 - val_loss: 0.6950 - val_acc: 0.8628\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9984\n",
      "Epoch 00139: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0092 - acc: 0.9984 - val_loss: 0.4710 - val_acc: 0.9026\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9983\n",
      "Epoch 00140: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0097 - acc: 0.9983 - val_loss: 0.5516 - val_acc: 0.8826\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9970\n",
      "Epoch 00141: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0134 - acc: 0.9970 - val_loss: 0.4583 - val_acc: 0.9012\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9908\n",
      "Epoch 00142: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0329 - acc: 0.9908 - val_loss: 0.6967 - val_acc: 0.8607\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9912\n",
      "Epoch 00143: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0338 - acc: 0.9912 - val_loss: 0.4491 - val_acc: 0.9110\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9992\n",
      "Epoch 00144: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0064 - acc: 0.9992 - val_loss: 0.4344 - val_acc: 0.9068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9992\n",
      "Epoch 00145: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0059 - acc: 0.9991 - val_loss: 0.4230 - val_acc: 0.9103\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9936\n",
      "Epoch 00146: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0243 - acc: 0.9936 - val_loss: 0.4850 - val_acc: 0.8956\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9961\n",
      "Epoch 00147: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0160 - acc: 0.9961 - val_loss: 0.4563 - val_acc: 0.9038\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9974\n",
      "Epoch 00148: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0117 - acc: 0.9973 - val_loss: 0.6237 - val_acc: 0.8689\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9907\n",
      "Epoch 00149: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0371 - acc: 0.9907 - val_loss: 0.4274 - val_acc: 0.9057\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9971\n",
      "Epoch 00150: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0131 - acc: 0.9971 - val_loss: 0.4597 - val_acc: 0.9057\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9965\n",
      "Epoch 00151: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0148 - acc: 0.9965 - val_loss: 0.4866 - val_acc: 0.8961\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9969\n",
      "Epoch 00152: val_loss did not improve from 0.39007\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0127 - acc: 0.9969 - val_loss: 0.4500 - val_acc: 0.9008\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.5005 - acc: 0.8839\n",
      "Loss: 0.5005481078741087 Accuracy: 0.88390446\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9064 - acc: 0.4202\n",
      "Epoch 00001: val_loss improved from inf to 1.57002, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/001-1.5700.hdf5\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 1.9064 - acc: 0.4202 - val_loss: 1.5700 - val_acc: 0.5292\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1789 - acc: 0.6473\n",
      "Epoch 00002: val_loss improved from 1.57002 to 1.11876, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/002-1.1188.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 1.1790 - acc: 0.6472 - val_loss: 1.1188 - val_acc: 0.6643\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8829 - acc: 0.7432\n",
      "Epoch 00003: val_loss improved from 1.11876 to 0.93099, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/003-0.9310.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.8828 - acc: 0.7432 - val_loss: 0.9310 - val_acc: 0.7202\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.8029\n",
      "Epoch 00004: val_loss improved from 0.93099 to 0.67536, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/004-0.6754.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.6932 - acc: 0.8029 - val_loss: 0.6754 - val_acc: 0.7922\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.8354\n",
      "Epoch 00005: val_loss improved from 0.67536 to 0.58965, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/005-0.5896.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.5735 - acc: 0.8353 - val_loss: 0.5896 - val_acc: 0.8281\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.8617\n",
      "Epoch 00006: val_loss improved from 0.58965 to 0.50644, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/006-0.5064.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.4892 - acc: 0.8617 - val_loss: 0.5064 - val_acc: 0.8465\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4219 - acc: 0.8814\n",
      "Epoch 00007: val_loss did not improve from 0.50644\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.4219 - acc: 0.8813 - val_loss: 0.7551 - val_acc: 0.7608\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8938\n",
      "Epoch 00008: val_loss improved from 0.50644 to 0.45444, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/008-0.4544.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.3761 - acc: 0.8938 - val_loss: 0.4544 - val_acc: 0.8614\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.9053\n",
      "Epoch 00009: val_loss improved from 0.45444 to 0.36583, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/009-0.3658.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.3321 - acc: 0.9053 - val_loss: 0.3658 - val_acc: 0.8973\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2979 - acc: 0.9155\n",
      "Epoch 00010: val_loss did not improve from 0.36583\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2979 - acc: 0.9156 - val_loss: 0.3824 - val_acc: 0.8863\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9243\n",
      "Epoch 00011: val_loss improved from 0.36583 to 0.36434, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/011-0.3643.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.2689 - acc: 0.9243 - val_loss: 0.3643 - val_acc: 0.8938\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9285\n",
      "Epoch 00012: val_loss did not improve from 0.36434\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.2507 - acc: 0.9285 - val_loss: 0.4364 - val_acc: 0.8742\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9341\n",
      "Epoch 00013: val_loss improved from 0.36434 to 0.32856, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/013-0.3286.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.2345 - acc: 0.9341 - val_loss: 0.3286 - val_acc: 0.9080\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9400\n",
      "Epoch 00014: val_loss did not improve from 0.32856\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2100 - acc: 0.9400 - val_loss: 0.4662 - val_acc: 0.8672\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9438\n",
      "Epoch 00015: val_loss did not improve from 0.32856\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1997 - acc: 0.9438 - val_loss: 0.5597 - val_acc: 0.8453\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9494\n",
      "Epoch 00016: val_loss did not improve from 0.32856\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1781 - acc: 0.9494 - val_loss: 0.4604 - val_acc: 0.8651\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9514\n",
      "Epoch 00017: val_loss did not improve from 0.32856\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1711 - acc: 0.9514 - val_loss: 0.3569 - val_acc: 0.8968\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9551\n",
      "Epoch 00018: val_loss improved from 0.32856 to 0.30867, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/018-0.3087.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1565 - acc: 0.9551 - val_loss: 0.3087 - val_acc: 0.9115\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9580\n",
      "Epoch 00019: val_loss did not improve from 0.30867\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1486 - acc: 0.9580 - val_loss: 0.4322 - val_acc: 0.8763\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9613\n",
      "Epoch 00020: val_loss did not improve from 0.30867\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1379 - acc: 0.9613 - val_loss: 0.4523 - val_acc: 0.8658\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9624\n",
      "Epoch 00021: val_loss improved from 0.30867 to 0.29506, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/021-0.2951.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1330 - acc: 0.9624 - val_loss: 0.2951 - val_acc: 0.9119\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9688\n",
      "Epoch 00022: val_loss did not improve from 0.29506\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1146 - acc: 0.9688 - val_loss: 0.4898 - val_acc: 0.8635\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9695\n",
      "Epoch 00023: val_loss did not improve from 0.29506\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1094 - acc: 0.9695 - val_loss: 0.3585 - val_acc: 0.8942\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9692\n",
      "Epoch 00024: val_loss did not improve from 0.29506\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1088 - acc: 0.9692 - val_loss: 0.3020 - val_acc: 0.9180\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9749\n",
      "Epoch 00025: val_loss did not improve from 0.29506\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0914 - acc: 0.9749 - val_loss: 0.3106 - val_acc: 0.9096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9745\n",
      "Epoch 00026: val_loss did not improve from 0.29506\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0910 - acc: 0.9745 - val_loss: 0.3310 - val_acc: 0.9038\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9746\n",
      "Epoch 00027: val_loss improved from 0.29506 to 0.27365, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/027-0.2737.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0960 - acc: 0.9746 - val_loss: 0.2737 - val_acc: 0.9192\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9811\n",
      "Epoch 00028: val_loss did not improve from 0.27365\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0731 - acc: 0.9810 - val_loss: 0.3441 - val_acc: 0.9040\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9741\n",
      "Epoch 00029: val_loss did not improve from 0.27365\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0918 - acc: 0.9741 - val_loss: 0.2907 - val_acc: 0.9178\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9845\n",
      "Epoch 00030: val_loss did not improve from 0.27365\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0606 - acc: 0.9845 - val_loss: 0.3396 - val_acc: 0.9036\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9790\n",
      "Epoch 00031: val_loss improved from 0.27365 to 0.25281, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/031-0.2528.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0770 - acc: 0.9791 - val_loss: 0.2528 - val_acc: 0.9250\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9841\n",
      "Epoch 00032: val_loss did not improve from 0.25281\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0603 - acc: 0.9841 - val_loss: 0.3226 - val_acc: 0.9131\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9803\n",
      "Epoch 00033: val_loss did not improve from 0.25281\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0721 - acc: 0.9803 - val_loss: 0.5184 - val_acc: 0.8607\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9894\n",
      "Epoch 00034: val_loss did not improve from 0.25281\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0441 - acc: 0.9894 - val_loss: 0.2669 - val_acc: 0.9278\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9868\n",
      "Epoch 00035: val_loss did not improve from 0.25281\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0510 - acc: 0.9868 - val_loss: 0.3246 - val_acc: 0.9147\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9873\n",
      "Epoch 00036: val_loss did not improve from 0.25281\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0500 - acc: 0.9873 - val_loss: 0.3562 - val_acc: 0.9059\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9849\n",
      "Epoch 00037: val_loss did not improve from 0.25281\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0562 - acc: 0.9849 - val_loss: 0.2900 - val_acc: 0.9243\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9904\n",
      "Epoch 00038: val_loss did not improve from 0.25281\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0396 - acc: 0.9904 - val_loss: 0.4185 - val_acc: 0.8826\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9905\n",
      "Epoch 00039: val_loss did not improve from 0.25281\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0383 - acc: 0.9904 - val_loss: 0.2666 - val_acc: 0.9304\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9862\n",
      "Epoch 00040: val_loss did not improve from 0.25281\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0525 - acc: 0.9861 - val_loss: 0.5277 - val_acc: 0.8656\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9871\n",
      "Epoch 00041: val_loss improved from 0.25281 to 0.24784, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/041-0.2478.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0495 - acc: 0.9871 - val_loss: 0.2478 - val_acc: 0.9371\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9926\n",
      "Epoch 00042: val_loss did not improve from 0.24784\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0317 - acc: 0.9926 - val_loss: 0.4987 - val_acc: 0.8684\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9865\n",
      "Epoch 00043: val_loss improved from 0.24784 to 0.24759, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/043-0.2476.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0505 - acc: 0.9865 - val_loss: 0.2476 - val_acc: 0.9338\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9920\n",
      "Epoch 00044: val_loss did not improve from 0.24759\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0346 - acc: 0.9920 - val_loss: 0.2861 - val_acc: 0.9245\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9939\n",
      "Epoch 00045: val_loss improved from 0.24759 to 0.22958, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/045-0.2296.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0266 - acc: 0.9939 - val_loss: 0.2296 - val_acc: 0.9418\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9940\n",
      "Epoch 00046: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0279 - acc: 0.9939 - val_loss: 0.5575 - val_acc: 0.8621\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9854\n",
      "Epoch 00047: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0528 - acc: 0.9854 - val_loss: 0.3102 - val_acc: 0.9171\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9896\n",
      "Epoch 00048: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0400 - acc: 0.9896 - val_loss: 0.2515 - val_acc: 0.9371\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9957\n",
      "Epoch 00049: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0212 - acc: 0.9957 - val_loss: 0.3193 - val_acc: 0.9210\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9899\n",
      "Epoch 00050: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0393 - acc: 0.9899 - val_loss: 0.2748 - val_acc: 0.9266\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9865\n",
      "Epoch 00051: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0516 - acc: 0.9865 - val_loss: 0.2345 - val_acc: 0.9413\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9908\n",
      "Epoch 00052: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0355 - acc: 0.9908 - val_loss: 0.2391 - val_acc: 0.9373\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9985\n",
      "Epoch 00053: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0113 - acc: 0.9984 - val_loss: 0.2510 - val_acc: 0.9373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9883\n",
      "Epoch 00054: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0444 - acc: 0.9883 - val_loss: 0.2583 - val_acc: 0.9366\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9907\n",
      "Epoch 00055: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0351 - acc: 0.9907 - val_loss: 0.2421 - val_acc: 0.9394\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9980\n",
      "Epoch 00056: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0124 - acc: 0.9980 - val_loss: 0.2554 - val_acc: 0.9415\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9947\n",
      "Epoch 00057: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0232 - acc: 0.9946 - val_loss: 0.3636 - val_acc: 0.9108\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9902\n",
      "Epoch 00058: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0362 - acc: 0.9902 - val_loss: 0.2374 - val_acc: 0.9401\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9964\n",
      "Epoch 00059: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0168 - acc: 0.9964 - val_loss: 0.2997 - val_acc: 0.9299\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9921\n",
      "Epoch 00060: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0307 - acc: 0.9921 - val_loss: 0.3059 - val_acc: 0.9208\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9959\n",
      "Epoch 00061: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0179 - acc: 0.9959 - val_loss: 0.2486 - val_acc: 0.9371\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9935\n",
      "Epoch 00062: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0252 - acc: 0.9935 - val_loss: 0.2936 - val_acc: 0.9273\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9958\n",
      "Epoch 00063: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0184 - acc: 0.9958 - val_loss: 0.2684 - val_acc: 0.9373\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9937\n",
      "Epoch 00064: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0244 - acc: 0.9937 - val_loss: 0.2621 - val_acc: 0.9308\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9893\n",
      "Epoch 00065: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0381 - acc: 0.9893 - val_loss: 0.2420 - val_acc: 0.9401\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9983\n",
      "Epoch 00066: val_loss improved from 0.22958 to 0.22174, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/066-0.2217.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0100 - acc: 0.9983 - val_loss: 0.2217 - val_acc: 0.9534\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9979\n",
      "Epoch 00067: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0104 - acc: 0.9979 - val_loss: 0.4747 - val_acc: 0.8889\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9904\n",
      "Epoch 00068: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0353 - acc: 0.9904 - val_loss: 0.6205 - val_acc: 0.8558\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9931\n",
      "Epoch 00069: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0268 - acc: 0.9931 - val_loss: 0.2399 - val_acc: 0.9406\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9980\n",
      "Epoch 00070: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0101 - acc: 0.9980 - val_loss: 0.2908 - val_acc: 0.9329\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9907\n",
      "Epoch 00071: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0339 - acc: 0.9907 - val_loss: 0.2478 - val_acc: 0.9406\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9928\n",
      "Epoch 00072: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0274 - acc: 0.9928 - val_loss: 0.2245 - val_acc: 0.9429\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9972\n",
      "Epoch 00073: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0130 - acc: 0.9972 - val_loss: 0.2254 - val_acc: 0.9439\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9951\n",
      "Epoch 00074: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0192 - acc: 0.9951 - val_loss: 0.2726 - val_acc: 0.9350\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9953\n",
      "Epoch 00075: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0195 - acc: 0.9953 - val_loss: 0.2348 - val_acc: 0.9474\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9988\n",
      "Epoch 00076: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0071 - acc: 0.9988 - val_loss: 0.2369 - val_acc: 0.9408\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9913\n",
      "Epoch 00077: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0298 - acc: 0.9913 - val_loss: 0.2238 - val_acc: 0.9460\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9969\n",
      "Epoch 00078: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0143 - acc: 0.9969 - val_loss: 0.2487 - val_acc: 0.9418\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9961\n",
      "Epoch 00079: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0159 - acc: 0.9961 - val_loss: 0.2549 - val_acc: 0.9357\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9953\n",
      "Epoch 00080: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0205 - acc: 0.9952 - val_loss: 0.3489 - val_acc: 0.9180\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9920\n",
      "Epoch 00081: val_loss did not improve from 0.22174\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0292 - acc: 0.9920 - val_loss: 0.2911 - val_acc: 0.9315\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9941\n",
      "Epoch 00082: val_loss improved from 0.22174 to 0.20613, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/082-0.2061.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0230 - acc: 0.9941 - val_loss: 0.2061 - val_acc: 0.9483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9990\n",
      "Epoch 00083: val_loss improved from 0.20613 to 0.20471, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/083-0.2047.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0066 - acc: 0.9990 - val_loss: 0.2047 - val_acc: 0.9499\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9965\n",
      "Epoch 00084: val_loss did not improve from 0.20471\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0152 - acc: 0.9964 - val_loss: 0.2626 - val_acc: 0.9422\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9890\n",
      "Epoch 00085: val_loss did not improve from 0.20471\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0385 - acc: 0.9889 - val_loss: 0.2483 - val_acc: 0.9401\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9926\n",
      "Epoch 00086: val_loss did not improve from 0.20471\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0269 - acc: 0.9926 - val_loss: 0.2182 - val_acc: 0.9474\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9989\n",
      "Epoch 00087: val_loss did not improve from 0.20471\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0068 - acc: 0.9989 - val_loss: 0.2081 - val_acc: 0.9513\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9965\n",
      "Epoch 00088: val_loss did not improve from 0.20471\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0143 - acc: 0.9965 - val_loss: 0.2092 - val_acc: 0.9469\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9964\n",
      "Epoch 00089: val_loss did not improve from 0.20471\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0139 - acc: 0.9964 - val_loss: 0.2375 - val_acc: 0.9457\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9952\n",
      "Epoch 00090: val_loss did not improve from 0.20471\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0196 - acc: 0.9952 - val_loss: 0.2274 - val_acc: 0.9469\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9992\n",
      "Epoch 00091: val_loss did not improve from 0.20471\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0055 - acc: 0.9992 - val_loss: 0.2482 - val_acc: 0.9446\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9902\n",
      "Epoch 00092: val_loss did not improve from 0.20471\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0330 - acc: 0.9902 - val_loss: 0.2356 - val_acc: 0.9432\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9978\n",
      "Epoch 00093: val_loss did not improve from 0.20471\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0096 - acc: 0.9977 - val_loss: 0.2236 - val_acc: 0.9478\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9932\n",
      "Epoch 00094: val_loss improved from 0.20471 to 0.20159, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/094-0.2016.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0230 - acc: 0.9932 - val_loss: 0.2016 - val_acc: 0.9546\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9989\n",
      "Epoch 00095: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0060 - acc: 0.9989 - val_loss: 0.2735 - val_acc: 0.9359\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9941\n",
      "Epoch 00096: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0215 - acc: 0.9941 - val_loss: 0.2607 - val_acc: 0.9376\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9932\n",
      "Epoch 00097: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0239 - acc: 0.9932 - val_loss: 0.2651 - val_acc: 0.9404\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9987\n",
      "Epoch 00098: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0070 - acc: 0.9987 - val_loss: 0.2066 - val_acc: 0.9525\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9964\n",
      "Epoch 00099: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0134 - acc: 0.9964 - val_loss: 0.3250 - val_acc: 0.9257\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9922\n",
      "Epoch 00100: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0274 - acc: 0.9922 - val_loss: 0.2104 - val_acc: 0.9511\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9955\n",
      "Epoch 00101: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0196 - acc: 0.9955 - val_loss: 0.2061 - val_acc: 0.9525\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9994\n",
      "Epoch 00102: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0044 - acc: 0.9994 - val_loss: 0.2069 - val_acc: 0.9553\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9967\n",
      "Epoch 00103: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0130 - acc: 0.9967 - val_loss: 0.2971 - val_acc: 0.9313\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9936\n",
      "Epoch 00104: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0235 - acc: 0.9935 - val_loss: 0.2086 - val_acc: 0.9532\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9944\n",
      "Epoch 00105: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0216 - acc: 0.9943 - val_loss: 0.2423 - val_acc: 0.9453\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9940\n",
      "Epoch 00106: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0217 - acc: 0.9940 - val_loss: 0.2415 - val_acc: 0.9469\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9998\n",
      "Epoch 00107: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0034 - acc: 0.9998 - val_loss: 0.2080 - val_acc: 0.9522\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9994\n",
      "Epoch 00108: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0036 - acc: 0.9994 - val_loss: 0.2327 - val_acc: 0.9478\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9954\n",
      "Epoch 00109: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0160 - acc: 0.9954 - val_loss: 0.4534 - val_acc: 0.8938\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9898\n",
      "Epoch 00110: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0381 - acc: 0.9898 - val_loss: 0.2198 - val_acc: 0.9502\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9990\n",
      "Epoch 00111: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0051 - acc: 0.9990 - val_loss: 0.2332 - val_acc: 0.9488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00112: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0031 - acc: 0.9996 - val_loss: 0.2094 - val_acc: 0.9520\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9929\n",
      "Epoch 00113: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0263 - acc: 0.9929 - val_loss: 0.2536 - val_acc: 0.9422\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9972\n",
      "Epoch 00114: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0114 - acc: 0.9971 - val_loss: 0.2495 - val_acc: 0.9418\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9961\n",
      "Epoch 00115: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0153 - acc: 0.9961 - val_loss: 0.2252 - val_acc: 0.9504\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9982\n",
      "Epoch 00116: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0080 - acc: 0.9982 - val_loss: 0.3033 - val_acc: 0.9329\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9965\n",
      "Epoch 00117: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0136 - acc: 0.9965 - val_loss: 0.2527 - val_acc: 0.9432\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 00118: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0034 - acc: 0.9996 - val_loss: 0.2221 - val_acc: 0.9478\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9931\n",
      "Epoch 00119: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0229 - acc: 0.9931 - val_loss: 0.2419 - val_acc: 0.9460\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9991\n",
      "Epoch 00120: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0048 - acc: 0.9991 - val_loss: 0.2828 - val_acc: 0.9352\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9915\n",
      "Epoch 00121: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0295 - acc: 0.9916 - val_loss: 0.2309 - val_acc: 0.9483\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9992\n",
      "Epoch 00122: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0046 - acc: 0.9992 - val_loss: 0.2388 - val_acc: 0.9443\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9926\n",
      "Epoch 00123: val_loss did not improve from 0.20159\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0269 - acc: 0.9926 - val_loss: 0.2466 - val_acc: 0.9439\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9996\n",
      "Epoch 00124: val_loss improved from 0.20159 to 0.19408, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/124-0.1941.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0035 - acc: 0.9996 - val_loss: 0.1941 - val_acc: 0.9557\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 00125: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0034 - acc: 0.9994 - val_loss: 0.2065 - val_acc: 0.9509\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9983\n",
      "Epoch 00126: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0069 - acc: 0.9983 - val_loss: 0.7048 - val_acc: 0.8574\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9935\n",
      "Epoch 00127: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0223 - acc: 0.9935 - val_loss: 0.2150 - val_acc: 0.9515\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9995\n",
      "Epoch 00128: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0033 - acc: 0.9995 - val_loss: 0.2231 - val_acc: 0.9502\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9962\n",
      "Epoch 00129: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0133 - acc: 0.9963 - val_loss: 0.2455 - val_acc: 0.9429\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9980\n",
      "Epoch 00130: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0087 - acc: 0.9980 - val_loss: 0.2701 - val_acc: 0.9390\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9916\n",
      "Epoch 00131: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0317 - acc: 0.9916 - val_loss: 0.2273 - val_acc: 0.9497\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9955\n",
      "Epoch 00132: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0166 - acc: 0.9955 - val_loss: 0.2070 - val_acc: 0.9546\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9985\n",
      "Epoch 00133: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0078 - acc: 0.9985 - val_loss: 0.2231 - val_acc: 0.9513\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9995\n",
      "Epoch 00134: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0030 - acc: 0.9995 - val_loss: 0.2068 - val_acc: 0.9515\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00135: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0031 - acc: 0.9996 - val_loss: 0.2444 - val_acc: 0.9464\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9950\n",
      "Epoch 00136: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0183 - acc: 0.9950 - val_loss: 0.2549 - val_acc: 0.9443\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9953\n",
      "Epoch 00137: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0165 - acc: 0.9953 - val_loss: 0.2048 - val_acc: 0.9511\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00138: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0033 - acc: 0.9993 - val_loss: 0.2028 - val_acc: 0.9529\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9997\n",
      "Epoch 00139: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0026 - acc: 0.9997 - val_loss: 0.2108 - val_acc: 0.9520\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9961\n",
      "Epoch 00140: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0137 - acc: 0.9961 - val_loss: 0.2938 - val_acc: 0.9345\n",
      "Epoch 141/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9920\n",
      "Epoch 00141: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0268 - acc: 0.9920 - val_loss: 0.2117 - val_acc: 0.9515\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9996\n",
      "Epoch 00142: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0027 - acc: 0.9996 - val_loss: 0.2053 - val_acc: 0.9532\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 00143: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0030 - acc: 0.9996 - val_loss: 0.2748 - val_acc: 0.9341\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9923\n",
      "Epoch 00144: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0273 - acc: 0.9923 - val_loss: 0.2073 - val_acc: 0.9541\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9996\n",
      "Epoch 00145: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0029 - acc: 0.9996 - val_loss: 0.2250 - val_acc: 0.9511\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9957\n",
      "Epoch 00146: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0159 - acc: 0.9957 - val_loss: 0.2195 - val_acc: 0.9529\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9983\n",
      "Epoch 00147: val_loss did not improve from 0.19408\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0071 - acc: 0.9983 - val_loss: 0.2115 - val_acc: 0.9543\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9984\n",
      "Epoch 00148: val_loss improved from 0.19408 to 0.19352, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/148-0.1935.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0065 - acc: 0.9984 - val_loss: 0.1935 - val_acc: 0.9546\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9956\n",
      "Epoch 00149: val_loss did not improve from 0.19352\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0157 - acc: 0.9956 - val_loss: 0.2274 - val_acc: 0.9495\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9973\n",
      "Epoch 00150: val_loss did not improve from 0.19352\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0110 - acc: 0.9973 - val_loss: 0.2370 - val_acc: 0.9497\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9944\n",
      "Epoch 00151: val_loss did not improve from 0.19352\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0188 - acc: 0.9944 - val_loss: 0.2142 - val_acc: 0.9553\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9997\n",
      "Epoch 00152: val_loss did not improve from 0.19352\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0026 - acc: 0.9997 - val_loss: 0.2392 - val_acc: 0.9502\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9942\n",
      "Epoch 00153: val_loss did not improve from 0.19352\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0221 - acc: 0.9941 - val_loss: 0.2314 - val_acc: 0.9513\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9964\n",
      "Epoch 00154: val_loss did not improve from 0.19352\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0124 - acc: 0.9964 - val_loss: 0.2323 - val_acc: 0.9504\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9959\n",
      "Epoch 00155: val_loss improved from 0.19352 to 0.19056, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv_checkpoint/155-0.1906.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0151 - acc: 0.9959 - val_loss: 0.1906 - val_acc: 0.9562\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 00156: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0027 - acc: 0.9997 - val_loss: 0.2115 - val_acc: 0.9548\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9976\n",
      "Epoch 00157: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0095 - acc: 0.9976 - val_loss: 0.2124 - val_acc: 0.9532\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00158: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0031 - acc: 0.9996 - val_loss: 0.2113 - val_acc: 0.9539\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9958\n",
      "Epoch 00159: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0145 - acc: 0.9958 - val_loss: 0.2552 - val_acc: 0.9469\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9966\n",
      "Epoch 00160: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0129 - acc: 0.9966 - val_loss: 0.2025 - val_acc: 0.9539\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9986\n",
      "Epoch 00161: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0074 - acc: 0.9986 - val_loss: 0.3037 - val_acc: 0.9390\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9967\n",
      "Epoch 00162: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0114 - acc: 0.9966 - val_loss: 0.5267 - val_acc: 0.8928\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9955\n",
      "Epoch 00163: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0176 - acc: 0.9955 - val_loss: 0.2024 - val_acc: 0.9534\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9986\n",
      "Epoch 00164: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.2074 - val_acc: 0.9536\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 00165: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0023 - acc: 0.9996 - val_loss: 0.2351 - val_acc: 0.9520\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9992\n",
      "Epoch 00166: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0043 - acc: 0.9992 - val_loss: 0.1971 - val_acc: 0.9567\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00167: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.2390 - val_acc: 0.9515\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9940\n",
      "Epoch 00168: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0210 - acc: 0.9940 - val_loss: 0.2609 - val_acc: 0.9420\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9958\n",
      "Epoch 00169: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0164 - acc: 0.9958 - val_loss: 0.2129 - val_acc: 0.9532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9987\n",
      "Epoch 00170: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0052 - acc: 0.9988 - val_loss: 0.2187 - val_acc: 0.9539\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9995\n",
      "Epoch 00171: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0029 - acc: 0.9995 - val_loss: 0.2011 - val_acc: 0.9553\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9965\n",
      "Epoch 00172: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0116 - acc: 0.9965 - val_loss: 0.3873 - val_acc: 0.9180\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9961\n",
      "Epoch 00173: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0141 - acc: 0.9961 - val_loss: 0.2368 - val_acc: 0.9474\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9955\n",
      "Epoch 00174: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0153 - acc: 0.9955 - val_loss: 0.2407 - val_acc: 0.9467\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9962\n",
      "Epoch 00175: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0133 - acc: 0.9963 - val_loss: 0.2072 - val_acc: 0.9539\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 00176: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1964 - val_acc: 0.9543\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9976\n",
      "Epoch 00177: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0085 - acc: 0.9976 - val_loss: 0.2100 - val_acc: 0.9557\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 00178: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.2312 - val_acc: 0.9506\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9958\n",
      "Epoch 00179: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0144 - acc: 0.9958 - val_loss: 0.2332 - val_acc: 0.9495\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9987\n",
      "Epoch 00180: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0058 - acc: 0.9987 - val_loss: 0.2548 - val_acc: 0.9448\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9945\n",
      "Epoch 00181: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0190 - acc: 0.9945 - val_loss: 0.2687 - val_acc: 0.9446\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9988\n",
      "Epoch 00182: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0052 - acc: 0.9988 - val_loss: 0.2205 - val_acc: 0.9550\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 00183: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0033 - acc: 0.9992 - val_loss: 0.2986 - val_acc: 0.9352\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9972\n",
      "Epoch 00184: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0113 - acc: 0.9972 - val_loss: 0.2191 - val_acc: 0.9532\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9973\n",
      "Epoch 00185: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0110 - acc: 0.9973 - val_loss: 0.2358 - val_acc: 0.9497\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00186: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0032 - acc: 0.9992 - val_loss: 0.2382 - val_acc: 0.9534\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9963\n",
      "Epoch 00187: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0127 - acc: 0.9963 - val_loss: 0.2242 - val_acc: 0.9506\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9960\n",
      "Epoch 00188: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0138 - acc: 0.9960 - val_loss: 0.2427 - val_acc: 0.9471\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9966\n",
      "Epoch 00189: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0123 - acc: 0.9966 - val_loss: 0.2030 - val_acc: 0.9546\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9995\n",
      "Epoch 00190: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0032 - acc: 0.9995 - val_loss: 0.1983 - val_acc: 0.9588\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9982\n",
      "Epoch 00191: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0073 - acc: 0.9982 - val_loss: 0.2104 - val_acc: 0.9553\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9958\n",
      "Epoch 00192: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0167 - acc: 0.9958 - val_loss: 0.2193 - val_acc: 0.9525\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 00193: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0024 - acc: 0.9996 - val_loss: 0.2053 - val_acc: 0.9595\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00194: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.2340 - val_acc: 0.9515\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9931\n",
      "Epoch 00195: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0244 - acc: 0.9931 - val_loss: 0.2224 - val_acc: 0.9541\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9986\n",
      "Epoch 00196: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0060 - acc: 0.9986 - val_loss: 0.2044 - val_acc: 0.9597\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9983\n",
      "Epoch 00197: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0059 - acc: 0.9983 - val_loss: 0.2508 - val_acc: 0.9478\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00198: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.2443 - val_acc: 0.9492\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9956\n",
      "Epoch 00199: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0152 - acc: 0.9956 - val_loss: 0.2125 - val_acc: 0.9569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 00200: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0027 - acc: 0.9995 - val_loss: 0.2479 - val_acc: 0.9518\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9964\n",
      "Epoch 00201: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0123 - acc: 0.9964 - val_loss: 0.2464 - val_acc: 0.9476\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00202: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.2065 - val_acc: 0.9571\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00203: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.2072 - val_acc: 0.9543\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9953\n",
      "Epoch 00204: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0163 - acc: 0.9953 - val_loss: 0.2638 - val_acc: 0.9464\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9989\n",
      "Epoch 00205: val_loss did not improve from 0.19056\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0054 - acc: 0.9989 - val_loss: 0.2378 - val_acc: 0.9520\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd8VFX6/99nJr1XWoIEBKkJoSNIsyCgAsoiKArqqosrlp+ru6xriWVX166rfJFVVCwgFkRXBEWaKIiA9BY6CS299zy/P85MMgkpkzIEwnm/XvOaueeec+9z79x7Pvd5TrlKRDAYDAaDoTYsTW2AwWAwGM4PjGAYDAaDwSmMYBgMBoPBKYxgGAwGg8EpjGAYDAaDwSmMYBgMBoPBKVwmGEqptkqplUqpXUqpnUqpB6rIo5RSbyil9iultimlejusm6aUird9prnKToPBYDA4h3LVOAylVGugtYhsVkr5A5uA8SKyyyHPGOA+YAwwAHhdRAYopUKAjUBfQGxl+4hImkuMNRgMBkOtuMzDEJETIrLZ9jsL2A1EVMo2DpgnmvVAkE1orgZ+EJFUm0j8AIxyla0Gg8FgqB23s7ETpVQU0Av4tdKqCOCYw3KCLa269BoJCwuTqKioBlhqMBgMFxabNm1KFpFwZ/K6XDCUUn7AF8CDIpLpgu3fDdwNcNFFF7Fx48bG3oXBYDA0W5RSR5zN69JeUkopd7RYfCwiX1aRJRFo67AcaUurLv0MRGSOiPQVkb7h4U6JpMFgMBjqgSt7SSngXWC3iLxSTbavgam23lIDgQwROQEsA0YqpYKVUsHASFuawWAwGJoIV4akBgO3AtuVUltsaY8CFwGIyGxgCbqH1H4gF7jdti5VKfUM8Jut3NMikupCWw0Gg8FQCy4TDBFZC6ha8ghwbzXr5gJzG2pHUVERCQkJ5OfnN3RTFyReXl5ERkbi7u7e1KYYDIYm5qz0kmpKEhIS8Pf3JyoqCh0lMziLiJCSkkJCQgLt27dvanMMBkMT0+ynBsnPzyc0NNSIRT1QShEaGmq8M4PBAFwAggEYsWgA5twZDAY7F4Rg1EZBwXGKizOa2gyDwWA4pzGCARQWnqS4uNHHFAKQnp7OrFmz6lV2zJgxpKenO50/Li6Ol156qV77MhgMhtowggHozlyumYSxJsEoLi6useySJUsICgpyhVkGg8FQZ4xgYI/Tu0YwZs6cyYEDB4iNjeWRRx5h1apVDBkyhLFjx9KtWzcAxo8fT58+fejevTtz5swpKxsVFUVycjKHDx+ma9eu3HXXXXTv3p2RI0eSl5dX4363bNnCwIEDiYmJ4frrryctTU/0+8Ybb9CtWzdiYmKYPHkyAKtXryY2NpbY2Fh69epFVlaWS86FwWA4v2n23WodiY9/kOzsLWekl5Rko5QbFotXnbfp5xdLp06vVbv++eefZ8eOHWzZove7atUqNm/ezI4dO8q6qs6dO5eQkBDy8vLo168fEyZMIDQ0tJLt8cyfP5///ve/3HjjjXzxxRfccsst1e536tSp/Oc//2HYsGE88cQTPPXUU7z22ms8//zzHDp0CE9Pz7Jw10svvcRbb73F4MGDyc7Oxsur7ufBYDA0f4yHAdQyvrDR6d+/f4VxDW+88QY9e/Zk4MCBHDt2jPj4+DPKtG/fntjYWAD69OnD4cOHq91+RkYG6enpDBs2DIBp06axZs0aAGJiYpgyZQofffQRbm76eWHw4ME89NBDvPHGG6Snp5elGwwGgyMXVM1QnSeQnb0dq9UXb+8OZ8UOX1/fst+rVq1i+fLlrFu3Dh8fH4YPH17luAdPT8+y31artdaQVHV8++23rFmzhm+++YZ//vOfbN++nZkzZ3LNNdewZMkSBg8ezLJly+jSpUu9tm8wGJovxsPAtW0Y/v7+NbYJZGRkEBwcjI+PD3v27GH9+vUN3mdgYCDBwcH89NNPAHz44YcMGzaM0tJSjh07xogRI/j3v/9NRkYG2dnZHDhwgOjoaP72t7/Rr18/9uzZ02AbDAZD8+OC8jCqR+GqV9WGhoYyePBgevTowejRo7nmmmsqrB81ahSzZ8+ma9eudO7cmYEDBzbKfj/44AOmT59Obm4uHTp04L333qOkpIRbbrmFjIwMRIT777+foKAgHn/8cVauXInFYqF79+6MHj26UWwwGAzNC5e907sp6Nu3r1R+gdLu3bvp2rVrjeVycnahlDs+Pp1cad55izPn0GAwnJ8opTaJSF9n8pqQFODKcRgGg8HQXDCCgWvbMAwGg6G5YAQDcGUbhsFgMDQXjGAAJiRlMBgMteOyXlJKqbnAtcBpEelRxfpHgCkOdnQFwm2vZz0MZAElQLGzDTINsBYjGAaDwVAzrvQw3gdGVbdSRF4UkVgRiQX+Dqyu9N7uEbb1LhYL04ZhMBgMzuAywRCRNUBqrRk1NwHzXWVL7ZxbbRh+fn51SjcYDIazQZO3YSilfNCeyBcOyQJ8r5TapJS6u5bydyulNiqlNiYlJdXXCoyHYTAYDDXT5IIBXAf8XCkcdZmI9AZGA/cqpYZWV1hE5ohIXxHpGx4eXk8TXDu9+VtvvVW2bH/JUXZ2NldccQW9e/cmOjqaxYsXO71NEeGRRx6hR48eREdH8+mnnwJw4sQJhg4dSmxsLD169OCnn36ipKSE2267rSzvq6++2ujHaDAYLgzOhalBJlMpHCUiibbv00qpRUB/YE2D9/Tgg7DlzOnNPUvzESkGaz1CPrGx8Fr105tPmjSJBx98kHvvvReAhQsXsmzZMry8vFi0aBEBAQEkJyczcOBAxo4d69Q7tL/88ku2bNnC1q1bSU5Opl+/fgwdOpRPPvmEq6++mn/84x+UlJSQm5vLli1bSExMZMeOHQB1eoOfwWAwONKkgqGUCgSGAbc4pPkCFhHJsv0eCTztaltcFZDq1asXp0+f5vjx4yQlJREcHEzbtm0pKiri0UcfZc2aNVgsFhITEzl16hStWrWqdZtr167lpptuwmq10rJlS4YNG8Zvv/1Gv379uOOOOygqKmL8+PHExsbSoUMHDh48yH333cc111zDyJEjXXSkBoOhuePKbrXzgeFAmFIqAXgScAcQkdm2bNcD34tIjkPRlsAi25O2G/CJiCxtFKOq8QQK849SVJSCv3+vRtlNZSZOnMjnn3/OyZMnmTRpEgAff/wxSUlJbNq0CXd3d6Kioqqc1rwuDB06lDVr1vDtt99y22238dBDDzF16lS2bt3KsmXLmD17NgsXLmTu3LmNcVgGg+ECw2WCISI3OZHnfXT3W8e0g0BP11hVHa5t9J40aRJ33XUXycnJrF69GtDTmrdo0QJ3d3dWrlzJkSNHnN7ekCFDePvtt5k2bRqpqamsWbOGF198kSNHjhAZGcldd91FQUEBmzdvZsyYMXh4eDBhwgQ6d+5c41v6DAaDoSbOhTaMJkd7M6Uu23737t3JysoiIiKC1q1bAzBlyhSuu+46oqOj6du3b51eWHT99dezbt06evbsiVKKF154gVatWvHBBx/w4osv4u7ujp+fH/PmzSMxMZHbb7+d0lJ9fM8995xLjtFgMDR/zPTmQEFBIoWFJ/Dz6+NUo/OFhpne3GBovpjpzeuM/TQ0H/E0GAyGxsYIBqDbMMAIhsFgMFSPEQwoC0M1p/CcwWAwNDZGMADjYRgMBkPtGMEAjGAYDAZD7RjBAIxgGAwGQ+0YwcC1bRjp6enMmjWrXmXHjBlj5n4yGAznDEYwAFd6GDUJRnFxcY1llyxZQlBQUKPbZDAYDPXBCAbgSsGYOXMmBw4cIDY2lkceeYRVq1YxZMgQxo4dS7du3QAYP348ffr0oXv37syZM6esbFRUFMnJyRw+fJiuXbty11130b17d0aOHEleXt4Z+/rmm28YMGAAvXr14sorr+TUqVMAZGdnc/vttxMdHU1MTAxffKFfPbJ06VJ69+5Nz549ueKKKxr92A0GQ/PigpoapJrZzRHxp7S0MxaLJ3Ud6F3L7OY8//zz7Nixgy22Ha9atYrNmzezY8cO2rdvD8DcuXMJCQkhLy+Pfv36MWHCBEJDQytsJz4+nvnz5/Pf//6XG2+8kS+++OKMeaEuu+wy1q9fj1KKd955hxdeeIGXX36ZZ555hsDAQLZv3w5AWloaSUlJ3HXXXaxZs4b27duTmursyxENBsOFygUlGNWhCgpRFs6av9W/f/8ysQB44403WLRoEQDHjh0jPj7+DMFo3749sbGxAPTp04fDhw+fsd2EhAQmTZrEiRMnKCwsLNvH8uXLWbBgQVm+4OBgvvnmG4YOHVqWJyQkpFGP0WAwND8uKMGozhOQzXsoCizF0q4zbm7+LrfD19e37PeqVatYvnw569atw8fHh+HDh1c5zbmnp2fZb6vVWmVI6r777uOhhx5i7NixrFq1iri4OJfYbzAYLkxMGwaARdmaLxq/DcPf35+srKxq12dkZBAcHIyPjw979uxh/fr19d5XRkYGERERAHzwwQdl6VdddVWF18SmpaUxcOBA1qxZw6FDhwBMSMpgMNSKEQwApWyzmze+YISGhjJ48GB69OjBI488csb6UaNGUVxcTNeuXZk5cyYDBw6s977i4uKYOHEiffr0ISwsrCz9scceIy0tjR49etCzZ09WrlxJeHg4c+bM4YYbbqBnz55lL3YyGAyG6jDTmwOyfRvFHoXQoSPu7qYba2XM9OYGQ/PlnJjeXCk1Vyl1Wim1o5r1w5VSGUqpLbbPEw7rRiml9iql9iulZrrKxjJcGJIyGAyG5oIrQ1LvA6NqyfOTiMTaPk8DKKWswFvAaKAbcJNSqpsL7QRlQRnBMBgMhhpxmWCIyBqgPi2p/YH9InJQRAqBBcC4RjWuMsbDMBgMhlpp6kbvS5VSW5VS3ymlutvSIoBjDnkSbGlVopS6Wym1USm1MSkpqX5WKAuUmvdhGAwGQ000pWBsBtqJSE/gP8BX9dmIiMwRkb4i0jc8PLx+llhMSMpgMBhqo8kEQ0QyRSTb9nsJ4K6UCgMSgbYOWSNtaa7DYjEhKYPBYKiFJhMMpVQrZZtXXCnV32ZLCvAb0Ekp1V4p5QFMBr52rTHnlofh5+fX1CYYDAbDGbhsahCl1HxgOBCmlEoAngTcAURkNvAH4B6lVDGQB0wW3YhQrJSaASwDrMBcEdnpKjuBMg/DtGEYDAZD9biyl9RNItJaRNxFJFJE3hWR2TaxQETeFJHuItJTRAaKyC8OZZeIyCUicrGI/NNVNpZhsaBcNNJ75syZFabliIuL46WXXiI7O5srrriC3r17Ex0dzeLFi2vdVnXToFc1TXl1U5obDAZDfbmgJh98cOmDbDlZxfzmBQVQWEjpFk8sFo86bTO2VSyvjap+fvNJkybx4IMPcu+99wKwcOFCli1bhpeXF4sWLSIgIIDk5GQGDhzI2LFjy97+VxVVTYNeWlpa5TTlVU1pbjAYDA3hghKM2ml8D6NXr16cPn2a48ePk5SURHBwMG3btqWoqIhHH32UNWvWYLFYSExM5NSpU7Rq1arabVU1DXpSUlKV05RXNaW5wWAwNIQLSjCq9QROnIDERAq6t8TTu23VeRrAxIkT+fzzzzl58mTZJH8ff/wxSUlJbNq0CXd3d6Kioqqc1tyOs9OgGwwGg6to6oF75wYW22koLXXJ5idNmsSCBQv4/PPPmThxIqCnIm/RogXu7u6sXLmSI0eO1LiN6qZBr26a8qqmNDcYDIaGYAQDKHsvq4t6SXXv3p2srCwiIiJo3bo1AFOmTGHjxo1ER0czb948unTpUuM2qpsGvbppyqua0txgMBgagpneHCA5GQ4fpuCSEDwDOrjQwvMTM725wdB8OSemNz+vcHFIymAwGJoDRjDA5SEpg8FgaA5cEIJRa9jNeBjV0pxClgaDoWE0e8Hw8vIiJSWl5orPLhimcqyAiJCSkoKXl1dTm2IwGM4Bmv04jMjISBISEqjxXRkFBZCcTHFpJm5JRjQc8fLyIjIysqnNMBgM5wDNXjDc3d3LRkFXy9atMHo0h1+OJeqh38+OYQaDwXCe0exDUk5hD7kUFDWtHQaDwXAOYwQDygRD5RvBMBgMhuowggHlgmE8DIPBYKgWIxhgBMNgMBicwAgGgLc3AKqguIkNMRgMhnMXlwmGUmquUuq0UmpHNeunKKW2KaW2K6V+UUr1dFh32Ja+RSm1saryjYq7O6KMYBgMBkNNuNLDeB8YVcP6Q8AwEYkGngHmVFo/QkRinZ0Uq0EohXhajWAYDAZDDbhsHIaIrFFKRdWw/heHxfVAk44OEw8rloKSpjTBYDAYzmnOlTaMPwLfOSwL8L1SapNS6u6aCiql7lZKbVRKbaxxNHctlHpaUUYwDAaDoVqafKS3UmoEWjAuc0i+TEQSlVItgB+UUntEZE1V5UVkDrZwVt++fes9r4d4uqEKC+tb3GAwGJo9TephKKVigHeAcSKSYk8XkUTb92lgEdDf1baIpxuWAjNbrcFgMFRHkwmGUuoi4EvgVhHZ55Duq5Tyt/8GRgJV9rRqTMTLHWUEw2AwGKrFZSEppdR8YDgQppRKAJ4E3AFEZDbwBBAKzFL6BUbFth5RLYFFtjQ34BMRWeoqO+2Ip7vxMAwGg6EGXNlL6qZa1t8J3FlF+kGg55klXIynO5YsM7W5wWAwVMe50kuqyREvdyyFIGJ6ShkMBkNVGMGwIZ4eWAqhtNT0lDIYDIaqMIJhQ3l52QQjt6lNMRgMhnMSIxh2vH2wFEJJSXZTW2IwGAznJEYw7BjBMBgMhhoxgmFDefliKYTi4qymNsVgMBjOSYxg2FDeWjBKjGAYDAZDlRjBsKG8/VGlUJKf1tSmGAwGwzmJEQwbKjAEgNL05Ca2xGAwGM5NjGDYUGGt9I+UU01riMFgMJyjGMGwYWmh398kDXinhsFgMDRnjGDYsIS3AUClptSS02AwGC5MjGDYUOHh+keyafQ2GAyGqjCCYScsDABLanoTG2IwGAznJkYw7Pj6UuquUKlmHIbBYDBUhREMO0pRHOSGJdVMDWIwGAxVYQTDgZJADyxpeU1thsFgMJyTuFQwlFJzlVKnlVJVvpNbad5QSu1XSm1TSvV2WDdNKRVv+0xzpZ12SoK9sKbnn41dGQwGw3mHU4KhlHpAKRVgq+DfVUptVkqNdKLo+8CoGtaPBjrZPncD/2fbXwj6HeADgP7Ak0qpYGdsbQglIT64pZsXKBkMBkNVOPtO7ztE5HWl1NVAMHAr8CHwfU2FRGSNUiqqhizjgHkiIsB6pVSQUqo1MBz4QURSAZRSP6CFZ76T9tYLCfbFLcO8otUZ8vNh1y7o1QuU0mlHj4K7O7RuXZ6vuBjcbFeZSHneyuzaBadPQ0QEdOpUnj8+HiIjwcdHpxUWQlKS3ofFAunpsHkz9O4NQUGQkgKlpRAYCB4eZ+7j6FGIjtb7sbN/Pxw6BJ07Q2io3ldJCfz6K/j5Qc+ekJkJOTl6v7t2QV6e3qdSkJsL69frdZ066eMVgT17YNs2uOQS6NFDnxvH87JsmbYnIACGDIGLLtLrtmyBrVv1dsaN0/YcO6ZtSU+HU6egZUs4flz/HjMGsrJg9269nbQ02L4dLrsMTpyAH38ET09dvlUr6NtXH+/vv0NsrD5foO0EfWybNunznJur/+vevcvzbtoEO3boY4iJ0dtzc9PnZc0a/XvMGAgJgSVLICoK2rbV9uXl6f+pRw/IyND/YYsWsHat3teVV2obcnP1vn75RR9z27b6WHx89HGBPv6EBP2fRUTo/3zbNjh8WP9fJSV6G56ecN11ej/Z2fq4ExK0/T16aPu8vXW+khI4eVL/nwDh4Xp7rVrp49+3T5/79u11nuPH9TUQFaX/x9xcnT88HLp21euKi/V/sm2bPt6ICH2u9+zR/1NpKVx+uU5bt07bDvr7kku03evWaRsCA2HDBn0ftGoFI0bo//LIERg/vrq7tfFwVjDst/kY4EMR2alUdbd+nYgAjjksJ9jSqks/0zCl7kZ7J1xkv+PqSWlIAG6ZpfqqsVobtK2zTXy8vlC7dtXL/v76JvjuO10henjA0KFw8cV6ef58vf7aa3We336DxER96IMGwT/+oS/wOXPgp59g7FiYNEnfKN99By+9pG+6G2/UF+r//qe3KaJvwhEj9IW9caPeXkqKtrFjR30jJCVBhw765i8uhvffLz+W8eP1Dbdypba1ZUu9n6NHYcUKXTl6eOgbPDtb7zMoSN/Qq1frZdCVS3Q0tGkDO3fqm93O1VfDqFG60l66tOK5VEpXekVFevm662DVKr3fdu30zQn6xr/oIr1t+00eGAj9+ulzd/p0+TY9PaFLF73NkhJdgTiuB+jTR/9/H39cfgzBwdqWmiYgCA7W56GoSIve8ePa1jZtdDn7cdgJC9P/h30fdgYP1pXXxo1nlgEtOEVFUFBQMT0gQIvq2rXl2/Ty0nadOFG93XZ8fHRFC/p/zMmpev8Nwd1dH3dN9ih15jmpDk/PM89DQ2nZUj8A1IfAQP2furraUuLEGVJKvYeusNsDPQErsEpE+jhRNgr4n4j0qGLd/4DnRWStbflH4G9oD8NLRJ61pT8O5InISzXtq2/fvrJx48Zaj6c6Uh4fSeizPyBJSSjbuIymprQUvv9e31BXXAFPPKGfKEaPhp9/1hVDq1Ywd66ueB3x8tJPh1Xh7q7z25/6u3fXlZ+I3l+JzdEKDdWV/3ff6RvZzsCB+mn21Vf1dnx8YMYMnf+HH7TIREXpivmXX3TlER0NBw7oizs8XFfgq1bpY3voIX1Ma9bAK6/oSrJfP/2k+tVX+lijomDYMF05HT2qKxX7dt97D/bu1cISHq6fXpOTtfeRlKSF6uqrtZitXavtTk3V25w2TYtpfLwul5OjK4M+fbTovfmmtq1vX/2kd/nl+ul56VK97bZtYcIELQJr1ugyMTEwfLj2wOLjdSW8e7euaNzc9PmfMEGfx6Qkfc7mzdNPnPfdpz8nT8KsWfr/GTpUP50HBGhP5tQpXQF6e+v/vkULfV5mzdL/4/jxsGCBFvjp0/U+s7P1Ofrf//ST69VXa7Hz9NTbmztXV9hDhmghb9tW/69Wqxbvbdt03t69td0Wiz7W5cu1N3bllXDvvfr/fOUVXTnfe68+z0lJ+hrz99dCuXOn/u+KivST9pAh4OsLX3+t/7/wcH0N9umjr+9jx/Rxp6XpfXl46GNu21ZXlImJej89emjRDAzUx2yx6GObN09fDx06aPs7dNDndetWfZ5zc/XH3V3/t/37632cPq0fEg4d0h5Uly76wWzPHv3/dumir//Dh3V5T0+d/8QJ/bBjsejz5+enrwml9LW7d6/+bwYM0OXnztXbnzBBp4M+H1u36nM3eLA+jvR0fe79/fX9s3q1zj9iREWvuS4opTaJSF+n8jopGBYgFjgoIum2NoZIEdnmRNkoqheMt9HCM9+2vBctFsOB4SLyp6ryVUdDBSP5tcmE/b9PKdn1O9ausfXeTl3IzNQXX3GxvkHWrYOFC/XFZ7XqCuyYzdeyV/KtW+unyJAQfdMdOKArvSlT9G+rVd88CQm6orvsMr2f77/XN03LlnDNNVpsVqzQN3q7duU27dwJn32mL+YRI7Tw5OToiv/AAX2jdO6sL/7Dh3VF1KmTvlnsFBdrO2rzQwsLdfmQkPK0khJ9ozmWrSmcVR+ys3Xl07Zt422zoYhoofLyampLDBcSrhCMwcAWEclRSt0C9AZeF5EjTpSNonrBuAaYgQ51DQDeEJH+NkHaZNsPwGagj71NozoaLBgf3UvYrbMoXPk1HsOvq/d2atxHso597t6tK99PPtEVtyODBulwgr3ivOEG/aS3aJEWhuHDtch07KifgvLzTSVjMJzrnMw+ib+HP74evgDkFuWSkptC20D91CIixKfG0y6wHZ5unjVtqlGpi2A424bxf0BPpVRP4C/AO8A8YFgthsxHewthSqkEdM8ndwARmQ0sQYvFfiAXuN22LlUp9Qzwm21TT9cmFo1CWAsAJPl4o2wuN1eHIvbu1SLx88/6tx1vby0GEyfq31arftLv2LHq7Tk2anXrVv77fBeLopIi3tzwJmM6jaFzWGeX7COrIIuDaQcJ9AokKiiqLL2guIBfjv1CblEuI9qPwMfdp8rypVLKV3u+4vNdn/PU8KfoFNqpbF1GfgYLdizg1p634uXmxaLdi9iXso+9KXs5lnmMuWPn4u/pz39+/Q83R99Mp9BOZBVksXjvYqzKyqiOowj2DqZUSvk14Vf8PPxIzErkRNYJJveYjLe7NwD7UvYR5hNGiHdIBbssylJhGShLe3vj28zfMZ8r2l/BPf3uIcwnjBWHVnAy+yRdwrrQu3VZT3Z2J+3mo20fsS91H31a9yHIK4hgr2DGdBqDv6c/xaXFfLvvW3ac3kF6fjphPmHc3ut2wn3C+WL3F2w5uYVOIZ2YFjuNb/Z+w1d7viKmZQy5RbkcyzzG8azj5BXn0cqvFRO7TSTcJ5z1Cev5X/z/6Nu6L/0j+pNRkIGPuw+bjm9i0Z5F+Hr4EuEfQSu/VhSUFHBx8MXcEnMLn+38jBDvEP7U908AZBZk8tiKx8goyMDP3Q9vd2+WHViGp9WTKdFTmN53OqdyTvHKulc4kX2C1LxUFIpP//ApoT6hgK6wv43/lo+2fURybjLp+ekIwnWXXMegtoNoG9CWLmFdUEqRlJPEX5f/lSvbX8nN0Tcz67dZFJQUEBUUxfqE9QR5BTEwciAxLWP4dMen/N/G/2Nn0k6sykp0y2hiWsawJH4JybnJTOw2kTb+bVh2YBl7kvcQ2yqW+RPmk5Gfwb/W/ostJ7cwuuNoJnSdwInsE8xcPhM/Dz9GRI3g70P+TnxKPHuS93DfgPsacos4hbMexmYR6a2UegJIFJF37Wkut7AONLgNY+NsQvvdQ96sJ/G+J65e28jK0qGf777ToSW79xASoj2HQYN0PDI2VschGzPM0hDyi/P54cAPXHvJtVTXnyG/OJ9fjv1CjxY9aOHboiy9qKSI/an72ZO8hwGRA2jj34ZT2afw9/SvtgK2k1eUx6TPJ/HNvm+4LfY23hv3Xln6kYwjdAkQNYlRAAAgAElEQVTrQnp+Oj8c+IHdybs5nXOaUO9QZvSfwaM/Psr3B7/njtg7iE+NJyEzgSs7XMmU6Cm0D25fto+fj/7M1R9dTU5RDhZl4daYW7mm0zVsO7WNNza8QWaBbrH2tHoSERCBVVlJz0+nlV8r2ge3J8Q7hOUHl5OQmQBAjxY9WP/H9Xi6ebL15FZuXXQru5N38/jQxwnzCeOBpQ8A0MqvFen56Vx98dV4u3uzYMcC3CxudArpxKH0Q+QX6wamAM8Atk3fxrqEddz0xU0Vzk/n0M5M7zudnad38u7v79I1vCs/3/Ezn2z/hPe2vMfmE5tp49+GqTFTGd9lPDcsvIH84nwGtx1MQUkBS/cvpV1gO45kHOHBAQ/y76v+jf9z/hSW6O7jw6OGc3fvuzmQdoAnVz2JQhEZEMmRjPLggbebNzEtYziVc4rD6YcB8HLzIr84n96te3N9l+t5fOXjZfmfHfEsz619jsKSQopKdet1iHcIEf4R+Lj7sC9lH2kOb7a8JPQSDqQeoETKeyhalZWRF4/EoiwkZiVyKvsUHlYPjmYcRSivs1686kUeHvQwH279kKlfTSUyIJKcwhwyCjIY1m4YOUU5bEjcwCWhl5CUk0RecR5RQVEEeQWxPmE9L498mZ4tezL+0/EoFFmFWfp/D2pPsHcwWQVZrD26tmyf7YPa0zW8K5uOb+JUzik8rZ78ud+feXX9q2U2uVvcy47bzsDIgdzQ5QYyCzJZn7iezSc20z+iPzEtYnjztzcB6NumL1d1uIoXf3mx7JoM9AxkaLuhrDi0gpwi3Yg4IELfY0vil1BQolveW/u15tADh+rlmbgiJLUaWArcAQwBTgNbRSS6zta5kIYKRurhxYS0H0/eU/fg/cSsOpXdtEn3KPrkEx0f9/PT3sOkSbphLCpKh5eaimMZx2gb2Jb9qfu58bMb+eambwj0CuTNDW9yS8wtPLTsIT7b9Rm/3PELl7a9lJLSEm764iba+Lfh1atfJb84n7ELxrL84HJAVwAeVg/cLe6czD5ZdoNc3v5yvrzxSy5+42J83H14d+y7XHXxVbz3+3u8seENnhnxDKsOr2LZgWWM6zyOz3d9zr6UfbT2b42/hz97ZuwB4Nk1z/L4yscZ1m4Y205tK6tggr2CSc9Px2qxUlxaTO/Wvdl8YjPBXsFEBUXx+8nfUShuibmFuePmciT9CAPfHUiwVzDPXv4svyb8ylu/vVV2o/2h2x+YGjMVLzcvfjj4A4lZiRSXFhPoGcjJ7JMcSj/EyeyTDG47mJujb8bfw59rPrmGMJ8wMgsyKSgpINgrmK7hXdl6ciseVg96t+7N4smL8fXw5d9r/83MH2cC8NDAhyiREo5kHOHi4Iu5oesNZBVkMerjUbw5+k02ntjI4j2LmX3tbEK9QykoKeCeb+/haMZRrMrKzdE3M3/HfPw9/EnLT6NP6z5c3v5y9qXsY/HexQC0DWjL0HZD2XxiM/nF+dzU4yaeHvE0A94ZQKhPKK+Pep2ub3XluSuew8vNixd+foET2brr0OQek3l91Ou08G1Ben46+cX5HEg9wGe7PmP76e1YlIUZ/WZwZYcr8fXw5Zu93zBuwTgEYWK3icy7fh5XfXgVa4+uJdgrmK3Tt+Juda8QhgHt1f187GcKigtoF9SObuHdOJ1zmoTMBIK9gsktyiXUJ5RWfq3OuI53Je1i0e5FjOo4ihd/eZFPd37K2tvX8sHWD1i4cyGpf0vFoiyUlJZgteguQ8sPLufOr+8k2DuYzyZ+RscQ7cIPfGcgWYVZ+Hv4k5CZwA1db6B3695MiZ6Cu7W8D/TJ7JNlD0SL9y7mZPZJgryCmDl4JrcuupUT2ScY3XE0b1/7NgmZCfRq3Yu8ojw2JG5g04lN9GvTjys7XFntg1hBcQHuVvcyrzA+JZ4l8Uto4duCqzteTYh3CHlFeSw7sIz84nwmdpuI1WLlaMZR5myaQ/fw7lzf9Xq83OoXaqiLYCAitX6AVsBDwBDb8kXAVGfKns1Pnz59pCGkp/0iJVYk9/4/OJU/NVXkr38V6dRJBES8vUVuv11k9WqRwsIGmVJGSWnJGWlpeWny0NKHZNfpXWVp//jxHzJu/jhJy0sry/PyLy9LTmGO/HjwRyEOWX9svby+/nUhDpm7ea68s+kdIQ5xf9pdiEOIQ2ZtmCUiIs+sfqYsbeqiqdL/v/1FxSl58ecX5V9r/iUzvp0hd399t0xbNE3++v1fZd6WefLwsoeFOGTCpxOEOKTD6x2EOGT8gvFiecoiXs96lW2z1+xeQhzS7tV28v3+7+Vfa/4lxCEpuSkiIjJ2/lgJfj5YIl6OkNEfjZa1R9ZKbmGuiIj8lvibXPPxNfLR1o9ERCQhI0Hyi/JFRORYxjF5aOlDQhxy65e3SuQrkRLy7xDZl7yv7FxlF2TL7yd+l/iU+Hr9J/O2zJPJn0+WR75/RD7c+qEczzwu+5L3idvTbmJ5yiLbTm4ry1tQXCA9ZvWQ3m/3lsLiMy+K0tJSiXotSq5fcL20e7WdXL/g+grri0uKJSU3RTLyM0RE5O2Nb0vYC2Hy7uZ3pbS0tCzfpzs+lT8s/IMcyzhWpc1TvpgiF716kXy1+yshDvk14dey7W9I2CA/HvyxwvacZdaGWTJx4cSy/yYhI0Eum3uZfLP3mzpvq65kF2SL17Ne8sB3D0iXN7vImI/HVJu3uKT4jHtpzsY5Zdfj2xvfrpcNa4+slSlfTCm7785HgI3iZB3rdGUMtASutX1aOFvubH4aKhhZWdulMADJue2qWvN++qlIeLiIxSIyapTIrFkiaTVcMzmFOXI883iFtN9P/C5FJUVly4+veFwufedSWXVolYiILNm3RAKfC5TX178uPx/9WWJnx8qi3Ytk6qKpQhwS8FyAfL//exERiXg5QohDomdFy67Tu2TMx2OEOOStDW/J9G+mC3HIcz89V1b2nv/dI3cuvlOCng+Saz+5Vh747gEJej5Ipn8zXbae3CrWp6xy8xc3y11f31VWsX+87eMaz0lGfoYEPR8kxCHXfnKt5BXlycPLHhYVp6TP230kKSdJnv/peVm2f5mI6MrdXtGsPLRSiEO+3fetiIhc/PrFMnHhxFr/h+q4f8n9QhzS4sUWsuXElnpvpy7M2jBL3lj/xhnpOYU5ZYJWFXcuvlM8n/EU4pD//PqfWvdTn4r96VVPC3HIkyufFOKQ9Lz0Om/jXOTaT66VFi+2KLu+60JGfob4/NNHIl+JrPH/ae40umAANwJHgA/Qjd2HgD84u5Oz9WmoYOTlHZbcNkju9QOqzZObKzJtmj5zAwaIbLHVRTmFOTL0vaGyNH7pGWWScpKkx6weEvZCmOQU5oiIyM7TO4U45M1f3xQRkU3HN4nlKYt4POMhxCGD3h0k3s96i/ez3kIc4vWsl1iesoj1KWtZhd/1za4S9VqUJGQkCHHIxIUTJej5IFFxSohDfP7pI5fNvUwiX4kU4pDrPrlOur3VTYhD+s7pK93f6i6jPxpdZufQ94bKoHcHyeMrHhfLUxZJzkmWktIS2Xpya5WeTlU89uNjFZ5gRUT2Ju+VzPzMGstlF2SL9SmrPPbjY5JTmCMqTslTq55yap9VUVRSJK+ue7XeXsTZZMH2BWVPujtO7XDJPhbuWCjEIb3f7i0tX2zpkn00BXYvmThk7ZG1dS6/aPciWXN4jQssO3+oi2A4G1X/B9BPRKaJyFT0/E6P11LmvMNq9aPYB8iqeorz48dh6LASPki7k+uemMuK1YXsd/+ClNwUPt72MWuOrKnQ+AU6PnnVh1exJ3kPybnJfLrjUwC+3P0lAIv3LqZUSvnzt38m3Cecww8c5qWrXuJ0zmk6BHcg/r54xncZT/fw7uy5dw9D2g2hf0R/Xhv1GjP6z+Bw+mE+3v4xAH+59C/suXcPf+z1R/466K/MHDyTtUfXkpCZQKBnIGuPrmVP8h683LzYcnILu5J2MTByYJmtMS1i2H5qO8sPLqdfm36E+oRiURZiWsZU6IlTE48Pe5yNd22kf0T/srRLQi/B39O/xnK+Hr7EtIxhXcI6diftRhC6h3d3ap9V4WZx48GBD5bFq89lruhwBQAtfFvQLbxbLbnrh7332eYTm13WE60puK7zdSgUnlZP+rZxLgzvyPgu4xnSbogLLGueONut1iIijhMZpNAMp0a3Wv0p8QX3zJwz1m3erKfHSA75Dnq/yze8S9vXHyE1L5UhFw0hPV+/qW/5weWcyj5FS7+WAHy8/WO2nNzC5xM/54lVTzB702xu73V7WSPlqsOr+GjbR/ya+Cvvj3uf1v6t+cugv/CXQX9BRFBKsWjSorLfK6auoFRKsVp0LxKAl355CXeLO7GtYvF08+S/Y/8LwP7U/Tyx6gkUir9c+heeWPUEADf1uIn3tujeSBUEo2UMWYVZrEtYx98v+3u9zqGH1YM+bWqdAKBKLo28lHnb5rHpxCYAureov2CcT4T5hDEiagSdQztX2zDaUDqFdEKhEIQuoV1cso+moIVvC67scCVuFrezOnbhQsXZSn+pUmqZUuo2pdRtwLfoMRTNCovFg2JfhcrKrZC+b5+eksJqhQF/fptWfq14avhTxLSM4a+D/spPR39i++nt/L+B/48SKWHhzoWA7hP/0i8v0bNlT27oegPT+0xnQ+IGPtr2ERuPb2R0x9EUlRZx75J7uTj4YqbETKmwX8fKw/5bKVXW+6NjSEc6BHcgKTepTCwc6RjSkUsjL2VIuyGM6zKuLH163+l6WygGRAwoS49pGVP2+4r2V9T7PNaX8V3Gk12YzXNrn8PD6nFeeAeNxY9Tf2TWNXXrmVcXvN29aRekh/M3Jw8D4KvJX/H5jZ83tRkXBE4Jhog8AswBYmyfOSLyN1ca1lSInycqq/wlSgUFMHmyHlG9cOkx1p5ewh2xd/DEsCdYOW0l/77q3/ypz5+ICorin5f/k5iWMXy47UNEhKX7l7I7eTcPD3oYpRS39ryViwIv4tZFtwK6D3mQVxDZhdk8POhh3CzOOnzljOygvQzHit+R/938P76a9BXdw7vj7+FPG/829GvTj1Z+rega3pVAr8CyvD1a9Chz7we1HVRnWxrKlR2upFt4Nw6nH6ZLWJd6nY/zFaWUy7wLO13CtGfRObR5CYaPu0+t430MjYPTd6SIfAF84UJbzglKA3ywOMzV8eyzejrkr7+GVSmfUCql3Nn7zgplZl87m8KSQjysHkzvM50/L/kzb296m5fXvUy7wHZM6j4JgCCvIDbdvYl7vr2HnMIcurfozrjO4/jh4A9M61m/d0SNvHgkszfNZkBk1YLhOCr4D93+gK+7L0opnhnxDH4efhXy+nr40iWsC5EBkWWji88mSinu738/07+d3qD2C0PVdA7tzNL9S5udh2E4i9TUIg5kAZlVfLKATGdb1s/Wp6G9pERETtzeVkrdlEhpqSQkiHi2OiiX3q27k97y5S3S7tV2NZYvLimWS9+5VIhDLE9ZyrrIVkd2Qbaczj5db3uLSopk9m+zJa8or97bcGRP0p5q+/KfDXIKc6TzfzrL3M1zm8yG5srqw6vlhk9vkOKS4qY2xXAOQR16STk10vt8oaEjvQFOPhhNq9d3QG4u0x4oZJ5XPwiNJ+1vaYz6aBR+Hn4sn7q8xm3sSd7DwHcG8vCgh3ls6GMNssdgMBhciSsmH7xgUIE6pn9kRxbzMu+BiHhAT0mwP3U/E7tNrHUbXcK6cPqR03hYPWrNazAYDOcLza5rbENRgXrmyn+8swu6fsnkS/4IwNqja0nJS3G6544RC4PB0NwwglEJFRRGJv4sPD0fa6kXb41/AR93n7JxE47TWhsMBsOFhBGMSliCWvCOxySKus5n1EWTCPEOoWtYV9YdWwdwQY0NMBgMBkeMYFTCGtKGj7p5gmcWj468G9AjjgVBoegQ3KGJLTQYDIamwQhGJVRQG3ZFJeNVEMylkZcClI0JaBvYtt5zzhsMBsP5jksFQyk1Sim1Vym1Xyk1s4r1ryqlttg++5RS6Q7rShzWfe1KOx3Zn9yJgtY76VAQUTby1i4YnUJM+4XBYLhwcVm3WqWUFXgLuApIAH5TSn0tIrvseUTk/znkvw/o5bCJPBGJdZV91bFmRwsI38XgrPJ5leyT4Jn2C4PBcCHjSg+jP7BfRA6KSCGwABhXQ/6bgPkutMcpvtt+ECylDCssn4DwIv9Ixh4PYNzJwBpKGgwGQ/PGlYIRARxzWE6wpZ2BUqod0B5Y4ZDspZTaqJRar5QaX91OlFJ32/JtTEpKarDRvyVsBiA2rbQszZKTy+I5mYyOb/DmDQaD4bzlXGn0ngx8LiIlDmntbMPVbwZeU0pdXFVBEZkjIn1FpG94eHiDjEhNhXTvTfjm+BCRll++IiNDf2dX/WIlg8FguBBwpWAkAm0dliNtaVUxmUrhKBFJtH0fBFZRsX3DJcTHA2020ik1AEumwzsxjGAYDAaDSwXjN6CTUqq9UsoDLQpn9HZSSnUBgoF1DmnBSilP2+8wYDCwq3LZxmbL7kwI30W/LCsq28HDyMzU30YwDAbDBYzLBENEioEZwDJgN7BQRHYqpZ5WSo11yDoZWCAVp83tCmxUSm0FVgLPO/auchUrDqwBSynXFAqWrMLyFcbDMDQHNmyA22+H0tLa8xoMVeDS2WpFZAmVXuUqIk9UWo6rotwvQLQrbauK39NXoIK8uNQahDXneNl7tMsEw+HFSgbDeccPP8D778Prr0NAQFNbYzgPOVcavc8JjrmtICRnMN7+IVhzoLjYNo7QeBiG5kB+fsVvg6GOGMGwcTo7mfygrXTxvBxLSEvcciE/77BeadowDM2BgoKK3wZDHTGCYePLzasAGNR6BJbIjqhSKDqkx2QYD8PQLLALhfEwDPXECIaNtfu3QqmFq7r3xa3nIABKt9le92raMAzNAbtQGA/DUE/MK1ptHExKhOxWdOvijpvXYJ2409Yxyy4Y+flQXAxu5rQZzkOMh2FoIMbDsHEqLxEyI2ndGlRoKIXhblj3HNEr7YIBkJPTNAYaDA3FtGEYGogRDBvpJYm450dgsZ2RgosD8Nhnm5vK3ugNph3DcP5iekkZGogRDBtZKgGvovK5EYs6t8HrYK4e5OToYRjBcC2lpdCpkx4vYGhcjIdhaCBGMICcwhyKrBn4lZYLRmnXjlgLoGT/Hi0Y/v56hWn4di0pKbB/P+ze3dSWND+Mh2FoIEYwgMQsPSdioKVcMFS0foFS8dbVWjAiI/UK42G4lhMn9HdeXtPa0RwxHoahgRjBABIztWCEuUeWpVmjBwJQsmWDbsOIsImJEQzXcvy4/s7NrTmfoe6YXlKGBmIEA0jITACghXe5h+EZ1pX8lmD5fSsUFRnBOFvYPQwjGI2PGYdhaCBGMCgPSbXxcxAMzwhy2oPbhn06wS4Ypg3DtRjBcB3GwzA0ECMYwLGMRMgPJDzItyzNYnGnoGMIbim2cRd1acMoKYHp02HHDhdY62JWr4YtW5pu/6YNw3WYNgxDAzFDloEjqYmQGUFQ64rppd06AKl6oU0b/e2MYMTHw9tvQ4sW0KNHo9rqcmbMgA4dYPHiptm/acNwHaaXlKGBGA8DOJaRAFkRBAVVTFc9HN4KGxICXl7OCcaePfp7//7GM/JskZkJaWlNt38TknIdxsMwNBAjGMCJbD0tSHBwxXT3mGGIsi0EBoKfn3NtGHv36u9zTTB27Sq3rTpyciqObD/bmJCU62iuHsbx4+WeqcGluFQwlFKjlFJ7lVL7lVIzq1h/m1IqSSm1xfa502HdNKVUvO0zzVU2lkopVjwgvd0ZHoZvWG/ybJGoMsFwxsM4m4IhAgsW6EkRa+NPf4IHHqg5T05OxZHtZxMR42G4ChEotL12uLkJxh//CHfeWXs+Q4NxWRuGUsoKvAVcBSQAvymlvq7i3dyfisiMSmVDgCeBvoAAm2xlGz1WYlEW/tPxEBNXc4ZgeHt3JLW9widR6iYY9pBUWpoeuRwa2thml/Prr3DTTdq+0aNrzpucXHNlUVKi1zeVYKSnl4dLjGA0LoUO76hvbiGpxETw8GhqKy4IXOlh9Af2i8hBESkEFgDjnCx7NfCDiKTaROIHYJSL7CwL2VcWDIvFnfzoFpT4uumpQfz9nfcw7N1wXe1lnDqlv1NTa8+bmVkeUtu160zb7JV0ZqZ+Ij3b2MMKbdqYkFRj4/ig0Nw8jLQ0Mz7qLOFKwYgAjjksJ9jSKjNBKbVNKfW5UqptHcuilLpbKbVRKbUxKSmpXoam217dXVkwALLuGsq2eS3BanXOw0hO1pX3Ndfo5doEY98+WLWqzjaXkZKiv53xChwF449/hAcfrLjefmwlJU0zjbs9HHXxxVq8mkK0miuOXkVz8zDS081rB84STd3o/Q0QJSIxaC/ig7puQETmiEhfEekbHh5eLyPS08v1oDI+IbFkhCRSVJTmXKO3PRw1ejRYLLqLbU08/TTcfHO97AbKPYvaBKOkRAuC3f6kJP1xxPGma4qwlKNglJZWDKMYGoajSDQnD6OoSF/XRjDOCq4UjESgrcNypC2tDBFJERH7lfwO0MfZso1Jerr2LpQ6c52/f38AsrI2Oudh2Bu8Y2Lgootq9zCSknRFWd/K0e5h1NazyW53drZ+cs/MPLPMuSIYHTrobxOWajwcRaI5eRj269SEpM4KrhSM34BOSqn2SikPYDLwtWMGpZTjULmxgH1O62XASKVUsFIqGBhpS3MJdsGoioCAfgBkZv7qXBvG3r26Aa5dO+jYsXbBsFf49e0W6GxIyi4OIuU9oSqXaWrBSE0Fd3c94BFMw3dj0lw9DHsDZFGR8UjPAi4TDBEpBmagK/rdwEIR2amUelopNdaW7X6l1E6l1FbgfuA2W9lU4Bm06PwGPG1LcwlpadULhptbID4+XcjK+rU8JFVTbP3YMWjbVse4OnasOiS1di18YIu+2Sv8hIT6Ge+sh+G4PilJ31yVyziKYVOMxcjM1L29fHz08oUkGHPmuHZ0fXP1MOwNkGDCUmcBl04NIiJLgCWV0p5w+P134O/VlJ0LzHWlfXZq8jAA/P0HkJq6BGl9BaqgAE6fhpYtq86cmFjeQ6pjR/3UnJqqR4qDFpSxY3X8a9q0xhMMZz0Mx33l5OjxG25u5ct2msLDyMiAgIBywbiQQlIvvACdO8M4ZzsS1hG7SPj6Nk8PA/T1W3n0raFRaepG73OC9PSar7OAgAEUFSVR2NFW6Vd+G9wHH8DVV+vfx4+XzzvVqZP+PnBAf4vA1Kn6Ik9N1Re4vRG6voLhbKO34/pEh+YgRyFpasG4kD2M9PSKT8uNjV0wAgKMh2GoN0YwqN3DCAgYAEBmhMMYBkc++wy+/15XspU9DChvxzhwQHeh7d5dL9t7VIHrQlKlpeWN3FXt61wSjIwMLRje3nr5QhEMEX0RuvKc272KwMDm62GYhm+XYwSDmtswAHx9o7Fa/Uj13qobvit7GJs2lX/n55cLRocOOvRkb8c4elR/X3ml/nYUnvoIhkjtIakpU+DWW6sXDMdyjjdcU3kYF2JIKitLd3s+Gx5GYGDz8jAqh6QMLuWCn95cRL8CIiys+jwWizshIWNITvmaS7p2QTlW9MePw8mT+vdPP+lvu2B4een3aNg9DLtg9NdddcsEw929foKRm1t+81dXwW/cCJ6e0Lt3eZpjSMqxnP2G8/NrWg/jQgtJ2YXibAlGc/IwTEjqrHLBexhK6frb3vW/OsLDb6Co6BSFHcMqehgbN5b/XrtWf0c4DErv1OlMwehjG26yc6f+7tq1foJh9y5atKh6Og8Rvd2TJ50PSXl46LmvmtLDuNBCUvan5Jwc3T3UFTiGpJqrh2FCUi7nghcMZwkJGYNSHmRFZusBZocOwebNOgxlsein8nXrdGZ7ozdU7Fp77JjuXXXRRXrZ7mH07Km36cyMs47YG7w7dNAhjcoVrH2ywZQU/dtq1enVhaRycvRxBAaefcEQOdPDuFBCUo6Vnqu6M1f2MJrLtCvp6eUTD9bHw3j0Uf2yM4NTGMFwEjc3f0JCRpIUbmuoHjAA+vWDDz+ELl30x37BVhaM5GR9YR89qsXC21s/SR88qPP07Kkbp+2hLWexexh296hyJe8oDPv3a09EqfIR1XCmh+Hr2zSCkZenRc+xDeNC8TAcwyquCks5ehjgOk/mbJOWVv765PoIxkcfweefN65NzRgjGHUgLOwGMtrY5l9KTdWjuQ8d0iEme6UdFqbbDOzYe0odOFAuGKA9DRH9dNS5s06ra1jKLhjt2+vvyk+nxxzmb9y7t3yK9pKScm+jcqO3r6+utOv7pJubCytW1L2c3Y4LsQ3D0cNwlWA4ehjQfNox0tPLBaM+Iank5IoPUIYaMYJRB0JDryO/lYXcoR1g7lzdlTYyUk80aBeMiEqT6trHYsTHVxSMVq3sG9Ujw6G8jcNZKguGvdL96itYtKiiYBw5oisLf3+9HBamG9srh6Qa6mG89x5ccUXFhnVnsAtUQIC2y2q9MENSrhaMgICKy+c7aWnQurX2nOvqYeTm6musrp79BYwRjDrg4RFGUMhwdrzoqQfgdeyoK/mbbioXDMdwFGjB8PGBr7/WF6hdHOwjxUND4ZJLtKexYUPVOz52rOKYDTtVCUZWFtx2GzzySEWPRURXFvYKIzDwTE+iMdow9u3T34cP162co4cB+pw1xMPIz9fdl+3tSucyjiLhqlBgfr4WYV/f8uXmgH3Ura9v3T0M+/2TkmLmoXISIxh1JCzsenJzd5OTY+spZZ/itjoPw9MTrrqqPE7qGJICLRje3nDppbByZXk5x0bJe+6BUaN02nPPlb/HIjVVV/D2ad0zM+Gdd3Slc+AA/P57RQELCCj3MAIDzxSGyh5GfRpG7e0yjt6NMzS2YOzcCT/+qNuYznXOlofh6VFITwwAACAASURBVKm7etuXz3dE9LmzC0ZdPYzk5PLf9heRGWrECEYdCQ+/HoDTpz+tuKI6wQC49tryRsaqQlIAl1+uK/i0NH0j3H67DnWJwPr1OqS0eze89hrMm1c+aC8kpNxrSEmBV18tF5Aff9R22fdRm2DY2zACA7W99XkKra9gOIakQItoQ0JS9mnmG/Jyqrpy4ED93rCYllY+15grBcPLq7x9rTl4GDk5uj0uKKjhgmHCUk5hBKOOeHpGEBx8NSdO/JfSUoeeJu3awf33w4QJZxayv30PqvYwAEaM0CKwZo0WhA8+gGXLtIjYXefnn9cTH6al6SeiU6d0W4T9qdzebvHaazr8UFysQ2B2cXIUDHt4qrpeUqDFJD5eT4jnTKhE5NzxMOyCsXu3Pmdngz/+Ee64o+7l0tP1daGUa3tJNTcPw+6ZBQc7966aytjvKzAN305iBKMeRETMoLDwOMnJX5UnWizw+usQHX1mgdatoW9f3U5hf/qv7GH076+fql94AWbM0BW9iPYYQJd1DK/s3KnFJDq6XARWrNANxuPG6Rc4QfWCUVtICnTl9dlnuv1l0aLaT8zJk+VPrnbBKCqCb7+t/Yas7GE0hmDYZ+Fdvbr+26kL+/fX/obFqkhL09eBM21HIrW/9bEqmqOH4fhuZeNhnBWMYNSD0NDReHm1JzHxTecLzZwJDzyghQXO9DA8PeGyy+CXX6BHD92e4eUFCxZoEbC/xtXeTXfZMv3k3K9f+ftli4r0sq+vHicCuhdXa9t7qioLRnWN3pdcope3bIFff9W/v/ii9mO0exeenrozwLZtEBWlQ3IzZ+p12dk6jFAZe0XZWCGpPXu01+bnd3bCUoWF5dPEVLb711/1IM/qsE9mFhhYu4fx1Vd6PE1dPThXeRjFxU03JYfdQwgJabhg1NfDOHwYnnxSj6O6ADCCUQ+UshIRMYOMjDVkZDjZC2fCBO092ImK0l7DxReXp732mq6Yf/5Zpw8apG/Inj3Lw1p33KErl48+0sv99BsBy7yC4cP198CB+ruyh2GvkAMCyp9o4+N1ZV9YqG+82FgtLKtXlwvG99/X/mRrF4yBA3WF9u67ujIcMEALYFGRHnNy331nls3M1Pu2jw/x8dH7mzABli6teb+VKS3VvbW6d9cifDY8jISE8k4CR45UXHfXXVUfsx17T5+goNoFY/VqXfl/+23d7LM3eje2h/H009qbbYqR4/brLSqqfiGp5GR93sPC6u9hzJ2rz0HlCUmbKUYw6kmbNn/C3T2Mw4efqt8GWrTQTyeOL8zp1g1uuKHcC7FX/n37asF48EEtGN276yciN7fy0FNlwbjuOv2CpmHDag9JXXEFTJyo03199XYHD9bideoUTJ6sxaS2SurgQR2HHzJEv9Xvxx+1WEybpgVk7lz9FD57tvZeHLFPC2LHxwe2boUvv4RXXnHypNpISNBP+Z0761Df7t2NG4KpqnJ0FAnHLsXFxdrb2bmz+krV3tPHGcHYulV/VxbRxYt1O0h15e0hKbuH0VjnY+1a/b87VrgpKWdHQOLj9bXarl39PIyUFC0WrVvX38Ow/x/bttWv/HmGSwVDKTVKKbVXKbVfKTWzivUPKaV2KaW2KaV+VEq1c1hXopTaYvt8XblsU2O1+tK27cOkpS0jI+OX+m2kdetycaiKESP0d79+OkTz6qs6lNWtm06PiSmvAAIC9M0zaJBeDgmB99/XFVF1ghEQoMNDx46Vh0zs/fSHDdOVPmihatVKt2fUxMGDOgRm95p27tTCYxexxx7TQhASottpHCsZ+8SDdry9y3uWrVhRMXyQl6dH13/5ZdV22Bu8O3fW4lpaWvU4lvoyY4be/9GjuoLOzq5eMA4e1MeRkVH1U2x+vv4EBelPTW0YIuUV1I8/Vhw78NZb+n9cvrzqsv+/vTMPr6q6Gve7770JGSGEhISEDMwzhEHUqsUBrbO18hOcbW2tFuv0OVFtHdpax89f69gqVqw4YR1wwGoVrYrImAAmBEJkzgCZ5zut7491b+4NJBBQklT2+zz3uefss4d19tlnrb32Pufs4JBU0MMIH5L68kt9b+dAlbwIrF2r28H/sjJtA/PnH1heB0NRkb6D5HIdvIeRlKRt+0A8jM2b4emn216P7jIYXezZHTKDYYxxAo8DpwGjgQuMMaP3iLYamCIi44HXgLAxG5pEJCfwO5seSFrabCIj0ygouASPp2r/CQ6UY45RpXjRRW3DgwswTZkSChs2TD2FoMLfM77DoTdX+FNS4T36IHFx+j9tmv5HRuoQ1cyZ8M47bXuwXi+88gqsW6f7xcX6GG/wSbDgOYwcqR7V7t1w+unw0EM6V5OVpe+NQPseBmg6n6/tpPvHH6uBe/DBveWHvQ0GhL4MXFOjHtC3eT9j4UItf9w4VTjnnKMGwxidbwo3GOGGas+FtyBUn3377n8OY/t29UZOOUWV4xdfaHhpqRoQ0Lmt9ujIw2hp0ZdQH3po3/MsInu/3FZeHjLkQYOxdKnmfTCfh+ksQaNaVBSa09ufh3H//frkYTi7d+sc4oF4GIWF2qavvFK9vGBHIWg4upLqajV2zz/fZUUeSg9jKlAkIsUi4gZeBtosWCwii0Uk+CjMUmDgIZTnO8flimPMmAW0tGyjoOBC/P4D/Nrs/jAGzj237bepIORhBOcvQId73nqr/XxycvQlv1Gj9h6SAp1QD84dBA3O5MmqtCdO1PIvukgVRnDy+7PPNL9Zs9SDmDdPFc6IEaG32Y3RFxKNCXkZP/mJvoleWKjzC7Nna+8suDxrkKDBuOoqNYavvho6FjzPpUvbjh2L6JNq996rXsyAAZrW5QoZtY8/1mGUyy7b/41WWLj3m+JlZaq4r7hCX8g84gh9FDo/X8vLymprMMLla89ghD8a2t6Q1BlnqMG9/fbQQl033KCG6e23df/VV9WLGjdODUaw17l5c+hJsz3nMIIexp//rIrXmI49yK1b1cgOH972i8rBOoWQwVi+XP+Dc1/fNcuXq5L/7DMdkgp+eic2Vs+1vcnnDz/Uhy5++9u2PfI9PYz99darq/XrAV6v3i/33qvhKSnfrYexZIneF6tXh8Lef18feAhvW598okb75psP7sm5g0FEDskPmAE8E7Z/CfDYPuI/BtwRtu8FVqCG5Mf7SHdlIN6KzMxM6Q527HhKFi9GCguvEr/ff+gLdLtF/vAHkZqaA0+blycSESFSVCTy1VciLpfIihUiU6eKgMgHH4TiPvKIyGuv6bbfLzJsmMgPfiDy4IOax7BhInPniiQna9oRI0S2bhVpbNT9sWNDeS1YIJKRIVJdHQorLxdJSREZM0Zk8GCRGTNCx269VfP48kuRO+8UMUZk2TIRn08kNVVk2jQRp1Nk9myRVau0Tv75T03zwx+K/Oc/obxGjxY56yzdvu46kehokeOP1//y8rb1U14usmGDSEWFyIABWu4jj+j5i4i8+66W8cknuv/++7ofEyNy9NEi06drXQa57DLNp08fkauvFrn9dj1+zjlaxhdfaPpFi0R+9zstz+fTtEVFemzYMP0fMkT/a2u1rvr0EamqEpk0SWTCBJG//lWPv/eeyIUX6vavf615jRqlaSorNfyRR/RaxcWJnHmmyGmniQwaFDrPILW1Iv37izgcmi68Xh95RMPGjxeZPFnDTjlFw4zZf/usrBTZuVO3V6/WNuL17jvNdddp/hddpP9/+YuG33+/7tfX636wDuvqRLKytL2CSG5uKK/oaJGbbgqdxzXX6HXoiJ/9TNvcsmXa/tTEiNxwg/7v3i2ydq3eM6WlIg0NIjNnirz11r7PaU9OPz2U99y5ImvWhPYTEvT+EhG59trQed1xx4GVEQawQjqr1zsb8UB/B2IwgIsDhqFXWFh64H8wsBkYsr8yJwcbbTdQVHSrLF6MbN/+ZLfJ0Gnc7tB28AYLKugvvug43V13hRruj36kykpEZOVKkV/+sq3yHTJE5MYb9y9LUOGCyBVXhMLfeEOVuterimfAAFWMn3+ucf/xD1W6wbTTpolkZ4uMGyfi8bQt4/zz1SCJqGI96SSR/HxVarffHorn96sydzo1H5dLDQCoYfJ4RO65p60yrK3V+CAya5bIz3+uCjbI1KkiJ56oxmT4cI07apTGv/dePU8QWbpU5H//V7d//GORBx4IKcHNm1WhQ+g8VqzQ/Zwc/X/2WZFvvgnVR3S0SGam1onfr+kuvliVGIj86U9qKGJiRIqLNT2ILF/etu7mztXwhQu1PubMCR274gqRpCRVmFFRWj+JiaqgQeSjjzq+7i+/LNKvn3Y2tm4VGThQ04wcKXLCCVrPe+LzheJFRoYMrYjIY4/pflmZyMcf63nl5oo8+aSGv/KK/v/hDxo/WA/33afnFqy3zMy9289DD4kcd5weD57/Qw/pflJSqA0fc0won4wMvY7Ba9QRDQ3aNm6+WcutqxPp1UvrdupUNeK//KWGBc/l5Zc17bhx2j4vuEDrsqGh43L2QU8xGEcD/wrbnwPMaSfedKAA6L+PvJ4DZuyvzO40GH6/T1avPlE++6yvtLTs6jY5DpoVK0TS07Vn1BHV1dqjy83duye6J5WVIs3NnSv7+uultafWEa++GroZnU7tzX3zjSqKhx5SZdaRkrr7bj22bZsq+6AyOu887aUHvZ6339Z4EyeGlIvPpzczqOE55xz1pMI54gg9fuutmgb05vX7ReLj1dhccYWGOxxqAKZPV+V3zDGqNOvqQko7eI5ZWSJTpmgZGzaokvzJT0Llnnyyxp0xI3Q9Lr1Ujdb27SElunGjSFqayuD1Sqs3CGqkgtcrmH9Vleb52msq38iRmv+0aWpwgxx5pCr3oNyLFoWMEagiS0pSg7lgQajXv3SpHp80Sa9baqru3323yLHHqpGLiFCZwgmmmzAhVE8bN+qxv/9d9zdt0joD9eimTVMD7ffrdZo6VWTJEvUQQeTpp1WuvDyRF1/UsH/+M1Tm66+H2sT114fa9Pr1Gj59ukhJSUieq69WLzQpSffHjdP/tWtDefp8Im++qdfo4YdDaU85RWT+fGn1YINlgxp7r1fv0TPP1M4ZiPzxj5rPtm3t3jadoacYDBdQDAwCIoE8YMwecSYCm4Bhe4T3DXobQBKwERi9vzK702CIiNTXr5PFi51SUPDTrhma+r7Q1KSu+/vvdxzH71el8Pvfi7zzzt7HP/1U5Kmn2k/72mvS6raHD6usXKkGZOxYNUjjx2tP3O1WRRR+DYOGwOXS4Z5wbrpJjz3xhMgLL+j2a6+pkoCQUQORc8/VNG+9FVIGTz+tYV9+qYrm73/XHmNQ+QZZvFgNR5C8PJFLLtlbsQbZsEHzePxxzW/2bA3v1UvDL7yw7RDQffdpeHZ26FxBvZ3w49u3a5lxcTrktXy5hp94ov6vWhUaRktN1V4yqMLetEnL7d1bjeScOXps5syQHMuWSetwzLvvquE56yztqUdEaD0EjWrQW16wQMPuuUf/BwxQY22MGiKR0DFQjwjUwwvi8aiRnjZN90tL1ZhPmtTWKxfRtnHGGXrNRdR7PP/8kFEsKNDOVUmJynnLLRpeWKjDukFvpn9/9Xj/9jcNi41VL83j0WszdKi08fxvuUWvywMPaPiSJe1f+wOgRxgMlYPTgQ0Bo3B7IOwe4OzA9r+BMiA38FsYCP8BsDZgZNYCV3SmvO42GCIiRUU3y+LFyJo1Z4nb3cGNbOlaCgvbKopwz+e993QeBVS5vPRS+3l4vSJHHaXxHn647bHgvMYHH+hQV1AZBX+ffCLy2Weaf9BYBZXB5MltlXbQSM2bp/ls2nTw5+33q/IfP16VzG23afibb+rQXnvnGBybf/JJnVtyuVTpiYTG0ocNE+nbV89n0SKtz8mTQ/XrdotcfrluL1+uSnTePE0zYoQq/Wuv1TwbG3XoLdyz9ft1SHPcOM2jf3/1KgYN0nkGn08V+dChoTTvvReq7xEj2g51rl+vcbZtU6X+0EOqmEGvSzgPPiit3u6oUTq0F+4ddERLS8de9xlnqLz33adzEP366TkHOwVBGYLDwpdeGkq7cKEOSQXzDnZCQI3unobsIOgxBqOrfz3BYPj9Ptm27f/LJ59EyNq1P7GeRk/h+edVYT733N7Hqqu1V7s/tz4/XxVXfn7bcL9fFWewd1ldrT3exx7T4Y1geFlZ23RlZaF5oPZoatq3PJ3hyiv1Nh85UucK9kdNjdaFiE7Kr1oVOub3q4d35pk6ZBU+gezz6ZDewoW6v3u39rLD+fDD0OR5YeG+5bj9do2XmBiaGA/nkUdCno+IGpwTTlCPZetWlSc7W4eS2uPNN9Uo7fnQQ0uLKmhQDyr4cMO3YenS0LzOmDE6lCqihuyFF0LxvF71gtat23d+b7+tBjj82nwLDsRgGI3//WDKlCmyYsWK7hYDgK1bH6C4+FZGjXqJlJRZ3S2O5XClsFDfrP/d7/TR3e5m/nx9d+E3v9l3vMJCfbR73rz2vwDdGQoL9fHj4NIDB8L77+sLiGPHHlzZeyKin6vJzNSXUnsQxpiVIjJl/zGxBuNQ4fd7Wb36GOrrcxkw4AoGDfo9ERH9ulssi+W/B79/319CsHwnHIjBsFfjEOFwuBg79i1SU39KSckz5OaeiMdTsf+EFotFscaix2GvyCGkV69URox4inHj3qGxsZBVq46hsrKDTzdYLBZLD8cajC4gMfEUxo9/DxEPa9acSn7+xXi9XfQqv8VisXxHWIPRRfTteyJTp+aTnX0X5eUvsWzZcIqKbsTjqexu0SwWi6VTWIPRhTgcvcjOvpOcnE+Ijz+SHTse5euvZ3z3Hy20WCyWQ4A1GN1AQsJxjBv3JiNGPEN19WI2bPglzc1b+D49sWaxWL5/uLpbgMOZ1NTLqK9fy/btD1Na+izgJCHheMaNW4jTGdPd4lksFksbrIfRzQwd+hBHHrmJIUMeJj39GqqrP2b9+p/S1LSZ3bvfoajoBlpadgDY+Q6LxdKtWA+jBxAdPZiMjBsB6NUrjeLiW9m1K7RgUGPjBlJTLyU//wJGjZpPSsoF3SWqxWI5jLEGo4eRkXEzcXE5tLTsIDIylYaGdRQX30JV1YeAsHXrvfTtO51t2x4gPf0aoqKy8HrrcLniu1t0i8XyPccajB6GMYbExFNa9xMTT2HXrgU0N29m4MAb+eabOaxceQQtLVuoqHiPfv3OZNu2Bxk79g2Sks7ZR84Wi8Xy7bBzGD0cY5zk5CzmiCPyycj4H3r1GkhLy1YyMm6lqWkD27Y9gMMRTXHxbxDxtaZrbNzI9u2P4fM1tIaJ+PH5GtsrZi86erHQPsllsRy+WA/jvwCnMxanMxaA0aMX4PVW0K/fGSQkTMPjqcDhiCQ/fyZr155DXd1XGBOB210CQGXl+4wd+yYeTxl5eSfT2FhIfPxkRo16Ebe7hKKiGxg+/El69z6itbzS0udZv/5yhgx5kIyM/wHUUGzZcg87djzB2LGv06fPMQA0N2/H6YwlIqIHfAnVYrEcUuzXar8HiPhZuXIK9fVrSE4+D6cznqioLByOSIqLbyM+fgpudzlebxVpaVdRUjKXyMgUvN4q3O5SIiKSyMn5jNjYkXi9tXz11TB8vnr8/kaysu4kO/tOCgt/TmnpszidcRgTwcSJXxAVlcXSpdmI+Bg27FFSUi5slamqajHR0UOIisrcp+x1dSupqvqYjIybMMYAUF+/hl690lu/7uv3e/F6K4mM7P+d1Vll5b+oq1tFRsZNOBwR+4zr93txOA5936qlpZSysucZOPA6HI5eh7w8iwXs12oPO4xxMGHChxx99BbGjHmFkSOfITv7t2Rm3sqQIQ9jTCSRkSmMH/8vhgx5gDFjFtDYuAGvt4YxY14HDCtXTqKo6AYKCi7C4yknJ2cxqamXs2XL3eTnz6S09FkyM29nypQ8jImgoOAiSkvn4fHsIjIyhYKCi6ioWBTwRO4lL+9E8vKm09y8lXXrzmXLlnsRERobC1u/2ltVtZjc3OMpLr6F3bvfAqC+fh0rVkziq6+GsnPn3wDYuPEalixJZd26c2lsLGq3DnbvfpsNG67B729pDfP7vezY8QQbN15PfX1ea7jbXU5+/iy++eY35OWdjMdT1WHd1tWtZMmS/pSXvwaocQ7H46ls86a+11vL9u2Ptj4KHY7XW7dX+iAiwvr1l1NcfCulpc+HlZ9Lc/OWDuULpg3+f/PNXVRWfgBAVdVHNDdv32fakGw1rFw5lS1b7u1U/Lbl+9m16594PNWtYY2NG+2HNg8AEf9/xWPzh9TDMMacCvwZcALPiMh9exzvBTwPTAYqgJkisjlwbA5wBeADrhWR/ba+w9XDOBgqKhbhdMaQkDCNpqbNFBffxq5dr2CMi4ED/4chQ+7D52sOrOmxiuTkGYwe/SrGGMrLXyU/fyYORxQxMSOZNGkpK1ZMxOdrJCHhh5SV/YO+fU+hqurfOByR+P3NAMTEjKSxcT0ORwxxcTnU1i4hJmY0Im4cjmimTMklL+9k6utXExeXQ3X1YtLSfsXOnU/Qp8+xNDSsw+GIZtiwJ6iq+gC3u5yoqGzS02ezcuUkvN5qkpNnkJ5+DbW1SyktfZ7GxnyMcSHiZfDg+8nMvIWCgksoL3+F7Oy72Lz5LhITT2fs2Ddwu0vw+93U1S2nouJtMjJuZtOmm6iq+oDIyFSysu6guHgOw4Y9TkzMcL7+eiYtLVuIiRnFxImfAw7WrDmVurqvcDiiGDToj2Rk3IjP18SmTTexc+cTGNMLl6s3Lldfxox5lbi4CQCUls5j/frLcThiiIoaxBFHrKW5eQvLl4/G6YwjJ+cTwBAZmUJERKKufmYMHk8Vq1cfS58+PyAx8TS+/vo8IiKSGTlyHmvXnkFc3AQmTVre6iE1N29l7doz6d9/FllZv2k1Nhs3/pqdOx8HYNCge8nMvK3V4wM1XLt2LaBXrwz69z+fiIhEAPx+N+vX/5Ty8hdJTj6fMWNewe9vYfnycTQ1bWLKlNVERqbidpcTF6eLEYkI9fWriIoajMezi5KSZ0lPvxqvt5qvv55BSsolpKdfg9tdRkzMCIxp26/VYdAYXK6+rTI2Nm6kpuZzkpPPw+Xq3Sa+iLBx42yam7eQnKz5h3uMfr+XlpYtuN27cDpjiI4euteLsyKCx1NBZGQS9fV5lJe/TGbmb/b5dKKID4+nCqczDqczqk1eVVUfEBeXQ2RkCgCFhb+grOxFJk78gvj4nNa4Pl8jbnc50dHZ7eQv1NT8h7i4iXud84HQIxZQMsY40fW8Twa2A8uBC0QkPyzOr4DxInKVMWYWcK6IzDTGjAZeAqYCaeja38MlfFa3HazB+Hb4fI04HFFtbtDm5q2UlDxNRsYtrTeHiJCXdxLV1YsZOfI5UlMvo7r6c3JzjwMM2dl3kpX1W7ZufYCtW//I6NELAgp8LmlpV9HYWEhd3Sr6959FevpsKisXUVBwEfHxU6irW8HQoY+SlnYV69b9mMrKd4mMTGPq1PW0tGwlN/cEPJ5dOByxREVl0dhYgMMRhYif9PRfsX37I62yx8dPITPzNhISTmDDhqvYtWsBvXsfQ23tF2Rl3cGgQb9n27aH2bTpJuLjp1JXtyysNgxOZxw+Xx0pKZdSVvYC4MfhiEXEjdMZi8uVSErKxWzdeh+xsaPxeHbjdpcxbNjjVFS8S0XFW6Sn/5qqqg9pbFzPgAG/wOXqg89Xz+7db+FwRDFx4pLWlzXj449gwIArKCz8GWPGvE5p6bNUVS3G5YrH7S4DBIcjlsTEH1FV9SGxseNwuRKorHwPAIcjhoiIJFpatgEOnM4YfL46hgx5mLS0q6mvz2P9+stoatoAQGbmHEpL/47TGU9TUxFpaVfj9VZSXv4yffpMIyvrDhISjqehYS25uSfg89UAavjHj/8AYyLIz59FTc2n9O59NLW1XzJhwkfU1i7jm2/m4HDEEBMzCre7BLd7J6mpPwt0KOZTVfVhwJAL4CM6egQALS1bWjsYAMnJM0lL+wW7dy+kb9/pVFV9yI4djwIQFTWYpKSzqatbTU3Np62yjRz5HPHxR+Dz1WOMi5KSZygquo6IiGQ8nl0kJJzI8OFP4ve7KS6+maqqjxFxt5bpcEQxdOijxMaOoqZmCUlJZ7N58z2Ul79E//6zqKh4F5+vlt69j2LkyOfxeHaxefPduN07iYhIYeDAa6moeJuSkr+j/V2IihrCkCEPEhGRxJYtf6Cq6gOiogaTk7OYpqYi8vJOAhxERWUxdOhfcLl643BEs379pTQ1FTF06J+prf2KhoY1jBgxl7i4iRQXz2HbtvuJihrMmDELiI+fdFD3fU8xGEcDd4nIjwL7cwBE5E9hcf4ViPOlMcYFlALJwG3hccPj7atMazC6jqamTezY8TiDB/+pdby9vPxVIiPTSEg4tjWe3+/Z7xyBiI+Cgktobv6G2NhxDBv2BA6HC6+3no0bf0VKyqUkJk4Hgj3Jz0hOnoHL1Ztdu16noOASsrPvJDPzFqqrP8fvbyYmZiRRUQPD5GhhzZpTqa1dTnb2XQwceD0OhwsRP2vWnE5t7RIGDryOqKgh9Oo1gKioQeTlTQeEqVM3sGPHozQ1bSI7+25yc4/H56tl4sQviI4eRFnZfAoKLiUhYRrZ2XeTkHAcIj7y8y9g164FREUNZvjwJ0hM/FGrPDU1S8nN/SEiHgD69PkhY8e+jtMZx9Klg1ofWhgy5GESE0+npORvxMSMorr6YyorP6Rv3+lUVr6Pz1fD4MH3UVPzJRUVbzFhwkeUlb1Iaelcxo1bxI4df6GyclFruQ5HDGPHvsmmTTfR0LCGuLjJRET0xeOpIifnY5zOOEpKnqG4eA5ebyU6OOCjV68McnL+Q3NzMevWnYvPV4cxToyJYPjwv5GcfB7Ll4+hpWUbIj769Tubfv1OY8OGq4iMTCMptEVmfgAACm5JREFU6VxKSv6KiBenM46srN+1Dk327n0k+fkXIOJmwoR/4/c3UV+/Bp+vjq1bg+rCAehwXlra1URHD6GiYhHV1R8TGzuW5OTziI2dwIYNVwY6FFEBw2MAQ79+ZzB27JuUlj7Hhg2/QkSHLl2uvqSm/ozY2LFERqbg89VSUvIMVVX/3qud9ut3FpWVi4iOHkZ6+rUUFV3bev0iI1Pp3fsH1NWtpKVlC+BgwIBfEBs7Gp+vjvLyV2hoWAuA0xnHwIHXs337owQNSkRECiNGPM2aNae2MV4REUlER4+gtvYLjHHhciXi9VbicvXD4ykjOXkmNTXa5o86ajMuV9w+77X26CkGYwZwqoj8PLB/CXCkiFwTFmddIM72wP4m4EjgLmCpiLwQCJ8LLBKR19op50rgSoDMzMzJW7bse7zX8v3D73fjcER2Ip4Hv79pL/fd7/cg4tlrGMLrrcHvb24dNgji8zUg4sXl6tMmLPgkW7hcVVUfkpBwUpshiSDV1Z9RU/MFERGJpKZe1mp4m5u3UFn5AX5/I2lpszuccG9u3kp19SekpFyM3++msXE98fE5+P0tNDSsIz5+Mh5PBaWlz+H3txAdPYyEhOOJjEymuXk71dUf0b//Re3m7/M1UVn5HnV1KzEmgtTUy4mOHgRAQ0M+5eUv4/c3k5JyaetQU0PD15SWPofDEUt6+mwiIhIpKZlLYuKpREVl4vXW4PHsxuXqR0REwh518R88nl0kJ7ddv7uiYhEtLdvo338WlZWLcDhiSEo6q821C++QeDwVVFS8R339KiIjB+D3N+N2lzBo0B9bh9EaG4uorv4Er7eS1NSfEhmZ3KZMER87dz4N6LIEpaXPEx8/keTk82hu3obL1ReXK46Ghnxqa7/E73eTknIJLlccfn8Lu3a9TkzMSOLjJ7aRs6xsPk5nHImJJ+Ny9aG+Po+dO/+Kz9fAwIHXEx8/kZaWnbS0bMfrraK5eQuJiacRGZnCjh1PkJBwPFFRGWzZ8id8vhpiYyeQnj4bj2c3DQ1r6Nv3pHbbyf44rAxGONbDsFgslgOjpzwltQPICNsfGAhrN05gSKoPOvndmbQWi8Vi6UIOpcFYDgwzxgwyxkQCs4CFe8RZCFwW2J4BfCzq8iwEZhljehljBgHDgGVYLBaLpds4ZG8jiYjXGHMN8C905uxZEfnaGHMPsEJEFgJzgX8YY4qAStSoEIj3KpAPeIHZ+3tCymKxWCyHFvumt8VisRzG9JQ5DIvFYrF8j7AGw2KxWCydwhoMi8VisXQKazAsFovF0im+V5PexphdwMG+6p0E7P4OxfkusDJ1np4ol5Wp8/REuQ4XmbJEJHn/0b5nBuPbYIxZ0dknBboKK1Pn6YlyWZk6T0+Uy8q0N3ZIymKxWCydwhoMi8VisXQKazBC/K27BWgHK1Pn6YlyWZk6T0+Uy8q0B3YOw2KxWCydwnoYFovFYukUh73BMMacaowpNMYUGWNu6yYZMowxi40x+caYr40x1wXC7zLG7DDG5AZ+p3eDbJuNMWsD5a8IhCUaYz40xmwM/PftQnlGhNVHrjGm1hhzfXfUlTHmWWNMeWBdl2BYu3VjlL8E2tkaY8zBrad5cDI9aIxZHyj3DWNMQiA82xjTFFZnT3WhTB1eL2PMnEA9FRpjftR+rodEplfC5NlsjMkNhHdJPQXK6kgXdGu7akVEDtsf+hXdTcBgIBLIA0Z3gxwDgEmB7Xh0LfTR6EJSN3VzHW0GkvYIewC4LbB9G3B/N16/UiCrO+oK+CEwCVi3v7oBTgcWoWuGHgV81YUynQK4Atv3h8mUHR6vi+up3esVaPd5QC9gUOD+dHaFTHscfxj4XVfWU6CsjnRBt7ar4O9w9zCmAkUiUiy6kO7LwDldLYSIlIjIqsB2HVAApHe1HAfAOcC8wPY84MfdJMdJwCYR6ZZ1eUXkP+hn+cPpqG7OAZ4XZSmQYIwZ0BUyicgHIuIN7C5FFyTrMjqop444B3hZRFpE5BugCL1Pu0wmY4wBzgde+q7L3R/70AXd2q6CHO4GIx3YFra/nW5W1MaYbGAi8FUg6JqAq/lsVw79hCHAB8aYlUbXTwdIEZGSwHYpkNJ+0kPOLNre1N1dV9Bx3fSUtvYztEcaZJAxZrUx5lNjzHFdLEt716sn1NNxQJmIbAwL6/J62kMX9Ih2dbgbjB6FMSYO+CdwvYjUAk8CQ4AcoAR1k7uaY0VkEnAaMNsY88Pwg6J+cZc/amd0FcezgQWBoJ5QV23orrrpCGPM7eiCZPMDQSVApohMBG4EXjTG9O4icXrc9QrjAtp2RLq8ntrRBa10Z7s63A1Gj1k73BgTgTaQ+SLyOoCIlImIT0T8wNMcAtd8f4jIjsB/OfBGQIayoNsb+C/varlQA7ZKRMoC8nV7XQXoqG66ta0ZYy4HzgQuCigcAsM+FYHtleh8wfCukGcf16u768kF/AR4JUzWLq2n9nQBPaRdHe4GozPrjh9yAmOmc4ECEfnfsPDwschzgXV7pj3EcsUaY+KD2+jk6TrarsV+GfBWV8oVoE0vsLvrKoyO6mYhcGngqZajgJqwIYZDijHmVOAW4GwRaQwLTzbGOAPbg4FhQHEXydTR9VoIzDLG9DLGDArItKwrZAowHVgvItuDAV1ZTx3pAnpKu+qKmf+e/EOfMtiA9hpu7yYZjkVdzDVAbuB3OvAPYG0gfCEwoIvlGow+sZIHfB2sH6Af8BGwEfg3kNjFcsUCFUCfsLAuryvUYJUAHnTs+IqO6gZ9iuXxQDtbC0zpQpmK0HHuYNt6KhD3vMB1zQVWAWd1oUwdXi/g9kA9FQKndZVMgfDngKv2iNsl9RQoqyNd0K3tKvizb3pbLBaLpVMc7kNSFovFYukk1mBYLBaLpVNYg2GxWCyWTmENhsVisVg6hTUYFovFYukU1mBYLD0AY8zxxph3ulsOi2VfWINhsVgslk5hDYbFcgAYYy42xiwLrIvwV2OM0xhTb4x5JLB+wUfGmORA3BxjzFITWociuIbBUGPMv40xecaYVcaYIYHs44wxrxldu2J+4K1fi6XHYA2GxdJJjDGjgJnAMSKSA/iAi9A3z1eIyBjgU+DOQJLngVtFZDz6Fm4wfD7wuIhMAH6AvnEM+mXS69H1DwYDxxzyk7JYDgBXdwtgsfwXcRIwGVge6PxHox+B8xP6WN0LwOvGmD5Agoh8GgifBywIfJsrXUTeABCRZoBAfssk8A0jo6u9ZQOfH/rTslg6hzUYFkvnMcA8EZnTJtCY3+4R72C/t9MStu3D3p+WHoYdkrJYOs9HwAxjTH9oXWc5C72PZgTiXAh8LiI1QFXYYjuXAJ+KrqK23Rjz40AevYwxMV16FhbLQWJ7MBZLJxGRfGPMHegKhA70S6ezgQZgauBYOTrPAfoZ6qcCBqEY+Gkg/BLgr8aYewJ5/L8uPA2L5aCxX6u1WL4lxph6EYnrbjkslkONHZKyWCwWS6ewHobFYrFYOoX1MCwWi8XSKazBsFgsFkunsAbDYrFYLJ3CGgyLxWKxdAprMCwWi8XSKazBsFgsFkun+D8Y5FYMPhfUlwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2650 - acc: 0.9406\n",
      "Loss: 0.26498853739153744 Accuracy: 0.9406023\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6603 - acc: 0.4951\n",
      "Epoch 00001: val_loss improved from inf to 1.27673, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/001-1.2767.hdf5\n",
      "36805/36805 [==============================] - 225s 6ms/sample - loss: 1.6603 - acc: 0.4951 - val_loss: 1.2767 - val_acc: 0.6131\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8492 - acc: 0.7391\n",
      "Epoch 00002: val_loss improved from 1.27673 to 1.18780, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/002-1.1878.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.8492 - acc: 0.7391 - val_loss: 1.1878 - val_acc: 0.6427\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5744 - acc: 0.8264\n",
      "Epoch 00003: val_loss improved from 1.18780 to 0.73617, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/003-0.7362.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.5744 - acc: 0.8264 - val_loss: 0.7362 - val_acc: 0.7717\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4349 - acc: 0.8692\n",
      "Epoch 00004: val_loss improved from 0.73617 to 0.42402, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/004-0.4240.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4350 - acc: 0.8692 - val_loss: 0.4240 - val_acc: 0.8712\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8940\n",
      "Epoch 00005: val_loss improved from 0.42402 to 0.38916, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/005-0.3892.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3489 - acc: 0.8940 - val_loss: 0.3892 - val_acc: 0.8777\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9129\n",
      "Epoch 00006: val_loss improved from 0.38916 to 0.33131, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/006-0.3313.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2898 - acc: 0.9129 - val_loss: 0.3313 - val_acc: 0.8975\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9248\n",
      "Epoch 00007: val_loss improved from 0.33131 to 0.31776, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/007-0.3178.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2484 - acc: 0.9248 - val_loss: 0.3178 - val_acc: 0.9057\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9383\n",
      "Epoch 00008: val_loss did not improve from 0.31776\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2103 - acc: 0.9383 - val_loss: 0.4362 - val_acc: 0.8661\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9424\n",
      "Epoch 00009: val_loss improved from 0.31776 to 0.25585, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/009-0.2559.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1923 - acc: 0.9425 - val_loss: 0.2559 - val_acc: 0.9161\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9539\n",
      "Epoch 00010: val_loss did not improve from 0.25585\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1556 - acc: 0.9539 - val_loss: 0.2989 - val_acc: 0.9136\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9587\n",
      "Epoch 00011: val_loss did not improve from 0.25585\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1401 - acc: 0.9586 - val_loss: 0.4329 - val_acc: 0.8733\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9586\n",
      "Epoch 00012: val_loss improved from 0.25585 to 0.25199, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/012-0.2520.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1380 - acc: 0.9586 - val_loss: 0.2520 - val_acc: 0.9243\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9660\n",
      "Epoch 00013: val_loss improved from 0.25199 to 0.24685, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/013-0.2469.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1158 - acc: 0.9660 - val_loss: 0.2469 - val_acc: 0.9264\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9715\n",
      "Epoch 00014: val_loss improved from 0.24685 to 0.23648, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/014-0.2365.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0995 - acc: 0.9716 - val_loss: 0.2365 - val_acc: 0.9320\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9754\n",
      "Epoch 00015: val_loss did not improve from 0.23648\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0866 - acc: 0.9753 - val_loss: 0.3799 - val_acc: 0.8868\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9762\n",
      "Epoch 00016: val_loss did not improve from 0.23648\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0844 - acc: 0.9762 - val_loss: 0.2920 - val_acc: 0.9096\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9788\n",
      "Epoch 00017: val_loss improved from 0.23648 to 0.23263, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/017-0.2326.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0739 - acc: 0.9788 - val_loss: 0.2326 - val_acc: 0.9331\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9786\n",
      "Epoch 00018: val_loss did not improve from 0.23263\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0749 - acc: 0.9786 - val_loss: 0.2982 - val_acc: 0.9180\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9838\n",
      "Epoch 00019: val_loss did not improve from 0.23263\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0589 - acc: 0.9838 - val_loss: 0.2468 - val_acc: 0.9313\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9875\n",
      "Epoch 00020: val_loss did not improve from 0.23263\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0477 - acc: 0.9874 - val_loss: 0.8233 - val_acc: 0.7687\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9852\n",
      "Epoch 00021: val_loss did not improve from 0.23263\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0570 - acc: 0.9852 - val_loss: 0.2576 - val_acc: 0.9276\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9846\n",
      "Epoch 00022: val_loss improved from 0.23263 to 0.21547, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/022-0.2155.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0571 - acc: 0.9846 - val_loss: 0.2155 - val_acc: 0.9345\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9898\n",
      "Epoch 00023: val_loss did not improve from 0.21547\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0402 - acc: 0.9898 - val_loss: 0.2352 - val_acc: 0.9322\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9865\n",
      "Epoch 00024: val_loss improved from 0.21547 to 0.20589, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/024-0.2059.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0496 - acc: 0.9865 - val_loss: 0.2059 - val_acc: 0.9434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9917\n",
      "Epoch 00025: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0333 - acc: 0.9917 - val_loss: 0.2495 - val_acc: 0.9329\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9922\n",
      "Epoch 00026: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0307 - acc: 0.9922 - val_loss: 0.2776 - val_acc: 0.9215\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9910\n",
      "Epoch 00027: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0349 - acc: 0.9910 - val_loss: 0.4639 - val_acc: 0.8656\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9877\n",
      "Epoch 00028: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0432 - acc: 0.9877 - val_loss: 0.2091 - val_acc: 0.9432\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9878\n",
      "Epoch 00029: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0431 - acc: 0.9878 - val_loss: 0.2866 - val_acc: 0.9215\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9920\n",
      "Epoch 00030: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0292 - acc: 0.9920 - val_loss: 0.2558 - val_acc: 0.9276\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9971\n",
      "Epoch 00031: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0156 - acc: 0.9971 - val_loss: 0.2788 - val_acc: 0.9241\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9911\n",
      "Epoch 00032: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0311 - acc: 0.9911 - val_loss: 0.5772 - val_acc: 0.8411\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9932\n",
      "Epoch 00033: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0278 - acc: 0.9932 - val_loss: 0.2300 - val_acc: 0.9383\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9955\n",
      "Epoch 00034: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0193 - acc: 0.9955 - val_loss: 0.2735 - val_acc: 0.9269\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9941\n",
      "Epoch 00035: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0232 - acc: 0.9941 - val_loss: 0.2417 - val_acc: 0.9345\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9969\n",
      "Epoch 00036: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0145 - acc: 0.9968 - val_loss: 0.3177 - val_acc: 0.9175\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9864\n",
      "Epoch 00037: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0460 - acc: 0.9864 - val_loss: 0.2571 - val_acc: 0.9313\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9972\n",
      "Epoch 00038: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0126 - acc: 0.9972 - val_loss: 0.2092 - val_acc: 0.9478\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9967\n",
      "Epoch 00039: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0143 - acc: 0.9967 - val_loss: 0.2428 - val_acc: 0.9357\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9899\n",
      "Epoch 00040: val_loss did not improve from 0.20589\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0326 - acc: 0.9899 - val_loss: 0.2214 - val_acc: 0.9411\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9964\n",
      "Epoch 00041: val_loss improved from 0.20589 to 0.20376, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/041-0.2038.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0162 - acc: 0.9964 - val_loss: 0.2038 - val_acc: 0.9467\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9950\n",
      "Epoch 00042: val_loss did not improve from 0.20376\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0202 - acc: 0.9950 - val_loss: 0.2286 - val_acc: 0.9429\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9967\n",
      "Epoch 00043: val_loss did not improve from 0.20376\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0157 - acc: 0.9966 - val_loss: 0.7066 - val_acc: 0.8227\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9879\n",
      "Epoch 00044: val_loss did not improve from 0.20376\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0419 - acc: 0.9879 - val_loss: 0.2223 - val_acc: 0.9429\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9985\n",
      "Epoch 00045: val_loss did not improve from 0.20376\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0080 - acc: 0.9985 - val_loss: 0.2841 - val_acc: 0.9259\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9912\n",
      "Epoch 00046: val_loss did not improve from 0.20376\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0296 - acc: 0.9911 - val_loss: 0.2203 - val_acc: 0.9429\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9940\n",
      "Epoch 00047: val_loss improved from 0.20376 to 0.19841, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/047-0.1984.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0211 - acc: 0.9940 - val_loss: 0.1984 - val_acc: 0.9506\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9955\n",
      "Epoch 00048: val_loss improved from 0.19841 to 0.19599, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/048-0.1960.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0170 - acc: 0.9955 - val_loss: 0.1960 - val_acc: 0.9564\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9986\n",
      "Epoch 00049: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0078 - acc: 0.9986 - val_loss: 0.2496 - val_acc: 0.9408\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9883\n",
      "Epoch 00050: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0365 - acc: 0.9883 - val_loss: 0.2432 - val_acc: 0.9404\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9963\n",
      "Epoch 00051: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0150 - acc: 0.9963 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9932\n",
      "Epoch 00052: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0260 - acc: 0.9932 - val_loss: 0.2601 - val_acc: 0.9352\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9982\n",
      "Epoch 00053: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0083 - acc: 0.9982 - val_loss: 0.2115 - val_acc: 0.9490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9986\n",
      "Epoch 00054: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0065 - acc: 0.9986 - val_loss: 0.2413 - val_acc: 0.9418\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9954\n",
      "Epoch 00055: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0169 - acc: 0.9954 - val_loss: 0.2855 - val_acc: 0.9313\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9937\n",
      "Epoch 00056: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0213 - acc: 0.9938 - val_loss: 0.2185 - val_acc: 0.9462\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9972\n",
      "Epoch 00057: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0119 - acc: 0.9972 - val_loss: 0.2558 - val_acc: 0.9376\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9962\n",
      "Epoch 00058: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0149 - acc: 0.9962 - val_loss: 0.2147 - val_acc: 0.9495\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9970\n",
      "Epoch 00059: val_loss did not improve from 0.19599\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0119 - acc: 0.9970 - val_loss: 0.2249 - val_acc: 0.9443\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9912\n",
      "Epoch 00060: val_loss improved from 0.19599 to 0.19113, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/060-0.1911.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0306 - acc: 0.9912 - val_loss: 0.1911 - val_acc: 0.9525\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9961\n",
      "Epoch 00061: val_loss did not improve from 0.19113\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0154 - acc: 0.9961 - val_loss: 0.1925 - val_acc: 0.9513\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9993\n",
      "Epoch 00062: val_loss improved from 0.19113 to 0.18997, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/062-0.1900.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0043 - acc: 0.9993 - val_loss: 0.1900 - val_acc: 0.9550\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9974\n",
      "Epoch 00063: val_loss did not improve from 0.18997\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0112 - acc: 0.9974 - val_loss: 0.2173 - val_acc: 0.9497\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9949\n",
      "Epoch 00064: val_loss did not improve from 0.18997\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0173 - acc: 0.9949 - val_loss: 0.2150 - val_acc: 0.9488\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9934\n",
      "Epoch 00065: val_loss did not improve from 0.18997\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0236 - acc: 0.9934 - val_loss: 0.1938 - val_acc: 0.9527\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9994\n",
      "Epoch 00066: val_loss improved from 0.18997 to 0.18629, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/066-0.1863.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0042 - acc: 0.9994 - val_loss: 0.1863 - val_acc: 0.9583\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9987\n",
      "Epoch 00067: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0055 - acc: 0.9987 - val_loss: 0.2849 - val_acc: 0.9306\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9967\n",
      "Epoch 00068: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0132 - acc: 0.9967 - val_loss: 0.2444 - val_acc: 0.9399\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9966\n",
      "Epoch 00069: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0132 - acc: 0.9966 - val_loss: 0.2129 - val_acc: 0.9488\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9984\n",
      "Epoch 00070: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0080 - acc: 0.9984 - val_loss: 0.2391 - val_acc: 0.9453\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9960\n",
      "Epoch 00071: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0136 - acc: 0.9960 - val_loss: 0.2370 - val_acc: 0.9441\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9940\n",
      "Epoch 00072: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0194 - acc: 0.9940 - val_loss: 0.2069 - val_acc: 0.9515\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9983\n",
      "Epoch 00073: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0076 - acc: 0.9983 - val_loss: 0.1964 - val_acc: 0.9525\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9988\n",
      "Epoch 00074: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0061 - acc: 0.9988 - val_loss: 0.1919 - val_acc: 0.9581\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9996\n",
      "Epoch 00075: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0026 - acc: 0.9996 - val_loss: 0.1943 - val_acc: 0.9576\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9946\n",
      "Epoch 00076: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0196 - acc: 0.9946 - val_loss: 0.2493 - val_acc: 0.9415\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9963\n",
      "Epoch 00077: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0125 - acc: 0.9963 - val_loss: 0.2355 - val_acc: 0.9460\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9988\n",
      "Epoch 00078: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0060 - acc: 0.9988 - val_loss: 0.1903 - val_acc: 0.9560\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9991\n",
      "Epoch 00079: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0047 - acc: 0.9991 - val_loss: 0.2395 - val_acc: 0.9474\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9969\n",
      "Epoch 00080: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0106 - acc: 0.9968 - val_loss: 0.3915 - val_acc: 0.9033\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9923\n",
      "Epoch 00081: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0255 - acc: 0.9923 - val_loss: 0.2054 - val_acc: 0.9495\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9983\n",
      "Epoch 00082: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0070 - acc: 0.9983 - val_loss: 0.2278 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9994\n",
      "Epoch 00083: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0037 - acc: 0.9994 - val_loss: 0.1967 - val_acc: 0.9567\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 00084: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0032 - acc: 0.9994 - val_loss: 0.2032 - val_acc: 0.9574\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9937\n",
      "Epoch 00085: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0206 - acc: 0.9938 - val_loss: 0.2579 - val_acc: 0.9383\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9987\n",
      "Epoch 00086: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0060 - acc: 0.9987 - val_loss: 0.1885 - val_acc: 0.9581\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9993\n",
      "Epoch 00087: val_loss did not improve from 0.18629\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0040 - acc: 0.9993 - val_loss: 0.2320 - val_acc: 0.9460\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9924\n",
      "Epoch 00088: val_loss improved from 0.18629 to 0.18273, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/088-0.1827.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0241 - acc: 0.9924 - val_loss: 0.1827 - val_acc: 0.9576\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9995\n",
      "Epoch 00089: val_loss improved from 0.18273 to 0.17501, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv_checkpoint/089-0.1750.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0030 - acc: 0.9995 - val_loss: 0.1750 - val_acc: 0.9602\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00090: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1808 - val_acc: 0.9602\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9969\n",
      "Epoch 00091: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0116 - acc: 0.9969 - val_loss: 0.2533 - val_acc: 0.9439\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9970\n",
      "Epoch 00092: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0108 - acc: 0.9970 - val_loss: 0.2390 - val_acc: 0.9464\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9955\n",
      "Epoch 00093: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0158 - acc: 0.9955 - val_loss: 0.2026 - val_acc: 0.9567\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00094: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0036 - acc: 0.9993 - val_loss: 0.2072 - val_acc: 0.9529\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9995\n",
      "Epoch 00095: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0028 - acc: 0.9995 - val_loss: 0.2338 - val_acc: 0.9462\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9958\n",
      "Epoch 00096: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0135 - acc: 0.9957 - val_loss: 0.2705 - val_acc: 0.9369\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9946\n",
      "Epoch 00097: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0193 - acc: 0.9946 - val_loss: 0.1781 - val_acc: 0.9602\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9990\n",
      "Epoch 00098: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0042 - acc: 0.9990 - val_loss: 0.1815 - val_acc: 0.9609\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9997\n",
      "Epoch 00099: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1786 - val_acc: 0.9602\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9957\n",
      "Epoch 00100: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0156 - acc: 0.9957 - val_loss: 0.1975 - val_acc: 0.9585\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00101: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0038 - acc: 0.9992 - val_loss: 0.2011 - val_acc: 0.9529\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00102: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1860 - val_acc: 0.9606\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9954\n",
      "Epoch 00103: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0169 - acc: 0.9954 - val_loss: 0.2036 - val_acc: 0.9555\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9985\n",
      "Epoch 00104: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0060 - acc: 0.9985 - val_loss: 0.2042 - val_acc: 0.9550\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9990\n",
      "Epoch 00105: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0043 - acc: 0.9990 - val_loss: 0.2339 - val_acc: 0.9462\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9980\n",
      "Epoch 00106: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0073 - acc: 0.9980 - val_loss: 0.3725 - val_acc: 0.9159\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9961\n",
      "Epoch 00107: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0120 - acc: 0.9961 - val_loss: 0.2053 - val_acc: 0.9536\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9940\n",
      "Epoch 00108: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0183 - acc: 0.9940 - val_loss: 0.1760 - val_acc: 0.9618\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 00109: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1839 - val_acc: 0.9616\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9996\n",
      "Epoch 00110: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0026 - acc: 0.9996 - val_loss: 0.2007 - val_acc: 0.9567\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00111: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.2004 - val_acc: 0.9574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9980\n",
      "Epoch 00112: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0076 - acc: 0.9980 - val_loss: 0.3354 - val_acc: 0.9241\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9936\n",
      "Epoch 00113: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0207 - acc: 0.9936 - val_loss: 0.2336 - val_acc: 0.9476\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9963\n",
      "Epoch 00114: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0141 - acc: 0.9963 - val_loss: 0.1948 - val_acc: 0.9599\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00115: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0034 - acc: 0.9992 - val_loss: 0.2080 - val_acc: 0.9588\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9981\n",
      "Epoch 00116: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0068 - acc: 0.9981 - val_loss: 0.2057 - val_acc: 0.9543\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 00117: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.2010 - val_acc: 0.9569\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9990\n",
      "Epoch 00118: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0042 - acc: 0.9990 - val_loss: 0.2154 - val_acc: 0.9539\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9964\n",
      "Epoch 00119: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0122 - acc: 0.9964 - val_loss: 0.3005 - val_acc: 0.9345\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9951\n",
      "Epoch 00120: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0172 - acc: 0.9951 - val_loss: 0.1835 - val_acc: 0.9576\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 00121: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1803 - val_acc: 0.9604\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 00122: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0023 - acc: 0.9996 - val_loss: 0.2114 - val_acc: 0.9560\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9947\n",
      "Epoch 00123: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0165 - acc: 0.9946 - val_loss: 0.2276 - val_acc: 0.9492\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9969\n",
      "Epoch 00124: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0111 - acc: 0.9969 - val_loss: 0.1909 - val_acc: 0.9606\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9997\n",
      "Epoch 00125: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1877 - val_acc: 0.9606\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 00126: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.2049 - val_acc: 0.9550\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9951\n",
      "Epoch 00127: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0169 - acc: 0.9951 - val_loss: 0.2229 - val_acc: 0.9525\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 00128: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0024 - acc: 0.9996 - val_loss: 0.1982 - val_acc: 0.9583\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 00129: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0022 - acc: 0.9997 - val_loss: 0.1904 - val_acc: 0.9578\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9965\n",
      "Epoch 00130: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0114 - acc: 0.9965 - val_loss: 0.1977 - val_acc: 0.9560\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9980\n",
      "Epoch 00131: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0080 - acc: 0.9980 - val_loss: 0.2053 - val_acc: 0.9567\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9962\n",
      "Epoch 00132: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0128 - acc: 0.9963 - val_loss: 0.1949 - val_acc: 0.9583\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 00133: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0027 - acc: 0.9995 - val_loss: 0.2000 - val_acc: 0.9550\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 00134: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0024 - acc: 0.9996 - val_loss: 0.1936 - val_acc: 0.9597\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 00135: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1977 - val_acc: 0.9599\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9961\n",
      "Epoch 00136: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0132 - acc: 0.9961 - val_loss: 0.4245 - val_acc: 0.9038\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9954\n",
      "Epoch 00137: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0162 - acc: 0.9954 - val_loss: 0.1994 - val_acc: 0.9590\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00138: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.2014 - val_acc: 0.9606\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
      "Epoch 00139: val_loss did not improve from 0.17501\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.2043 - val_acc: 0.9574\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2362 - acc: 0.9464\n",
      "Loss: 0.23617102964350864 Accuracy: 0.94641745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_tanh_BN'\n",
    "\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 1024000)           4096000   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 20,493,136\n",
      "Trainable params: 18,444,880\n",
      "Non-trainable params: 2,048,256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 11.2598 - acc: 0.1047\n",
      "Loss: 11.25984339788324 Accuracy: 0.104672894\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 341312)            1365248   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 6,864,592\n",
      "Trainable params: 6,181,456\n",
      "Non-trainable params: 683,136\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 8.1143 - acc: 0.2692\n",
      "Loss: 8.11433346610698 Accuracy: 0.26915887\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 113728)            454912    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 2,338,128\n",
      "Trainable params: 2,109,904\n",
      "Non-trainable params: 228,224\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 4.3486 - acc: 0.3192\n",
      "Loss: 4.348554058114564 Accuracy: 0.3192108\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 37888)             151552    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 846,544\n",
      "Trainable params: 769,744\n",
      "Non-trainable params: 76,800\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 2.4855 - acc: 0.5333\n",
      "Loss: 2.4855427053734767 Accuracy: 0.53333336\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 25216)             100864    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 668,112\n",
      "Trainable params: 616,144\n",
      "Non-trainable params: 51,968\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.8931 - acc: 0.5034\n",
      "Loss: 1.893113769474802 Accuracy: 0.5034268\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 8320)              33280     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 429,776\n",
      "Trainable params: 411,088\n",
      "Non-trainable params: 18,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.1385 - acc: 0.7570\n",
      "Loss: 1.1384778298569123 Accuracy: 0.7570093\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 2688)              10752     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 416,720\n",
      "Trainable params: 408,784\n",
      "Non-trainable params: 7,936\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.5005 - acc: 0.8839\n",
      "Loss: 0.5005481078741087 Accuracy: 0.88390446\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 480,464\n",
      "Trainable params: 475,600\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2650 - acc: 0.9406\n",
      "Loss: 0.26498853739153744 Accuracy: 0.9406023\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_179 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_180 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_181 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_182 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_183 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_184 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_185 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_186 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_187 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_188 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_189 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_190 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_191 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_192 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_193 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_194 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_195 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_196 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_197 ( (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 770,256\n",
      "Trainable params: 765,136\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2362 - acc: 0.9464\n",
      "Loss: 0.23617102964350864 Accuracy: 0.94641745\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_tanh_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 1024000)           4096000   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 20,493,136\n",
      "Trainable params: 18,444,880\n",
      "Non-trainable params: 2,048,256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 12.0779 - acc: 0.2301\n",
      "Loss: 12.077900381078354 Accuracy: 0.23011422\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 341312)            1365248   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 6,864,592\n",
      "Trainable params: 6,181,456\n",
      "Non-trainable params: 683,136\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 10.3158 - acc: 0.2876\n",
      "Loss: 10.315817500200598 Accuracy: 0.28764278\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 113728)            454912    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 2,338,128\n",
      "Trainable params: 2,109,904\n",
      "Non-trainable params: 228,224\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 10.2560 - acc: 0.2619\n",
      "Loss: 10.25601249944384 Accuracy: 0.26188993\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 37888)             151552    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 846,544\n",
      "Trainable params: 769,744\n",
      "Non-trainable params: 76,800\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 3.4309 - acc: 0.5279\n",
      "Loss: 3.430877184050848 Accuracy: 0.52793354\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 25216)             100864    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 668,112\n",
      "Trainable params: 616,144\n",
      "Non-trainable params: 51,968\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 2.2186 - acc: 0.6106\n",
      "Loss: 2.218603184611874 Accuracy: 0.6105919\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 8320)              33280     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 429,776\n",
      "Trainable params: 411,088\n",
      "Non-trainable params: 18,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 1.3336 - acc: 0.7485\n",
      "Loss: 1.3336144921935607 Accuracy: 0.74849427\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 2688)              10752     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 416,720\n",
      "Trainable params: 408,784\n",
      "Non-trainable params: 7,936\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.5790 - acc: 0.8793\n",
      "Loss: 0.578952280581307 Accuracy: 0.8793354\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 480,464\n",
      "Trainable params: 475,600\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.3320 - acc: 0.9308\n",
      "Loss: 0.33197585187527434 Accuracy: 0.93084115\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_179 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_180 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_181 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_182 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_183 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_184 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_185 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_186 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_187 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_188 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_189 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_190 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_191 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_192 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_193 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_194 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_195 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_196 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_197 ( (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 770,256\n",
      "Trainable params: 765,136\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.2795 - acc: 0.9448\n",
      "Loss: 0.2794641652818564 Accuracy: 0.944756\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
