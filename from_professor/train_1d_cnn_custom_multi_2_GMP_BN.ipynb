{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 64)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 128)          512         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2064        batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,176\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 128)          512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2064        batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 65,616\n",
      "Trainable params: 64,848\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 64)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 192)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 192)          768         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           3088        batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 108,496\n",
      "Trainable params: 107,344\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 256)          1024        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           4112        batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 192,336\n",
      "Trainable params: 190,800\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256)          0           global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 256)          1024        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           4112        batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 274,896\n",
      "Trainable params: 273,104\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 256)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 256)          1024        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           4112        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 357,456\n",
      "Trainable params: 355,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0975 - acc: 0.3231\n",
      "Epoch 00001: val_loss improved from inf to 2.04945, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/001-2.0495.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 2.0974 - acc: 0.3231 - val_loss: 2.0495 - val_acc: 0.3305\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5718 - acc: 0.4865\n",
      "Epoch 00002: val_loss improved from 2.04945 to 1.46002, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/002-1.4600.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.5718 - acc: 0.4865 - val_loss: 1.4600 - val_acc: 0.5260\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3338 - acc: 0.5751\n",
      "Epoch 00003: val_loss improved from 1.46002 to 1.23701, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/003-1.2370.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.3337 - acc: 0.5751 - val_loss: 1.2370 - val_acc: 0.6070\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1653 - acc: 0.6350\n",
      "Epoch 00004: val_loss improved from 1.23701 to 1.12628, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/004-1.1263.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1654 - acc: 0.6349 - val_loss: 1.1263 - val_acc: 0.6504\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0435 - acc: 0.6779\n",
      "Epoch 00005: val_loss improved from 1.12628 to 0.99553, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/005-0.9955.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0435 - acc: 0.6780 - val_loss: 0.9955 - val_acc: 0.6928\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9541 - acc: 0.7107\n",
      "Epoch 00006: val_loss improved from 0.99553 to 0.92042, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/006-0.9204.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9543 - acc: 0.7106 - val_loss: 0.9204 - val_acc: 0.7205\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8792 - acc: 0.7357\n",
      "Epoch 00007: val_loss improved from 0.92042 to 0.87590, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/007-0.8759.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8791 - acc: 0.7357 - val_loss: 0.8759 - val_acc: 0.7331\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8168 - acc: 0.7542\n",
      "Epoch 00008: val_loss improved from 0.87590 to 0.81944, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/008-0.8194.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8167 - acc: 0.7542 - val_loss: 0.8194 - val_acc: 0.7424\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7673 - acc: 0.7691\n",
      "Epoch 00009: val_loss improved from 0.81944 to 0.81005, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/009-0.8101.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7672 - acc: 0.7691 - val_loss: 0.8101 - val_acc: 0.7447\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7250 - acc: 0.7801\n",
      "Epoch 00010: val_loss improved from 0.81005 to 0.69740, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/010-0.6974.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7249 - acc: 0.7801 - val_loss: 0.6974 - val_acc: 0.7841\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6855 - acc: 0.7932\n",
      "Epoch 00011: val_loss did not improve from 0.69740\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6856 - acc: 0.7932 - val_loss: 0.7197 - val_acc: 0.7801\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6563 - acc: 0.8035\n",
      "Epoch 00012: val_loss did not improve from 0.69740\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6563 - acc: 0.8035 - val_loss: 0.6976 - val_acc: 0.7808\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6290 - acc: 0.8101\n",
      "Epoch 00013: val_loss did not improve from 0.69740\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6290 - acc: 0.8102 - val_loss: 0.7419 - val_acc: 0.7573\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.8183\n",
      "Epoch 00014: val_loss improved from 0.69740 to 0.65179, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/014-0.6518.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6027 - acc: 0.8183 - val_loss: 0.6518 - val_acc: 0.8008\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5816 - acc: 0.8252\n",
      "Epoch 00015: val_loss improved from 0.65179 to 0.64431, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/015-0.6443.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5817 - acc: 0.8251 - val_loss: 0.6443 - val_acc: 0.7959\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5643 - acc: 0.8288\n",
      "Epoch 00016: val_loss improved from 0.64431 to 0.64103, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/016-0.6410.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5643 - acc: 0.8287 - val_loss: 0.6410 - val_acc: 0.8116\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.8364\n",
      "Epoch 00017: val_loss improved from 0.64103 to 0.57021, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/017-0.5702.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5456 - acc: 0.8364 - val_loss: 0.5702 - val_acc: 0.8314\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.8389\n",
      "Epoch 00018: val_loss did not improve from 0.57021\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5306 - acc: 0.8389 - val_loss: 0.6302 - val_acc: 0.8088\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.8439\n",
      "Epoch 00019: val_loss improved from 0.57021 to 0.56080, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/019-0.5608.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5172 - acc: 0.8440 - val_loss: 0.5608 - val_acc: 0.8290\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8476\n",
      "Epoch 00020: val_loss did not improve from 0.56080\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5031 - acc: 0.8475 - val_loss: 0.5686 - val_acc: 0.8206\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4939 - acc: 0.8504\n",
      "Epoch 00021: val_loss did not improve from 0.56080\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4939 - acc: 0.8504 - val_loss: 0.5741 - val_acc: 0.8155\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8560\n",
      "Epoch 00022: val_loss improved from 0.56080 to 0.56080, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/022-0.5608.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4767 - acc: 0.8559 - val_loss: 0.5608 - val_acc: 0.8211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4687 - acc: 0.8579\n",
      "Epoch 00023: val_loss did not improve from 0.56080\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4687 - acc: 0.8579 - val_loss: 0.5739 - val_acc: 0.8279\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.8618\n",
      "Epoch 00024: val_loss did not improve from 0.56080\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4544 - acc: 0.8618 - val_loss: 0.5612 - val_acc: 0.8300\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4443 - acc: 0.8646\n",
      "Epoch 00025: val_loss improved from 0.56080 to 0.53880, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/025-0.5388.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4446 - acc: 0.8646 - val_loss: 0.5388 - val_acc: 0.8409\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.8649\n",
      "Epoch 00026: val_loss improved from 0.53880 to 0.53423, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/026-0.5342.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4409 - acc: 0.8649 - val_loss: 0.5342 - val_acc: 0.8369\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.8709\n",
      "Epoch 00027: val_loss did not improve from 0.53423\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4275 - acc: 0.8709 - val_loss: 0.5833 - val_acc: 0.8213\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8756\n",
      "Epoch 00028: val_loss did not improve from 0.53423\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4173 - acc: 0.8755 - val_loss: 0.5381 - val_acc: 0.8355\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4136 - acc: 0.8737\n",
      "Epoch 00029: val_loss did not improve from 0.53423\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4137 - acc: 0.8737 - val_loss: 0.5561 - val_acc: 0.8274\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8777\n",
      "Epoch 00030: val_loss did not improve from 0.53423\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4064 - acc: 0.8777 - val_loss: 0.5604 - val_acc: 0.8307\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3989 - acc: 0.8784\n",
      "Epoch 00031: val_loss improved from 0.53423 to 0.50967, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/031-0.5097.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3988 - acc: 0.8784 - val_loss: 0.5097 - val_acc: 0.8360\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8819\n",
      "Epoch 00032: val_loss did not improve from 0.50967\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3895 - acc: 0.8819 - val_loss: 0.5477 - val_acc: 0.8337\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3837 - acc: 0.8819\n",
      "Epoch 00033: val_loss did not improve from 0.50967\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3837 - acc: 0.8819 - val_loss: 0.5606 - val_acc: 0.8239\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8838\n",
      "Epoch 00034: val_loss did not improve from 0.50967\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3767 - acc: 0.8838 - val_loss: 0.5123 - val_acc: 0.8437\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8873\n",
      "Epoch 00035: val_loss did not improve from 0.50967\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3707 - acc: 0.8873 - val_loss: 0.5373 - val_acc: 0.8397\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3638 - acc: 0.8905\n",
      "Epoch 00036: val_loss did not improve from 0.50967\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3637 - acc: 0.8905 - val_loss: 0.5312 - val_acc: 0.8365\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8911\n",
      "Epoch 00037: val_loss did not improve from 0.50967\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3572 - acc: 0.8911 - val_loss: 0.5500 - val_acc: 0.8307\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.8930\n",
      "Epoch 00038: val_loss did not improve from 0.50967\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3551 - acc: 0.8930 - val_loss: 0.5212 - val_acc: 0.8386\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8939\n",
      "Epoch 00039: val_loss did not improve from 0.50967\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3471 - acc: 0.8938 - val_loss: 0.5287 - val_acc: 0.8393\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8960\n",
      "Epoch 00040: val_loss improved from 0.50967 to 0.48939, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_3_conv_checkpoint/040-0.4894.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3404 - acc: 0.8960 - val_loss: 0.4894 - val_acc: 0.8491\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8984\n",
      "Epoch 00041: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3363 - acc: 0.8984 - val_loss: 0.5214 - val_acc: 0.8423\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8985\n",
      "Epoch 00042: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3324 - acc: 0.8984 - val_loss: 0.6374 - val_acc: 0.8078\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.8981\n",
      "Epoch 00043: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3298 - acc: 0.8981 - val_loss: 0.5035 - val_acc: 0.8484\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.9014\n",
      "Epoch 00044: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3243 - acc: 0.9014 - val_loss: 0.5494 - val_acc: 0.8372\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9040\n",
      "Epoch 00045: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3179 - acc: 0.9040 - val_loss: 0.5726 - val_acc: 0.8281\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.9051\n",
      "Epoch 00046: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3149 - acc: 0.9051 - val_loss: 0.5052 - val_acc: 0.8453\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9074\n",
      "Epoch 00047: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3105 - acc: 0.9074 - val_loss: 0.6741 - val_acc: 0.8039\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3051 - acc: 0.9078\n",
      "Epoch 00048: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3051 - acc: 0.9078 - val_loss: 0.5362 - val_acc: 0.8407\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9068\n",
      "Epoch 00049: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3014 - acc: 0.9068 - val_loss: 0.5428 - val_acc: 0.8367\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9094\n",
      "Epoch 00050: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2983 - acc: 0.9094 - val_loss: 0.5002 - val_acc: 0.8521\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9115\n",
      "Epoch 00051: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2944 - acc: 0.9116 - val_loss: 0.5619 - val_acc: 0.8325\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9130\n",
      "Epoch 00052: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2879 - acc: 0.9130 - val_loss: 0.5597 - val_acc: 0.8355\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9142\n",
      "Epoch 00053: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2847 - acc: 0.9142 - val_loss: 0.5575 - val_acc: 0.8311\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9137\n",
      "Epoch 00054: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2830 - acc: 0.9137 - val_loss: 0.5606 - val_acc: 0.8300\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9139\n",
      "Epoch 00055: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2821 - acc: 0.9139 - val_loss: 0.5027 - val_acc: 0.8444\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9158\n",
      "Epoch 00056: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2757 - acc: 0.9158 - val_loss: 0.5395 - val_acc: 0.8400\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9179\n",
      "Epoch 00057: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2704 - acc: 0.9179 - val_loss: 0.5604 - val_acc: 0.8321\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.9192\n",
      "Epoch 00058: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2667 - acc: 0.9192 - val_loss: 0.5375 - val_acc: 0.8395\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2648 - acc: 0.9209\n",
      "Epoch 00059: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2649 - acc: 0.9208 - val_loss: 0.5593 - val_acc: 0.8362\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9202\n",
      "Epoch 00060: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2618 - acc: 0.9202 - val_loss: 0.5613 - val_acc: 0.8397\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.9217\n",
      "Epoch 00061: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2581 - acc: 0.9217 - val_loss: 0.5518 - val_acc: 0.8409\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9223\n",
      "Epoch 00062: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2554 - acc: 0.9222 - val_loss: 0.5140 - val_acc: 0.8470\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.9237\n",
      "Epoch 00063: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2533 - acc: 0.9237 - val_loss: 0.5439 - val_acc: 0.8453\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9243\n",
      "Epoch 00064: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2502 - acc: 0.9243 - val_loss: 0.6230 - val_acc: 0.8251\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9255\n",
      "Epoch 00065: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2441 - acc: 0.9254 - val_loss: 0.6032 - val_acc: 0.8237\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9242\n",
      "Epoch 00066: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2471 - acc: 0.9242 - val_loss: 0.5558 - val_acc: 0.8330\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9266\n",
      "Epoch 00067: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2403 - acc: 0.9265 - val_loss: 0.5283 - val_acc: 0.8437\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9286\n",
      "Epoch 00068: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2349 - acc: 0.9286 - val_loss: 0.5763 - val_acc: 0.8344\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9279\n",
      "Epoch 00069: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2332 - acc: 0.9279 - val_loss: 0.5428 - val_acc: 0.8314\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9304\n",
      "Epoch 00070: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2323 - acc: 0.9303 - val_loss: 0.5263 - val_acc: 0.8456\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9264\n",
      "Epoch 00071: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2379 - acc: 0.9263 - val_loss: 0.5571 - val_acc: 0.8388\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9307\n",
      "Epoch 00072: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2247 - acc: 0.9307 - val_loss: 0.5633 - val_acc: 0.8376\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9332\n",
      "Epoch 00073: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2221 - acc: 0.9332 - val_loss: 0.5567 - val_acc: 0.8435\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9326\n",
      "Epoch 00074: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2210 - acc: 0.9326 - val_loss: 0.5548 - val_acc: 0.8444\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9338\n",
      "Epoch 00075: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2165 - acc: 0.9338 - val_loss: 0.5509 - val_acc: 0.8390\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9358\n",
      "Epoch 00076: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2125 - acc: 0.9358 - val_loss: 0.5707 - val_acc: 0.8360\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9360\n",
      "Epoch 00077: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2119 - acc: 0.9360 - val_loss: 0.5762 - val_acc: 0.8293\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9375\n",
      "Epoch 00078: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2059 - acc: 0.9375 - val_loss: 0.5408 - val_acc: 0.8442\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9363\n",
      "Epoch 00079: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2068 - acc: 0.9363 - val_loss: 0.6172 - val_acc: 0.8246\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9367\n",
      "Epoch 00080: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2088 - acc: 0.9366 - val_loss: 0.5519 - val_acc: 0.8432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9368\n",
      "Epoch 00081: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2052 - acc: 0.9367 - val_loss: 0.5599 - val_acc: 0.8358\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9395\n",
      "Epoch 00082: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2001 - acc: 0.9395 - val_loss: 0.5885 - val_acc: 0.8309\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9402\n",
      "Epoch 00083: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1967 - acc: 0.9401 - val_loss: 0.5868 - val_acc: 0.8339\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9390\n",
      "Epoch 00084: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1993 - acc: 0.9391 - val_loss: 0.5505 - val_acc: 0.8479\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9409\n",
      "Epoch 00085: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1942 - acc: 0.9409 - val_loss: 0.5477 - val_acc: 0.8470\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9401\n",
      "Epoch 00086: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1949 - acc: 0.9401 - val_loss: 0.5612 - val_acc: 0.8360\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9408\n",
      "Epoch 00087: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1950 - acc: 0.9408 - val_loss: 0.5480 - val_acc: 0.8465\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9427\n",
      "Epoch 00088: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1874 - acc: 0.9427 - val_loss: 0.5747 - val_acc: 0.8353\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9423\n",
      "Epoch 00089: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1895 - acc: 0.9423 - val_loss: 0.6191 - val_acc: 0.8246\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9442\n",
      "Epoch 00090: val_loss did not improve from 0.48939\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1825 - acc: 0.9442 - val_loss: 0.5521 - val_acc: 0.8404\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 553us/sample - loss: 0.5636 - acc: 0.8258\n",
      "Loss: 0.5635875388221702 Accuracy: 0.82575285\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9947 - acc: 0.3668\n",
      "Epoch 00001: val_loss improved from inf to 1.86121, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/001-1.8612.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.9947 - acc: 0.3668 - val_loss: 1.8612 - val_acc: 0.4263\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3548 - acc: 0.5738\n",
      "Epoch 00002: val_loss improved from 1.86121 to 1.21262, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/002-1.2126.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.3548 - acc: 0.5738 - val_loss: 1.2126 - val_acc: 0.6124\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0797 - acc: 0.6683\n",
      "Epoch 00003: val_loss improved from 1.21262 to 0.99054, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/003-0.9905.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0797 - acc: 0.6683 - val_loss: 0.9905 - val_acc: 0.6918\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9121 - acc: 0.7260\n",
      "Epoch 00004: val_loss improved from 0.99054 to 0.85294, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/004-0.8529.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9120 - acc: 0.7260 - val_loss: 0.8529 - val_acc: 0.7428\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7947 - acc: 0.7640\n",
      "Epoch 00005: val_loss improved from 0.85294 to 0.76162, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/005-0.7616.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7947 - acc: 0.7641 - val_loss: 0.7616 - val_acc: 0.7685\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7050 - acc: 0.7934\n",
      "Epoch 00006: val_loss improved from 0.76162 to 0.68083, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/006-0.6808.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7050 - acc: 0.7934 - val_loss: 0.6808 - val_acc: 0.7957\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6326 - acc: 0.8142\n",
      "Epoch 00007: val_loss improved from 0.68083 to 0.60842, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/007-0.6084.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6326 - acc: 0.8142 - val_loss: 0.6084 - val_acc: 0.8139\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.8330\n",
      "Epoch 00008: val_loss improved from 0.60842 to 0.58829, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/008-0.5883.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5748 - acc: 0.8329 - val_loss: 0.5883 - val_acc: 0.8185\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.8466\n",
      "Epoch 00009: val_loss improved from 0.58829 to 0.56496, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/009-0.5650.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5311 - acc: 0.8466 - val_loss: 0.5650 - val_acc: 0.8309\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4922 - acc: 0.8570\n",
      "Epoch 00010: val_loss did not improve from 0.56496\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4921 - acc: 0.8570 - val_loss: 0.5703 - val_acc: 0.8218\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4643 - acc: 0.8643\n",
      "Epoch 00011: val_loss improved from 0.56496 to 0.50575, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/011-0.5058.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4643 - acc: 0.8643 - val_loss: 0.5058 - val_acc: 0.8472\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8751\n",
      "Epoch 00012: val_loss improved from 0.50575 to 0.46690, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/012-0.4669.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4317 - acc: 0.8750 - val_loss: 0.4669 - val_acc: 0.8612\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8811\n",
      "Epoch 00013: val_loss did not improve from 0.46690\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4100 - acc: 0.8812 - val_loss: 0.4699 - val_acc: 0.8577\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.8872\n",
      "Epoch 00014: val_loss improved from 0.46690 to 0.44056, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/014-0.4406.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3881 - acc: 0.8872 - val_loss: 0.4406 - val_acc: 0.8672\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8943\n",
      "Epoch 00015: val_loss improved from 0.44056 to 0.42317, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/015-0.4232.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3681 - acc: 0.8943 - val_loss: 0.4232 - val_acc: 0.8677\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8994\n",
      "Epoch 00016: val_loss did not improve from 0.42317\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3491 - acc: 0.8993 - val_loss: 0.4556 - val_acc: 0.8626\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.9017\n",
      "Epoch 00017: val_loss did not improve from 0.42317\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3390 - acc: 0.9017 - val_loss: 0.4858 - val_acc: 0.8481\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.9086\n",
      "Epoch 00018: val_loss did not improve from 0.42317\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3221 - acc: 0.9085 - val_loss: 0.4251 - val_acc: 0.8717\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.9115\n",
      "Epoch 00019: val_loss did not improve from 0.42317\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3097 - acc: 0.9115 - val_loss: 0.4267 - val_acc: 0.8684\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9166\n",
      "Epoch 00020: val_loss improved from 0.42317 to 0.40820, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/020-0.4082.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2920 - acc: 0.9166 - val_loss: 0.4082 - val_acc: 0.8761\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9190\n",
      "Epoch 00021: val_loss did not improve from 0.40820\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2833 - acc: 0.9190 - val_loss: 0.4466 - val_acc: 0.8595\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9241\n",
      "Epoch 00022: val_loss did not improve from 0.40820\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2687 - acc: 0.9241 - val_loss: 0.4300 - val_acc: 0.8705\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9246\n",
      "Epoch 00023: val_loss improved from 0.40820 to 0.40314, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/023-0.4031.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2621 - acc: 0.9246 - val_loss: 0.4031 - val_acc: 0.8761\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9312\n",
      "Epoch 00024: val_loss did not improve from 0.40314\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2472 - acc: 0.9312 - val_loss: 0.4416 - val_acc: 0.8626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9327\n",
      "Epoch 00025: val_loss did not improve from 0.40314\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2389 - acc: 0.9326 - val_loss: 0.4166 - val_acc: 0.8672\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9366\n",
      "Epoch 00026: val_loss did not improve from 0.40314\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2289 - acc: 0.9366 - val_loss: 0.4291 - val_acc: 0.8651\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9374\n",
      "Epoch 00027: val_loss improved from 0.40314 to 0.38326, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/027-0.3833.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2236 - acc: 0.9374 - val_loss: 0.3833 - val_acc: 0.8838\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9406\n",
      "Epoch 00028: val_loss improved from 0.38326 to 0.37986, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/028-0.3799.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2114 - acc: 0.9405 - val_loss: 0.3799 - val_acc: 0.8838\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9417\n",
      "Epoch 00029: val_loss did not improve from 0.37986\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2089 - acc: 0.9417 - val_loss: 0.4031 - val_acc: 0.8807\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9444\n",
      "Epoch 00030: val_loss did not improve from 0.37986\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1967 - acc: 0.9444 - val_loss: 0.3982 - val_acc: 0.8784\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9476\n",
      "Epoch 00031: val_loss did not improve from 0.37986\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1908 - acc: 0.9475 - val_loss: 0.4028 - val_acc: 0.8765\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9477\n",
      "Epoch 00032: val_loss did not improve from 0.37986\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1872 - acc: 0.9477 - val_loss: 0.3836 - val_acc: 0.8847\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9530\n",
      "Epoch 00033: val_loss improved from 0.37986 to 0.37775, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/033-0.3777.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1744 - acc: 0.9530 - val_loss: 0.3777 - val_acc: 0.8866\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9514\n",
      "Epoch 00034: val_loss did not improve from 0.37775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1732 - acc: 0.9513 - val_loss: 0.4318 - val_acc: 0.8728\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9527\n",
      "Epoch 00035: val_loss did not improve from 0.37775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1705 - acc: 0.9527 - val_loss: 0.4890 - val_acc: 0.8526\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9576\n",
      "Epoch 00036: val_loss did not improve from 0.37775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1584 - acc: 0.9576 - val_loss: 0.3963 - val_acc: 0.8824\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9584\n",
      "Epoch 00037: val_loss did not improve from 0.37775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1539 - acc: 0.9584 - val_loss: 0.4190 - val_acc: 0.8682\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9620\n",
      "Epoch 00038: val_loss did not improve from 0.37775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1454 - acc: 0.9620 - val_loss: 0.3972 - val_acc: 0.8905\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9615\n",
      "Epoch 00039: val_loss did not improve from 0.37775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1426 - acc: 0.9615 - val_loss: 0.3947 - val_acc: 0.8814\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9641\n",
      "Epoch 00040: val_loss did not improve from 0.37775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1344 - acc: 0.9641 - val_loss: 0.4147 - val_acc: 0.8756\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9640\n",
      "Epoch 00041: val_loss did not improve from 0.37775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1341 - acc: 0.9640 - val_loss: 0.4354 - val_acc: 0.8693\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9651\n",
      "Epoch 00042: val_loss did not improve from 0.37775\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1303 - acc: 0.9651 - val_loss: 0.4623 - val_acc: 0.8640\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9684\n",
      "Epoch 00043: val_loss improved from 0.37775 to 0.37413, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_4_conv_checkpoint/043-0.3741.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1227 - acc: 0.9684 - val_loss: 0.3741 - val_acc: 0.8901\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9684\n",
      "Epoch 00044: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1205 - acc: 0.9684 - val_loss: 0.4013 - val_acc: 0.8828\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9712\n",
      "Epoch 00045: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1132 - acc: 0.9711 - val_loss: 0.4219 - val_acc: 0.8749\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9699\n",
      "Epoch 00046: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1131 - acc: 0.9699 - val_loss: 0.4135 - val_acc: 0.8798\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9739\n",
      "Epoch 00047: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1039 - acc: 0.9738 - val_loss: 0.4037 - val_acc: 0.8833\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9712\n",
      "Epoch 00048: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1132 - acc: 0.9712 - val_loss: 0.4451 - val_acc: 0.8721\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9752\n",
      "Epoch 00049: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0999 - acc: 0.9751 - val_loss: 0.4697 - val_acc: 0.8670\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9714\n",
      "Epoch 00050: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1086 - acc: 0.9713 - val_loss: 0.3952 - val_acc: 0.8873\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9765\n",
      "Epoch 00051: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0951 - acc: 0.9765 - val_loss: 0.4006 - val_acc: 0.8859\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9763\n",
      "Epoch 00052: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0959 - acc: 0.9763 - val_loss: 0.4052 - val_acc: 0.8884\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9779\n",
      "Epoch 00053: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0891 - acc: 0.9779 - val_loss: 0.4262 - val_acc: 0.8826\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9772\n",
      "Epoch 00054: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0913 - acc: 0.9772 - val_loss: 0.3992 - val_acc: 0.8875\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9796\n",
      "Epoch 00055: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0847 - acc: 0.9796 - val_loss: 0.4183 - val_acc: 0.8826\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9816\n",
      "Epoch 00056: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0776 - acc: 0.9816 - val_loss: 0.4309 - val_acc: 0.8782\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9785\n",
      "Epoch 00057: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0863 - acc: 0.9784 - val_loss: 0.4110 - val_acc: 0.8840\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9778\n",
      "Epoch 00058: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0867 - acc: 0.9778 - val_loss: 0.4275 - val_acc: 0.8828\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9835\n",
      "Epoch 00059: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0718 - acc: 0.9835 - val_loss: 0.4378 - val_acc: 0.8775\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9817\n",
      "Epoch 00060: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0745 - acc: 0.9816 - val_loss: 0.4172 - val_acc: 0.8896\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9826\n",
      "Epoch 00061: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0730 - acc: 0.9826 - val_loss: 0.4481 - val_acc: 0.8714\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9808\n",
      "Epoch 00062: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0738 - acc: 0.9808 - val_loss: 0.4253 - val_acc: 0.8819\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9833\n",
      "Epoch 00063: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0702 - acc: 0.9833 - val_loss: 0.4284 - val_acc: 0.8859\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9869\n",
      "Epoch 00064: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0609 - acc: 0.9869 - val_loss: 0.4084 - val_acc: 0.8898\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9825\n",
      "Epoch 00065: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0692 - acc: 0.9825 - val_loss: 0.4319 - val_acc: 0.8889\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9834\n",
      "Epoch 00066: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0675 - acc: 0.9834 - val_loss: 0.4307 - val_acc: 0.8870\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9833\n",
      "Epoch 00067: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0688 - acc: 0.9833 - val_loss: 0.4535 - val_acc: 0.8772\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9888\n",
      "Epoch 00068: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0556 - acc: 0.9888 - val_loss: 0.4496 - val_acc: 0.8817\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9856\n",
      "Epoch 00069: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0617 - acc: 0.9856 - val_loss: 0.4590 - val_acc: 0.8833\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9891\n",
      "Epoch 00070: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0520 - acc: 0.9891 - val_loss: 0.4408 - val_acc: 0.8863\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9875\n",
      "Epoch 00071: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0545 - acc: 0.9875 - val_loss: 0.4231 - val_acc: 0.8901\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9885\n",
      "Epoch 00072: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0534 - acc: 0.9885 - val_loss: 0.4588 - val_acc: 0.8803\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9892\n",
      "Epoch 00073: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0513 - acc: 0.9892 - val_loss: 0.4589 - val_acc: 0.8779\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9884\n",
      "Epoch 00074: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0511 - acc: 0.9883 - val_loss: 0.5053 - val_acc: 0.8691\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9870\n",
      "Epoch 00075: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0541 - acc: 0.9870 - val_loss: 0.4738 - val_acc: 0.8807\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9876\n",
      "Epoch 00076: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0518 - acc: 0.9876 - val_loss: 0.4353 - val_acc: 0.8859\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9912\n",
      "Epoch 00077: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0428 - acc: 0.9913 - val_loss: 0.4671 - val_acc: 0.8742\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9908\n",
      "Epoch 00078: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0423 - acc: 0.9908 - val_loss: 0.5157 - val_acc: 0.8649\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9910\n",
      "Epoch 00079: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0424 - acc: 0.9910 - val_loss: 0.4478 - val_acc: 0.8845\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9909\n",
      "Epoch 00080: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0435 - acc: 0.9909 - val_loss: 0.4458 - val_acc: 0.8849\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9906\n",
      "Epoch 00081: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0436 - acc: 0.9905 - val_loss: 0.5881 - val_acc: 0.8444\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9864\n",
      "Epoch 00082: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0547 - acc: 0.9864 - val_loss: 0.4734 - val_acc: 0.8796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9907\n",
      "Epoch 00083: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0412 - acc: 0.9906 - val_loss: 0.4617 - val_acc: 0.8861\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9865\n",
      "Epoch 00084: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0516 - acc: 0.9864 - val_loss: 0.4754 - val_acc: 0.8807\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9886\n",
      "Epoch 00085: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0472 - acc: 0.9886 - val_loss: 0.4537 - val_acc: 0.8817\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9938\n",
      "Epoch 00086: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0323 - acc: 0.9938 - val_loss: 0.5105 - val_acc: 0.8786\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9896\n",
      "Epoch 00087: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0449 - acc: 0.9896 - val_loss: 0.4827 - val_acc: 0.8817\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9892\n",
      "Epoch 00088: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0424 - acc: 0.9892 - val_loss: 0.4591 - val_acc: 0.8863\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9939\n",
      "Epoch 00089: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0323 - acc: 0.9939 - val_loss: 0.4658 - val_acc: 0.8838\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9926\n",
      "Epoch 00090: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0358 - acc: 0.9925 - val_loss: 0.4722 - val_acc: 0.8845\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9926\n",
      "Epoch 00091: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0348 - acc: 0.9926 - val_loss: 0.5110 - val_acc: 0.8754\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9939\n",
      "Epoch 00092: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0319 - acc: 0.9938 - val_loss: 0.5703 - val_acc: 0.8607\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9854\n",
      "Epoch 00093: val_loss did not improve from 0.37413\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0541 - acc: 0.9854 - val_loss: 0.4834 - val_acc: 0.8828\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 602us/sample - loss: 0.4251 - acc: 0.8654\n",
      "Loss: 0.4251129043003233 Accuracy: 0.8654206\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6348 - acc: 0.4916\n",
      "Epoch 00001: val_loss improved from inf to 1.45596, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/001-1.4560.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.6348 - acc: 0.4916 - val_loss: 1.4560 - val_acc: 0.5190\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8423 - acc: 0.7541\n",
      "Epoch 00002: val_loss improved from 1.45596 to 0.70768, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/002-0.7077.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8423 - acc: 0.7541 - val_loss: 0.7077 - val_acc: 0.8011\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6054 - acc: 0.8288\n",
      "Epoch 00003: val_loss improved from 0.70768 to 0.52970, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/003-0.5297.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6055 - acc: 0.8288 - val_loss: 0.5297 - val_acc: 0.8551\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4841 - acc: 0.8646\n",
      "Epoch 00004: val_loss improved from 0.52970 to 0.44064, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/004-0.4406.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4843 - acc: 0.8646 - val_loss: 0.4406 - val_acc: 0.8742\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8858\n",
      "Epoch 00005: val_loss improved from 0.44064 to 0.38566, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/005-0.3857.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4075 - acc: 0.8858 - val_loss: 0.3857 - val_acc: 0.8870\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.9005\n",
      "Epoch 00006: val_loss improved from 0.38566 to 0.35345, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/006-0.3535.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3535 - acc: 0.9004 - val_loss: 0.3535 - val_acc: 0.9012\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9134\n",
      "Epoch 00007: val_loss improved from 0.35345 to 0.34357, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/007-0.3436.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3121 - acc: 0.9134 - val_loss: 0.3436 - val_acc: 0.9022\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.9228\n",
      "Epoch 00008: val_loss improved from 0.34357 to 0.33105, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/008-0.3311.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2806 - acc: 0.9228 - val_loss: 0.3311 - val_acc: 0.9033\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9308\n",
      "Epoch 00009: val_loss improved from 0.33105 to 0.27937, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/009-0.2794.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2533 - acc: 0.9307 - val_loss: 0.2794 - val_acc: 0.9164\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9365\n",
      "Epoch 00010: val_loss did not improve from 0.27937\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2323 - acc: 0.9365 - val_loss: 0.2955 - val_acc: 0.9124\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9442\n",
      "Epoch 00011: val_loss did not improve from 0.27937\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2104 - acc: 0.9442 - val_loss: 0.3018 - val_acc: 0.9073\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9461\n",
      "Epoch 00012: val_loss improved from 0.27937 to 0.25673, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/012-0.2567.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1966 - acc: 0.9461 - val_loss: 0.2567 - val_acc: 0.9210\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9520\n",
      "Epoch 00013: val_loss did not improve from 0.25673\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1781 - acc: 0.9520 - val_loss: 0.2713 - val_acc: 0.9161\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9568\n",
      "Epoch 00014: val_loss did not improve from 0.25673\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1658 - acc: 0.9568 - val_loss: 0.2774 - val_acc: 0.9138\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9602\n",
      "Epoch 00015: val_loss did not improve from 0.25673\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1531 - acc: 0.9601 - val_loss: 0.2609 - val_acc: 0.9227\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9643\n",
      "Epoch 00016: val_loss did not improve from 0.25673\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1401 - acc: 0.9643 - val_loss: 0.2829 - val_acc: 0.9096\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9659\n",
      "Epoch 00017: val_loss did not improve from 0.25673\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1321 - acc: 0.9659 - val_loss: 0.2600 - val_acc: 0.9173\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9707\n",
      "Epoch 00018: val_loss improved from 0.25673 to 0.25472, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/018-0.2547.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1172 - acc: 0.9707 - val_loss: 0.2547 - val_acc: 0.9180\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9733\n",
      "Epoch 00019: val_loss improved from 0.25472 to 0.24471, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/019-0.2447.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1081 - acc: 0.9733 - val_loss: 0.2447 - val_acc: 0.9243\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9757\n",
      "Epoch 00020: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1006 - acc: 0.9757 - val_loss: 0.2458 - val_acc: 0.9238\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9796\n",
      "Epoch 00021: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0904 - acc: 0.9796 - val_loss: 0.2553 - val_acc: 0.9189\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9817\n",
      "Epoch 00022: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0833 - acc: 0.9817 - val_loss: 0.2571 - val_acc: 0.9182\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9813\n",
      "Epoch 00023: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0800 - acc: 0.9813 - val_loss: 0.2592 - val_acc: 0.9229\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9823\n",
      "Epoch 00024: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0774 - acc: 0.9823 - val_loss: 0.2467 - val_acc: 0.9255\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9870\n",
      "Epoch 00025: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0632 - acc: 0.9870 - val_loss: 0.2568 - val_acc: 0.9248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9891\n",
      "Epoch 00026: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0569 - acc: 0.9891 - val_loss: 0.2795 - val_acc: 0.9180\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9895\n",
      "Epoch 00027: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0546 - acc: 0.9895 - val_loss: 0.2634 - val_acc: 0.9213\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9886\n",
      "Epoch 00028: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0556 - acc: 0.9886 - val_loss: 0.2708 - val_acc: 0.9245\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9919\n",
      "Epoch 00029: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0479 - acc: 0.9919 - val_loss: 0.2628 - val_acc: 0.9236\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9932\n",
      "Epoch 00030: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0417 - acc: 0.9932 - val_loss: 0.2462 - val_acc: 0.9243\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9941\n",
      "Epoch 00031: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0385 - acc: 0.9941 - val_loss: 0.2776 - val_acc: 0.9213\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9923\n",
      "Epoch 00032: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0426 - acc: 0.9923 - val_loss: 0.2593 - val_acc: 0.9224\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9935\n",
      "Epoch 00033: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0377 - acc: 0.9935 - val_loss: 0.2720 - val_acc: 0.9178\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9914\n",
      "Epoch 00034: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0428 - acc: 0.9914 - val_loss: 0.2554 - val_acc: 0.9301\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9966\n",
      "Epoch 00035: val_loss did not improve from 0.24471\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0269 - acc: 0.9966 - val_loss: 0.2717 - val_acc: 0.9264\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9929\n",
      "Epoch 00036: val_loss improved from 0.24471 to 0.24288, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_5_conv_checkpoint/036-0.2429.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0380 - acc: 0.9929 - val_loss: 0.2429 - val_acc: 0.9285\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9979\n",
      "Epoch 00037: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0225 - acc: 0.9979 - val_loss: 0.2653 - val_acc: 0.9262\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9967\n",
      "Epoch 00038: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0243 - acc: 0.9967 - val_loss: 0.2766 - val_acc: 0.9213\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9937\n",
      "Epoch 00039: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0317 - acc: 0.9937 - val_loss: 0.3059 - val_acc: 0.9175\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9944\n",
      "Epoch 00040: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0293 - acc: 0.9944 - val_loss: 0.2552 - val_acc: 0.9269\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9963\n",
      "Epoch 00041: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0237 - acc: 0.9963 - val_loss: 0.2671 - val_acc: 0.9290\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9960\n",
      "Epoch 00042: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0236 - acc: 0.9960 - val_loss: 0.2531 - val_acc: 0.9292\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9988\n",
      "Epoch 00043: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0154 - acc: 0.9988 - val_loss: 0.2484 - val_acc: 0.9320\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9980\n",
      "Epoch 00044: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0179 - acc: 0.9980 - val_loss: 0.3007 - val_acc: 0.9213\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9983\n",
      "Epoch 00045: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0157 - acc: 0.9983 - val_loss: 0.2577 - val_acc: 0.9294\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9971\n",
      "Epoch 00046: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0183 - acc: 0.9971 - val_loss: 0.2690 - val_acc: 0.9290\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9986\n",
      "Epoch 00047: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0148 - acc: 0.9986 - val_loss: 0.3548 - val_acc: 0.9138\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9945\n",
      "Epoch 00048: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0254 - acc: 0.9945 - val_loss: 0.2734 - val_acc: 0.9299\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9984\n",
      "Epoch 00049: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0135 - acc: 0.9984 - val_loss: 0.2726 - val_acc: 0.9243\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9988\n",
      "Epoch 00050: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0116 - acc: 0.9988 - val_loss: 0.3411 - val_acc: 0.9161\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9935\n",
      "Epoch 00051: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0282 - acc: 0.9935 - val_loss: 0.3055 - val_acc: 0.9180\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9985\n",
      "Epoch 00052: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0125 - acc: 0.9985 - val_loss: 0.3361 - val_acc: 0.9101\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9977\n",
      "Epoch 00053: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0154 - acc: 0.9977 - val_loss: 0.3008 - val_acc: 0.9224\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9959\n",
      "Epoch 00054: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0193 - acc: 0.9959 - val_loss: 0.2663 - val_acc: 0.9327\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9989\n",
      "Epoch 00055: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0115 - acc: 0.9989 - val_loss: 0.2740 - val_acc: 0.9297\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9988\n",
      "Epoch 00056: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0104 - acc: 0.9988 - val_loss: 0.2678 - val_acc: 0.9324\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9980\n",
      "Epoch 00057: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0130 - acc: 0.9980 - val_loss: 0.2936 - val_acc: 0.9266\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9972\n",
      "Epoch 00058: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0143 - acc: 0.9972 - val_loss: 0.3073 - val_acc: 0.9248\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9970\n",
      "Epoch 00059: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0157 - acc: 0.9970 - val_loss: 0.2830 - val_acc: 0.9287\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9971\n",
      "Epoch 00060: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0147 - acc: 0.9971 - val_loss: 0.2899 - val_acc: 0.9269\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9995\n",
      "Epoch 00061: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0071 - acc: 0.9994 - val_loss: 0.3161 - val_acc: 0.9262\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9954\n",
      "Epoch 00062: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0199 - acc: 0.9954 - val_loss: 0.2845 - val_acc: 0.9306\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9996\n",
      "Epoch 00063: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0068 - acc: 0.9996 - val_loss: 0.2825 - val_acc: 0.9311\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9977\n",
      "Epoch 00064: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0124 - acc: 0.9977 - val_loss: 0.2808 - val_acc: 0.9278\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9991\n",
      "Epoch 00065: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0085 - acc: 0.9991 - val_loss: 0.2896 - val_acc: 0.9341\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9996\n",
      "Epoch 00066: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0054 - acc: 0.9996 - val_loss: 0.3025 - val_acc: 0.9280\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9949\n",
      "Epoch 00067: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0204 - acc: 0.9949 - val_loss: 0.3206 - val_acc: 0.9236\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9994\n",
      "Epoch 00068: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0064 - acc: 0.9993 - val_loss: 0.2749 - val_acc: 0.9306\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9954\n",
      "Epoch 00069: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0177 - acc: 0.9954 - val_loss: 0.2871 - val_acc: 0.9271\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9993\n",
      "Epoch 00070: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0066 - acc: 0.9993 - val_loss: 0.2799 - val_acc: 0.9299\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9961\n",
      "Epoch 00071: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0158 - acc: 0.9961 - val_loss: 0.2794 - val_acc: 0.9313\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9995\n",
      "Epoch 00072: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0057 - acc: 0.9995 - val_loss: 0.3004 - val_acc: 0.9276\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9957\n",
      "Epoch 00073: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0170 - acc: 0.9957 - val_loss: 0.2863 - val_acc: 0.9273\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9996\n",
      "Epoch 00074: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0052 - acc: 0.9996 - val_loss: 0.2776 - val_acc: 0.9331\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9995\n",
      "Epoch 00075: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0059 - acc: 0.9995 - val_loss: 0.2873 - val_acc: 0.9329\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9988\n",
      "Epoch 00076: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0080 - acc: 0.9988 - val_loss: 0.3007 - val_acc: 0.9299\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9993\n",
      "Epoch 00077: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0060 - acc: 0.9993 - val_loss: 0.3145 - val_acc: 0.9206\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9952\n",
      "Epoch 00078: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0195 - acc: 0.9952 - val_loss: 0.2854 - val_acc: 0.9304\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9996\n",
      "Epoch 00079: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0048 - acc: 0.9996 - val_loss: 0.2962 - val_acc: 0.9315\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9954\n",
      "Epoch 00080: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0186 - acc: 0.9954 - val_loss: 0.2747 - val_acc: 0.9350\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9996\n",
      "Epoch 00081: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0045 - acc: 0.9996 - val_loss: 0.2792 - val_acc: 0.9317\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9997\n",
      "Epoch 00082: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0043 - acc: 0.9997 - val_loss: 0.2727 - val_acc: 0.9369\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9968\n",
      "Epoch 00083: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0136 - acc: 0.9968 - val_loss: 0.3196 - val_acc: 0.9311\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9996\n",
      "Epoch 00084: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0047 - acc: 0.9995 - val_loss: 0.3263 - val_acc: 0.9264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9936\n",
      "Epoch 00085: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0226 - acc: 0.9936 - val_loss: 0.2993 - val_acc: 0.9292\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9996\n",
      "Epoch 00086: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0047 - acc: 0.9996 - val_loss: 0.2980 - val_acc: 0.9348\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 634us/sample - loss: 0.2825 - acc: 0.9221\n",
      "Loss: 0.28251868444315986 Accuracy: 0.92211837\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2951 - acc: 0.6087\n",
      "Epoch 00001: val_loss improved from inf to 1.00217, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/001-1.0022.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.2950 - acc: 0.6088 - val_loss: 1.0022 - val_acc: 0.7470\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.8476\n",
      "Epoch 00002: val_loss improved from 1.00217 to 0.46507, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/002-0.4651.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.5467 - acc: 0.8476 - val_loss: 0.4651 - val_acc: 0.8733\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8938\n",
      "Epoch 00003: val_loss improved from 0.46507 to 0.36947, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/003-0.3695.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.3781 - acc: 0.8937 - val_loss: 0.3695 - val_acc: 0.8938\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9145\n",
      "Epoch 00004: val_loss improved from 0.36947 to 0.30117, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/004-0.3012.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.3024 - acc: 0.9145 - val_loss: 0.3012 - val_acc: 0.9166\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9303\n",
      "Epoch 00005: val_loss did not improve from 0.30117\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.2499 - acc: 0.9303 - val_loss: 0.3223 - val_acc: 0.9038\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9411\n",
      "Epoch 00006: val_loss improved from 0.30117 to 0.27480, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/006-0.2748.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.2166 - acc: 0.9411 - val_loss: 0.2748 - val_acc: 0.9196\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9469\n",
      "Epoch 00007: val_loss improved from 0.27480 to 0.24336, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/007-0.2434.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1938 - acc: 0.9469 - val_loss: 0.2434 - val_acc: 0.9317\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9551\n",
      "Epoch 00008: val_loss improved from 0.24336 to 0.22684, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/008-0.2268.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1663 - acc: 0.9551 - val_loss: 0.2268 - val_acc: 0.9276\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9598\n",
      "Epoch 00009: val_loss improved from 0.22684 to 0.21316, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/009-0.2132.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1500 - acc: 0.9598 - val_loss: 0.2132 - val_acc: 0.9338\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9641\n",
      "Epoch 00010: val_loss improved from 0.21316 to 0.20110, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/010-0.2011.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1336 - acc: 0.9641 - val_loss: 0.2011 - val_acc: 0.9376\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9683\n",
      "Epoch 00011: val_loss did not improve from 0.20110\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1195 - acc: 0.9683 - val_loss: 0.2082 - val_acc: 0.9362\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9718\n",
      "Epoch 00012: val_loss did not improve from 0.20110\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1074 - acc: 0.9717 - val_loss: 0.2155 - val_acc: 0.9331\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9746\n",
      "Epoch 00013: val_loss did not improve from 0.20110\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0985 - acc: 0.9746 - val_loss: 0.2081 - val_acc: 0.9343\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9780\n",
      "Epoch 00014: val_loss improved from 0.20110 to 0.19439, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/014-0.1944.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0883 - acc: 0.9780 - val_loss: 0.1944 - val_acc: 0.9427\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9819\n",
      "Epoch 00015: val_loss did not improve from 0.19439\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0758 - acc: 0.9818 - val_loss: 0.2433 - val_acc: 0.9257\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9795\n",
      "Epoch 00016: val_loss did not improve from 0.19439\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0803 - acc: 0.9795 - val_loss: 0.2043 - val_acc: 0.9352\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9849\n",
      "Epoch 00017: val_loss did not improve from 0.19439\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0651 - acc: 0.9849 - val_loss: 0.2068 - val_acc: 0.9406\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9857\n",
      "Epoch 00018: val_loss improved from 0.19439 to 0.18729, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/018-0.1873.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0622 - acc: 0.9857 - val_loss: 0.1873 - val_acc: 0.9439\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9884\n",
      "Epoch 00019: val_loss did not improve from 0.18729\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0534 - acc: 0.9884 - val_loss: 0.1878 - val_acc: 0.9425\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9924\n",
      "Epoch 00020: val_loss improved from 0.18729 to 0.17848, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/020-0.1785.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0424 - acc: 0.9924 - val_loss: 0.1785 - val_acc: 0.9469\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9937\n",
      "Epoch 00021: val_loss did not improve from 0.17848\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0371 - acc: 0.9937 - val_loss: 0.2061 - val_acc: 0.9383\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9924\n",
      "Epoch 00022: val_loss did not improve from 0.17848\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0400 - acc: 0.9924 - val_loss: 0.2091 - val_acc: 0.9385\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9952\n",
      "Epoch 00023: val_loss improved from 0.17848 to 0.17737, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/023-0.1774.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0324 - acc: 0.9952 - val_loss: 0.1774 - val_acc: 0.9499\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9955\n",
      "Epoch 00024: val_loss did not improve from 0.17737\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0280 - acc: 0.9955 - val_loss: 0.2021 - val_acc: 0.9469\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9968\n",
      "Epoch 00025: val_loss did not improve from 0.17737\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0241 - acc: 0.9968 - val_loss: 0.1918 - val_acc: 0.9457\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9936\n",
      "Epoch 00026: val_loss did not improve from 0.17737\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0330 - acc: 0.9936 - val_loss: 0.1998 - val_acc: 0.9446\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9966\n",
      "Epoch 00027: val_loss did not improve from 0.17737\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0228 - acc: 0.9966 - val_loss: 0.2145 - val_acc: 0.9380\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9959\n",
      "Epoch 00028: val_loss did not improve from 0.17737\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0247 - acc: 0.9959 - val_loss: 0.2113 - val_acc: 0.9448\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9980\n",
      "Epoch 00029: val_loss did not improve from 0.17737\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0172 - acc: 0.9980 - val_loss: 0.1821 - val_acc: 0.9471\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9987\n",
      "Epoch 00030: val_loss did not improve from 0.17737\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0139 - acc: 0.9987 - val_loss: 0.2097 - val_acc: 0.9387\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9961\n",
      "Epoch 00031: val_loss did not improve from 0.17737\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0214 - acc: 0.9961 - val_loss: 0.1987 - val_acc: 0.9467\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9977\n",
      "Epoch 00032: val_loss did not improve from 0.17737\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0167 - acc: 0.9976 - val_loss: 0.2139 - val_acc: 0.9380\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9910\n",
      "Epoch 00033: val_loss did not improve from 0.17737\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0371 - acc: 0.9910 - val_loss: 0.1876 - val_acc: 0.9478\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9970\n",
      "Epoch 00034: val_loss improved from 0.17737 to 0.17616, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/034-0.1762.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0176 - acc: 0.9970 - val_loss: 0.1762 - val_acc: 0.9511\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9994\n",
      "Epoch 00035: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0087 - acc: 0.9994 - val_loss: 0.1840 - val_acc: 0.9485\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9993\n",
      "Epoch 00036: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0086 - acc: 0.9993 - val_loss: 0.2072 - val_acc: 0.9432\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9986\n",
      "Epoch 00037: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0116 - acc: 0.9986 - val_loss: 0.1954 - val_acc: 0.9457\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9991\n",
      "Epoch 00038: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0099 - acc: 0.9991 - val_loss: 0.2123 - val_acc: 0.9432\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9974\n",
      "Epoch 00039: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0155 - acc: 0.9974 - val_loss: 0.3372 - val_acc: 0.9180\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9950\n",
      "Epoch 00040: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0216 - acc: 0.9950 - val_loss: 0.1865 - val_acc: 0.9488\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9985\n",
      "Epoch 00041: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0112 - acc: 0.9985 - val_loss: 0.1961 - val_acc: 0.9471\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9990\n",
      "Epoch 00042: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0084 - acc: 0.9990 - val_loss: 0.2304 - val_acc: 0.9383\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9959\n",
      "Epoch 00043: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0202 - acc: 0.9959 - val_loss: 0.1853 - val_acc: 0.9513\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9995\n",
      "Epoch 00044: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0061 - acc: 0.9995 - val_loss: 0.2000 - val_acc: 0.9469\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9995\n",
      "Epoch 00045: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0059 - acc: 0.9995 - val_loss: 0.1976 - val_acc: 0.9509\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9982\n",
      "Epoch 00046: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0113 - acc: 0.9981 - val_loss: 0.2631 - val_acc: 0.9304\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9947\n",
      "Epoch 00047: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0217 - acc: 0.9947 - val_loss: 0.2363 - val_acc: 0.9371\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9996\n",
      "Epoch 00048: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0052 - acc: 0.9996 - val_loss: 0.2012 - val_acc: 0.9474\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9990\n",
      "Epoch 00049: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0074 - acc: 0.9990 - val_loss: 0.1928 - val_acc: 0.9488\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9996\n",
      "Epoch 00050: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0047 - acc: 0.9996 - val_loss: 0.1918 - val_acc: 0.9515\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9960\n",
      "Epoch 00051: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0159 - acc: 0.9960 - val_loss: 0.1871 - val_acc: 0.9502\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9998\n",
      "Epoch 00052: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0039 - acc: 0.9998 - val_loss: 0.2039 - val_acc: 0.9497\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9997\n",
      "Epoch 00053: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0039 - acc: 0.9997 - val_loss: 0.2191 - val_acc: 0.9469\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9970\n",
      "Epoch 00054: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0140 - acc: 0.9970 - val_loss: 0.2167 - val_acc: 0.9462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9962\n",
      "Epoch 00055: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0168 - acc: 0.9962 - val_loss: 0.2202 - val_acc: 0.9434\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9956\n",
      "Epoch 00056: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0169 - acc: 0.9956 - val_loss: 0.1901 - val_acc: 0.9511\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9998\n",
      "Epoch 00057: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0034 - acc: 0.9998 - val_loss: 0.1767 - val_acc: 0.9555\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 00058: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0025 - acc: 0.9999 - val_loss: 0.1911 - val_acc: 0.9506\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9936\n",
      "Epoch 00059: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0231 - acc: 0.9936 - val_loss: 0.2281 - val_acc: 0.9450\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9979\n",
      "Epoch 00060: val_loss did not improve from 0.17616\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0109 - acc: 0.9979 - val_loss: 0.1862 - val_acc: 0.9534\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9998\n",
      "Epoch 00061: val_loss improved from 0.17616 to 0.17325, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_6_conv_checkpoint/061-0.1733.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0030 - acc: 0.9998 - val_loss: 0.1733 - val_acc: 0.9560\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 00062: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0025 - acc: 0.9999 - val_loss: 0.1894 - val_acc: 0.9548\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9995\n",
      "Epoch 00063: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0044 - acc: 0.9995 - val_loss: 0.1896 - val_acc: 0.9511\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9994\n",
      "Epoch 00064: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0046 - acc: 0.9994 - val_loss: 0.2299 - val_acc: 0.9427\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9974\n",
      "Epoch 00065: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0115 - acc: 0.9974 - val_loss: 0.2631 - val_acc: 0.9294\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9982\n",
      "Epoch 00066: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0087 - acc: 0.9982 - val_loss: 0.2163 - val_acc: 0.9504\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00067: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0026 - acc: 0.9997 - val_loss: 0.2399 - val_acc: 0.9425\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9957\n",
      "Epoch 00068: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0167 - acc: 0.9957 - val_loss: 0.2014 - val_acc: 0.9509\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9996\n",
      "Epoch 00069: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0036 - acc: 0.9996 - val_loss: 0.2016 - val_acc: 0.9511\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00070: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0025 - acc: 0.9998 - val_loss: 0.1902 - val_acc: 0.9541\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 00071: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0029 - acc: 0.9997 - val_loss: 0.2874 - val_acc: 0.9290\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9938\n",
      "Epoch 00072: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0223 - acc: 0.9938 - val_loss: 0.2024 - val_acc: 0.9504\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9995\n",
      "Epoch 00073: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0043 - acc: 0.9995 - val_loss: 0.2144 - val_acc: 0.9490\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9957\n",
      "Epoch 00074: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0159 - acc: 0.9956 - val_loss: 0.1935 - val_acc: 0.9518\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9979\n",
      "Epoch 00075: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0095 - acc: 0.9979 - val_loss: 0.1886 - val_acc: 0.9536\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9995\n",
      "Epoch 00076: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0035 - acc: 0.9995 - val_loss: 0.1868 - val_acc: 0.9567\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00077: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.1899 - val_acc: 0.9522\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 00078: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0026 - acc: 0.9998 - val_loss: 0.2241 - val_acc: 0.9469\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9995\n",
      "Epoch 00079: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0034 - acc: 0.9995 - val_loss: 0.2508 - val_acc: 0.9425\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9968\n",
      "Epoch 00080: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0126 - acc: 0.9968 - val_loss: 0.2698 - val_acc: 0.9369\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9962\n",
      "Epoch 00081: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0145 - acc: 0.9963 - val_loss: 0.1822 - val_acc: 0.9527\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9991\n",
      "Epoch 00082: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0048 - acc: 0.9991 - val_loss: 0.1820 - val_acc: 0.9567\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00083: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0032 - acc: 0.9996 - val_loss: 0.2011 - val_acc: 0.9515\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9960\n",
      "Epoch 00084: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0143 - acc: 0.9960 - val_loss: 0.1973 - val_acc: 0.9546\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9977\n",
      "Epoch 00085: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0101 - acc: 0.9977 - val_loss: 0.2013 - val_acc: 0.9529\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 00086: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0024 - acc: 0.9998 - val_loss: 0.1920 - val_acc: 0.9543\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9990\n",
      "Epoch 00087: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0054 - acc: 0.9990 - val_loss: 0.1866 - val_acc: 0.9553\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9962\n",
      "Epoch 00088: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0135 - acc: 0.9962 - val_loss: 0.1850 - val_acc: 0.9581\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00089: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1884 - val_acc: 0.9571\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9999\n",
      "Epoch 00090: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0022 - acc: 0.9999 - val_loss: 0.1939 - val_acc: 0.9569\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9997\n",
      "Epoch 00091: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0024 - acc: 0.9997 - val_loss: 0.2313 - val_acc: 0.9453\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9983\n",
      "Epoch 00092: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0066 - acc: 0.9983 - val_loss: 0.2012 - val_acc: 0.9541\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9997\n",
      "Epoch 00093: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0025 - acc: 0.9997 - val_loss: 0.2448 - val_acc: 0.9443\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 00094: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0029 - acc: 0.9997 - val_loss: 0.2019 - val_acc: 0.9529\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9991\n",
      "Epoch 00095: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0047 - acc: 0.9991 - val_loss: 0.7898 - val_acc: 0.8626\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9966\n",
      "Epoch 00096: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0121 - acc: 0.9966 - val_loss: 0.2954 - val_acc: 0.9327\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9995\n",
      "Epoch 00097: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0035 - acc: 0.9995 - val_loss: 0.1934 - val_acc: 0.9546\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9997\n",
      "Epoch 00098: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0025 - acc: 0.9997 - val_loss: 0.2241 - val_acc: 0.9455\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00099: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0035 - acc: 0.9993 - val_loss: 0.3314 - val_acc: 0.9324\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9960\n",
      "Epoch 00100: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0139 - acc: 0.9960 - val_loss: 0.1970 - val_acc: 0.9548\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9976\n",
      "Epoch 00101: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0093 - acc: 0.9976 - val_loss: 0.2003 - val_acc: 0.9529\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9987\n",
      "Epoch 00102: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0066 - acc: 0.9987 - val_loss: 0.1932 - val_acc: 0.9541\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00103: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1928 - val_acc: 0.9532\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9999\n",
      "Epoch 00104: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0016 - acc: 0.9999 - val_loss: 0.1998 - val_acc: 0.9520\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9972\n",
      "Epoch 00105: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0106 - acc: 0.9972 - val_loss: 0.1940 - val_acc: 0.9550\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00106: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1948 - val_acc: 0.9543\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9995\n",
      "Epoch 00107: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0033 - acc: 0.9995 - val_loss: 0.1985 - val_acc: 0.9539\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00108: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.2027 - val_acc: 0.9522\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9997\n",
      "Epoch 00109: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0025 - acc: 0.9997 - val_loss: 0.2217 - val_acc: 0.9481\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9959\n",
      "Epoch 00110: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0137 - acc: 0.9959 - val_loss: 0.1930 - val_acc: 0.9557\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 00111: val_loss did not improve from 0.17325\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0017 - acc: 0.9999 - val_loss: 0.2000 - val_acc: 0.9541\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 690us/sample - loss: 0.1879 - acc: 0.9493\n",
      "Loss: 0.1879012505151525 Accuracy: 0.949325\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0641 - acc: 0.6801\n",
      "Epoch 00001: val_loss improved from inf to 0.82575, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_7_conv_checkpoint/001-0.8257.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 1.0643 - acc: 0.6800 - val_loss: 0.8257 - val_acc: 0.7787\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8829\n",
      "Epoch 00002: val_loss improved from 0.82575 to 0.31702, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_7_conv_checkpoint/002-0.3170.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4042 - acc: 0.8829 - val_loss: 0.3170 - val_acc: 0.9157\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9166\n",
      "Epoch 00003: val_loss improved from 0.31702 to 0.25113, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_7_conv_checkpoint/003-0.2511.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2846 - acc: 0.9166 - val_loss: 0.2511 - val_acc: 0.9311\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9377\n",
      "Epoch 00004: val_loss improved from 0.25113 to 0.20460, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_7_conv_checkpoint/004-0.2046.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2193 - acc: 0.9377 - val_loss: 0.2046 - val_acc: 0.9429\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9478\n",
      "Epoch 00005: val_loss improved from 0.20460 to 0.19923, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_7_conv_checkpoint/005-0.1992.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1844 - acc: 0.9478 - val_loss: 0.1992 - val_acc: 0.9394\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9557\n",
      "Epoch 00006: val_loss improved from 0.19923 to 0.17092, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_7_conv_checkpoint/006-0.1709.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1546 - acc: 0.9557 - val_loss: 0.1709 - val_acc: 0.9506\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9648\n",
      "Epoch 00007: val_loss did not improve from 0.17092\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1311 - acc: 0.9648 - val_loss: 0.1725 - val_acc: 0.9499\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9670\n",
      "Epoch 00008: val_loss did not improve from 0.17092\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1193 - acc: 0.9669 - val_loss: 0.1734 - val_acc: 0.9460\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9716\n",
      "Epoch 00009: val_loss improved from 0.17092 to 0.15995, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_7_conv_checkpoint/009-0.1600.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1033 - acc: 0.9716 - val_loss: 0.1600 - val_acc: 0.9541\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9767\n",
      "Epoch 00010: val_loss did not improve from 0.15995\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0895 - acc: 0.9767 - val_loss: 0.1622 - val_acc: 0.9495\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9825\n",
      "Epoch 00011: val_loss did not improve from 0.15995\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0725 - acc: 0.9825 - val_loss: 0.1756 - val_acc: 0.9478\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9836\n",
      "Epoch 00012: val_loss improved from 0.15995 to 0.14757, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_7_conv_checkpoint/012-0.1476.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0675 - acc: 0.9836 - val_loss: 0.1476 - val_acc: 0.9534\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9875\n",
      "Epoch 00013: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0546 - acc: 0.9875 - val_loss: 0.1630 - val_acc: 0.9515\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9898\n",
      "Epoch 00014: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0478 - acc: 0.9897 - val_loss: 0.1676 - val_acc: 0.9469\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9904\n",
      "Epoch 00015: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0459 - acc: 0.9903 - val_loss: 0.1753 - val_acc: 0.9488\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9910\n",
      "Epoch 00016: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0427 - acc: 0.9910 - val_loss: 0.1629 - val_acc: 0.9509\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9954\n",
      "Epoch 00017: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0284 - acc: 0.9953 - val_loss: 0.1804 - val_acc: 0.9474\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9928\n",
      "Epoch 00018: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0364 - acc: 0.9927 - val_loss: 0.1691 - val_acc: 0.9502\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9943\n",
      "Epoch 00019: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0296 - acc: 0.9943 - val_loss: 0.1944 - val_acc: 0.9429\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9953\n",
      "Epoch 00020: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0267 - acc: 0.9953 - val_loss: 0.1494 - val_acc: 0.9564\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9971\n",
      "Epoch 00021: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0193 - acc: 0.9971 - val_loss: 0.1796 - val_acc: 0.9478\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9973\n",
      "Epoch 00022: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0187 - acc: 0.9973 - val_loss: 0.1632 - val_acc: 0.9520\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9940\n",
      "Epoch 00023: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0283 - acc: 0.9940 - val_loss: 0.1524 - val_acc: 0.9576\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9979\n",
      "Epoch 00024: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0147 - acc: 0.9979 - val_loss: 0.1641 - val_acc: 0.9509\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9976\n",
      "Epoch 00025: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0158 - acc: 0.9975 - val_loss: 0.1558 - val_acc: 0.9548\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9968\n",
      "Epoch 00026: val_loss did not improve from 0.14757\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0194 - acc: 0.9968 - val_loss: 0.1621 - val_acc: 0.9509\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9961\n",
      "Epoch 00027: val_loss improved from 0.14757 to 0.14582, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_7_conv_checkpoint/027-0.1458.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0203 - acc: 0.9961 - val_loss: 0.1458 - val_acc: 0.9555\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9993\n",
      "Epoch 00028: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0078 - acc: 0.9993 - val_loss: 0.1692 - val_acc: 0.9557\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9994\n",
      "Epoch 00029: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0068 - acc: 0.9994 - val_loss: 0.1654 - val_acc: 0.9557\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9992\n",
      "Epoch 00030: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0072 - acc: 0.9992 - val_loss: 0.1766 - val_acc: 0.9483\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9961\n",
      "Epoch 00031: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0178 - acc: 0.9961 - val_loss: 0.2247 - val_acc: 0.9364\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9970\n",
      "Epoch 00032: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0149 - acc: 0.9970 - val_loss: 0.1524 - val_acc: 0.9585\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9975\n",
      "Epoch 00033: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0125 - acc: 0.9975 - val_loss: 0.1498 - val_acc: 0.9569\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9991\n",
      "Epoch 00034: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0072 - acc: 0.9991 - val_loss: 0.1731 - val_acc: 0.9541\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9990\n",
      "Epoch 00035: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0069 - acc: 0.9990 - val_loss: 0.1786 - val_acc: 0.9525\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9977\n",
      "Epoch 00036: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0108 - acc: 0.9977 - val_loss: 0.1673 - val_acc: 0.9534\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9980\n",
      "Epoch 00037: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0097 - acc: 0.9980 - val_loss: 0.1687 - val_acc: 0.9567\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9997\n",
      "Epoch 00038: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0038 - acc: 0.9997 - val_loss: 0.1651 - val_acc: 0.9562\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9990\n",
      "Epoch 00039: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0068 - acc: 0.9990 - val_loss: 0.2460 - val_acc: 0.9366\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9930\n",
      "Epoch 00040: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0247 - acc: 0.9930 - val_loss: 0.1468 - val_acc: 0.9620\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9998\n",
      "Epoch 00041: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0036 - acc: 0.9998 - val_loss: 0.1714 - val_acc: 0.9562\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9995\n",
      "Epoch 00042: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0041 - acc: 0.9995 - val_loss: 0.1638 - val_acc: 0.9578\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9977\n",
      "Epoch 00043: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0111 - acc: 0.9977 - val_loss: 0.1824 - val_acc: 0.9553\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9994\n",
      "Epoch 00044: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0048 - acc: 0.9994 - val_loss: 0.1602 - val_acc: 0.9590\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9998\n",
      "Epoch 00045: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0029 - acc: 0.9998 - val_loss: 0.1616 - val_acc: 0.9562\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9962\n",
      "Epoch 00046: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0133 - acc: 0.9962 - val_loss: 0.1911 - val_acc: 0.9495\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9990\n",
      "Epoch 00047: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0067 - acc: 0.9990 - val_loss: 0.1747 - val_acc: 0.9543\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9969\n",
      "Epoch 00048: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0116 - acc: 0.9969 - val_loss: 0.1515 - val_acc: 0.9602\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9995\n",
      "Epoch 00049: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0036 - acc: 0.9995 - val_loss: 0.1484 - val_acc: 0.9613\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 00050: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0029 - acc: 0.9997 - val_loss: 0.1613 - val_acc: 0.9595\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9988\n",
      "Epoch 00051: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0058 - acc: 0.9988 - val_loss: 0.1945 - val_acc: 0.9513\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9908\n",
      "Epoch 00052: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0298 - acc: 0.9908 - val_loss: 0.1758 - val_acc: 0.9555\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9996\n",
      "Epoch 00053: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0038 - acc: 0.9996 - val_loss: 0.1473 - val_acc: 0.9627\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9998\n",
      "Epoch 00054: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0023 - acc: 0.9998 - val_loss: 0.1580 - val_acc: 0.9606\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 00055: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0026 - acc: 0.9998 - val_loss: 0.1725 - val_acc: 0.9585\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9998\n",
      "Epoch 00056: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0022 - acc: 0.9998 - val_loss: 0.1895 - val_acc: 0.9550\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9969\n",
      "Epoch 00057: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0110 - acc: 0.9969 - val_loss: 0.1525 - val_acc: 0.9599\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9957\n",
      "Epoch 00058: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0150 - acc: 0.9957 - val_loss: 0.1461 - val_acc: 0.9627\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 00059: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0027 - acc: 0.9997 - val_loss: 0.1583 - val_acc: 0.9578\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 00060: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0019 - acc: 0.9999 - val_loss: 0.1467 - val_acc: 0.9646\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9997\n",
      "Epoch 00061: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0025 - acc: 0.9997 - val_loss: 0.1782 - val_acc: 0.9571\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9978\n",
      "Epoch 00062: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0077 - acc: 0.9978 - val_loss: 0.3058 - val_acc: 0.9227\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9971\n",
      "Epoch 00063: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0109 - acc: 0.9971 - val_loss: 0.1506 - val_acc: 0.9602\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9970\n",
      "Epoch 00064: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0114 - acc: 0.9970 - val_loss: 0.1534 - val_acc: 0.9602\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9980\n",
      "Epoch 00065: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0087 - acc: 0.9980 - val_loss: 0.1511 - val_acc: 0.9613\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00066: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1650 - val_acc: 0.9590\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00067: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1676 - val_acc: 0.9595\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9974\n",
      "Epoch 00068: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0101 - acc: 0.9974 - val_loss: 0.1567 - val_acc: 0.9576\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00069: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1487 - val_acc: 0.9620\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00070: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1958 - val_acc: 0.9515\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9951\n",
      "Epoch 00071: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0185 - acc: 0.9951 - val_loss: 0.1659 - val_acc: 0.9613\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9992\n",
      "Epoch 00072: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0042 - acc: 0.9992 - val_loss: 0.1499 - val_acc: 0.9644\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9992\n",
      "Epoch 00073: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0041 - acc: 0.9992 - val_loss: 0.1588 - val_acc: 0.9632\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 00074: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0013 - acc: 0.9999 - val_loss: 0.1840 - val_acc: 0.9574\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00075: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1613 - val_acc: 0.9602\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9976\n",
      "Epoch 00076: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0091 - acc: 0.9975 - val_loss: 0.1770 - val_acc: 0.9576\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9960\n",
      "Epoch 00077: val_loss did not improve from 0.14582\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0142 - acc: 0.9960 - val_loss: 0.1516 - val_acc: 0.9623\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 688us/sample - loss: 0.1756 - acc: 0.9462\n",
      "Loss: 0.17561202078655377 Accuracy: 0.9462098\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0398 - acc: 0.6812\n",
      "Epoch 00001: val_loss improved from inf to 0.76252, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/001-0.7625.hdf5\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.0399 - acc: 0.6812 - val_loss: 0.7625 - val_acc: 0.7768\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8778\n",
      "Epoch 00002: val_loss improved from 0.76252 to 0.31461, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/002-0.3146.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4000 - acc: 0.8778 - val_loss: 0.3146 - val_acc: 0.9019\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2771 - acc: 0.9157\n",
      "Epoch 00003: val_loss improved from 0.31461 to 0.23947, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/003-0.2395.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2771 - acc: 0.9156 - val_loss: 0.2395 - val_acc: 0.9273\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9358\n",
      "Epoch 00004: val_loss did not improve from 0.23947\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2163 - acc: 0.9358 - val_loss: 0.2708 - val_acc: 0.9126\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9450\n",
      "Epoch 00005: val_loss improved from 0.23947 to 0.21868, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/005-0.2187.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1838 - acc: 0.9450 - val_loss: 0.2187 - val_acc: 0.9304\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9569\n",
      "Epoch 00006: val_loss improved from 0.21868 to 0.18284, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/006-0.1828.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1478 - acc: 0.9569 - val_loss: 0.1828 - val_acc: 0.9429\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9630\n",
      "Epoch 00007: val_loss improved from 0.18284 to 0.17969, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/007-0.1797.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1276 - acc: 0.9630 - val_loss: 0.1797 - val_acc: 0.9420\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9689\n",
      "Epoch 00008: val_loss did not improve from 0.17969\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1101 - acc: 0.9689 - val_loss: 0.2179 - val_acc: 0.9366\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9737\n",
      "Epoch 00009: val_loss did not improve from 0.17969\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0939 - acc: 0.9737 - val_loss: 0.1856 - val_acc: 0.9439\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9777\n",
      "Epoch 00010: val_loss improved from 0.17969 to 0.17075, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/010-0.1707.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0841 - acc: 0.9777 - val_loss: 0.1707 - val_acc: 0.9492\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9790\n",
      "Epoch 00011: val_loss did not improve from 0.17075\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0762 - acc: 0.9790 - val_loss: 0.2146 - val_acc: 0.9322\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9850\n",
      "Epoch 00012: val_loss improved from 0.17075 to 0.15909, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/012-0.1591.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0591 - acc: 0.9850 - val_loss: 0.1591 - val_acc: 0.9520\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9886\n",
      "Epoch 00013: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0500 - acc: 0.9886 - val_loss: 0.1783 - val_acc: 0.9439\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9906\n",
      "Epoch 00014: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0425 - acc: 0.9906 - val_loss: 0.1998 - val_acc: 0.9385\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9921\n",
      "Epoch 00015: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0376 - acc: 0.9921 - val_loss: 0.1667 - val_acc: 0.9471\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9910\n",
      "Epoch 00016: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0385 - acc: 0.9910 - val_loss: 0.1687 - val_acc: 0.9506\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9948\n",
      "Epoch 00017: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0270 - acc: 0.9948 - val_loss: 0.1854 - val_acc: 0.9441\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9937\n",
      "Epoch 00018: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0275 - acc: 0.9937 - val_loss: 0.1766 - val_acc: 0.9502\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9952\n",
      "Epoch 00019: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0241 - acc: 0.9952 - val_loss: 0.1991 - val_acc: 0.9450\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9923\n",
      "Epoch 00020: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0321 - acc: 0.9923 - val_loss: 0.1681 - val_acc: 0.9511\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9946\n",
      "Epoch 00021: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0244 - acc: 0.9946 - val_loss: 0.1967 - val_acc: 0.9476\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9949\n",
      "Epoch 00022: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0234 - acc: 0.9949 - val_loss: 0.1717 - val_acc: 0.9511\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9987\n",
      "Epoch 00023: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0114 - acc: 0.9987 - val_loss: 0.1784 - val_acc: 0.9502\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9972\n",
      "Epoch 00024: val_loss improved from 0.15909 to 0.15689, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/024-0.1569.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0155 - acc: 0.9972 - val_loss: 0.1569 - val_acc: 0.9548\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9970\n",
      "Epoch 00025: val_loss did not improve from 0.15689\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0153 - acc: 0.9970 - val_loss: 0.1934 - val_acc: 0.9446\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9967\n",
      "Epoch 00026: val_loss did not improve from 0.15689\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0159 - acc: 0.9967 - val_loss: 0.2111 - val_acc: 0.9373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9979\n",
      "Epoch 00027: val_loss did not improve from 0.15689\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0118 - acc: 0.9979 - val_loss: 0.2034 - val_acc: 0.9488\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9967\n",
      "Epoch 00028: val_loss did not improve from 0.15689\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0153 - acc: 0.9967 - val_loss: 0.2133 - val_acc: 0.9418\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9906\n",
      "Epoch 00029: val_loss improved from 0.15689 to 0.15645, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/029-0.1565.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0322 - acc: 0.9906 - val_loss: 0.1565 - val_acc: 0.9574\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9994\n",
      "Epoch 00030: val_loss improved from 0.15645 to 0.15331, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/030-0.1533.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0060 - acc: 0.9994 - val_loss: 0.1533 - val_acc: 0.9604\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9993\n",
      "Epoch 00031: val_loss did not improve from 0.15331\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0056 - acc: 0.9993 - val_loss: 0.1734 - val_acc: 0.9562\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9964\n",
      "Epoch 00032: val_loss did not improve from 0.15331\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0154 - acc: 0.9964 - val_loss: 0.2436 - val_acc: 0.9378\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9928\n",
      "Epoch 00033: val_loss did not improve from 0.15331\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0259 - acc: 0.9928 - val_loss: 0.1854 - val_acc: 0.9499\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9993\n",
      "Epoch 00034: val_loss did not improve from 0.15331\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0061 - acc: 0.9992 - val_loss: 0.1886 - val_acc: 0.9511\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9954\n",
      "Epoch 00035: val_loss did not improve from 0.15331\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0187 - acc: 0.9954 - val_loss: 0.1892 - val_acc: 0.9506\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9957\n",
      "Epoch 00036: val_loss did not improve from 0.15331\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0172 - acc: 0.9957 - val_loss: 0.1785 - val_acc: 0.9564\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9995\n",
      "Epoch 00037: val_loss did not improve from 0.15331\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0045 - acc: 0.9995 - val_loss: 0.1663 - val_acc: 0.9592\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9996\n",
      "Epoch 00038: val_loss did not improve from 0.15331\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0039 - acc: 0.9996 - val_loss: 0.1920 - val_acc: 0.9562\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9983\n",
      "Epoch 00039: val_loss did not improve from 0.15331\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0080 - acc: 0.9983 - val_loss: 0.3290 - val_acc: 0.9210\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9907\n",
      "Epoch 00040: val_loss improved from 0.15331 to 0.15082, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_BN_8_conv_checkpoint/040-0.1508.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0320 - acc: 0.9907 - val_loss: 0.1508 - val_acc: 0.9616\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9994\n",
      "Epoch 00041: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0048 - acc: 0.9994 - val_loss: 0.1600 - val_acc: 0.9590\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9996\n",
      "Epoch 00042: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0036 - acc: 0.9996 - val_loss: 0.2029 - val_acc: 0.9506\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9947\n",
      "Epoch 00043: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0184 - acc: 0.9947 - val_loss: 0.1914 - val_acc: 0.9502\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9993\n",
      "Epoch 00044: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.1717 - val_acc: 0.9548\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9968\n",
      "Epoch 00045: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0135 - acc: 0.9967 - val_loss: 0.1973 - val_acc: 0.9515\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9965\n",
      "Epoch 00046: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0129 - acc: 0.9965 - val_loss: 0.2023 - val_acc: 0.9539\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9928\n",
      "Epoch 00047: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0251 - acc: 0.9928 - val_loss: 0.1800 - val_acc: 0.9532\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9992\n",
      "Epoch 00048: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0045 - acc: 0.9992 - val_loss: 0.1827 - val_acc: 0.9550\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 00049: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0029 - acc: 0.9997 - val_loss: 0.1776 - val_acc: 0.9571\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00050: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0043 - acc: 0.9992 - val_loss: 0.2283 - val_acc: 0.9467\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9907\n",
      "Epoch 00051: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0298 - acc: 0.9907 - val_loss: 0.1658 - val_acc: 0.9597\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9995\n",
      "Epoch 00052: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0037 - acc: 0.9995 - val_loss: 0.1576 - val_acc: 0.9606\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9997\n",
      "Epoch 00053: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0031 - acc: 0.9997 - val_loss: 0.1641 - val_acc: 0.9606\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 00054: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0020 - acc: 0.9999 - val_loss: 0.1700 - val_acc: 0.9620\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9964\n",
      "Epoch 00055: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0130 - acc: 0.9964 - val_loss: 0.2286 - val_acc: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9988\n",
      "Epoch 00056: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0059 - acc: 0.9988 - val_loss: 0.1746 - val_acc: 0.9595\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00057: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0040 - acc: 0.9991 - val_loss: 0.1918 - val_acc: 0.9567\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9987\n",
      "Epoch 00058: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0053 - acc: 0.9987 - val_loss: 0.2714 - val_acc: 0.9404\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9987\n",
      "Epoch 00059: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0062 - acc: 0.9987 - val_loss: 0.1969 - val_acc: 0.9564\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 00060: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0046 - acc: 0.9991 - val_loss: 0.2707 - val_acc: 0.9352\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9966\n",
      "Epoch 00061: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0122 - acc: 0.9966 - val_loss: 0.1924 - val_acc: 0.9550\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9992\n",
      "Epoch 00062: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0044 - acc: 0.9992 - val_loss: 0.1700 - val_acc: 0.9620\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9995\n",
      "Epoch 00063: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0032 - acc: 0.9995 - val_loss: 0.2281 - val_acc: 0.9464\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9986\n",
      "Epoch 00064: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.2031 - val_acc: 0.9564\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9956\n",
      "Epoch 00065: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0136 - acc: 0.9956 - val_loss: 0.1746 - val_acc: 0.9583\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9948\n",
      "Epoch 00066: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0164 - acc: 0.9948 - val_loss: 0.1757 - val_acc: 0.9592\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9995\n",
      "Epoch 00067: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0033 - acc: 0.9995 - val_loss: 0.1656 - val_acc: 0.9604\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9979\n",
      "Epoch 00068: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0081 - acc: 0.9979 - val_loss: 0.1809 - val_acc: 0.9574\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9989\n",
      "Epoch 00069: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0052 - acc: 0.9989 - val_loss: 0.1780 - val_acc: 0.9604\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00070: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1929 - val_acc: 0.9574\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 00071: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1818 - val_acc: 0.9599\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9967\n",
      "Epoch 00072: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0110 - acc: 0.9967 - val_loss: 0.2199 - val_acc: 0.9504\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9989\n",
      "Epoch 00073: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0055 - acc: 0.9988 - val_loss: 0.2030 - val_acc: 0.9518\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9947\n",
      "Epoch 00074: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0176 - acc: 0.9947 - val_loss: 0.1722 - val_acc: 0.9602\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9995\n",
      "Epoch 00075: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0030 - acc: 0.9995 - val_loss: 0.1863 - val_acc: 0.9590\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9956\n",
      "Epoch 00076: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0155 - acc: 0.9956 - val_loss: 0.1767 - val_acc: 0.9583\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9991\n",
      "Epoch 00077: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0050 - acc: 0.9991 - val_loss: 0.1889 - val_acc: 0.9553\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 00078: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0084 - acc: 0.9977 - val_loss: 0.1784 - val_acc: 0.9599\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9997\n",
      "Epoch 00079: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1773 - val_acc: 0.9620\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9988\n",
      "Epoch 00080: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0048 - acc: 0.9988 - val_loss: 0.1930 - val_acc: 0.9560\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00081: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0035 - acc: 0.9992 - val_loss: 0.2244 - val_acc: 0.9541\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9962\n",
      "Epoch 00082: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0131 - acc: 0.9963 - val_loss: 0.1813 - val_acc: 0.9583\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00083: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1801 - val_acc: 0.9578\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00084: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0030 - acc: 0.9994 - val_loss: 0.1886 - val_acc: 0.9574\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9977\n",
      "Epoch 00085: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0086 - acc: 0.9977 - val_loss: 0.1910 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9978\n",
      "Epoch 00086: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0088 - acc: 0.9978 - val_loss: 0.1798 - val_acc: 0.9623\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 00087: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1908 - val_acc: 0.9585\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9983\n",
      "Epoch 00088: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0068 - acc: 0.9983 - val_loss: 0.1826 - val_acc: 0.9599\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 00089: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.2058 - val_acc: 0.9578\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9982\n",
      "Epoch 00090: val_loss did not improve from 0.15082\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0068 - acc: 0.9982 - val_loss: 0.1736 - val_acc: 0.9592\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 764us/sample - loss: 0.2088 - acc: 0.9433\n",
      "Loss: 0.20877599846784448 Accuracy: 0.94330215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GMP_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 128)          512         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,176\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 713us/sample - loss: 0.5636 - acc: 0.8258\n",
      "Loss: 0.5635875388221702 Accuracy: 0.82575285\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 128)          512         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2064        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 65,616\n",
      "Trainable params: 64,848\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 717us/sample - loss: 0.4251 - acc: 0.8654\n",
      "Loss: 0.4251129043003233 Accuracy: 0.8654206\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 192)          768         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           3088        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 108,496\n",
      "Trainable params: 107,344\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 745us/sample - loss: 0.2825 - acc: 0.9221\n",
      "Loss: 0.28251868444315986 Accuracy: 0.92211837\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 256)          1024        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           4112        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 192,336\n",
      "Trainable params: 190,800\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 791us/sample - loss: 0.1879 - acc: 0.9493\n",
      "Loss: 0.1879012505151525 Accuracy: 0.949325\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 256)          1024        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           4112        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 274,896\n",
      "Trainable params: 273,104\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 785us/sample - loss: 0.1756 - acc: 0.9462\n",
      "Loss: 0.17561202078655377 Accuracy: 0.9462098\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 256)          1024        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4112        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 357,456\n",
      "Trainable params: 355,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 818us/sample - loss: 0.2088 - acc: 0.9433\n",
      "Loss: 0.20877599846784448 Accuracy: 0.94330215\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GMP_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 128)          512         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,176\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 798us/sample - loss: 0.6854 - acc: 0.8102\n",
      "Loss: 0.6853611973587226 Accuracy: 0.81017655\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 128)          512         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2064        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 65,616\n",
      "Trainable params: 64,848\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 865us/sample - loss: 0.5342 - acc: 0.8669\n",
      "Loss: 0.5341947954020396 Accuracy: 0.86687434\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 192)          768         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           3088        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 108,496\n",
      "Trainable params: 107,344\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 878us/sample - loss: 0.3312 - acc: 0.9225\n",
      "Loss: 0.3312351103636087 Accuracy: 0.92253375\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 256)          1024        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           4112        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 192,336\n",
      "Trainable params: 190,800\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 903us/sample - loss: 0.2020 - acc: 0.9522\n",
      "Loss: 0.20200930233398764 Accuracy: 0.9522326\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 256)          1024        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           4112        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 274,896\n",
      "Trainable params: 273,104\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1872 - acc: 0.9468\n",
      "Loss: 0.1872489244012725 Accuracy: 0.94683284\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 256)          1024        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4112        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 357,456\n",
      "Trainable params: 355,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 971us/sample - loss: 0.2142 - acc: 0.9472\n",
      "Loss: 0.21416281622748432 Accuracy: 0.94724816\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_BN_2'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
