{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, \n",
    "                      padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 1024000)           4096000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 20,493,136\n",
      "Trainable params: 18,444,880\n",
      "Non-trainable params: 2,048,256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 341312)            1365248   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 6,864,592\n",
      "Trainable params: 6,181,456\n",
      "Non-trainable params: 683,136\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 113728)            454912    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 2,338,128\n",
      "Trainable params: 2,109,904\n",
      "Non-trainable params: 228,224\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 37888)             151552    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 846,544\n",
      "Trainable params: 769,744\n",
      "Non-trainable params: 76,800\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 25216)             100864    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 668,112\n",
      "Trainable params: 616,144\n",
      "Non-trainable params: 51,968\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 8320)              33280     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 429,776\n",
      "Trainable params: 411,088\n",
      "Non-trainable params: 18,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 2688)              10752     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 416,720\n",
      "Trainable params: 408,784\n",
      "Non-trainable params: 7,936\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 480,464\n",
      "Trainable params: 475,600\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 770,256\n",
      "Trainable params: 765,136\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4693 - acc: 0.5404\n",
      "Epoch 00001: val_loss improved from inf to 1.04390, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/001-1.0439.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 1.4693 - acc: 0.5404 - val_loss: 1.0439 - val_acc: 0.6648\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5755 - acc: 0.8183\n",
      "Epoch 00002: val_loss improved from 1.04390 to 0.40446, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/002-0.4045.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.5755 - acc: 0.8183 - val_loss: 0.4045 - val_acc: 0.8786\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.8780\n",
      "Epoch 00003: val_loss did not improve from 0.40446\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.3826 - acc: 0.8780 - val_loss: 0.4935 - val_acc: 0.8430\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9115\n",
      "Epoch 00004: val_loss improved from 0.40446 to 0.29747, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/004-0.2975.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2844 - acc: 0.9115 - val_loss: 0.2975 - val_acc: 0.9082\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9286\n",
      "Epoch 00005: val_loss improved from 0.29747 to 0.26891, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/005-0.2689.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.2288 - acc: 0.9285 - val_loss: 0.2689 - val_acc: 0.9168\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9401\n",
      "Epoch 00006: val_loss did not improve from 0.26891\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1938 - acc: 0.9401 - val_loss: 0.2810 - val_acc: 0.9143\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9464\n",
      "Epoch 00007: val_loss improved from 0.26891 to 0.21082, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/007-0.2108.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1704 - acc: 0.9464 - val_loss: 0.2108 - val_acc: 0.9373\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9569\n",
      "Epoch 00008: val_loss did not improve from 0.21082\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1373 - acc: 0.9569 - val_loss: 0.2298 - val_acc: 0.9313\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9637\n",
      "Epoch 00009: val_loss did not improve from 0.21082\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1178 - acc: 0.9637 - val_loss: 0.2132 - val_acc: 0.9359\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9649\n",
      "Epoch 00010: val_loss improved from 0.21082 to 0.17836, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/010-0.1784.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1093 - acc: 0.9649 - val_loss: 0.1784 - val_acc: 0.9427\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9732\n",
      "Epoch 00011: val_loss did not improve from 0.17836\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0873 - acc: 0.9731 - val_loss: 0.2093 - val_acc: 0.9362\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9706\n",
      "Epoch 00012: val_loss did not improve from 0.17836\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0934 - acc: 0.9706 - val_loss: 0.1924 - val_acc: 0.9427\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9782\n",
      "Epoch 00013: val_loss improved from 0.17836 to 0.17534, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/013-0.1753.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0726 - acc: 0.9782 - val_loss: 0.1753 - val_acc: 0.9471\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9764\n",
      "Epoch 00014: val_loss did not improve from 0.17534\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0756 - acc: 0.9764 - val_loss: 0.1914 - val_acc: 0.9422\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9845\n",
      "Epoch 00015: val_loss improved from 0.17534 to 0.16847, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/015-0.1685.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0539 - acc: 0.9845 - val_loss: 0.1685 - val_acc: 0.9492\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9833\n",
      "Epoch 00016: val_loss did not improve from 0.16847\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0553 - acc: 0.9833 - val_loss: 0.1760 - val_acc: 0.9439\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9811\n",
      "Epoch 00017: val_loss improved from 0.16847 to 0.16288, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/017-0.1629.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0596 - acc: 0.9811 - val_loss: 0.1629 - val_acc: 0.9527\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9874\n",
      "Epoch 00018: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0473 - acc: 0.9874 - val_loss: 0.1804 - val_acc: 0.9455\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9911\n",
      "Epoch 00019: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0325 - acc: 0.9911 - val_loss: 0.1885 - val_acc: 0.9448\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9902\n",
      "Epoch 00020: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0359 - acc: 0.9902 - val_loss: 0.1790 - val_acc: 0.9467\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9902\n",
      "Epoch 00021: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0328 - acc: 0.9902 - val_loss: 0.2108 - val_acc: 0.9406\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9927\n",
      "Epoch 00022: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0284 - acc: 0.9927 - val_loss: 0.2413 - val_acc: 0.9404\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9918\n",
      "Epoch 00023: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0295 - acc: 0.9918 - val_loss: 0.1887 - val_acc: 0.9481\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9895\n",
      "Epoch 00024: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0347 - acc: 0.9895 - val_loss: 0.1712 - val_acc: 0.9529\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9945\n",
      "Epoch 00025: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0207 - acc: 0.9945 - val_loss: 0.1738 - val_acc: 0.9499\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9940\n",
      "Epoch 00026: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0219 - acc: 0.9940 - val_loss: 0.2014 - val_acc: 0.9462\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9943\n",
      "Epoch 00027: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0209 - acc: 0.9943 - val_loss: 0.1907 - val_acc: 0.9506\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9938\n",
      "Epoch 00028: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0216 - acc: 0.9938 - val_loss: 0.2370 - val_acc: 0.9357\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9886\n",
      "Epoch 00029: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0375 - acc: 0.9886 - val_loss: 0.1641 - val_acc: 0.9543\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9971\n",
      "Epoch 00030: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0126 - acc: 0.9971 - val_loss: 0.1807 - val_acc: 0.9502\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9911\n",
      "Epoch 00031: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0300 - acc: 0.9911 - val_loss: 0.1709 - val_acc: 0.9534\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9966\n",
      "Epoch 00032: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0129 - acc: 0.9966 - val_loss: 0.1720 - val_acc: 0.9555\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9953\n",
      "Epoch 00033: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0179 - acc: 0.9952 - val_loss: 0.1809 - val_acc: 0.9536\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9929\n",
      "Epoch 00034: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0249 - acc: 0.9929 - val_loss: 0.1776 - val_acc: 0.9536\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9978\n",
      "Epoch 00035: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0093 - acc: 0.9978 - val_loss: 0.1726 - val_acc: 0.9555\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9982\n",
      "Epoch 00036: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0082 - acc: 0.9982 - val_loss: 0.1934 - val_acc: 0.9550\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9915\n",
      "Epoch 00037: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0258 - acc: 0.9915 - val_loss: 0.2012 - val_acc: 0.9513\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9970\n",
      "Epoch 00038: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0115 - acc: 0.9970 - val_loss: 0.1820 - val_acc: 0.9536\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9974\n",
      "Epoch 00039: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0101 - acc: 0.9974 - val_loss: 0.2285 - val_acc: 0.9448\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9964\n",
      "Epoch 00040: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0135 - acc: 0.9964 - val_loss: 0.1926 - val_acc: 0.9534\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9962\n",
      "Epoch 00041: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0130 - acc: 0.9962 - val_loss: 0.2179 - val_acc: 0.9520\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9955\n",
      "Epoch 00042: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0146 - acc: 0.9955 - val_loss: 0.2451 - val_acc: 0.9469\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9975\n",
      "Epoch 00043: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0099 - acc: 0.9975 - val_loss: 0.1832 - val_acc: 0.9571\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9953\n",
      "Epoch 00044: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0160 - acc: 0.9953 - val_loss: 0.1729 - val_acc: 0.9553\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9956\n",
      "Epoch 00045: val_loss did not improve from 0.16288\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0145 - acc: 0.9956 - val_loss: 0.2078 - val_acc: 0.9478\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9948\n",
      "Epoch 00046: val_loss improved from 0.16288 to 0.15663, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/046-0.1566.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0165 - acc: 0.9948 - val_loss: 0.1566 - val_acc: 0.9595\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9981\n",
      "Epoch 00047: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0080 - acc: 0.9981 - val_loss: 0.1791 - val_acc: 0.9520\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9943\n",
      "Epoch 00048: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0188 - acc: 0.9943 - val_loss: 0.1767 - val_acc: 0.9569\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9949\n",
      "Epoch 00049: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0167 - acc: 0.9949 - val_loss: 0.1591 - val_acc: 0.9599\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9986\n",
      "Epoch 00050: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0063 - acc: 0.9986 - val_loss: 0.1848 - val_acc: 0.9557\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9988\n",
      "Epoch 00051: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0056 - acc: 0.9988 - val_loss: 0.1733 - val_acc: 0.9592\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9969\n",
      "Epoch 00052: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0120 - acc: 0.9968 - val_loss: 0.2013 - val_acc: 0.9527\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9947\n",
      "Epoch 00053: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0170 - acc: 0.9947 - val_loss: 0.1942 - val_acc: 0.9546\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9989\n",
      "Epoch 00054: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0051 - acc: 0.9989 - val_loss: 0.1662 - val_acc: 0.9581\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9957\n",
      "Epoch 00055: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.1703 - val_acc: 0.9567\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9977\n",
      "Epoch 00056: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0082 - acc: 0.9977 - val_loss: 0.1796 - val_acc: 0.9599\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9987\n",
      "Epoch 00057: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0051 - acc: 0.9988 - val_loss: 0.1899 - val_acc: 0.9548\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9983\n",
      "Epoch 00058: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0067 - acc: 0.9983 - val_loss: 0.2028 - val_acc: 0.9527\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9957\n",
      "Epoch 00059: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0138 - acc: 0.9957 - val_loss: 0.1962 - val_acc: 0.9536\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9949\n",
      "Epoch 00060: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0171 - acc: 0.9948 - val_loss: 0.1878 - val_acc: 0.9569\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9954\n",
      "Epoch 00061: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0149 - acc: 0.9954 - val_loss: 0.1618 - val_acc: 0.9623\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9994\n",
      "Epoch 00062: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0040 - acc: 0.9994 - val_loss: 0.1628 - val_acc: 0.9611\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9980\n",
      "Epoch 00063: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0075 - acc: 0.9980 - val_loss: 0.1762 - val_acc: 0.9606\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9967\n",
      "Epoch 00064: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0110 - acc: 0.9967 - val_loss: 0.1840 - val_acc: 0.9599\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986\n",
      "Epoch 00065: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.1657 - val_acc: 0.9616\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9983\n",
      "Epoch 00066: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0063 - acc: 0.9983 - val_loss: 0.2244 - val_acc: 0.9548\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9971\n",
      "Epoch 00067: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0091 - acc: 0.9971 - val_loss: 0.2220 - val_acc: 0.9513\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9974\n",
      "Epoch 00068: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0086 - acc: 0.9974 - val_loss: 0.1738 - val_acc: 0.9606\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9987\n",
      "Epoch 00069: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0051 - acc: 0.9988 - val_loss: 0.1970 - val_acc: 0.9555\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9968\n",
      "Epoch 00070: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0101 - acc: 0.9968 - val_loss: 0.1900 - val_acc: 0.9597\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9972\n",
      "Epoch 00071: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0089 - acc: 0.9972 - val_loss: 0.1976 - val_acc: 0.9543\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9977\n",
      "Epoch 00072: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0069 - acc: 0.9977 - val_loss: 0.2013 - val_acc: 0.9567\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9975\n",
      "Epoch 00073: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0080 - acc: 0.9975 - val_loss: 0.1974 - val_acc: 0.9585\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9990\n",
      "Epoch 00074: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0035 - acc: 0.9990 - val_loss: 0.2333 - val_acc: 0.9495\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9975\n",
      "Epoch 00075: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0086 - acc: 0.9975 - val_loss: 0.2012 - val_acc: 0.9550\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 00076: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0078 - acc: 0.9977 - val_loss: 0.1931 - val_acc: 0.9574\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9965\n",
      "Epoch 00077: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0104 - acc: 0.9965 - val_loss: 0.1750 - val_acc: 0.9588\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00078: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0037 - acc: 0.9993 - val_loss: 0.1780 - val_acc: 0.9627\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00079: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0032 - acc: 0.9991 - val_loss: 0.1977 - val_acc: 0.9597\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9946\n",
      "Epoch 00080: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0169 - acc: 0.9946 - val_loss: 0.2024 - val_acc: 0.9550\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00081: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0032 - acc: 0.9992 - val_loss: 0.1815 - val_acc: 0.9602\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 00082: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1823 - val_acc: 0.9602\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9967\n",
      "Epoch 00083: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0110 - acc: 0.9967 - val_loss: 0.1905 - val_acc: 0.9569\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00084: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0033 - acc: 0.9992 - val_loss: 0.1880 - val_acc: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9961\n",
      "Epoch 00085: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0133 - acc: 0.9960 - val_loss: 0.1751 - val_acc: 0.9616\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979\n",
      "Epoch 00086: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0080 - acc: 0.9979 - val_loss: 0.1749 - val_acc: 0.9620\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 00087: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1866 - val_acc: 0.9592\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00088: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0027 - acc: 0.9992 - val_loss: 0.1642 - val_acc: 0.9644\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9958\n",
      "Epoch 00089: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0129 - acc: 0.9958 - val_loss: 0.1849 - val_acc: 0.9597\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9968\n",
      "Epoch 00090: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0118 - acc: 0.9968 - val_loss: 0.1781 - val_acc: 0.9630\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 00091: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1734 - val_acc: 0.9639\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00092: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1893 - val_acc: 0.9616\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00093: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1905 - val_acc: 0.9630\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9982\n",
      "Epoch 00094: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0063 - acc: 0.9982 - val_loss: 0.1875 - val_acc: 0.9560\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 00095: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0044 - acc: 0.9988 - val_loss: 0.1946 - val_acc: 0.9562\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9969\n",
      "Epoch 00096: val_loss did not improve from 0.15663\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0104 - acc: 0.9969 - val_loss: 0.1751 - val_acc: 0.9637\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.2128 - acc: 0.9493\n",
      "Loss: 0.2127867239213095 Accuracy: 0.949325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model_name = '1D_CNN_custom_conv_3_VGG_BN_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 1024000)           4096000   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 20,493,136\n",
      "Trainable params: 18,444,880\n",
      "Non-trainable params: 2,048,256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 893us/sample - loss: 9.6000 - acc: 0.3674\n",
      "Loss: 9.600019411085055 Accuracy: 0.36739355\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 341312)            1365248   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 6,864,592\n",
      "Trainable params: 6,181,456\n",
      "Non-trainable params: 683,136\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 3.7139 - acc: 0.3668\n",
      "Loss: 3.713918998110208 Accuracy: 0.3667705\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 113728)            454912    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 2,338,128\n",
      "Trainable params: 2,109,904\n",
      "Non-trainable params: 228,224\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 2.0577 - acc: 0.4933\n",
      "Loss: 2.0576771995112657 Accuracy: 0.49325025\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 37888)             151552    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 846,544\n",
      "Trainable params: 769,744\n",
      "Non-trainable params: 76,800\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 2ms/sample - loss: 1.2689 - acc: 0.6521\n",
      "Loss: 1.268929294584201 Accuracy: 0.65212876\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 25216)             100864    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 668,112\n",
      "Trainable params: 616,144\n",
      "Non-trainable params: 51,968\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.9899 - acc: 0.7047\n",
      "Loss: 0.9898805419851563 Accuracy: 0.7046729\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 8320)              33280     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 429,776\n",
      "Trainable params: 411,088\n",
      "Non-trainable params: 18,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.6074 - acc: 0.8349\n",
      "Loss: 0.6074317322589403 Accuracy: 0.83489096\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 2688)              10752     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 416,720\n",
      "Trainable params: 408,784\n",
      "Non-trainable params: 7,936\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.3032 - acc: 0.9286\n",
      "Loss: 0.3031660705959562 Accuracy: 0.9285566\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 480,464\n",
      "Trainable params: 475,600\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2028 - acc: 0.9423\n",
      "Loss: 0.2028081684592852 Accuracy: 0.9422638\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 770,256\n",
      "Trainable params: 765,136\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2128 - acc: 0.9493\n",
      "Loss: 0.2127867239213095 Accuracy: 0.949325\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.6790 - acc: 0.2589\n",
      "Epoch 00001: val_loss improved from inf to 11.60893, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_1_conv_checkpoint/001-11.6089.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 10.6797 - acc: 0.2588 - val_loss: 11.6089 - val_acc: 0.2197\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.1265 - acc: 0.3773\n",
      "Epoch 00002: val_loss improved from 11.60893 to 10.65462, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_1_conv_checkpoint/002-10.6546.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 9.1264 - acc: 0.3773 - val_loss: 10.6546 - val_acc: 0.2830\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.1076 - acc: 0.4510\n",
      "Epoch 00003: val_loss improved from 10.65462 to 9.58048, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_1_conv_checkpoint/003-9.5805.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 8.1078 - acc: 0.4509 - val_loss: 9.5805 - val_acc: 0.3513\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.3375 - acc: 0.5049\n",
      "Epoch 00004: val_loss did not improve from 9.58048\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 7.3380 - acc: 0.5049 - val_loss: 10.0138 - val_acc: 0.3326\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.9323 - acc: 0.5326\n",
      "Epoch 00005: val_loss did not improve from 9.58048\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 6.9326 - acc: 0.5326 - val_loss: 10.1718 - val_acc: 0.3152\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.4530 - acc: 0.5682\n",
      "Epoch 00006: val_loss did not improve from 9.58048\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 6.4536 - acc: 0.5681 - val_loss: 9.8482 - val_acc: 0.3429\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.0470 - acc: 0.5965\n",
      "Epoch 00007: val_loss did not improve from 9.58048\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 6.0479 - acc: 0.5964 - val_loss: 10.7675 - val_acc: 0.2944\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.8702 - acc: 0.6097\n",
      "Epoch 00008: val_loss did not improve from 9.58048\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 5.8711 - acc: 0.6096 - val_loss: 10.3800 - val_acc: 0.3226\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.5962 - acc: 0.6279\n",
      "Epoch 00009: val_loss did not improve from 9.58048\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 5.5959 - acc: 0.6279 - val_loss: 10.0390 - val_acc: 0.3364\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.3387 - acc: 0.6453\n",
      "Epoch 00010: val_loss improved from 9.58048 to 9.55441, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_1_conv_checkpoint/010-9.5544.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 5.3399 - acc: 0.6452 - val_loss: 9.5544 - val_acc: 0.3683\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.1416 - acc: 0.6604\n",
      "Epoch 00011: val_loss did not improve from 9.55441\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 5.1431 - acc: 0.6603 - val_loss: 10.4440 - val_acc: 0.3196\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9667 - acc: 0.6722\n",
      "Epoch 00012: val_loss did not improve from 9.55441\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 4.9673 - acc: 0.6722 - val_loss: 9.8461 - val_acc: 0.3517\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.7950 - acc: 0.6850\n",
      "Epoch 00013: val_loss did not improve from 9.55441\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 4.7961 - acc: 0.6849 - val_loss: 10.4282 - val_acc: 0.3233\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.6676 - acc: 0.6931\n",
      "Epoch 00014: val_loss did not improve from 9.55441\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 4.6674 - acc: 0.6931 - val_loss: 9.8602 - val_acc: 0.3485\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.5149 - acc: 0.7033\n",
      "Epoch 00015: val_loss did not improve from 9.55441\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 4.5153 - acc: 0.7033 - val_loss: 10.4270 - val_acc: 0.3242\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.4579 - acc: 0.7078\n",
      "Epoch 00016: val_loss did not improve from 9.55441\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 4.4591 - acc: 0.7077 - val_loss: 9.9316 - val_acc: 0.3506\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.2953 - acc: 0.7191\n",
      "Epoch 00017: val_loss did not improve from 9.55441\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 4.2960 - acc: 0.7191 - val_loss: 10.7373 - val_acc: 0.3112\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.2102 - acc: 0.7254\n",
      "Epoch 00018: val_loss did not improve from 9.55441\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 4.2097 - acc: 0.7254 - val_loss: 10.3573 - val_acc: 0.3219\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.1715 - acc: 0.7275\n",
      "Epoch 00019: val_loss improved from 9.55441 to 9.49746, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_1_conv_checkpoint/019-9.4975.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 4.1718 - acc: 0.7275 - val_loss: 9.4975 - val_acc: 0.3680\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.0439 - acc: 0.7371\n",
      "Epoch 00020: val_loss did not improve from 9.49746\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 4.0446 - acc: 0.7370 - val_loss: 10.0106 - val_acc: 0.3452\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.9863 - acc: 0.7403\n",
      "Epoch 00021: val_loss did not improve from 9.49746\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.9875 - acc: 0.7402 - val_loss: 11.2568 - val_acc: 0.2809\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.9909 - acc: 0.7405\n",
      "Epoch 00022: val_loss did not improve from 9.49746\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.9911 - acc: 0.7404 - val_loss: 10.4282 - val_acc: 0.3273\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.8897 - acc: 0.7475\n",
      "Epoch 00023: val_loss did not improve from 9.49746\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.8896 - acc: 0.7475 - val_loss: 10.1591 - val_acc: 0.3340\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.7846 - acc: 0.7553\n",
      "Epoch 00024: val_loss improved from 9.49746 to 9.32690, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_1_conv_checkpoint/024-9.3269.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.7854 - acc: 0.7552 - val_loss: 9.3269 - val_acc: 0.3864\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.7527 - acc: 0.7564\n",
      "Epoch 00025: val_loss did not improve from 9.32690\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.7531 - acc: 0.7564 - val_loss: 10.9455 - val_acc: 0.3051\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.7107 - acc: 0.7598\n",
      "Epoch 00026: val_loss did not improve from 9.32690\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.7111 - acc: 0.7598 - val_loss: 10.1618 - val_acc: 0.3389\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.6144 - acc: 0.7670\n",
      "Epoch 00027: val_loss did not improve from 9.32690\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.6152 - acc: 0.7669 - val_loss: 10.7640 - val_acc: 0.3103\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.6188 - acc: 0.7656\n",
      "Epoch 00028: val_loss did not improve from 9.32690\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.6188 - acc: 0.7656 - val_loss: 10.4060 - val_acc: 0.3333\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.5398 - acc: 0.7721\n",
      "Epoch 00029: val_loss did not improve from 9.32690\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.5398 - acc: 0.7721 - val_loss: 9.4816 - val_acc: 0.3806\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4927 - acc: 0.7748\n",
      "Epoch 00030: val_loss did not improve from 9.32690\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.4931 - acc: 0.7748 - val_loss: 10.1892 - val_acc: 0.3475\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3892 - acc: 0.7819\n",
      "Epoch 00031: val_loss did not improve from 9.32690\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.3892 - acc: 0.7819 - val_loss: 9.5890 - val_acc: 0.3776\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4182 - acc: 0.7804\n",
      "Epoch 00032: val_loss did not improve from 9.32690\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.4184 - acc: 0.7804 - val_loss: 9.3797 - val_acc: 0.3864\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4359 - acc: 0.7785\n",
      "Epoch 00033: val_loss improved from 9.32690 to 9.24055, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_1_conv_checkpoint/033-9.2405.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.4372 - acc: 0.7784 - val_loss: 9.2405 - val_acc: 0.3932\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3326 - acc: 0.7854\n",
      "Epoch 00034: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.3330 - acc: 0.7853 - val_loss: 10.2705 - val_acc: 0.3382\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2613 - acc: 0.7915\n",
      "Epoch 00035: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.2618 - acc: 0.7914 - val_loss: 10.1023 - val_acc: 0.3480\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2601 - acc: 0.7908\n",
      "Epoch 00036: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.2605 - acc: 0.7908 - val_loss: 11.6788 - val_acc: 0.2553\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2283 - acc: 0.7934\n",
      "Epoch 00037: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.2283 - acc: 0.7934 - val_loss: 9.7014 - val_acc: 0.3701\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2219 - acc: 0.7939\n",
      "Epoch 00038: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.2224 - acc: 0.7939 - val_loss: 11.0275 - val_acc: 0.2975\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1979 - acc: 0.7951\n",
      "Epoch 00039: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.1988 - acc: 0.7951 - val_loss: 10.7726 - val_acc: 0.3096\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2111 - acc: 0.7943\n",
      "Epoch 00040: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.2125 - acc: 0.7942 - val_loss: 10.1497 - val_acc: 0.3431\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1479 - acc: 0.7977\n",
      "Epoch 00041: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.1479 - acc: 0.7977 - val_loss: 10.0817 - val_acc: 0.3454\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0788 - acc: 0.8033\n",
      "Epoch 00042: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.0784 - acc: 0.8033 - val_loss: 10.3409 - val_acc: 0.3329\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1044 - acc: 0.8016\n",
      "Epoch 00043: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.1052 - acc: 0.8015 - val_loss: 10.1783 - val_acc: 0.3429\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0674 - acc: 0.8044\n",
      "Epoch 00044: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.0670 - acc: 0.8044 - val_loss: 10.4717 - val_acc: 0.3280\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0383 - acc: 0.8062\n",
      "Epoch 00045: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.0387 - acc: 0.8062 - val_loss: 10.7293 - val_acc: 0.3133\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0387 - acc: 0.8060\n",
      "Epoch 00046: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.0401 - acc: 0.8059 - val_loss: 11.5639 - val_acc: 0.2728\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0409 - acc: 0.8057\n",
      "Epoch 00047: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.0414 - acc: 0.8057 - val_loss: 10.4065 - val_acc: 0.3340\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9991 - acc: 0.8083\n",
      "Epoch 00048: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.0000 - acc: 0.8083 - val_loss: 10.5068 - val_acc: 0.3280\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9520 - acc: 0.8117\n",
      "Epoch 00049: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.9525 - acc: 0.8116 - val_loss: 11.0735 - val_acc: 0.2916\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9152 - acc: 0.8140\n",
      "Epoch 00050: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.9161 - acc: 0.8139 - val_loss: 9.4029 - val_acc: 0.3892\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9270 - acc: 0.8130\n",
      "Epoch 00051: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.9270 - acc: 0.8130 - val_loss: 10.5039 - val_acc: 0.3275\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9019 - acc: 0.8157\n",
      "Epoch 00052: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.9020 - acc: 0.8157 - val_loss: 10.9619 - val_acc: 0.3063\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8701 - acc: 0.8171\n",
      "Epoch 00053: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.8697 - acc: 0.8171 - val_loss: 9.6851 - val_acc: 0.3715\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8328 - acc: 0.8200\n",
      "Epoch 00054: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.8333 - acc: 0.8199 - val_loss: 11.4389 - val_acc: 0.2751\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8479 - acc: 0.8189\n",
      "Epoch 00055: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.8479 - acc: 0.8189 - val_loss: 9.5369 - val_acc: 0.3836\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8353 - acc: 0.8191\n",
      "Epoch 00056: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.8358 - acc: 0.8190 - val_loss: 10.2233 - val_acc: 0.3454\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7898 - acc: 0.8228\n",
      "Epoch 00057: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.7907 - acc: 0.8228 - val_loss: 9.2487 - val_acc: 0.3986\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7991 - acc: 0.8218\n",
      "Epoch 00058: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.7992 - acc: 0.8218 - val_loss: 9.3610 - val_acc: 0.3916\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7599 - acc: 0.8244\n",
      "Epoch 00059: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.7597 - acc: 0.8244 - val_loss: 10.5326 - val_acc: 0.3247\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7722 - acc: 0.8235\n",
      "Epoch 00060: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.7731 - acc: 0.8234 - val_loss: 10.3407 - val_acc: 0.3382\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7233 - acc: 0.8273\n",
      "Epoch 00061: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.7247 - acc: 0.8272 - val_loss: 11.5133 - val_acc: 0.2739\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6990 - acc: 0.8288\n",
      "Epoch 00062: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.6989 - acc: 0.8288 - val_loss: 9.4789 - val_acc: 0.3890\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7643 - acc: 0.8243\n",
      "Epoch 00063: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.7652 - acc: 0.8243 - val_loss: 10.1229 - val_acc: 0.3457\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6934 - acc: 0.8292\n",
      "Epoch 00064: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.6935 - acc: 0.8292 - val_loss: 10.5914 - val_acc: 0.3266\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7124 - acc: 0.8282\n",
      "Epoch 00065: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.7120 - acc: 0.8283 - val_loss: 10.4170 - val_acc: 0.3301\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7016 - acc: 0.8285\n",
      "Epoch 00066: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.7029 - acc: 0.8284 - val_loss: 10.2561 - val_acc: 0.3399\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7203 - acc: 0.8265\n",
      "Epoch 00067: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.7205 - acc: 0.8265 - val_loss: 12.4164 - val_acc: 0.2229\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6760 - acc: 0.8295\n",
      "Epoch 00068: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.6765 - acc: 0.8294 - val_loss: 12.0523 - val_acc: 0.2446\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6765 - acc: 0.8304\n",
      "Epoch 00069: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.6775 - acc: 0.8303 - val_loss: 9.6436 - val_acc: 0.3778\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6566 - acc: 0.8315\n",
      "Epoch 00070: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.6572 - acc: 0.8314 - val_loss: 10.1099 - val_acc: 0.3510\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6222 - acc: 0.8344\n",
      "Epoch 00071: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.6223 - acc: 0.8344 - val_loss: 9.4302 - val_acc: 0.3904\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6053 - acc: 0.8353\n",
      "Epoch 00072: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.6054 - acc: 0.8353 - val_loss: 9.3293 - val_acc: 0.4000\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6058 - acc: 0.8353\n",
      "Epoch 00073: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.6057 - acc: 0.8353 - val_loss: 10.7090 - val_acc: 0.3205\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6077 - acc: 0.8349\n",
      "Epoch 00074: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.6091 - acc: 0.8348 - val_loss: 9.5759 - val_acc: 0.3811\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5998 - acc: 0.8354\n",
      "Epoch 00075: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.5994 - acc: 0.8354 - val_loss: 10.6046 - val_acc: 0.3263\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5921 - acc: 0.8359\n",
      "Epoch 00076: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.5926 - acc: 0.8358 - val_loss: 9.8126 - val_acc: 0.3669\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5810 - acc: 0.8367\n",
      "Epoch 00077: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.5811 - acc: 0.8367 - val_loss: 10.6049 - val_acc: 0.3259\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5850 - acc: 0.8368\n",
      "Epoch 00078: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.5851 - acc: 0.8368 - val_loss: 9.6629 - val_acc: 0.3771\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5778 - acc: 0.8373\n",
      "Epoch 00079: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.5778 - acc: 0.8373 - val_loss: 9.3845 - val_acc: 0.3955\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5543 - acc: 0.8388\n",
      "Epoch 00080: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.5544 - acc: 0.8388 - val_loss: 11.2681 - val_acc: 0.2872\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5135 - acc: 0.8413\n",
      "Epoch 00081: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.5136 - acc: 0.8413 - val_loss: 11.0229 - val_acc: 0.2982\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5375 - acc: 0.8397\n",
      "Epoch 00082: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.5376 - acc: 0.8397 - val_loss: 12.1551 - val_acc: 0.2374\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5257 - acc: 0.8405\n",
      "Epoch 00083: val_loss did not improve from 9.24055\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.5258 - acc: 0.8405 - val_loss: 9.8835 - val_acc: 0.3671\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 830us/sample - loss: 9.6762 - acc: 0.3686\n",
      "Loss: 9.676190913429142 Accuracy: 0.36863968\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.9252 - acc: 0.3487\n",
      "Epoch 00001: val_loss improved from inf to 3.59976, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_2_conv_checkpoint/001-3.5998.hdf5\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 3.9253 - acc: 0.3487 - val_loss: 3.5998 - val_acc: 0.2909\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5518 - acc: 0.7114\n",
      "Epoch 00002: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 1.5518 - acc: 0.7114 - val_loss: 3.7600 - val_acc: 0.4116\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7629 - acc: 0.8651\n",
      "Epoch 00003: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.7628 - acc: 0.8651 - val_loss: 4.4128 - val_acc: 0.3725\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.9161\n",
      "Epoch 00004: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.5308 - acc: 0.9161 - val_loss: 4.1187 - val_acc: 0.4130\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.9328\n",
      "Epoch 00005: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.4420 - acc: 0.9328 - val_loss: 4.6950 - val_acc: 0.3941\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.9352\n",
      "Epoch 00006: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.4219 - acc: 0.9351 - val_loss: 4.7663 - val_acc: 0.4093\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4485 - acc: 0.9268\n",
      "Epoch 00007: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.4489 - acc: 0.9268 - val_loss: 5.9759 - val_acc: 0.3496\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.9461\n",
      "Epoch 00008: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3672 - acc: 0.9460 - val_loss: 5.9925 - val_acc: 0.3618\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.9408\n",
      "Epoch 00009: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3984 - acc: 0.9407 - val_loss: 5.7554 - val_acc: 0.3846\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.9486\n",
      "Epoch 00010: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3637 - acc: 0.9485 - val_loss: 6.2107 - val_acc: 0.3755\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.9529\n",
      "Epoch 00011: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3383 - acc: 0.9529 - val_loss: 5.7931 - val_acc: 0.4151\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.9563\n",
      "Epoch 00012: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3268 - acc: 0.9562 - val_loss: 6.1039 - val_acc: 0.4020\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.9560\n",
      "Epoch 00013: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3305 - acc: 0.9560 - val_loss: 6.6044 - val_acc: 0.3615\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3123 - acc: 0.9590\n",
      "Epoch 00014: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3127 - acc: 0.9590 - val_loss: 6.2766 - val_acc: 0.3920\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9648\n",
      "Epoch 00015: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2882 - acc: 0.9647 - val_loss: 6.1388 - val_acc: 0.4172\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3273 - acc: 0.9573\n",
      "Epoch 00016: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.3278 - acc: 0.9573 - val_loss: 6.8001 - val_acc: 0.3848\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9630\n",
      "Epoch 00017: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2918 - acc: 0.9630 - val_loss: 6.5496 - val_acc: 0.4179\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9657\n",
      "Epoch 00018: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2845 - acc: 0.9657 - val_loss: 6.3519 - val_acc: 0.4232\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9702\n",
      "Epoch 00019: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2545 - acc: 0.9702 - val_loss: 6.7723 - val_acc: 0.4034\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9665\n",
      "Epoch 00020: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2783 - acc: 0.9665 - val_loss: 6.8711 - val_acc: 0.4081\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9690\n",
      "Epoch 00021: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2738 - acc: 0.9690 - val_loss: 6.6543 - val_acc: 0.4263\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9701\n",
      "Epoch 00022: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2629 - acc: 0.9700 - val_loss: 6.5598 - val_acc: 0.4291\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9712\n",
      "Epoch 00023: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2621 - acc: 0.9712 - val_loss: 7.7453 - val_acc: 0.3701\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9718\n",
      "Epoch 00024: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2513 - acc: 0.9718 - val_loss: 6.6183 - val_acc: 0.4288\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9761\n",
      "Epoch 00025: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2256 - acc: 0.9760 - val_loss: 6.7884 - val_acc: 0.4312\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9747\n",
      "Epoch 00026: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2338 - acc: 0.9747 - val_loss: 7.1312 - val_acc: 0.4111\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9747\n",
      "Epoch 00027: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2374 - acc: 0.9747 - val_loss: 7.7245 - val_acc: 0.3867\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9723\n",
      "Epoch 00028: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2538 - acc: 0.9723 - val_loss: 7.5191 - val_acc: 0.3888\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9746\n",
      "Epoch 00029: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2378 - acc: 0.9746 - val_loss: 7.0785 - val_acc: 0.4214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9721\n",
      "Epoch 00030: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2595 - acc: 0.9722 - val_loss: 6.7688 - val_acc: 0.4309\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9790\n",
      "Epoch 00031: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2080 - acc: 0.9790 - val_loss: 6.7510 - val_acc: 0.4274\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9768\n",
      "Epoch 00032: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2318 - acc: 0.9768 - val_loss: 7.0581 - val_acc: 0.4195\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9745\n",
      "Epoch 00033: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2392 - acc: 0.9745 - val_loss: 6.9509 - val_acc: 0.4333\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9780\n",
      "Epoch 00034: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2183 - acc: 0.9780 - val_loss: 6.8346 - val_acc: 0.4428\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9807\n",
      "Epoch 00035: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2013 - acc: 0.9806 - val_loss: 7.2731 - val_acc: 0.4223\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9783\n",
      "Epoch 00036: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2219 - acc: 0.9782 - val_loss: 7.1963 - val_acc: 0.4132\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9816\n",
      "Epoch 00037: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.1971 - acc: 0.9816 - val_loss: 7.2968 - val_acc: 0.4216\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9778\n",
      "Epoch 00038: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2190 - acc: 0.9778 - val_loss: 8.0272 - val_acc: 0.3846\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9783\n",
      "Epoch 00039: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2213 - acc: 0.9783 - val_loss: 7.2483 - val_acc: 0.4207\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9755\n",
      "Epoch 00040: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2416 - acc: 0.9755 - val_loss: 7.7889 - val_acc: 0.3969\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9797\n",
      "Epoch 00041: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2030 - acc: 0.9797 - val_loss: 7.1067 - val_acc: 0.4410\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1886 - acc: 0.9824\n",
      "Epoch 00042: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.1894 - acc: 0.9823 - val_loss: 7.4241 - val_acc: 0.4205\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9789\n",
      "Epoch 00043: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2172 - acc: 0.9788 - val_loss: 7.3768 - val_acc: 0.4179\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9781\n",
      "Epoch 00044: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2253 - acc: 0.9780 - val_loss: 7.1742 - val_acc: 0.4344\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9799\n",
      "Epoch 00045: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2080 - acc: 0.9799 - val_loss: 7.3748 - val_acc: 0.4235\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9812\n",
      "Epoch 00046: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.1984 - acc: 0.9811 - val_loss: 7.1382 - val_acc: 0.4347\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9789\n",
      "Epoch 00047: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2125 - acc: 0.9788 - val_loss: 7.1075 - val_acc: 0.4403\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9800\n",
      "Epoch 00048: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2094 - acc: 0.9799 - val_loss: 7.2862 - val_acc: 0.4330\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9816\n",
      "Epoch 00049: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.1996 - acc: 0.9816 - val_loss: 7.5954 - val_acc: 0.4142\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9839\n",
      "Epoch 00050: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.1827 - acc: 0.9839 - val_loss: 7.5978 - val_acc: 0.4151\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9843\n",
      "Epoch 00051: val_loss did not improve from 3.59976\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.1766 - acc: 0.9843 - val_loss: 7.3044 - val_acc: 0.4305\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 3.7823 - acc: 0.2573\n",
      "Loss: 3.7823052178043195 Accuracy: 0.25732088\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2307 - acc: 0.4379\n",
      "Epoch 00001: val_loss improved from inf to 1.92702, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_3_conv_checkpoint/001-1.9270.hdf5\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 2.2305 - acc: 0.4379 - val_loss: 1.9270 - val_acc: 0.4503\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8173 - acc: 0.7748\n",
      "Epoch 00002: val_loss improved from 1.92702 to 1.84604, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_3_conv_checkpoint/002-1.8460.hdf5\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.8174 - acc: 0.7748 - val_loss: 1.8460 - val_acc: 0.5465\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.9113\n",
      "Epoch 00003: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.3447 - acc: 0.9112 - val_loss: 1.9581 - val_acc: 0.5409\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9654\n",
      "Epoch 00004: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1791 - acc: 0.9653 - val_loss: 1.8666 - val_acc: 0.5698\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9792\n",
      "Epoch 00005: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1276 - acc: 0.9792 - val_loss: 2.0567 - val_acc: 0.5577\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9832\n",
      "Epoch 00006: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1046 - acc: 0.9832 - val_loss: 2.0839 - val_acc: 0.5663\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9840\n",
      "Epoch 00007: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0935 - acc: 0.9841 - val_loss: 2.5390 - val_acc: 0.5092\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9824\n",
      "Epoch 00008: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1011 - acc: 0.9824 - val_loss: 2.2094 - val_acc: 0.5602\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9832\n",
      "Epoch 00009: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0889 - acc: 0.9832 - val_loss: 2.3321 - val_acc: 0.5588\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9789\n",
      "Epoch 00010: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.1017 - acc: 0.9789 - val_loss: 2.6773 - val_acc: 0.5157\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9846\n",
      "Epoch 00011: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0788 - acc: 0.9846 - val_loss: 2.5807 - val_acc: 0.5525\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9891\n",
      "Epoch 00012: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0646 - acc: 0.9891 - val_loss: 2.5456 - val_acc: 0.5584\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9885\n",
      "Epoch 00013: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0617 - acc: 0.9885 - val_loss: 2.6706 - val_acc: 0.5537\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9836\n",
      "Epoch 00014: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0766 - acc: 0.9835 - val_loss: 2.7402 - val_acc: 0.5504\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9800\n",
      "Epoch 00015: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0860 - acc: 0.9800 - val_loss: 3.1778 - val_acc: 0.5283\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9789\n",
      "Epoch 00016: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0961 - acc: 0.9788 - val_loss: 3.0144 - val_acc: 0.5274\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9859\n",
      "Epoch 00017: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0689 - acc: 0.9858 - val_loss: 2.9850 - val_acc: 0.5509\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9831\n",
      "Epoch 00018: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0797 - acc: 0.9830 - val_loss: 3.0035 - val_acc: 0.5502\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9818\n",
      "Epoch 00019: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0841 - acc: 0.9817 - val_loss: 3.0205 - val_acc: 0.5500\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9858\n",
      "Epoch 00020: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0675 - acc: 0.9858 - val_loss: 3.0430 - val_acc: 0.5551\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9932\n",
      "Epoch 00021: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0433 - acc: 0.9932 - val_loss: 2.9746 - val_acc: 0.5628\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9933\n",
      "Epoch 00022: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0441 - acc: 0.9933 - val_loss: 3.0791 - val_acc: 0.5584\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9900\n",
      "Epoch 00023: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0579 - acc: 0.9900 - val_loss: 3.3885 - val_acc: 0.5292\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9913\n",
      "Epoch 00024: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0525 - acc: 0.9913 - val_loss: 3.4095 - val_acc: 0.5472\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9869\n",
      "Epoch 00025: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0651 - acc: 0.9869 - val_loss: 3.0756 - val_acc: 0.5667\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9932\n",
      "Epoch 00026: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0440 - acc: 0.9932 - val_loss: 3.4495 - val_acc: 0.5274\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9905\n",
      "Epoch 00027: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0546 - acc: 0.9905 - val_loss: 3.3640 - val_acc: 0.5416\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9881\n",
      "Epoch 00028: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0648 - acc: 0.9880 - val_loss: 3.2705 - val_acc: 0.5525\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9920\n",
      "Epoch 00029: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0526 - acc: 0.9920 - val_loss: 3.4117 - val_acc: 0.5597\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9936\n",
      "Epoch 00030: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0460 - acc: 0.9936 - val_loss: 3.2476 - val_acc: 0.5593\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9923\n",
      "Epoch 00031: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0508 - acc: 0.9923 - val_loss: 3.6732 - val_acc: 0.5232\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9872\n",
      "Epoch 00032: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0751 - acc: 0.9872 - val_loss: 3.6764 - val_acc: 0.5246\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9889\n",
      "Epoch 00033: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0595 - acc: 0.9889 - val_loss: 3.5322 - val_acc: 0.5549\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9901\n",
      "Epoch 00034: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0574 - acc: 0.9900 - val_loss: 3.7180 - val_acc: 0.5386\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9931\n",
      "Epoch 00035: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0461 - acc: 0.9931 - val_loss: 3.5372 - val_acc: 0.5588\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9921\n",
      "Epoch 00036: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0505 - acc: 0.9921 - val_loss: 3.5795 - val_acc: 0.5537\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9889\n",
      "Epoch 00037: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0640 - acc: 0.9889 - val_loss: 3.7527 - val_acc: 0.5411\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9921\n",
      "Epoch 00038: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0509 - acc: 0.9921 - val_loss: 3.6369 - val_acc: 0.5574\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9904\n",
      "Epoch 00039: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0557 - acc: 0.9904 - val_loss: 3.8527 - val_acc: 0.5365\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9897\n",
      "Epoch 00040: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0551 - acc: 0.9897 - val_loss: 3.7677 - val_acc: 0.5500\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9936\n",
      "Epoch 00041: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0446 - acc: 0.9936 - val_loss: 3.7895 - val_acc: 0.5439\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9962\n",
      "Epoch 00042: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0361 - acc: 0.9962 - val_loss: 3.5529 - val_acc: 0.5733\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9908\n",
      "Epoch 00043: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0544 - acc: 0.9908 - val_loss: 3.7275 - val_acc: 0.5488\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9895\n",
      "Epoch 00044: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0580 - acc: 0.9895 - val_loss: 3.9797 - val_acc: 0.5490\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9904\n",
      "Epoch 00045: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0557 - acc: 0.9904 - val_loss: 3.9913 - val_acc: 0.5437\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9914\n",
      "Epoch 00046: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0545 - acc: 0.9914 - val_loss: 3.7591 - val_acc: 0.5563\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9916\n",
      "Epoch 00047: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0541 - acc: 0.9916 - val_loss: 3.8621 - val_acc: 0.5535\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9920\n",
      "Epoch 00048: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0496 - acc: 0.9920 - val_loss: 3.9882 - val_acc: 0.5588\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9912\n",
      "Epoch 00049: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0522 - acc: 0.9912 - val_loss: 3.9137 - val_acc: 0.5642\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9940\n",
      "Epoch 00050: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0427 - acc: 0.9940 - val_loss: 4.0515 - val_acc: 0.5502\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9922\n",
      "Epoch 00051: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0516 - acc: 0.9922 - val_loss: 3.9588 - val_acc: 0.5658\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9940\n",
      "Epoch 00052: val_loss did not improve from 1.84604\n",
      "36805/36805 [==============================] - 151s 4ms/sample - loss: 0.0427 - acc: 0.9939 - val_loss: 3.9357 - val_acc: 0.5616\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XecVNX5+PHPmdnZ3mHpHVFgF5ayCgQFxKCIBrvYRaN8zdeu0WiiRk3MVxOTiD3Ye/mBxoaiqAgaNQKCIqDSO+yysGzfKc/vjzMz23dny2x93q/Xfd0pt5x7d/Y+95x7ihERlFJKKQBHaydAKaVU26FBQSmlVJAGBaWUUkEaFJRSSgVpUFBKKRWkQUEppVSQBgWllFJBGhSUUkoFaVBQSikVFNHaCWiorl27yoABA1o7GUop1a6sWLEiR0TS6luu3QWFAQMGsHz58tZOhlJKtSvGmK2hLKfFR0oppYI0KCillArSoKCUUiqo3T1TqInb7WbHjh2UlJS0dlLarejoaPr06YPL5WrtpCilWlGHCAo7duwgISGBAQMGYIxp7eS0OyLC/v372bFjBwMHDmzt5CilWlGHKD4qKSmhS5cuGhAayRhDly5dNKellOoYQQHQgNBEev6UUtACQcEY4zTGfGuMebeG76KMMa8ZYzYYY742xgwId3qUUqpNWLAA2mCbq5bIKVwLrKvlu18DB0TkMOCfwH0tkJ5md/DgQR599NFGrTtjxgwOHjwY8vJ33nkn999/f6P2pZRqI556Cs48EyZMgLlzQaS1UxQU1qBgjOkDnAQ8WcsipwDP+V/PB44z7bAco66g4PF46lx34cKFJCcnhyNZSqm26IMP4H/+B6ZNg5NOguuug7POgkOHWjtlQPhzCg8ANwO+Wr7vDWwHEBEPkAd0CXOamt0tt9zCxo0bGTVqFDfddBNLlizhmGOOYebMmQwfPhyAU089lbFjx5Kens68efOC6w4YMICcnBy2bNnCsGHDuPzyy0lPT+f444+nuLi4zv2uWrWK8ePHM3LkSE477TQOHDgAwIMPPsjw4cMZOXIk55xzDgCfffYZo0aNYtSoUYwePZr8/PwwnQ2l2rDSUti6Fb7+Gv79b3j8cfjjH+2FuqmWL4d1tRWK+K1caQPAiBG2+OjNN+Fvf7NpycqC775rejqaKGxVUo0xJwP7RGSFMWZKE7c1B5gD0K9fvzqX/fnn6ygoWNWU3VUTHz+KIUMeqPX7e++9lzVr1rBqld3vkiVLWLlyJWvWrAlW8Xz66adJTU2luLiYI488kjPOOIMuXSrHv59//plXXnmFJ554grPPPpsFCxZwwQUX1Lrfiy66iIceeojJkydzxx13cNddd/HAAw9w7733snnzZqKiooJFU/fffz+PPPIIEydOpKCggOjo6KaeFqXal+uus0U1NUlMhM2bITW1cdt+7z047TTw+eD6622giY+vvMyWLTZnkJpql09IsJ//9rcwbhzMmmXnjz0Gs2c3Lh3NIJw5hYnATGPMFuBVYKox5sUqy+wE+gIYYyKAJGB/1Q2JyDwRyRKRrLS0ejv5axOOOuqoSnX+H3zwQTIzMxk/fjzbt2/n559/rrbOwIEDGTVqFABjx45ly5YttW4/Ly+PgwcPMnnyZAAuvvhili5dCsDIkSM5//zzefHFF4mIsHF/4sSJ3HDDDTz44IMcPHgw+LlSncL69fDgg3DqqfDkk/Duu7BiBezcae/e8/PtHXtjfPwxnHEGjBwJl1wC998Pw4fbXEDgWcGBAzBjBhQXw8KF0KtX5W0ccwx8+y384hd2G+npcOGF8Pe/wyefQG5u046/AcJ2ZRCRW4FbAfw5hd+KSNXb3reBi4EvgTOBT0Sa9sSlrjv6lhQXFxd8vWTJEhYvXsyXX35JbGwsU6ZMqbFNQFRUVPC10+mst/ioNu+99x5Lly7lnXfe4Z577uH777/nlltu4aSTTmLhwoVMnDiRRYsWMXTo0EZtX6l25957IToa5s2DqjeWvXrBuefaXMS110KPHqFv9/PPYeZMOPxwWLQIunSxd/m/+Q2cfjqcfLINEnPmwMaNdpn09Jq31b07fPghPPwwfPQRfPopvFjhPrpfP7jhBpvGMGrxdgrGmLuNMTP9b58CuhhjNgA3ALe0dHqaQ0JCQp1l9Hl5eaSkpBAbG8v69ev56quvmrzPpKQkUlJSWLZsGQAvvPACkydPxufzsX37do499ljuu+8+8vLyKCgoYOPGjYwYMYLf/e53HHnkkaxfv77JaVCqXdiyxV5c58ypHhAC7roLysrgnntC3+4339i7/7597UU8UBw8caLNhdx/v72wDx0KS5fCs8/ClCl1b9PptBf9d9+FHTtg714bKO67z263S/gfubZIGYKILAGW+F/fUeHzEuCslkhDOHXp0oWJEyeSkZHBiSeeyEknnVTp++nTp/P4448zbNgwjjjiCMaPH98s+33uuee44oorKCoqYtCgQTzzzDN4vV4uuOAC8vLyEBGuueYakpOTuf322/n0009xOBykp6dz4oknNksalGrz/vpXcDhs2X1tDjsMLr0U/vUvu1z//nVvc/VqOOEEG2Q+/tje5VfkcsGNN8LZZ8Ptt8P48TY30lDdutlaStOmNXzdRjJNLK1pcVlZWVJ1kJ1169YxbNiwVkpRx6HnUXU4u3fDwIFw0UW26Kgu27fDkCFw3nnw9NO1L7d2LUyebIujli2DdjISpDFmhYhk1bdch+nmQinVTnz8MVx9NeyvVqek+f397+B2w+9+V/+yffvaZwHPPQc//ljzMv/5jy0CioiwD4DbSUBoCA0KSqmWU1QEF19sH6aOGGEfvIbL/v22HcJ558HgwaGtc+utEBMDd9xR/buXX4apUyEpCT77zOYqOiANCkqplvP3v9tqoI89ZuvrT58O11xjq2o2t7lzobDQXuhD1a2bbWfw+uvgb3eEiH0Qff75th3BV1/Z2kYdlAYFpVTL2LXLVg0980y44gpbe+faa+Ghh2DsWFtPP8DrhZ9+gvnz7V17fc8Dqjp0yG739NNtm4GGuPFGSE6G226DkhK44AK4806bw6lYy6iD0hZMSqmWcfvttnz/3nvt+5gYeOAB28p39mx7F37aabZl8Zo1lXMPxtiGXRkZoe3r0Ufh4EH4/e8bns7kZPsM4tZb4cgjbVr+8he45Rabjg5OcwpKqaZZuNB20ZCTU/syq1fDM8/YoqKq5fvTpsH339scxLJltsuJK66wywdaHcfH27v1UBQVwT/+YYumxo5t3DFdfbWtZrphA/y//2cDRCcICKA5hVYTHx9PQUFByJ8rFRYej61J01jbt9uy9oMH4YcfbPFKz56VlxGxLXFTU22RTE1SU+2D3Npcfz3cfbctYho9uu40PfEEZGfDH/7QsGOpKC7OPkw2pkM/P6iJ5hSU6qwee8w2vmpsz5w+ny32cbttf0JbtsCkSbBtW+Xl3nvPVt+8805bNNMY119v162pVlBF27bZZaZOhaOPbty+Ao44otMFBNCg0CxuueUWHnnkkeD7wEA4BQUFHHfccYwZM4YRI0bw1ltvhbxNEeGmm24iIyODESNG8NprrwGwe/duJk2axKhRo8jIyGDZsmV4vV5mz54dXPaf//xnsx+jauNEbDcNbndoy2/aZFvuHjxYfmFvqLlz7cX+n/+EX//a5hKys23nbhs22GXcbrufI46wYwg0VnKy3c6779pur2vi89mHwT6fDVKqUTpe8dF115VXJWsuo0bZB2K1mDVrFtdddx1XXnklAK+//jqLFi0iOjqaN998k8TERHJychg/fjwzZ84MaTzkN954g1WrVrF69WpycnI48sgjmTRpEi+//DInnHACf/jDH/B6vRQVFbFq1Sp27tzJmjVrABo0kptqh9ats52w5eXZ8QFKS21AELHdMb/9dt197IjA5ZfbfnYeesiWn//f/9V/F17RmjW2nH3mTLjsMvvZhAm2r5/jj7eBYfFi+/7HH22aXK4mHTbXXGMD0B131Ny+4YEHYMkSO6pZhR6KVcN0vKDQCkaPHs2+ffvYtWsX2dnZpKSk0LdvX9xuN7///e9ZunQpDoeDnTt3snfvXnqE0Avj559/zrnnnovT6aR79+5MnjyZb775hiOPPJJLL70Ut9vNqaeeyqhRoxg0aBCbNm3i6quv5qSTTuL4449vgaNWreaVV+yd/pw5EBVlp8hIO3/pJduN89df2/58avLkk/YO//HH7d37l1/Cn/5kL/D+rtvrVFpqnyMkJdny+4o3OaNH27L4X/7SdgUhYotyTj656cedkGBrBd18s+2dtGLxUCBInXKK7XpaNZ6ItKtp7NixUtXatWurfdbSbr/9dpk7d67ceuutMnfuXBEReeaZZ+Tss8+WsrIyERHp37+/bN68WURE4uLiatxO4PPrrrtOnnrqqeDnF1xwgbz11lsiIrJz506ZN2+eZGZmynPPPSciIvn5+TJ//nw55ZRT5JJLLmnUMbSF86hCkJUl8otf1Pzdhg0iXbqIHHGESG5u9e+3bxdJTBSZMkXE67Wf5eSI9OghMnKkSGlp/fu/6SYREHnnndqX+flnkX79RIwR+fbb+rcZqsJCke7dbfoDSkpEMjNFunUT2bu3+fbVwQDLJYRrbKtf5Bs6tdWgsGbNGpkwYYIMGTJEdu3aJSIiDzzwgFx11VUiIvLJJ58IEHJQWLBggRx//PHi8Xhk37590q9fP9m9e7ds2bJFPB6PiIg89NBDcu2110p2drbk5eWJiMj3338vmZmZjTqGtnAeVT327rX/tnffXfsyS5eKuFwixx0n4r8hERERn0/k5JNFYmJs8Kjorbfsdu+4o+79L1liL/Rz5tSf1j17RD7/vP7lGmruXJvWjz+272+5xb5/++3m31cHokGhFWRkZMiUCncw2dnZMn78eMnIyJDZs2fL0KFDQw4KPp9Pfvvb30p6erpkZGTIq6++KiIizz77rKSnp8uoUaPk6KOPlk2bNsmqVatk9OjRkpmZKZmZmbJw4cJGpb+tnEdVh+eft/+233xT93LPPGOX+5//scFAROSll+xnf/97zetceKGI0ymyYkXN3+/bZ+/+hwwRKSho9CE0WXGxSJ8+Nre0dKkNUpdd1nrpaSc0KKgG0/PYDpx7ri0mCRT91OV3v7P/4g88YHMYXbqIjBsn4s9pVpObK9Kzp0hGhi2SEbEX/1deEZk50+Y+nE6Rr75qvuNprMces8eWnCwyaJDIoUOtnaI2L9SgELYqqcaYaGPMf40xq40xPxhj7qphmdnGmGxjzCr/dFm40qNUu+f12lo3J5xgB42pz1/+YsckvuEG+6D30CFbM8fprHn5lBT74HjNGls76fzzbavec8+1LYuvvto2Hhs3rnmPqzEuvdR2W33oEDz/vH0IrZpFOGsflQJTRaTAGOMCPjfGvC8iVceifE1ErgpjOpTqGL75xg7gHuqoeQ4HvPCCrR76zTe2RXBt4wMHBPohevZZ28r4/PNt19PHHBNaIGopkZHw5pt2yMqJE1s7NR1K2IKCP7sS6K/B5Z/a1zBvSrUl779vL8wNqXIcH2/7Jpo/P/TGY489Zu/Ex42zF9+2atSo0KrQqgYJa+g3xjiNMauAfcBHIlJTU8QzjDHfGWPmG2P61rKdOcaY5caY5dnZ2eFMslJt1/vvw1FHNbzr5p49bdFPqBf46GibM2jLAUGFTViDgoh4RWQU0Ac4yhhTtd/bd4ABIjIS+Ah4rpbtzBORLBHJSktLC2eSlWqbsrNh+fLQi46UaqQWKSQUkYPAp8D0Kp/vF5FS/9sngUb2c6tUKzh40LbcfeQR23I3nBYtsvvQoKDCLJy1j9KMMcn+1zHANGB9lWUq9rE7E1gXrvSE08GDB3n00Ucbte6MGTO0r6L26s037SD0V11la/fs3dv4bdUXVD74wPZo2tjxAZQKUThzCj2BT40x3wHfYJ8pvGuMudsYM9O/zDX+6qqrgWuA2WFMT9jUFRQ8Hk+d6y5cuJDkxnYnrJqmuNhWsXz+edufzowZtjfPUAeTX7AA+vWzncp98okdiP6ddxqejg8+gD594MMPa/7e52tYVVSlmiKUxgxtaWqLjddmzZol0dHRkpmZKb/97W/l008/laOPPlp+9atfyZAhQ0RE5JRTTpExY8bI8OHD5V//+ldw3f79+0t2drZs3rxZhg4dKpdddpkMHz5cpk2bJkVFRdX29fbbb8tRRx0lo0aNkuOOO0727NkjIrbvo9mzZ0tGRoaMGDFC5s+fLyIi77//vowePVpGjhwpU6dOrfM4Wvs8tpiSEpGpU0UcDtsACkQiI23/OSkpIiecUP828vLsOtdfb9//8IPIqFHlrYhDbfG7apVIfLxdLy1NZOfO6st8/bX9/sUXQz9Gpaqgs7ZovvZakcmTm3e69tq6T/bmzZslPT09+P7TTz+V2NhY2bRpU/Cz/fv3i4hIUVGRpKenS05OjohUDgpOp1O+9XcedtZZZ8kLL7xQbV+5ubni83db8MQTT8gNN9wgIiI333yzXFshobm5ubJv3z7p06dPMB2BNNSm0wSF116zP/0rrxR5/XWRtWtF3G773e9/b4PF7t11byPQZUTFvn1KSkRuvtl2uzBkiMjKlXVvY+dO211D794iixeLxMaKHHts9RbHd95pt5md3fBjVcov1KCgedEwOeqooxhYoU/3Bx98kMzMTMaPH8/27dv5+eefq60zcOBARvnrXY8dO5YtW7ZUW2bHjh2ccMIJjBgxgr/97W/88MMPACxevDg4ngNASkoKX331FZMmTQqmIzU1tTkPsf164gno3x8efBDOOguGDSsfkvLCC21xzSuv1L2NBQtsVc8JE8o/i4qC++6zzxmKiux3Tz1V8/oFBfY5xIEDduCY446zD6w//RTuuafysu+/bweQ79q18cesVIg63HgKdYyF06Li4uKCr5csWcLixYv58ssviY2NZcqUKZSUlFRbJyoqKvja6XRSXFxcbZmrr76aG264gZkzZ7JkyRLuDHUwc2Vt3mwHf7nrrprL54cOhaws2xL4+utr3kZhob1QX3JJzds49lhYudK2BL7sMvjPf+DhhyEmxn7v9drvVq+2g88EGmBdfLENKHfdZccimDwZcnLgv/9t2AA4SjWB5hSaQUJCAvn5+bV+n5eXR0pKCrGxsaxfv56vvqra00fo8vLy6N27NwDPPVferGPatGmVhgQ9cOAA48ePZ+nSpWzevBmA3NzcRu+3w3j6aXshr2sglgsusA+g/bmwaj74wD6kPuOM2rfRrZt9OPyHP9h9/uIXdmAcgBtvtA+k58613UoEGAOPPgqDB9ugkZNjHz5rVVTVgjQoNIMuXbowceJEMjIyuOmmm6p9P336dDweD8OGDeOWW25h/Pjxjd7XnXfeyVlnncXYsWPpWqE44bbbbuPAgQNkZGSQmZnJp59+SlpaGvPmzeP0008nMzOTWbNmNXq/HYLHYy/Q06dD3xobz1vnnms7jXvhhZq/X7DAtiqeNKnu/Tmd8Oc/2+KhrVthzBjb1cTcuXDttbYqa1UJCfD66zYgzJ5tu6jo0sXmXpRqCaE8eGhLU1usfdRRdPjz+M479uHwG2/Uv+yMGfYhcNUuqktKRBISRH7964bte9MmkTFj7P5/9avau68OeOghu6wxIued17B9KVUD9EGzUlU8+aTtCjqU8YIvvND2wLlkSeXPP/oI8vPrLjqqycCB8MUX9gH2K6/U3n11wJVXwmmnadGRanEaFFTnsHu3LcaZPRtcrvqXP+UUW5RTtQhpwQI7YP1xxzU8DdHRcM45UKESQq2MsUVdf/lLwwOQUk2gQUF1Ds8+a2v9/PrXoS0fEwNnnmm7nC4qsp+53fDWW/CrX7VMD6LJyXDrreW1lpRqARoUVOMsW2arWrYHPp8tOpoyBYYMCX29Cy+07Qneesu+/+wz265A79xVB6ZBQTWciK22ecIJUEMjvDZnyRJbHfSyBo72OnmyraUUKEJasMAW/ZxwQrMnUam2QoOCarj162HbNnsXPWsWlJbWv05revJJWxRz+ukNW8/hsMNRfvihfSbx5pu20zwtzlEdmAaFVhIfH9/aSWi8QG+ec+faRl41tM1oM/bvt3f4F17YuIv5BRfYZxFXXWW7xtaiI9XBdbhuLlQLWLQIDj8crrnGdhvxwAMwdSqcemprp6y6F1+EsrKGFx0FpKfD6NHwxhu2b6MZM5o3fUq1MZpTaAa33HJLpS4m7rzzTu6//34KCgo47rjjGDNmDCNGjOCtwAPLOpx66qmMHTuW9PR05s2bF/z8gw8+YMyYMWRmZnKcvzpkQUEBl1xyCSNGjGDkyJEsWLCg+Q+uqpISW0YfKFe/7z7b2vaSS2yr3bakuBj+9S/bmdzIkY3fzoUX2vkJJ9hqqkp1YGHLKRhjooGlQJR/P/NF5I9VlokCnscOw7kfmCUiW5qy3+s+uI5Ve1Y1ZRPVjOoxigem197T3qxZs7juuuuCvZS+/vrrLFq0iOjoaN58800SExPJyclh/PjxzJw5E2NMrdt6+umnSU1Npbi4mCOPPJIzzjgDn8/H5ZdfztKlSxk4cGCwD6M//elPJCUl8f333wO2v6Ow+/xze7ENBIXISHj1VduFwznnwNKlobUDCLd9+2xbg/Xr4f/9v6Zt67zzbPALtTqrUu1YOIuPSoGpIlJgjHEBnxtj3heRir3B/Ro4ICKHGWPOAe4D2l0HPaNHj2bfvn3s2rWL7OxsUlJS6Nu3L263m9///vcsXboUh8PBzp072bt3Lz169Kh1Ww8++CBvvvkmQLCL7ezs7Bq7wF68eDGvvvpqcN2UlJQwHqXfokU2EEyZUv7Z4MG2O+pZs+C22+wFtLnk5NjeRFetsvMffrCdy/3xj7V3Jb1+vS3m2b3btjNo6APmqrp3hz17mrYNpdqJsAUFf18bBf63Lv9UdSDaU4A7/a/nAw8bY4x/3Uap644+nM466yzmz5/Pnj17gh3PvfTSS2RnZ7NixQpcLhcDBgyoscvsgFC72G5VixbB0UdXb5V79tl2LIC//tUOUXn66Xa8gVCUldlnExs2VJ6+/x527ixfrmdP287gscdsNdHbboOrr7Zl/QGffWa7h3C5bDHXuHFNPmSlOpOwPmg2xjiBFcBhwCMi8nWVRXoD2wFExGOMyQO6ADnhTFc4zJo1i8svv5ycnBw+++wzwHZz3a1bN1wuF59++ilb6ylzr62L7fHjx/O///u/bN68OVh8lJqaGuwu+wH/IBIHDhwIb25h1y57oa4tJ/CPf8CKFbamzlVX2Qv4pEnlk8sFP/5Yfdq61TYwC0hIgMMOs+MSZGba8QYyM+3A9QBr19oaTzfdZAPEX/9qg9BLL8Gll9p133vP9jeklGqQsAYFEfECo4wxycCbxpgMEVnT0O0YY+YAcwD69evXzKlsHunp6eTn59O7d296+u+Qzz//fH71q18xYsQIsrKyGDp0aJ3bmD59Oo8//jjDhg3jiCOOCHaxXbELbJ/PR7du3fjoo4+47bbbuPLKK8nIyMDpdPLHP/6R05taVFKXQFXU2hpvxcTYVs6rVtlnC599Zmvt1DT6WFycrcE0bpx9kHvYYeVT166275/aDB9uL/offgg33GC7o0hPt0VLxx5rq6C2RFGaUh2QaUJJTcN2ZMwdQJGI3F/hs0XAnSLypTEmAtgDpNVVfJSVlSXLly+v9Nm6desYNmxYmFLeedR7Hs891xbJ7NpV90W7Ip/PXqyXLbPvjzjCTr17h76NugTGSLjrLtub6KOPtky/REq1M8aYFSJS78Ac4ax9lAa4ReSgMSYGmIZ9kFzR28DFwJfAmcAnTXmeoMLI67XdRp90UsMu5g4HjBhhp3CIiIA5c+Dyy5snyCjVyYWz+Kgn8Jz/uYIDeF1E3jXG3I0d7OFt4CngBWPMBiAXOCeM6VFNsXKlbR18/PGtnZKaaUBQqlmEs/bRd8DoGj6/o8LrEuCsZtpfnfX/Vd3qzaAtWmTn06aFPzFKqVbTIVo0R0dHs3///vovbKpGIsL+/fuJjo6ufaFFi2wDtW7dWi5hSqkW1yH6PurTpw87duwgOzu7tZPSbkVHR9OnT5+av8zLgy+/hJtvbtlEKaVaXIcICi6XK9jaV4XBJ5/YB806joBSHV6HKD5SYbZoEcTHw4QJrZ0SpVSYaVBQdROxQWHqVK3/r1QnoEFB1W3DBtiyRYuOlOokNCiougWqompQUKpT0KCgavfVV/DQQ7Zr7MGDWzs1SqkWoEFBVbdjhx2beMIEyM+3gUEp1SloUFDliovhT3+yHdbNnw+//z389JPtaE4p1SloUGjvnn7aDnCzd2/TtvP++zB0KNxxhw0C69bBPffYqqhKqU5Dg0J75vPB3XfbMYizsuCbbxq3nbffhpkzITHRdo09f74OUKNUJ6VBoT378ks7atnNN4PTCcccA88807BtfPABnHWW7dfoiy9g8uTwpFUp1S5oUGjPXn7ZjnZ2222wfLkdO/nSS+HKK+24x/X5+GM7nvHw4TY4JCaGP81KqTZNg0J75XbD66/bYp+EBDuE5Qcf2HGLH33UtkDevbv29Zcts+sOHmwHz9HhK5VSaFBovxYvhpwcOO+88s8iIuwg9q+8YgfFGTDADorzwAO2FlHAV1/BjBnQt6/NLXTt2uLJV0q1TWELCsaYvsaYT40xa40xPxhjrq1hmSnGmDxjzCr/dEdN21I1ePlle3c/fXr17845B1asgKuvtm0Orr/eVjM97DD43/+163TvbgNC9+4tn3alVJsVzpyCB7hRRIYD44ErjTHDa1humYiM8k93hzE9bctDD9mLdGlpw9ctKoI334Qzz6y9k7phw+D++2HtWti0CR55xFY5ffZZSE62AaF37yYdglKq4wlbUBCR3SKy0v86H1gH6FUo4LXXYONGWLiw4eu+8w4UFlYuOqrLwIE2h/Duu5Cba9sg9O/f8P0qpTq8FnmmYIwZgB2v+esavp5gjFltjHnfGJPeEulpdYcO2XJ9gOefb/j6L79s7/KPOabh60ZH2xpLSilVg7AHBWNMPLAAuE5EDlX5eiXQX0S9f3UZAAAgAElEQVQygYeAf9eyjTnGmOXGmOUdYsjNzz6zI5llZcF779kHxqHKzbWtj2fNsm0TlFKqGYU1KBhjXNiA8JKIvFH1exE5JCIF/tcLAZcxplpVGBGZJyJZIpKVlpYWziS3jMWL7d36o4/aqqWvvRb6ugsW2HVCLTpSSqkGCGftIwM8BawTkX/UskwP/3IYY47yp2d/uNLUZixebIt+jjwSMjPhuedCX/fll+Hww20LZKWUambhzClMBC4EplaocjrDGHOFMeYK/zJnAmuMMauBB4FzRETCmKbWt2uXrRH0y1/a9xddZPssWreu/nV37LBFT+edBzaWKqVUs4oI14ZF5HOgziuXiDwMPByuNLRJH39s54GgcN55tu+iF16Av/yl7nVfe82OmXzuueFNo1Kq09IWzS3to49sC+LMTPu+Rw871OULL9heT+vyyiv24fThh4c/nUqpTkmDQksSsc8TjjsOHBVO/UUX2aKhJUtqX/fHH20rZX3ArJQKIw0KLWndOttJXaDoKCAwlkFtbRZEbP9FxtiqqEopFSYaFFrS4sV2XjUoxMTY0dPmz7ctlau68054/HHbKrlXr7AnUynVeWlQaEmLF9uuqgcMqP7dRRfZgPBGleYcd99tp0sugQcfbJFkKqU6Lw0KLcXtts8MquYSAo4+2vZRVLEI6Z574I9/tAHjiScqP4dQSqkw0KtMS/nvfyE/H6ZNq/l7Y+zF/+OP7UPne++1I6pdcAE8/bR2aaGUahGdJijk5LzLV18NpLh4U+skYPFie+E/9tjal7nwQvtQ+bTT4NZbbU2jZ5/VgKCUajGdJigY46SkZAtlZXtbJwGLF8PYsZCaWvsygwfDxIl2vOVzzrHdX2hAUEq1oE4TFCIjuwHgdu9r+Z3n59uusmt7nlDR3/9uaxu98IIdXlMppVpQSEHBGHOtMSbRWE8ZY1YaY44Pd+Kak8tlg0JZWSsEhaVLweMJLSiMG2cfLmtAUEq1glBzCpf6x0I4HkjBdnR3b9hSFQYul+1yu1VyCh99ZAe3mTix5fetlFINEGpQCHRsNwN4QUR+oJ7O7toapzMapzOxdXIKga6yo6Nbft9KKdUAoQaFFcaYD7FBYZExJgGop/e2ticyslvL5xR274Yffgit6EgppVpZqAXXvwZGAZtEpMgYkwpcEr5khYfL1a3lcgqHDsHXX9ueTUGDglKqXQg1KEwAVolIoTHmAmAMMDd8yQqPyMhuFBdvDM/GS0rseAdffAFffmlzByLlbRNGjQrPfpVSqhmFWnz0GFBkjMkEbgQ2ArV06WkZY/oaYz41xqw1xvxgjLm2hmWMMeZBY8wGY8x3xpiwjjEZ1pzCZZfB7Nnw+uvQu7etVvrhh3DgAHzyiXZRoZRqF0LNKXhERIwxpwAPi8hTxphf17cOcKOIrPQ/g1hhjPlIRNZWWOZEYIh/GocNPuMaeAwhs88UshHxYUwzXqQ/+ABeesm2Qv7znzUAKKXarVCvXvnGmFuxVVHfM/aK6qprBRHZLSIr/a/zgXVA7yqLnQI8L9ZXQLIxpmeDjqABbFsFH253bvNttKAArrgChg617Qs0ICil2rFQr2CzgFJse4U9QB/gb6HuxBgzABgNfF3lq97A9grvd1A9cGCMmWOMWW6MWZ6dnR3qbqsJS6vmO+6ArVttL6ZRUc23XaWUagUhBQV/IHgJSDLGnAyUiEidzxQCjDHxwALgOn8DuAYTkXkikiUiWWlpaY3ZBNDAVs3Z2bbmkMdT+zLffANz59qcwtFHNzpdSinVVoTazcXZwH+Bs4Czga+NMWeGsJ4LGxBeEpE3alhkJ9C3wvs+/s/CIrI0Dgghp7Bli219fN55MHUq7KwhSW43XH459Ohhu7lWSqkOINTioz8AR4rIxSJyEXAUcHtdKxhjDPAUsE5E/lHLYm8DF/lrIY0H8kRkd4hpapgPPyQ240S6LYay0jp6Sl271t71Z2fbEc9WrrTVST/8sPJy//gHrF4NDz8MSUlhSbJSSrW0UIOCQ0Qq3l7vD2HdidgH01ONMav80wxjzBXGmCv8yywENgEbgCeA/21A2humf38YMoTh90DKZY/Anj3Vl/n6a9sdhdcLn30Gt99uu7Hu0QOmT7fvvV7YsMFWOT3tNDsppVQHYUSk/oWM+RswEvA3z2UW8J2I/C6MaatRVlaWLF++vHEre71sviaJ/k+W4IhPsnf555xjG5gtXgynngrdu9sO7AYNKl+vqAiuvtqOgDZlig0Mq1fDunXQq1ezHJdSSoWTMWaFiGTVt1xI7RRE5CZjzBnYu3+AeSLyZlMS2CqcTrIvHoD7hJ4c/pd8+8zg9dfhpJPgyivhiCNg0SLoWaVWbGwsPPUUTJoEv/kNFBfD449rQFBKdTghd9ovIguwD43btcjIbhT0KbbdUfz977ZK6b//bR8sv/MOpKTUvvLFF8ORR8KSJfYhs1JKdTB1BgVjTD5QU/mSAUREEsOSqjByubpRULDSDnN5883wq1/BW2/BNdfYHEF9hg+3k1JKdUB1BgURSWiphLSUyMgq/R8NG2YnpZRSnWeM5gCXqxtebx4+X2lrJ0UppdqcThcUAl1dlJU1vrsMpZTqqDpdUAh0ddEqYzUrpVQb1+mCQnlOQYOCUkpV1emCguYUlFKqdp0uKGhOQSmlatfpgoLTmYAxUZpTUEqpGnS6oGCMqd5WQSmlFNAJgwLY5wqaU1BKqeo6ZVDQnIJSStWsUwYFzSkopVTNwhYUjDFPG2P2GWPW1PL9FGNMXoUBeO4IV1qqCuQUQhlLQimlOpNw5hSeBabXs8wyERnln+4OY1oqcbm6IVKK15vfUrtUSql2IWxBQUSWArnh2n5TBNoquN3a/5FSSlXU2s8UJhhjVhtj3jfGpLfUTgOtmvVhs1JKVRbyyGthsBLoLyIFxpgZwL+BITUtaIyZA8wB6NevX5N3XJ5T0KCglFIVtVpOQUQOiUiB//VCwGWM6VrLsvNEJEtEstLS0pq8b80pKKVUzVotKBhjehhjjP/1Uf607G+JfUdG2sCiOQWllKosbMVHxphXgClAV2PMDuCPgAtARB4HzgR+Y4zxAMXAOdJCdUQdjiicziTNKSilVBVhCwoicm493z8MPByu/dcnMlIbsCmlVFWtXfuo1bhc2tWFUkpV1WmDguYUlFKquk4bFDSnoJRS1XXaoGBzCjmIeFs7KUop1WZ02qBg2yr4cLvbZE8cSinVKjptUNBWzUopVV2nDQraqlkpparrtEFBcwpKKVVdpw0KmlNQSqnqOnFQSAUcmlNQSqkKOm1QMMaBy5WmOQWllKqg0wYF0FbNSilVVacOCppTUEqpyjp1UNCcglJKVdapg4L2f6SUUpV16qAQGdkNr/cQXm9JaydFKaXahLAFBWPM08aYfcaYNbV8b4wxDxpjNhhjvjPGjAlXWmoTaKvgdme39K6VUqpNCmdO4Vlgeh3fnwgM8U9zgMfCmJYaaatmpZSqLJzDcS41xgyoY5FTgOf94zJ/ZYxJNsb0FJHd4UpTVdqqWan2QwTcbjuVldm5MRAXBzEx9rVqurAFhRD0BrZXeL/D/1m1oGCMmYPNTdCvX79mS0Bz5BREIC8Ptm+HvXuhuNhOJSXlU2kp+HzVJxGIiLCT01n+2uGoeVlj7I+/6uRwQEFB+ZSfb+eFhTYtRUWV54F/Joej8jwyEmJj7T9ZbGz565gYiIqC6Gg7BV67XHZdh8OmP/C6tNSek8B06JCd+3x2H1WnwDEHthGYu932/AXOZ+Dc5ufDwYN2m4F5Xp5dviYRETbNVScAr7fy5PPZ72Ji7PEHznF0tE3DoUN2/4GpoMCmNTLSno+q84pT4FhLS8t/GxV/Kx5PeToCr32+6ucmMAX+PoG/V+BvVXEbge14PHZbgW0GXnu99gJbVmbTFXjtdpf/PStOTqc9jsA5DLx2Osu3EdhOaandb8Xfd8V5Tb9Bn6/y/07gHAW2VZtAcAhM0dGVz1VgCuwjlMnrtf93IuX/61XnFaeAwLkKHJfDUfk8V5yMKf+/r3h+RKr//TweuPlm+MtfGn25CklrBoWQicg8YB5AVlaW1LN4yBqaU/D5YNEi+Pe/YetWGwi2bbMXhrYocGELTIH3ERH2WAIXChE7LyuzgaOw0M6Liuw/ZVMZA4mJ5ReOwNTQbQQuzomJkJQEyckwYICdJyXZC1RVgX+uwMUqMJWU2G1WvWgEglogAB04ALt22eUD++7Sxe43IQHi4+25q3j3WvHCWnEqKrLzQNBJTLTbDATdqhfNQHqqXsQD+ysutn+rwN8rN9d+VvEGo+r2XK7KAdzpLL+4By7wgeAV+F1UnDyeyhf/wOT12otxSkrlwBsRUTnIVZwHLqYVf4MVb3wCNyGB81NT0PX5ys9B4EaooKA8TVUDvs9X+fgrXryrfh6YAr+/2uYVp4rHFDgur9eeh6o3Qy6X3Ubggl9xcjiq3yw6nTB5ckP/+xquNYPCTqBvhfd9/J+1GKczHocjut4HzcXF8NJL8I9/wLp19iJ02GFwxBHwy19C37526tnTXnQDP+TAPDKy5h8c1HxH5/VWXj7wo/X5yi9Wgbv+4mL7eeACFR9vX8fGlu+jKbze8txOxXnFO9uKd6Ber/0HTkoqn+Ljq6el4sW64l1sxe24XJXPoRYPKBV+rRkU3gauMsa8CowD8lryeQKAMabOtgr79sGjj9opOxtGjYIXXoCzz675rrQxAncBoUpMbJ79hsrpLM+SNydjyotVlFJtR9iCgjHmFWAK0NUYswP4I+ACEJHHgYXADGADUARcEq601KW2Vs2vvAKXXGLvZE8+GW64AaZM0btVpVTHFs7aR+fW870AV4Zr/6GyOYW9lT5bvBguuggmTIB582Do0FZKnFJKtbB28aA5nCIju1FY+H3w/apVcPrpMGwYvP22fX6glFKdRacPCoFnCiLCtm2GE0+0D0cXLtSAoJTqfDp9UIiM7IZIKdnZ+UyfnkhJCXz+OfTp09opU0qplqdBIbIXpaXRnHKKk02b4KOPID29tVOllFKto9MHhfj4X3DPPS/y9dexvPYaTJrU2ilSSqnW06m7zgZ49dX+LFt2Bjfd9DRnndXaqVFKqdbV6YPC88/DoEG7Ofnk3yLibe3kKKVUq+rUQWHLFli6FM4+ex9e70EKCla3dpKUUqpVdeqg8PLLdn7xxT0BOHDgk1ZMjVJKtb5OGxREbD9GRx8NQ4d2IzZ2KAcPalBQSnVunTYofPstrF8PF1xg3ycnTyUvbxk+Xy2d8iulVCfQaYPCiy/aHjoDNY6Sk4/F6y0gP3956yZMKaVaUacMCh6P7QX1pJMgNdV+lpw8BYCDBz9tvYQppVQr65RB4ZNPYM8euPDC8s8iI7sSF5epD5uVUp1apwwKL75oO7ubMaPy5ykpx3Lo0Bf4fKW1rusTHzlFOXh92qZBqdYiIvyY8yPf7PyGEk8zjBmrgsLazYUxZjowF3ACT4rIvVW+nw38jfJhOB8WkSfDmabCQnjjDTjvPDvUY0XJyVPZseMBDh36iuRkOxiq2+tmxe4VLNu6jGXblvHF9i/ILc7FaZz0iO9Bz4Se9EroRa/4XiRFJ3Gg+AC5JbnkFttpf9F+YlwxXHnklVw25jJiXbG1pq2grIAXv3uRL3d8icM4cBqnnRx23iuhF2enn83g1MH1Huf2vO2szV5LQVkB+WX5FJQV2Nel+fjER6wrlhhXDLGu2OAUHRFNlDOKqIio4DzSGUmP+B50je3a6HMuIuzM38mB4gM4jANjjJ1jgu8BDP65/32pp5RCdyGFZYUUuYuCr/NK8zhYcjA4HSg5QF5JHg7jIMYVQ0yEPa7APL1bOsf0O4bDUg8Lbrs2Xp+XLQe3sC5nHeuy17E2Zy3rstex6cAmxvQcwxnDzuDUoaeSFpcW8vH7xEducS57C/ayr3AfB0sOkleaR15JXvB1fmk+gmAwGGOC8+iIaCb1n8S0QdNIiEpo8Ln3iY+12WtZm72W3gm9GZw6mO5x3audBxFhV/4ufsj+gR/2/cCOQzuIjogmLjKOOFdccJ4YlWh/7wm9SItLw2Eq31e6vW625W1j44GNbDqwiYKyAoanDSejWwZ9E/vWev5FJHge4iPjSYhKIMJRfnny+rx8v+97lm5dyrJty1i6dSn7Cu3gWBGOCDK6ZZDVM4uxvcaS1SuLAckDMBgECW4/sGxcZByRzupDJwZu+Hbl72LnoZ3syt9FsaeYSGdktcnj81BQVkBhWWHwd1lQVmBfuwsrfVdQVkBCZAL9kvrRN7Ev/ZL62ddJfSkoK2Dzgc1sPuifDmxma95WPD4PMRExREdEB3/TMa4YZqXPYvao2Q3+HTSECZysZt+wMU7gJ2AasAP4BjhXRNZWWGY2kCUiV4W63aysLFm+vPEPg196ydY4+uwzOOYY4W//+Rt//eKvOB1O4l2x4N5CSmwfuiQOp9RTyje7vqHIXQTAkNQhHNPvGNK7pbO/aD+7C3azK39XcJ5XkkdKTApdYrqQGpManDbkbuCL7V/QNbYr1467liuPvJKUmJRgmn7M+ZFHv3mUZ1c/y6HSQ/RK6EWEIwKvz4tXvMH5geIDCMK43uM4f8T5zMqYRbe4boD90a/cvZK3f3ybd356h2/3fFvj8TuMA4dx4PF5GnTeeiX0IrN7JiO7jySzeyaZPTLpldArGLwC2zXGsC1vGyt3r2Tl7pV8u+dbVu5eSU5RTiP/YrWLj4wnOTqZlOgUkqKT8ImPYncxxZ5iitxFFLuLKSgroNhTDECP+B4c0+8YJvWfxC/6/oIidxE/7f+Jn/b/xM+5P/PT/p/YkLuh0p1nj/geDOs6jH5J/Vi2bRmbDmzCYRxM6j+JM4adwcwjZuL1edmWt41tedvYfmh78PWegj3sLbSBoK7zHeeKIyEqAYdxICIIEpwXlBVQ5C4i0hnJlAFTOHnIyZx0+EkMShlU47YCQWDJliUs2bKEz7Z+Vu3cx7piGZQyiEEpg0iNSeWn/T/xw74fyCvNq7RMqacUbx2t/CMcEfSMtzdFsa5YNh/czLa8bfjEV+PyiVGJZHTLICMtg25x3diVv4vth7az/dB2dhzaQUFZQaXlo5xRJEQlkBCZQG5xbjB9/ZP6M6n/JCb1n0RKdAord69k+e7lLN+1nNzi3FrTW5HL4QoGuvjIeEo8JezK34W7CbUPA9uquN24yDhiXbEcKj3Etrxt7Di0gzJvWY3r90roxcDkgQxIHkCkM5JiTzElnpLgb7rYXcxFmRdx1VEhXy4rMcasEJGsepcLY1CYANwpIif4398KICL/V2GZ2bRwUDjxRFi7Fr5bn89l717K/LXzmX7YdPon9aegrICd+96j2Asm2g63dlSvozim/zEc3e9oesT3aPR+P9/2Ofd+fi/v/fwe8ZHxXDH2Csb1GccTK5/gw40f4nK4ODv9bK466irG9R5X4x3V9rztvLrmVV76/iVW712N0ziZNnga/RL78d7P77EzfycO4+AXfX/BzMNnMqHvBJKikoiPjA9O0RHRGGPw+DwUu+3FMzCVeEoo9ZZS6imtNN+Wt43v9n7H6r2rWZe9LuR/HJfDRUa3DMb0HMPoHqPpHt8dEcEnPgQ7D1xAAr/Dind2URFRle5SY12xxEXGkRSVRFJ0UqU7ydr4xMf6nPXBnN7SrUvZfmh7tXQelnoYQ7oM4fDUwxnadSjD04YztOvQSsFbRFi9dzUL1i5gwboFrMtZV+M+02LT6JvUl57xPeke153u8d2D825x3UiNSQ0eQ2JUYp3H4fa6+c/2//DOT+/w7k/v8uP+HwEYlDKImIiYSjcNXp83mIsCe/GcMmAKUwZMIbN7JnsK9gTv4Dcd2MTGAxvZX7Sfw7sczvC04aSnpZPeLZ30tHTS4tIQEcq8ZcE74UJ3IQdLDrI7fzc78+2d9M78new8tJMidxEDkgcwOGUwg1MHMyhlEINTBhPjimFt9lrW7FvD93u/Z022nR8sOUjPhJ70SexD38S+wXliVCKF7kLyS/PJL8snvzSfAncBsRGxHNP/GI7pdwz9k/vXeK5EhK15W1m+azm78ndVy3kaDG6fu/JdfVkhBe4CopxR9E7oTa+EXvRO9M8TehPrisXtc1PmLQtOpZ5SIhwRlQJAjCumWq6ptt/jvsJ9bM+zNw+BAN0/uT/REdH1rt8UbSEonAlMF5HL/O8vBMZVDAD+oPB/QDY2V3G9iGyvYXNBTQkKe/dCr15w2S0/8Xnv01ifs577fnkfN064MfjD2bjxFnbs+AdHH30Ap7OZR6sHvtv7Hfd9cR+vrnkVn/jondCb32T9hsvGXEb3+O4hb2fNvjW89N1LvLzmZXKLczlh8AnMPGImM4bMaFJRT33KvGWsz1nP6j2rySnKCV7YK0494nswpucY0rul15hNb21bD27lqx1fkRSdxOFdDqdfUr+QAkxV67LX8eHGD4mPjKd/cn/6JfWjT2KfOosIm+rn/T/z3s/v8fm2z/GJL1i0GJjHumKZ0GcCkwdMZkDygLCloylEBK94G3XOVeO1l6DQBSgQkVJjzP8As0Rkag3bmgPMAejXr9/YrVu3NipNc+fCdY++Q/xFFxDlcvHama9x3KDjKi2Tm/sh3313AiNHLiI19fhG7ScUmw5sYmPuRo4deGyT/jkCxQyh3KUopTqvUINCOK8kO4G+Fd73ofyBMgAisl9EAlV9ngTG1rQhEZknIlkikpWWFvoDvop84uO+/94J583k8LTDWDFnRbWAAJCUNBFjXGGvmjooZRDTBk9r8t1S4KGtUko1h3BeTb4BhhhjBhpjIoFzgLcrLmCM6Vnh7Uyg5kLaZvDnhU+x+/C7yIq4mM8v+bzWckmnM47ExHHaiE0p1SmFrVBPRDzGmKuARdgqqU+LyA/GmLuB5SLyNnCNMWYm4AFygdnhSs/wstnEv5/MW2+cSYyr7mqJyclT2br1z3g8eUREJIUrSUop1eaE7ZlCuDTlQbPHAxEhhMEDB5awevWxZGS8Q9euJzdqX0op1Za0hWcKbU4oAQEgMXE8Dke0dqWtlOp0OlVQCJXTGU1i4kTtB0kp1eloUKhFSspUCgtXU1i4tv6FlVKqg9CgUIsePS7F5Upj7dpz8XqLWzs5SinVIjQo1CIqqgdDhz5HYeF3bNz429ZOjlJKtQgNCnXo0uVE+vS5kV27HiU7+83WTo5SSoWdBoV6DBr0FxISsvjxx19TUrKttZOjlFJhpUGhHg5HJMOGvYKIm7Vrz8PXwC6nlVKqPdGgEILY2MM4/PDHOXToC7Zuvbu1k6OUUmGjQSFE3bufT48es9m69c8cOKD9IimlOiYNCg1w2GEPERNzOGvXzmL79gfwePLqX0kppdoRDQoNEBERT0bGG8TEDGHjxuv5z39689NPv6Gw8IfWTppSSjULHfqogeLihjNmzBfk569k586H2b37GXbtepzk5Kn07Hk5iYnjiY7uX+8A8Uop1RZ1ql5Sw6GsLIfdu59k165HKS21I4lGRKQQHz+K+PjRxMePIi4ug8jInkRGpmGMs87tifgQ8eEIcfAdWxvKh8PR9oa9VEq1Ha0+HGe4tLWgEODzeSgoWEF+/rcUFHxLQcEqCgu/w+crqbCUA5erK5GRPYiM7I7TGY/Hk4fHcxCP54B/ngcILlc3oqJ6EhnZi6ioXkRG9sLhiKK0dBdlZTspLbVTWdkejHEQGzvcH4gCwSgTlyultuQ28hhLcbv343bn+OfZlJXtw+3OrvA6B5crlcTEX5CUNIH4+LE4nXUPSC7iw+crwecrxustwucrxucrISIihaio3hgdWU6pJtOg0Ab4fB6Ki3+kqGg9ZWV7KCvb65/sa6+3gIiIZFyuFCIikoMTOCgr2+0PALsoK9tNWdleQIiISCYysjdRUeWTiIeCgtUUFHxLWdme4P4jI3sSHd2fqKh+Feb9cLm64nBEYUwkDkdkcO7x5FNaupWSki2UlJTPy8p243bn4PXm13KkBperCy5XGi5XV0pLd1FSstF+Y1zEx48hKWkCTmciZWV7cbv3Bs+F270Pr7eg1nNoTBQxMQOJjh5MTIyd4uIyiI8fg8uV3Hx/rFp4vYX+v8NeoqL6NKpoUMSH252L270Pj+cgERGpREX1wulMaHIxo8/npqhoLfn5K8jPX0Fx8QYiI3v6z1n5FBXVKyzB1ecro7R0F17vISIje+JyddWi0zaqTQQFY8x0YC525LUnReTeKt9HAc9jx2beD8wSkS11bbM9BYXm5PN5ECnD6Yytc7nS0j0UFq4mP/9biot/oqRkG6Wl2ygp2Ub5cNj1MyYiGEwiI3sRGWkv+NWnNFyuLtWKxcrK9nLo0Ffk5X3JoUP/IT//G3y+Un9OqTsuV3ciI7sTGdkNpzMJpzMWhyMGhyMGpzMGhyMatzuH4uKNFaYN+HyFwX1ERw8mIWFscIqISAEEER/g8xfFeXG791Faut1/Lrb7p52AA6czDqczPjh3OGLweHL9ObFdeL2Va5g5nQnExaUTFzeCuLgMYmOH4fMV+3NJeykr2xcMduW5qBzAV+0cOxxxwVxgZGQP/01BIk5nIhERSUREJOJwxOLzleLzFflzUYV4vUV4PLnk539LYeHqYG7U6UwgJmaI/6ZjV9W9YYwLYyJwOOzcvo72/y16+P8ePfxpSUGktELOrRivtxivN4/S0h3BKXCzUv67ifLfrPQhKqo3ERGp/vUD6bdzkTLABCcbSIw/Pd2IjOzm/53Y1y5XGhERqbhcqUREpOBwuIL79HgOUVT0I0VFP1Jc/BNFRT9SVrYXn68EkVJ/LrTU/97nP/7yyW7LiTFOf+Cs+DoQSMvTaF+7/L/ZWP/v1f5+jTH4fGWIuINzkTIcjrga/39E3Ljd+/F49vtz3/vxeHIxJhKXq4v/mLsEp6ioPrhcXRNrE1AAAAj+SURBVEL6H66q1YOCsVeJn4BpwA7smM3nisjaCsv8LzBSRK4wxpwDnCYis+rabmcNCk0lIrjd2ZSUbMXjyfX/YMsqzR2OGKKjB/hzFb3qff7REIGW4KE+K6mJiFBWttcf9FYEp9LSrSGt73BEExXV1z/1BmxOwOstCM59viL/nXxv/wXbzl2ubpSWbqWwcA2FhWsoKPgej2d/tX04nUnBi5i9qKUF37tc3YiISPYHHZsLrJgb9HgO4fUeqlLkWBMnERGJxMWN9AfELBISxhITc1gwN+D1llBaupXi4s2UlGymrGyX/2/t8U9uRDx4vUX+ALaHsrI9/gBW2zXB4HQm+C/4fYmO7ht87XQm+HO3O/wB1QYNj+dghYtnIPDHBp+B2QAuwcmmxxZFVg3Ilc9zPBERqYiUVcodgyOYM7L7isLhiPZPUYDDf+xufD538LWIFxEv9mai4lyw18jySUT86xfh9dqAFyj6BPHnvF0YExkMOoFAXj9DREQSPl8ZPl9RtW/79v0tgwf/LYTt1LDlEINCOGsfHQVsEJFN/gS9CpwCVByg4BTgTv/r+cDDxhgj7a1Mqx0wxvjvvrq1yv6bEgwCjDFERfUgKqoHqaknBD8vK8uhoGCV/5/IVLjDs69drjSiovr6czTNU7QRCFDFxT/idMbjctkLf33PT0Lh85Xh9ebj8eTj9RbgcEThdMYFL6z2Drfu43A6o4mNPYLY2CMauG8Pbnc2Hs8B/4W0PPdmL3ItVzTk85VSVpaN2x3IcR3A48nF7c4Nzo1xEht7BDExh/vng/wX/5YnInWeH5/P4093TnAyxhXMBUREdMHlSgnejHm9Jf7l9wdzE9HRg8N+HOEMCr2B7RXe7wDG1baMiHiMMXlAFyAnjOlSHUxkZFdSU3/ZovusGKCam8MRicPRpdHFBE3bdwRRUT2JiurZ4vuunpYooqP7EB3dp7WTEpL6AqbDEdGgGzOnMxqn01Y0aUntolqHMWaOMWa5MWZ5dnZ2aydHKaU6rHAGhZ1A3wrv+/g/q3EZY0wEkIR94FyJiMwTkSwRyUpLSwtTcpVSSoUzKHwDDDHGDDTGRALnAG9XWeZt4GL/6zOBT/R5glJKtZ6wPVPwPyO4CliErZL6tIj8YIy5G1guIm8DTwEvGGM2ALnYwKGUUqqVhLXvIxFZCCys8tkdFV6XAGeFMw1KKaVC1y4eNCullGoZGhSUUkoFaVBQSikV1O46xDPGZAOh9WtQXVc6V8M4Pd6OqzMdK+jxNof+IlJvnf52FxSawhizPJS+PzoKPd6OqzMdK+jxtiQtPlJKKRWkQUEppVRQZwsK81o7AS1Mj7fj6kzHCnq8LaZTPVNQSilVt86WU1BKKVWHThMUjDHTjTE/GmM2GGNuae30NDdjzNPGmH3GmDUVPks1xnxkjPnZP09pzTQ2F2NMX2PMp8aYtcaYH4wx1/o/76jHG22M+a8xZrX/eO/yfz7QGPO1/zf9mr/jyQ7BGOM0xnxrjHnX/74jH+sWY8z3xphVxpjl/s9a7bfcKYKCf2jQR4ATgeHAucaY4a2bqmb3LDC9yme3AB+LyBDgY//7jsAD3Cgiw4HxwJX+v2dHPd5SYKqIZAKjgOnGmPHAfcA/ReQw4ADw61ZMY3O7FlhX4X1HPlaAY0VkVIVqqK32W+4UQYEKQ4OKHTE8MDRohyEiS7E9zVZ0CvCc//VzwKktmqgwEZHdIrLS/zofe/HoTcc9XhGRAv9bl38SYCp2GFvoQMdrjOkDnAQ86X9v6KDHWodW+y13lqBQ09CgvVspLS2pu4js9r/eA3RvzcSEgzFmADAa+JoOfLz+4pRVwD7gI2AjcFBEPP5FOtJv+gHgZsDnf9+FjnusYAP8h8aYFcaYOf7PWu23HNaus1XbISJijOlQVc2MMfHAAuA6ETlUcYzcjna8IuKF/9/e/YRYVYZxHP/+ygjTaEgMIlGxFkUgE4FgGgyFLkKkRX8glWjdpkUQRiEIbhMXQi5aGI2ShaPrMhl0Ef2VinIVLXThbCowKMJ+Ld73nG4zgcMwc8907u+zuXPfczm8D5wzzznv4TwP45LGgCngwY6ntCQk7QJmbH8paaLr+QzJdttXJd0DfCTp8uDGYR/Lo3KnMJ/WoH10TdK9APVzpuP5LBpJt1ESwqTt03W4t/E2bP8CnAe2AmO1jS3055jeBuyW9BNlmfcJ4Aj9jBUA21fr5wwl4W+hw2N5VJLCfFqD9tFgu9MXgbMdzmXR1DXmd4AfbL81sKmv8a6tdwhIWgnsoDxHOU9pYws9idf2ftvrbG+knKef2N5DD2MFkLRK0p3N38BO4Ds6PJZH5uU1SU9R1iqb1qCHOp7SopJ0EpigVFe8BhwAzgCngPWUyrLP2Z79MPp/R9J24ALwLf+sO79Oea7Qx3g3Ux423kq5kDtl+6CkTZSr6buBr4G9tv/obqaLqy4fvWp7V19jrXFN1a8rgBO2D0laQ0fH8sgkhYiIuLlRWT6KiIh5SFKIiIhWkkJERLSSFCIiopWkEBERrSSFiCGSNNFU/oxYjpIUIiKilaQQ8R8k7a09DC5JOlYL0l2XdLj2NDgnaW397bikTyV9I2mqqX0v6QFJH9c+CF9Jur/ufrWkDyVdljSpwaJNER1LUoiYRdJDwPPANtvjwA1gD7AK+ML2w8A05a1xgHeB12xvprxl3YxPAkdrH4THgKbq5SPAK5TeHpso9X4iloVUSY2Y60ngUeDzehG/klKQ7C/g/fqb94DTku4CxmxP1/HjwAe1ns19tqcAbP8OUPf3me0r9fslYCNwcenDiri5JIWIuQQct73/X4PSm7N+t9AaMYM1e26Q8zCWkSwfRcx1Dnim1rdv+uVuoJwvTaXOF4CLtn8Ffpb0eB3fB0zXjnBXJD1d93G7pDuGGkXEAuQKJWIW299LeoPSDesW4E/gZeA3YEvdNkN57gCltPHb9Z/+j8BLdXwfcEzSwbqPZ4cYRsSCpEpqxDxJum57ddfziFhKWT6KiIhW7hQiIqKVO4WIiGglKURERCtJISIiWkkKERHRSlKIiIhWkkJERLT+BnN97PfeDHc7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 1.9808 - acc: 0.5047\n",
      "Loss: 1.9808175353617683 Accuracy: 0.5046729\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9201 - acc: 0.4499\n",
      "Epoch 00001: val_loss improved from inf to 1.70522, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_4_conv_checkpoint/001-1.7052.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 1.9203 - acc: 0.4499 - val_loss: 1.7052 - val_acc: 0.4815\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1052 - acc: 0.6702\n",
      "Epoch 00002: val_loss improved from 1.70522 to 1.37478, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_4_conv_checkpoint/002-1.3748.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 1.1053 - acc: 0.6702 - val_loss: 1.3748 - val_acc: 0.6161\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7499 - acc: 0.7753\n",
      "Epoch 00003: val_loss improved from 1.37478 to 1.20664, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_4_conv_checkpoint/003-1.2066.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.7502 - acc: 0.7752 - val_loss: 1.2066 - val_acc: 0.6525\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4997 - acc: 0.8536\n",
      "Epoch 00004: val_loss did not improve from 1.20664\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.4998 - acc: 0.8536 - val_loss: 1.2311 - val_acc: 0.6518\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.9087\n",
      "Epoch 00005: val_loss improved from 1.20664 to 1.19192, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_4_conv_checkpoint/005-1.1919.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.3354 - acc: 0.9086 - val_loss: 1.1919 - val_acc: 0.6839\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9457\n",
      "Epoch 00006: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.2284 - acc: 0.9456 - val_loss: 1.2558 - val_acc: 0.6697\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9651\n",
      "Epoch 00007: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1720 - acc: 0.9651 - val_loss: 1.2345 - val_acc: 0.6816\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9722\n",
      "Epoch 00008: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1393 - acc: 0.9722 - val_loss: 1.2725 - val_acc: 0.6723\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9810\n",
      "Epoch 00009: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1093 - acc: 0.9809 - val_loss: 1.2970 - val_acc: 0.6820\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9858\n",
      "Epoch 00010: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0854 - acc: 0.9857 - val_loss: 1.2845 - val_acc: 0.6949\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9807\n",
      "Epoch 00011: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0995 - acc: 0.9807 - val_loss: 1.3107 - val_acc: 0.6813\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9840\n",
      "Epoch 00012: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0820 - acc: 0.9840 - val_loss: 1.3460 - val_acc: 0.6820\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9920\n",
      "Epoch 00013: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0559 - acc: 0.9919 - val_loss: 1.4466 - val_acc: 0.6678\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9901\n",
      "Epoch 00014: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0602 - acc: 0.9901 - val_loss: 1.4063 - val_acc: 0.6744\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9939\n",
      "Epoch 00015: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0456 - acc: 0.9939 - val_loss: 1.5263 - val_acc: 0.6744\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9880\n",
      "Epoch 00016: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0644 - acc: 0.9880 - val_loss: 1.4885 - val_acc: 0.6818\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9892\n",
      "Epoch 00017: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0580 - acc: 0.9892 - val_loss: 1.4797 - val_acc: 0.6767\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9936\n",
      "Epoch 00018: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0401 - acc: 0.9935 - val_loss: 1.5843 - val_acc: 0.6674\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9881\n",
      "Epoch 00019: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0561 - acc: 0.9881 - val_loss: 1.5155 - val_acc: 0.6816\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9943\n",
      "Epoch 00020: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0362 - acc: 0.9943 - val_loss: 1.5211 - val_acc: 0.6844\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9933\n",
      "Epoch 00021: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0403 - acc: 0.9933 - val_loss: 1.6176 - val_acc: 0.6753\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9917\n",
      "Epoch 00022: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0418 - acc: 0.9917 - val_loss: 1.5696 - val_acc: 0.6790\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9935\n",
      "Epoch 00023: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0355 - acc: 0.9935 - val_loss: 1.7185 - val_acc: 0.6604\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9919\n",
      "Epoch 00024: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0405 - acc: 0.9919 - val_loss: 1.6262 - val_acc: 0.6823\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9980\n",
      "Epoch 00025: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0198 - acc: 0.9980 - val_loss: 1.6115 - val_acc: 0.6907\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9963\n",
      "Epoch 00026: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0270 - acc: 0.9963 - val_loss: 1.7046 - val_acc: 0.6741\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9927\n",
      "Epoch 00027: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0365 - acc: 0.9927 - val_loss: 1.6437 - val_acc: 0.6837\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9963\n",
      "Epoch 00028: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0232 - acc: 0.9963 - val_loss: 1.7523 - val_acc: 0.6753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9922\n",
      "Epoch 00029: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0364 - acc: 0.9921 - val_loss: 1.8698 - val_acc: 0.6564\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9889\n",
      "Epoch 00030: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0466 - acc: 0.9888 - val_loss: 1.7705 - val_acc: 0.6713\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9942\n",
      "Epoch 00031: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0316 - acc: 0.9941 - val_loss: 1.7353 - val_acc: 0.6846\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9951\n",
      "Epoch 00032: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0285 - acc: 0.9950 - val_loss: 1.7957 - val_acc: 0.6781\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9936\n",
      "Epoch 00033: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0317 - acc: 0.9936 - val_loss: 1.7281 - val_acc: 0.6785\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9951\n",
      "Epoch 00034: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0262 - acc: 0.9951 - val_loss: 1.8287 - val_acc: 0.6629\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9968\n",
      "Epoch 00035: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0216 - acc: 0.9967 - val_loss: 1.8365 - val_acc: 0.6888\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9959\n",
      "Epoch 00036: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0253 - acc: 0.9958 - val_loss: 1.8509 - val_acc: 0.6629\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9917\n",
      "Epoch 00037: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0407 - acc: 0.9917 - val_loss: 2.0002 - val_acc: 0.6594\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9947\n",
      "Epoch 00038: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0269 - acc: 0.9946 - val_loss: 1.8912 - val_acc: 0.6676\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9949\n",
      "Epoch 00039: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0265 - acc: 0.9949 - val_loss: 1.8622 - val_acc: 0.6811\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9917\n",
      "Epoch 00040: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0364 - acc: 0.9917 - val_loss: 1.8601 - val_acc: 0.6802\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9976\n",
      "Epoch 00041: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0170 - acc: 0.9976 - val_loss: 1.9696 - val_acc: 0.6653\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9917\n",
      "Epoch 00042: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0379 - acc: 0.9917 - val_loss: 1.9312 - val_acc: 0.6718\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9968\n",
      "Epoch 00043: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0209 - acc: 0.9968 - val_loss: 1.8727 - val_acc: 0.6818\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9951\n",
      "Epoch 00044: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0246 - acc: 0.9951 - val_loss: 1.9341 - val_acc: 0.6778\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9952\n",
      "Epoch 00045: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0242 - acc: 0.9952 - val_loss: 1.9530 - val_acc: 0.6730\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9971\n",
      "Epoch 00046: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0197 - acc: 0.9971 - val_loss: 2.0472 - val_acc: 0.6615\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9925\n",
      "Epoch 00047: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0326 - acc: 0.9925 - val_loss: 2.0757 - val_acc: 0.6699\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9981\n",
      "Epoch 00048: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0142 - acc: 0.9981 - val_loss: 1.9919 - val_acc: 0.6788\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9966\n",
      "Epoch 00049: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0197 - acc: 0.9966 - val_loss: 2.0827 - val_acc: 0.6634\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9940\n",
      "Epoch 00050: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0269 - acc: 0.9940 - val_loss: 2.0068 - val_acc: 0.6690\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9930\n",
      "Epoch 00051: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0305 - acc: 0.9930 - val_loss: 2.1787 - val_acc: 0.6536\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9970\n",
      "Epoch 00052: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0179 - acc: 0.9970 - val_loss: 2.2774 - val_acc: 0.6497\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9933\n",
      "Epoch 00053: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0282 - acc: 0.9932 - val_loss: 2.1398 - val_acc: 0.6601\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9938\n",
      "Epoch 00054: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0276 - acc: 0.9938 - val_loss: 2.1038 - val_acc: 0.6681\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9979\n",
      "Epoch 00055: val_loss did not improve from 1.19192\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0134 - acc: 0.9979 - val_loss: 2.1526 - val_acc: 0.6636\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 1.3364 - acc: 0.6432\n",
      "Loss: 1.3364254650916143 Accuracy: 0.6431983\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8111 - acc: 0.4643\n",
      "Epoch 00001: val_loss improved from inf to 1.49466, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_5_conv_checkpoint/001-1.4947.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 1.8111 - acc: 0.4642 - val_loss: 1.4947 - val_acc: 0.5320\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1281 - acc: 0.6648\n",
      "Epoch 00002: val_loss improved from 1.49466 to 1.15129, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_5_conv_checkpoint/002-1.1513.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 1.1283 - acc: 0.6648 - val_loss: 1.1513 - val_acc: 0.6811\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8738 - acc: 0.7400\n",
      "Epoch 00003: val_loss improved from 1.15129 to 1.03346, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_5_conv_checkpoint/003-1.0335.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.8740 - acc: 0.7400 - val_loss: 1.0335 - val_acc: 0.7107\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6879 - acc: 0.7911\n",
      "Epoch 00004: val_loss improved from 1.03346 to 0.97241, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_5_conv_checkpoint/004-0.9724.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.6881 - acc: 0.7910 - val_loss: 0.9724 - val_acc: 0.7198\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.8374\n",
      "Epoch 00005: val_loss did not improve from 0.97241\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.5376 - acc: 0.8374 - val_loss: 0.9783 - val_acc: 0.7242\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8783\n",
      "Epoch 00006: val_loss did not improve from 0.97241\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.4125 - acc: 0.8783 - val_loss: 1.0176 - val_acc: 0.7349\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.9082\n",
      "Epoch 00007: val_loss improved from 0.97241 to 0.95411, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_5_conv_checkpoint/007-0.9541.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.3214 - acc: 0.9082 - val_loss: 0.9541 - val_acc: 0.7382\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9277\n",
      "Epoch 00008: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.2591 - acc: 0.9276 - val_loss: 0.9866 - val_acc: 0.7403\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9542\n",
      "Epoch 00009: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.1851 - acc: 0.9542 - val_loss: 0.9647 - val_acc: 0.7498\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9622\n",
      "Epoch 00010: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.1589 - acc: 0.9622 - val_loss: 1.0260 - val_acc: 0.7379\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9696\n",
      "Epoch 00011: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.1360 - acc: 0.9696 - val_loss: 1.0047 - val_acc: 0.7526\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9784\n",
      "Epoch 00012: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.1075 - acc: 0.9784 - val_loss: 1.0640 - val_acc: 0.7345\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9811\n",
      "Epoch 00013: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0946 - acc: 0.9811 - val_loss: 1.0210 - val_acc: 0.7417\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9864\n",
      "Epoch 00014: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0767 - acc: 0.9864 - val_loss: 1.0494 - val_acc: 0.7452\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9801\n",
      "Epoch 00015: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0904 - acc: 0.9801 - val_loss: 1.1462 - val_acc: 0.7254\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9871\n",
      "Epoch 00016: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0708 - acc: 0.9871 - val_loss: 1.0509 - val_acc: 0.7470\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9914\n",
      "Epoch 00017: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0526 - acc: 0.9914 - val_loss: 1.2807 - val_acc: 0.7179\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9889\n",
      "Epoch 00018: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0599 - acc: 0.9889 - val_loss: 1.1525 - val_acc: 0.7326\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9854\n",
      "Epoch 00019: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0675 - acc: 0.9854 - val_loss: 1.3045 - val_acc: 0.7007\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9878\n",
      "Epoch 00020: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0596 - acc: 0.9878 - val_loss: 1.1724 - val_acc: 0.7389\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9899\n",
      "Epoch 00021: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0519 - acc: 0.9898 - val_loss: 1.2280 - val_acc: 0.7333\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9890\n",
      "Epoch 00022: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0542 - acc: 0.9890 - val_loss: 1.1332 - val_acc: 0.7473\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9939\n",
      "Epoch 00023: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0388 - acc: 0.9938 - val_loss: 1.2722 - val_acc: 0.7335\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9855\n",
      "Epoch 00024: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0617 - acc: 0.9855 - val_loss: 1.1951 - val_acc: 0.7454\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9882\n",
      "Epoch 00025: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0529 - acc: 0.9882 - val_loss: 1.1842 - val_acc: 0.7407\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9902\n",
      "Epoch 00026: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0450 - acc: 0.9902 - val_loss: 1.2696 - val_acc: 0.7251\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9963\n",
      "Epoch 00027: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0271 - acc: 0.9963 - val_loss: 1.1327 - val_acc: 0.7529\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9950\n",
      "Epoch 00028: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0306 - acc: 0.9950 - val_loss: 1.2963 - val_acc: 0.7284\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9911\n",
      "Epoch 00029: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0433 - acc: 0.9911 - val_loss: 1.4451 - val_acc: 0.7177\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9906\n",
      "Epoch 00030: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0439 - acc: 0.9906 - val_loss: 1.2909 - val_acc: 0.7340\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9915\n",
      "Epoch 00031: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0396 - acc: 0.9914 - val_loss: 1.2387 - val_acc: 0.7454\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9900\n",
      "Epoch 00032: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0444 - acc: 0.9900 - val_loss: 1.3511 - val_acc: 0.7319\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9919\n",
      "Epoch 00033: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0382 - acc: 0.9918 - val_loss: 1.2703 - val_acc: 0.7517\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9938\n",
      "Epoch 00034: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0312 - acc: 0.9938 - val_loss: 1.2377 - val_acc: 0.7563\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9918\n",
      "Epoch 00035: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0379 - acc: 0.9918 - val_loss: 1.2929 - val_acc: 0.7526\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9964\n",
      "Epoch 00036: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0239 - acc: 0.9964 - val_loss: 1.2877 - val_acc: 0.7468\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9881\n",
      "Epoch 00037: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0485 - acc: 0.9881 - val_loss: 1.2160 - val_acc: 0.7626\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9946\n",
      "Epoch 00038: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0300 - acc: 0.9946 - val_loss: 1.4477 - val_acc: 0.7284\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9945\n",
      "Epoch 00039: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0299 - acc: 0.9944 - val_loss: 1.3727 - val_acc: 0.7361\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9903\n",
      "Epoch 00040: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0413 - acc: 0.9903 - val_loss: 1.3148 - val_acc: 0.7538\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9967\n",
      "Epoch 00041: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0227 - acc: 0.9967 - val_loss: 1.4124 - val_acc: 0.7345\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9939\n",
      "Epoch 00042: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0303 - acc: 0.9939 - val_loss: 1.2588 - val_acc: 0.7657\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9969\n",
      "Epoch 00043: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0205 - acc: 0.9969 - val_loss: 1.3256 - val_acc: 0.7522\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9954\n",
      "Epoch 00044: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0263 - acc: 0.9954 - val_loss: 1.5802 - val_acc: 0.7205\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9862\n",
      "Epoch 00045: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0527 - acc: 0.9862 - val_loss: 1.3133 - val_acc: 0.7598\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9961\n",
      "Epoch 00046: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0225 - acc: 0.9961 - val_loss: 1.3214 - val_acc: 0.7587\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9962\n",
      "Epoch 00047: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0220 - acc: 0.9962 - val_loss: 1.2908 - val_acc: 0.7626\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9923\n",
      "Epoch 00048: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0333 - acc: 0.9923 - val_loss: 1.3261 - val_acc: 0.7524\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9922\n",
      "Epoch 00049: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0341 - acc: 0.9921 - val_loss: 1.3408 - val_acc: 0.7536\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9933\n",
      "Epoch 00050: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0304 - acc: 0.9933 - val_loss: 1.2915 - val_acc: 0.7638\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9984\n",
      "Epoch 00051: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0144 - acc: 0.9984 - val_loss: 1.3028 - val_acc: 0.7575\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9974\n",
      "Epoch 00052: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0182 - acc: 0.9974 - val_loss: 1.6930 - val_acc: 0.7065\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9885\n",
      "Epoch 00053: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0437 - acc: 0.9885 - val_loss: 1.4746 - val_acc: 0.7403\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9970\n",
      "Epoch 00054: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0197 - acc: 0.9970 - val_loss: 1.5445 - val_acc: 0.7345\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9926\n",
      "Epoch 00055: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0331 - acc: 0.9925 - val_loss: 1.3606 - val_acc: 0.7554\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9930\n",
      "Epoch 00056: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0299 - acc: 0.9929 - val_loss: 1.4461 - val_acc: 0.7512\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9952\n",
      "Epoch 00057: val_loss did not improve from 0.95411\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0231 - acc: 0.9951 - val_loss: 1.3891 - val_acc: 0.7487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.0303 - acc: 0.7092\n",
      "Loss: 1.030256156634195 Accuracy: 0.7092419\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8616 - acc: 0.4354\n",
      "Epoch 00001: val_loss improved from inf to 1.53369, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_6_conv_checkpoint/001-1.5337.hdf5\n",
      "36805/36805 [==============================] - 189s 5ms/sample - loss: 1.8615 - acc: 0.4354 - val_loss: 1.5337 - val_acc: 0.5164\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1312 - acc: 0.6581\n",
      "Epoch 00002: val_loss improved from 1.53369 to 1.05902, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_6_conv_checkpoint/002-1.0590.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 1.1313 - acc: 0.6580 - val_loss: 1.0590 - val_acc: 0.6876\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8969 - acc: 0.7334\n",
      "Epoch 00003: val_loss improved from 1.05902 to 0.98289, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_6_conv_checkpoint/003-0.9829.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.8969 - acc: 0.7334 - val_loss: 0.9829 - val_acc: 0.7121\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7433 - acc: 0.7835\n",
      "Epoch 00004: val_loss improved from 0.98289 to 0.84359, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_6_conv_checkpoint/004-0.8436.hdf5\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.7434 - acc: 0.7835 - val_loss: 0.8436 - val_acc: 0.7438\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6464 - acc: 0.8130\n",
      "Epoch 00005: val_loss improved from 0.84359 to 0.69395, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_6_conv_checkpoint/005-0.6940.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.6464 - acc: 0.8130 - val_loss: 0.6940 - val_acc: 0.8025\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5687 - acc: 0.8336\n",
      "Epoch 00006: val_loss improved from 0.69395 to 0.65861, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_6_conv_checkpoint/006-0.6586.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.5689 - acc: 0.8336 - val_loss: 0.6586 - val_acc: 0.8206\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4974 - acc: 0.8562\n",
      "Epoch 00007: val_loss improved from 0.65861 to 0.65232, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_6_conv_checkpoint/007-0.6523.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.4977 - acc: 0.8562 - val_loss: 0.6523 - val_acc: 0.8195\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4476 - acc: 0.8721\n",
      "Epoch 00008: val_loss did not improve from 0.65232\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4477 - acc: 0.8721 - val_loss: 0.7237 - val_acc: 0.7876\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8834\n",
      "Epoch 00009: val_loss did not improve from 0.65232\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.3949 - acc: 0.8833 - val_loss: 0.6624 - val_acc: 0.8146\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.9030\n",
      "Epoch 00010: val_loss improved from 0.65232 to 0.58089, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_6_conv_checkpoint/010-0.5809.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.3427 - acc: 0.9031 - val_loss: 0.5809 - val_acc: 0.8435\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9167\n",
      "Epoch 00011: val_loss did not improve from 0.58089\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.2925 - acc: 0.9167 - val_loss: 0.5824 - val_acc: 0.8397\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9277\n",
      "Epoch 00012: val_loss improved from 0.58089 to 0.56591, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_6_conv_checkpoint/012-0.5659.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.2579 - acc: 0.9277 - val_loss: 0.5659 - val_acc: 0.8465\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9317\n",
      "Epoch 00013: val_loss did not improve from 0.56591\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2420 - acc: 0.9316 - val_loss: 0.5971 - val_acc: 0.8425\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9445\n",
      "Epoch 00014: val_loss did not improve from 0.56591\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2046 - acc: 0.9445 - val_loss: 0.5791 - val_acc: 0.8402\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9535\n",
      "Epoch 00015: val_loss did not improve from 0.56591\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1719 - acc: 0.9535 - val_loss: 0.5867 - val_acc: 0.8328\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9593\n",
      "Epoch 00016: val_loss did not improve from 0.56591\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1545 - acc: 0.9593 - val_loss: 0.6087 - val_acc: 0.8386\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9667\n",
      "Epoch 00017: val_loss did not improve from 0.56591\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1309 - acc: 0.9667 - val_loss: 0.6288 - val_acc: 0.8409\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9630\n",
      "Epoch 00018: val_loss improved from 0.56591 to 0.52928, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_6_conv_checkpoint/018-0.5293.hdf5\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.1363 - acc: 0.9630 - val_loss: 0.5293 - val_acc: 0.8628\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9768\n",
      "Epoch 00019: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0983 - acc: 0.9768 - val_loss: 0.5580 - val_acc: 0.8579\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9785\n",
      "Epoch 00020: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0909 - acc: 0.9785 - val_loss: 0.6106 - val_acc: 0.8456\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9742\n",
      "Epoch 00021: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1027 - acc: 0.9742 - val_loss: 0.5898 - val_acc: 0.8425\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9807\n",
      "Epoch 00022: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0844 - acc: 0.9806 - val_loss: 0.6523 - val_acc: 0.8325\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9819\n",
      "Epoch 00023: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0790 - acc: 0.9819 - val_loss: 0.5361 - val_acc: 0.8707\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9860\n",
      "Epoch 00024: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0645 - acc: 0.9860 - val_loss: 0.6039 - val_acc: 0.8521\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9870\n",
      "Epoch 00025: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0599 - acc: 0.9869 - val_loss: 0.5766 - val_acc: 0.8598\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9777\n",
      "Epoch 00026: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0846 - acc: 0.9776 - val_loss: 0.5568 - val_acc: 0.8637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9825\n",
      "Epoch 00027: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0709 - acc: 0.9824 - val_loss: 0.7253 - val_acc: 0.8309\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9866\n",
      "Epoch 00028: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0602 - acc: 0.9866 - val_loss: 0.5529 - val_acc: 0.8689\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9925\n",
      "Epoch 00029: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0414 - acc: 0.9925 - val_loss: 0.6033 - val_acc: 0.8539\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9836\n",
      "Epoch 00030: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0649 - acc: 0.9835 - val_loss: 0.5902 - val_acc: 0.8609\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9870\n",
      "Epoch 00031: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0555 - acc: 0.9870 - val_loss: 0.6099 - val_acc: 0.8516\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9933\n",
      "Epoch 00032: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0352 - acc: 0.9933 - val_loss: 0.5399 - val_acc: 0.8703\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9937\n",
      "Epoch 00033: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0354 - acc: 0.9937 - val_loss: 0.6563 - val_acc: 0.8477\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9877\n",
      "Epoch 00034: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0541 - acc: 0.9876 - val_loss: 0.6890 - val_acc: 0.8472\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9861\n",
      "Epoch 00035: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0539 - acc: 0.9861 - val_loss: 0.5458 - val_acc: 0.8754\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9949\n",
      "Epoch 00036: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0305 - acc: 0.9949 - val_loss: 0.6137 - val_acc: 0.8537\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9955\n",
      "Epoch 00037: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0273 - acc: 0.9955 - val_loss: 0.5589 - val_acc: 0.8721\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9941\n",
      "Epoch 00038: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0307 - acc: 0.9941 - val_loss: 0.7252 - val_acc: 0.8404\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9878\n",
      "Epoch 00039: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0514 - acc: 0.9877 - val_loss: 0.5666 - val_acc: 0.8677\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9891\n",
      "Epoch 00040: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0456 - acc: 0.9890 - val_loss: 0.6453 - val_acc: 0.8642\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9855\n",
      "Epoch 00041: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0543 - acc: 0.9855 - val_loss: 0.5536 - val_acc: 0.8733\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9964\n",
      "Epoch 00042: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0218 - acc: 0.9964 - val_loss: 0.6317 - val_acc: 0.8633\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9969\n",
      "Epoch 00043: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0209 - acc: 0.9968 - val_loss: 0.6346 - val_acc: 0.8647\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9845\n",
      "Epoch 00044: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0587 - acc: 0.9844 - val_loss: 0.7156 - val_acc: 0.8502\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9910\n",
      "Epoch 00045: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0391 - acc: 0.9909 - val_loss: 0.5905 - val_acc: 0.8751\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9922\n",
      "Epoch 00046: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0336 - acc: 0.9922 - val_loss: 0.5878 - val_acc: 0.8719\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9973\n",
      "Epoch 00047: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0175 - acc: 0.9973 - val_loss: 0.6656 - val_acc: 0.8609\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9944\n",
      "Epoch 00048: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0276 - acc: 0.9944 - val_loss: 0.6378 - val_acc: 0.8663\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9955\n",
      "Epoch 00049: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0243 - acc: 0.9955 - val_loss: 0.6824 - val_acc: 0.8598\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9920\n",
      "Epoch 00050: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0330 - acc: 0.9920 - val_loss: 0.6478 - val_acc: 0.8668\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9960\n",
      "Epoch 00051: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0221 - acc: 0.9959 - val_loss: 0.8029 - val_acc: 0.8516\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9877\n",
      "Epoch 00052: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0481 - acc: 0.9877 - val_loss: 0.6916 - val_acc: 0.8560\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9920\n",
      "Epoch 00053: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0327 - acc: 0.9919 - val_loss: 0.5773 - val_acc: 0.8793\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9952\n",
      "Epoch 00054: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0236 - acc: 0.9952 - val_loss: 0.5775 - val_acc: 0.8779\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9975\n",
      "Epoch 00055: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0160 - acc: 0.9975 - val_loss: 0.6817 - val_acc: 0.8595\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9874\n",
      "Epoch 00056: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0464 - acc: 0.9873 - val_loss: 0.6448 - val_acc: 0.8724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9903\n",
      "Epoch 00057: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0402 - acc: 0.9902 - val_loss: 0.6165 - val_acc: 0.8756\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9954\n",
      "Epoch 00058: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0229 - acc: 0.9954 - val_loss: 0.5976 - val_acc: 0.8791\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9981\n",
      "Epoch 00059: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0138 - acc: 0.9981 - val_loss: 0.6198 - val_acc: 0.8786\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9973\n",
      "Epoch 00060: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0162 - acc: 0.9972 - val_loss: 0.6294 - val_acc: 0.8747\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9899\n",
      "Epoch 00061: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0406 - acc: 0.9899 - val_loss: 0.6137 - val_acc: 0.8724\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9974\n",
      "Epoch 00062: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0165 - acc: 0.9974 - val_loss: 0.6386 - val_acc: 0.8684\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9966\n",
      "Epoch 00063: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0187 - acc: 0.9966 - val_loss: 0.6733 - val_acc: 0.8719\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9861\n",
      "Epoch 00064: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0491 - acc: 0.9860 - val_loss: 0.6538 - val_acc: 0.8686\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9919\n",
      "Epoch 00065: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0334 - acc: 0.9918 - val_loss: 0.6433 - val_acc: 0.8682\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9946\n",
      "Epoch 00066: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0252 - acc: 0.9946 - val_loss: 0.5880 - val_acc: 0.8824\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9980\n",
      "Epoch 00067: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0137 - acc: 0.9980 - val_loss: 0.6087 - val_acc: 0.8777\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9976\n",
      "Epoch 00068: val_loss did not improve from 0.52928\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0146 - acc: 0.9976 - val_loss: 0.6425 - val_acc: 0.8703\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.6444 - acc: 0.8258\n",
      "Loss: 0.6444138161366108 Accuracy: 0.82575285\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9638 - acc: 0.3945\n",
      "Epoch 00001: val_loss improved from inf to 1.67748, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/001-1.6775.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 1.9638 - acc: 0.3945 - val_loss: 1.6775 - val_acc: 0.4675\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1099 - acc: 0.6658\n",
      "Epoch 00002: val_loss improved from 1.67748 to 0.93467, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/002-0.9347.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 1.1099 - acc: 0.6658 - val_loss: 0.9347 - val_acc: 0.7312\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8124 - acc: 0.7629\n",
      "Epoch 00003: val_loss improved from 0.93467 to 0.69815, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/003-0.6981.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.8124 - acc: 0.7629 - val_loss: 0.6981 - val_acc: 0.7987\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6370 - acc: 0.8172\n",
      "Epoch 00004: val_loss improved from 0.69815 to 0.63787, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/004-0.6379.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.6372 - acc: 0.8171 - val_loss: 0.6379 - val_acc: 0.8157\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.8493\n",
      "Epoch 00005: val_loss improved from 0.63787 to 0.47954, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/005-0.4795.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.5225 - acc: 0.8492 - val_loss: 0.4795 - val_acc: 0.8628\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.8724\n",
      "Epoch 00006: val_loss improved from 0.47954 to 0.43350, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/006-0.4335.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.4431 - acc: 0.8724 - val_loss: 0.4335 - val_acc: 0.8730\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3736 - acc: 0.8929\n",
      "Epoch 00007: val_loss improved from 0.43350 to 0.41310, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/007-0.4131.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3736 - acc: 0.8929 - val_loss: 0.4131 - val_acc: 0.8826\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.9074\n",
      "Epoch 00008: val_loss improved from 0.41310 to 0.35044, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/008-0.3504.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3234 - acc: 0.9075 - val_loss: 0.3504 - val_acc: 0.8987\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9181\n",
      "Epoch 00009: val_loss did not improve from 0.35044\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2821 - acc: 0.9181 - val_loss: 0.4180 - val_acc: 0.8824\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9270\n",
      "Epoch 00010: val_loss improved from 0.35044 to 0.33263, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/010-0.3326.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2499 - acc: 0.9269 - val_loss: 0.3326 - val_acc: 0.9071\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9342\n",
      "Epoch 00011: val_loss did not improve from 0.33263\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2288 - acc: 0.9342 - val_loss: 0.3501 - val_acc: 0.9050\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9421\n",
      "Epoch 00012: val_loss did not improve from 0.33263\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2021 - acc: 0.9421 - val_loss: 0.3860 - val_acc: 0.8940\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9478\n",
      "Epoch 00013: val_loss did not improve from 0.33263\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1821 - acc: 0.9478 - val_loss: 0.3407 - val_acc: 0.9085\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9537\n",
      "Epoch 00014: val_loss improved from 0.33263 to 0.29487, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/014-0.2949.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1603 - acc: 0.9536 - val_loss: 0.2949 - val_acc: 0.9168\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9542\n",
      "Epoch 00015: val_loss did not improve from 0.29487\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1572 - acc: 0.9542 - val_loss: 0.3155 - val_acc: 0.9150\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9629\n",
      "Epoch 00016: val_loss improved from 0.29487 to 0.27136, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/016-0.2714.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1332 - acc: 0.9628 - val_loss: 0.2714 - val_acc: 0.9243\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9664\n",
      "Epoch 00017: val_loss did not improve from 0.27136\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1180 - acc: 0.9664 - val_loss: 0.3119 - val_acc: 0.9159\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9688\n",
      "Epoch 00018: val_loss did not improve from 0.27136\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1103 - acc: 0.9688 - val_loss: 0.3013 - val_acc: 0.9119\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9704\n",
      "Epoch 00019: val_loss did not improve from 0.27136\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1053 - acc: 0.9704 - val_loss: 0.2954 - val_acc: 0.9152\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9760\n",
      "Epoch 00020: val_loss did not improve from 0.27136\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0891 - acc: 0.9760 - val_loss: 0.3251 - val_acc: 0.9126\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9773\n",
      "Epoch 00021: val_loss did not improve from 0.27136\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0843 - acc: 0.9773 - val_loss: 0.2867 - val_acc: 0.9206\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9793\n",
      "Epoch 00022: val_loss did not improve from 0.27136\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0762 - acc: 0.9793 - val_loss: 0.2831 - val_acc: 0.9199\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9760\n",
      "Epoch 00023: val_loss did not improve from 0.27136\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0832 - acc: 0.9759 - val_loss: 0.3538 - val_acc: 0.9119\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9775\n",
      "Epoch 00024: val_loss improved from 0.27136 to 0.25158, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_7_conv_checkpoint/024-0.2516.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0799 - acc: 0.9774 - val_loss: 0.2516 - val_acc: 0.9313\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9875\n",
      "Epoch 00025: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0532 - acc: 0.9875 - val_loss: 0.2601 - val_acc: 0.9313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9875\n",
      "Epoch 00026: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0510 - acc: 0.9874 - val_loss: 0.3195 - val_acc: 0.9152\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9862\n",
      "Epoch 00027: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0538 - acc: 0.9862 - val_loss: 0.3126 - val_acc: 0.9203\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9836\n",
      "Epoch 00028: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0605 - acc: 0.9836 - val_loss: 0.3472 - val_acc: 0.9119\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9927\n",
      "Epoch 00029: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0344 - acc: 0.9927 - val_loss: 0.2769 - val_acc: 0.9306\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9844\n",
      "Epoch 00030: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0564 - acc: 0.9844 - val_loss: 0.3203 - val_acc: 0.9203\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9884\n",
      "Epoch 00031: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0466 - acc: 0.9883 - val_loss: 0.3756 - val_acc: 0.9073\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9838\n",
      "Epoch 00032: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0611 - acc: 0.9837 - val_loss: 0.2591 - val_acc: 0.9320\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9909\n",
      "Epoch 00033: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0379 - acc: 0.9909 - val_loss: 0.2607 - val_acc: 0.9299\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9960\n",
      "Epoch 00034: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0231 - acc: 0.9960 - val_loss: 0.3346 - val_acc: 0.9266\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9940\n",
      "Epoch 00035: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0286 - acc: 0.9940 - val_loss: 0.2805 - val_acc: 0.9287\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9880\n",
      "Epoch 00036: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0454 - acc: 0.9880 - val_loss: 0.3049 - val_acc: 0.9250\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9943\n",
      "Epoch 00037: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0274 - acc: 0.9942 - val_loss: 0.2922 - val_acc: 0.9266\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9876\n",
      "Epoch 00038: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0450 - acc: 0.9876 - val_loss: 0.2882 - val_acc: 0.9297\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9947\n",
      "Epoch 00039: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0239 - acc: 0.9946 - val_loss: 0.2686 - val_acc: 0.9315\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9924\n",
      "Epoch 00040: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0288 - acc: 0.9924 - val_loss: 0.2992 - val_acc: 0.9255\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9920\n",
      "Epoch 00041: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0311 - acc: 0.9920 - val_loss: 0.2746 - val_acc: 0.9317\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9937\n",
      "Epoch 00042: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0262 - acc: 0.9937 - val_loss: 0.2767 - val_acc: 0.9304\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9945\n",
      "Epoch 00043: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0224 - acc: 0.9945 - val_loss: 0.2591 - val_acc: 0.9378\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9863\n",
      "Epoch 00044: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0463 - acc: 0.9863 - val_loss: 0.3162 - val_acc: 0.9269\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9973\n",
      "Epoch 00045: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0150 - acc: 0.9973 - val_loss: 0.2978 - val_acc: 0.9352\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9971\n",
      "Epoch 00046: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0160 - acc: 0.9970 - val_loss: 0.3007 - val_acc: 0.9285\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9876\n",
      "Epoch 00047: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0423 - acc: 0.9876 - val_loss: 0.2854 - val_acc: 0.9331\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9969\n",
      "Epoch 00048: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0150 - acc: 0.9969 - val_loss: 0.3133 - val_acc: 0.9271\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9970\n",
      "Epoch 00049: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0150 - acc: 0.9970 - val_loss: 0.2894 - val_acc: 0.9406\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9973\n",
      "Epoch 00050: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0144 - acc: 0.9973 - val_loss: 0.2952 - val_acc: 0.9322\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9946\n",
      "Epoch 00051: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0217 - acc: 0.9946 - val_loss: 0.3014 - val_acc: 0.9320\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9950\n",
      "Epoch 00052: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0195 - acc: 0.9949 - val_loss: 0.3108 - val_acc: 0.9306\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9913\n",
      "Epoch 00053: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0317 - acc: 0.9913 - val_loss: 0.2900 - val_acc: 0.9299\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9917\n",
      "Epoch 00054: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0289 - acc: 0.9917 - val_loss: 0.2690 - val_acc: 0.9397\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9977\n",
      "Epoch 00055: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0115 - acc: 0.9977 - val_loss: 0.2771 - val_acc: 0.9390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9946\n",
      "Epoch 00056: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0208 - acc: 0.9946 - val_loss: 0.4164 - val_acc: 0.9108\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9959\n",
      "Epoch 00057: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0172 - acc: 0.9959 - val_loss: 0.3576 - val_acc: 0.9236\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9934\n",
      "Epoch 00058: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0251 - acc: 0.9934 - val_loss: 0.2596 - val_acc: 0.9362\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9964\n",
      "Epoch 00059: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0144 - acc: 0.9963 - val_loss: 0.3027 - val_acc: 0.9313\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9929\n",
      "Epoch 00060: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0263 - acc: 0.9929 - val_loss: 0.3011 - val_acc: 0.9285\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9963\n",
      "Epoch 00061: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0153 - acc: 0.9963 - val_loss: 0.2521 - val_acc: 0.9420\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9985\n",
      "Epoch 00062: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0091 - acc: 0.9985 - val_loss: 0.2788 - val_acc: 0.9378\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9944\n",
      "Epoch 00063: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0220 - acc: 0.9944 - val_loss: 0.2775 - val_acc: 0.9352\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9940\n",
      "Epoch 00064: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0221 - acc: 0.9940 - val_loss: 0.2637 - val_acc: 0.9420\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9990\n",
      "Epoch 00065: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0072 - acc: 0.9990 - val_loss: 0.2928 - val_acc: 0.9376\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9993\n",
      "Epoch 00066: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0056 - acc: 0.9993 - val_loss: 0.2947 - val_acc: 0.9336\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9945\n",
      "Epoch 00067: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0193 - acc: 0.9945 - val_loss: 0.3963 - val_acc: 0.9154\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9895\n",
      "Epoch 00068: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0355 - acc: 0.9895 - val_loss: 0.3414 - val_acc: 0.9259\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9939\n",
      "Epoch 00069: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0225 - acc: 0.9939 - val_loss: 0.2530 - val_acc: 0.9439\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9991\n",
      "Epoch 00070: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0058 - acc: 0.9991 - val_loss: 0.2695 - val_acc: 0.9422\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9991\n",
      "Epoch 00071: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0058 - acc: 0.9991 - val_loss: 0.2850 - val_acc: 0.9378\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9980\n",
      "Epoch 00072: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0099 - acc: 0.9980 - val_loss: 0.3585 - val_acc: 0.9173\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9966\n",
      "Epoch 00073: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0137 - acc: 0.9966 - val_loss: 0.4038 - val_acc: 0.9133\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9955\n",
      "Epoch 00074: val_loss did not improve from 0.25158\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0168 - acc: 0.9955 - val_loss: 0.3175 - val_acc: 0.9329\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.3201 - acc: 0.9128\n",
      "Loss: 0.32009967512307014 Accuracy: 0.9127726\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9040 - acc: 0.4209\n",
      "Epoch 00001: val_loss improved from inf to 1.59480, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/001-1.5948.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 1.9039 - acc: 0.4209 - val_loss: 1.5948 - val_acc: 0.4910\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8922 - acc: 0.7382\n",
      "Epoch 00002: val_loss improved from 1.59480 to 0.66022, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/002-0.6602.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.8924 - acc: 0.7381 - val_loss: 0.6602 - val_acc: 0.8132\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5703 - acc: 0.8353\n",
      "Epoch 00003: val_loss improved from 0.66022 to 0.48601, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/003-0.4860.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.5703 - acc: 0.8353 - val_loss: 0.4860 - val_acc: 0.8658\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8777\n",
      "Epoch 00004: val_loss improved from 0.48601 to 0.42410, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/004-0.4241.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.4177 - acc: 0.8777 - val_loss: 0.4241 - val_acc: 0.8737\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3331 - acc: 0.9021\n",
      "Epoch 00005: val_loss improved from 0.42410 to 0.33720, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/005-0.3372.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3330 - acc: 0.9021 - val_loss: 0.3372 - val_acc: 0.9040\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9186\n",
      "Epoch 00006: val_loss improved from 0.33720 to 0.31565, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/006-0.3157.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2784 - acc: 0.9185 - val_loss: 0.3157 - val_acc: 0.9033\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9304\n",
      "Epoch 00007: val_loss improved from 0.31565 to 0.24748, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/007-0.2475.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2372 - acc: 0.9304 - val_loss: 0.2475 - val_acc: 0.9273\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9398\n",
      "Epoch 00008: val_loss improved from 0.24748 to 0.23709, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/008-0.2371.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2051 - acc: 0.9397 - val_loss: 0.2371 - val_acc: 0.9301\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9437\n",
      "Epoch 00009: val_loss did not improve from 0.23709\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1888 - acc: 0.9437 - val_loss: 0.2418 - val_acc: 0.9283\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9532\n",
      "Epoch 00010: val_loss improved from 0.23709 to 0.22615, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/010-0.2262.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1592 - acc: 0.9532 - val_loss: 0.2262 - val_acc: 0.9334\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9596\n",
      "Epoch 00011: val_loss improved from 0.22615 to 0.22295, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/011-0.2230.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1386 - acc: 0.9596 - val_loss: 0.2230 - val_acc: 0.9352\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9626\n",
      "Epoch 00012: val_loss improved from 0.22295 to 0.21803, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/012-0.2180.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1283 - acc: 0.9626 - val_loss: 0.2180 - val_acc: 0.9348\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9650\n",
      "Epoch 00013: val_loss improved from 0.21803 to 0.20070, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/013-0.2007.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1167 - acc: 0.9650 - val_loss: 0.2007 - val_acc: 0.9427\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9685\n",
      "Epoch 00014: val_loss improved from 0.20070 to 0.19895, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/014-0.1989.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.1066 - acc: 0.9684 - val_loss: 0.1989 - val_acc: 0.9425\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9715\n",
      "Epoch 00015: val_loss did not improve from 0.19895\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0975 - acc: 0.9715 - val_loss: 0.2079 - val_acc: 0.9413\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9748\n",
      "Epoch 00016: val_loss improved from 0.19895 to 0.18841, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/016-0.1884.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0872 - acc: 0.9748 - val_loss: 0.1884 - val_acc: 0.9411\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9788\n",
      "Epoch 00017: val_loss improved from 0.18841 to 0.17481, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/017-0.1748.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0742 - acc: 0.9788 - val_loss: 0.1748 - val_acc: 0.9497\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9771\n",
      "Epoch 00018: val_loss did not improve from 0.17481\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0792 - acc: 0.9771 - val_loss: 0.1962 - val_acc: 0.9443\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9826\n",
      "Epoch 00019: val_loss improved from 0.17481 to 0.16283, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/019-0.1628.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0637 - acc: 0.9826 - val_loss: 0.1628 - val_acc: 0.9509\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9842\n",
      "Epoch 00020: val_loss did not improve from 0.16283\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0596 - acc: 0.9842 - val_loss: 0.1942 - val_acc: 0.9434\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9826\n",
      "Epoch 00021: val_loss did not improve from 0.16283\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0636 - acc: 0.9826 - val_loss: 0.1735 - val_acc: 0.9525\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9894\n",
      "Epoch 00022: val_loss did not improve from 0.16283\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0438 - acc: 0.9894 - val_loss: 0.1702 - val_acc: 0.9522\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9859\n",
      "Epoch 00023: val_loss improved from 0.16283 to 0.15969, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/023-0.1597.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0517 - acc: 0.9859 - val_loss: 0.1597 - val_acc: 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9887\n",
      "Epoch 00024: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0417 - acc: 0.9887 - val_loss: 0.2077 - val_acc: 0.9450\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9871\n",
      "Epoch 00025: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0471 - acc: 0.9871 - val_loss: 0.2232 - val_acc: 0.9404\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9920\n",
      "Epoch 00026: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0342 - acc: 0.9920 - val_loss: 0.2237 - val_acc: 0.9415\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9932\n",
      "Epoch 00027: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0290 - acc: 0.9932 - val_loss: 0.1723 - val_acc: 0.9499\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9924\n",
      "Epoch 00028: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0307 - acc: 0.9924 - val_loss: 0.1753 - val_acc: 0.9536\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9900\n",
      "Epoch 00029: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0377 - acc: 0.9900 - val_loss: 0.2018 - val_acc: 0.9499\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9951\n",
      "Epoch 00030: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0224 - acc: 0.9951 - val_loss: 0.2093 - val_acc: 0.9404\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9887\n",
      "Epoch 00031: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0428 - acc: 0.9888 - val_loss: 0.1871 - val_acc: 0.9485\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9965\n",
      "Epoch 00032: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0178 - acc: 0.9965 - val_loss: 0.1738 - val_acc: 0.9522\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9919\n",
      "Epoch 00033: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0319 - acc: 0.9918 - val_loss: 0.1833 - val_acc: 0.9506\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9891\n",
      "Epoch 00034: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0374 - acc: 0.9891 - val_loss: 0.1888 - val_acc: 0.9548\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9930\n",
      "Epoch 00035: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0255 - acc: 0.9930 - val_loss: 0.1651 - val_acc: 0.9581\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9926\n",
      "Epoch 00036: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0296 - acc: 0.9925 - val_loss: 0.1845 - val_acc: 0.9515\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9927\n",
      "Epoch 00037: val_loss improved from 0.15969 to 0.14979, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_8_conv_checkpoint/037-0.1498.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0291 - acc: 0.9927 - val_loss: 0.1498 - val_acc: 0.9583\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9948\n",
      "Epoch 00038: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0208 - acc: 0.9947 - val_loss: 0.2097 - val_acc: 0.9481\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9931\n",
      "Epoch 00039: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0257 - acc: 0.9931 - val_loss: 0.1642 - val_acc: 0.9569\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9977\n",
      "Epoch 00040: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0126 - acc: 0.9977 - val_loss: 0.2257 - val_acc: 0.9478\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9971\n",
      "Epoch 00041: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0134 - acc: 0.9971 - val_loss: 0.1838 - val_acc: 0.9543\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9974\n",
      "Epoch 00042: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0129 - acc: 0.9974 - val_loss: 0.2033 - val_acc: 0.9481\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9951\n",
      "Epoch 00043: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0196 - acc: 0.9951 - val_loss: 0.1832 - val_acc: 0.9534\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9910\n",
      "Epoch 00044: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0313 - acc: 0.9910 - val_loss: 0.1855 - val_acc: 0.9520\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9955\n",
      "Epoch 00045: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0177 - acc: 0.9955 - val_loss: 0.2022 - val_acc: 0.9485\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9950\n",
      "Epoch 00046: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0199 - acc: 0.9950 - val_loss: 0.1643 - val_acc: 0.9588\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9984\n",
      "Epoch 00047: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0094 - acc: 0.9984 - val_loss: 0.1627 - val_acc: 0.9546\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9980\n",
      "Epoch 00048: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0099 - acc: 0.9980 - val_loss: 0.1676 - val_acc: 0.9588\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9966\n",
      "Epoch 00049: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0134 - acc: 0.9966 - val_loss: 0.2255 - val_acc: 0.9509\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9954\n",
      "Epoch 00050: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0178 - acc: 0.9954 - val_loss: 0.2333 - val_acc: 0.9434\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9956\n",
      "Epoch 00051: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0176 - acc: 0.9956 - val_loss: 0.1762 - val_acc: 0.9576\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9984\n",
      "Epoch 00052: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0080 - acc: 0.9984 - val_loss: 0.2398 - val_acc: 0.9464\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9970\n",
      "Epoch 00053: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0113 - acc: 0.9970 - val_loss: 0.1854 - val_acc: 0.9543\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9969\n",
      "Epoch 00054: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0126 - acc: 0.9969 - val_loss: 0.1914 - val_acc: 0.9555\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9976\n",
      "Epoch 00055: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0106 - acc: 0.9976 - val_loss: 0.1770 - val_acc: 0.9574\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9961\n",
      "Epoch 00056: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0132 - acc: 0.9961 - val_loss: 0.2480 - val_acc: 0.9422\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9938\n",
      "Epoch 00057: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0207 - acc: 0.9938 - val_loss: 0.2640 - val_acc: 0.9383\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9979\n",
      "Epoch 00058: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0097 - acc: 0.9979 - val_loss: 0.1788 - val_acc: 0.9590\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9963\n",
      "Epoch 00059: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0149 - acc: 0.9963 - val_loss: 0.1791 - val_acc: 0.9578\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9990\n",
      "Epoch 00060: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0053 - acc: 0.9990 - val_loss: 0.1659 - val_acc: 0.9616\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9995\n",
      "Epoch 00061: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0041 - acc: 0.9995 - val_loss: 0.1838 - val_acc: 0.9569\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9926\n",
      "Epoch 00062: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0258 - acc: 0.9926 - val_loss: 0.1709 - val_acc: 0.9590\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9973\n",
      "Epoch 00063: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0116 - acc: 0.9973 - val_loss: 0.1854 - val_acc: 0.9543\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9993\n",
      "Epoch 00064: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0048 - acc: 0.9993 - val_loss: 0.1652 - val_acc: 0.9602\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9956\n",
      "Epoch 00065: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0162 - acc: 0.9956 - val_loss: 0.1913 - val_acc: 0.9574\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9992\n",
      "Epoch 00066: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0052 - acc: 0.9992 - val_loss: 0.1608 - val_acc: 0.9599\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9979\n",
      "Epoch 00067: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0087 - acc: 0.9979 - val_loss: 0.2099 - val_acc: 0.9529\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9976\n",
      "Epoch 00068: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0088 - acc: 0.9976 - val_loss: 0.1906 - val_acc: 0.9583\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9983\n",
      "Epoch 00069: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0078 - acc: 0.9983 - val_loss: 0.2055 - val_acc: 0.9520\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9965\n",
      "Epoch 00070: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0123 - acc: 0.9965 - val_loss: 0.1942 - val_acc: 0.9583\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9983\n",
      "Epoch 00071: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0071 - acc: 0.9983 - val_loss: 0.1927 - val_acc: 0.9574\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9978\n",
      "Epoch 00072: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0088 - acc: 0.9978 - val_loss: 0.1883 - val_acc: 0.9588\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9966\n",
      "Epoch 00073: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0114 - acc: 0.9965 - val_loss: 0.2548 - val_acc: 0.9408\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9925\n",
      "Epoch 00074: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0248 - acc: 0.9924 - val_loss: 0.1731 - val_acc: 0.9557\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9937\n",
      "Epoch 00075: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0213 - acc: 0.9937 - val_loss: 0.1694 - val_acc: 0.9611\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9987\n",
      "Epoch 00076: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0068 - acc: 0.9987 - val_loss: 0.1648 - val_acc: 0.9588\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9997\n",
      "Epoch 00077: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0031 - acc: 0.9997 - val_loss: 0.1615 - val_acc: 0.9618\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9992\n",
      "Epoch 00078: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0042 - acc: 0.9992 - val_loss: 0.1725 - val_acc: 0.9620\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9971\n",
      "Epoch 00079: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0103 - acc: 0.9971 - val_loss: 0.1973 - val_acc: 0.9550\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9980\n",
      "Epoch 00080: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0083 - acc: 0.9980 - val_loss: 0.2243 - val_acc: 0.9515\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9973\n",
      "Epoch 00081: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0107 - acc: 0.9972 - val_loss: 0.1726 - val_acc: 0.9588\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9947\n",
      "Epoch 00082: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0171 - acc: 0.9946 - val_loss: 0.1722 - val_acc: 0.9581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9980\n",
      "Epoch 00083: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0084 - acc: 0.9980 - val_loss: 0.1812 - val_acc: 0.9562\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 00084: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0028 - acc: 0.9996 - val_loss: 0.1566 - val_acc: 0.9632\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9995\n",
      "Epoch 00085: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0029 - acc: 0.9995 - val_loss: 0.1746 - val_acc: 0.9604\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9979\n",
      "Epoch 00086: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0083 - acc: 0.9978 - val_loss: 0.2466 - val_acc: 0.9495\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9948\n",
      "Epoch 00087: val_loss did not improve from 0.14979\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0170 - acc: 0.9948 - val_loss: 0.1686 - val_acc: 0.9602\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2005 - acc: 0.9435\n",
      "Loss: 0.20054632041289985 Accuracy: 0.9435099\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4880 - acc: 0.5399\n",
      "Epoch 00001: val_loss improved from inf to 1.08879, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/001-1.0888.hdf5\n",
      "36805/36805 [==============================] - 228s 6ms/sample - loss: 1.4879 - acc: 0.5398 - val_loss: 1.0888 - val_acc: 0.6457\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5916 - acc: 0.8145\n",
      "Epoch 00002: val_loss improved from 1.08879 to 0.47684, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/002-0.4768.hdf5\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.5916 - acc: 0.8144 - val_loss: 0.4768 - val_acc: 0.8460\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8755\n",
      "Epoch 00003: val_loss improved from 0.47684 to 0.35432, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/003-0.3543.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3925 - acc: 0.8755 - val_loss: 0.3543 - val_acc: 0.8910\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9053\n",
      "Epoch 00004: val_loss improved from 0.35432 to 0.28269, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/004-0.2827.hdf5\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.3021 - acc: 0.9053 - val_loss: 0.2827 - val_acc: 0.9157\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9270\n",
      "Epoch 00005: val_loss did not improve from 0.28269\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.2348 - acc: 0.9270 - val_loss: 0.3063 - val_acc: 0.9073\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9403\n",
      "Epoch 00006: val_loss improved from 0.28269 to 0.26484, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/006-0.2648.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1933 - acc: 0.9403 - val_loss: 0.2648 - val_acc: 0.9227\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9485\n",
      "Epoch 00007: val_loss did not improve from 0.26484\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1643 - acc: 0.9484 - val_loss: 0.3028 - val_acc: 0.9126\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9545\n",
      "Epoch 00008: val_loss improved from 0.26484 to 0.25914, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/008-0.2591.hdf5\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1440 - acc: 0.9544 - val_loss: 0.2591 - val_acc: 0.9194\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9599\n",
      "Epoch 00009: val_loss did not improve from 0.25914\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1259 - acc: 0.9598 - val_loss: 0.2814 - val_acc: 0.9154\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9660\n",
      "Epoch 00010: val_loss improved from 0.25914 to 0.21873, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/010-0.2187.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1097 - acc: 0.9659 - val_loss: 0.2187 - val_acc: 0.9297\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9707\n",
      "Epoch 00011: val_loss improved from 0.21873 to 0.21216, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/011-0.2122.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0962 - acc: 0.9707 - val_loss: 0.2122 - val_acc: 0.9359\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9757\n",
      "Epoch 00012: val_loss improved from 0.21216 to 0.18407, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/012-0.1841.hdf5\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0825 - acc: 0.9757 - val_loss: 0.1841 - val_acc: 0.9425\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9797\n",
      "Epoch 00013: val_loss did not improve from 0.18407\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0689 - acc: 0.9797 - val_loss: 0.1861 - val_acc: 0.9448\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9812\n",
      "Epoch 00014: val_loss improved from 0.18407 to 0.17554, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/014-0.1755.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0637 - acc: 0.9812 - val_loss: 0.1755 - val_acc: 0.9478\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9844\n",
      "Epoch 00015: val_loss did not improve from 0.17554\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0540 - acc: 0.9844 - val_loss: 0.2398 - val_acc: 0.9283\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9860\n",
      "Epoch 00016: val_loss did not improve from 0.17554\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0498 - acc: 0.9860 - val_loss: 0.2060 - val_acc: 0.9399\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9807\n",
      "Epoch 00017: val_loss did not improve from 0.17554\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0627 - acc: 0.9807 - val_loss: 0.2522 - val_acc: 0.9294\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9909\n",
      "Epoch 00018: val_loss did not improve from 0.17554\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0349 - acc: 0.9908 - val_loss: 0.2665 - val_acc: 0.9269\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9772\n",
      "Epoch 00019: val_loss did not improve from 0.17554\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0719 - acc: 0.9772 - val_loss: 0.1834 - val_acc: 0.9495\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9914\n",
      "Epoch 00020: val_loss did not improve from 0.17554\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0324 - acc: 0.9914 - val_loss: 0.1825 - val_acc: 0.9474\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9876\n",
      "Epoch 00021: val_loss did not improve from 0.17554\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0444 - acc: 0.9876 - val_loss: 0.1808 - val_acc: 0.9520\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9947\n",
      "Epoch 00022: val_loss improved from 0.17554 to 0.17174, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/022-0.1717.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0237 - acc: 0.9947 - val_loss: 0.1717 - val_acc: 0.9532\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9943\n",
      "Epoch 00023: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0227 - acc: 0.9943 - val_loss: 0.2164 - val_acc: 0.9478\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9919\n",
      "Epoch 00024: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0273 - acc: 0.9919 - val_loss: 0.2369 - val_acc: 0.9385\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9883\n",
      "Epoch 00025: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0392 - acc: 0.9883 - val_loss: 0.1809 - val_acc: 0.9522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9958\n",
      "Epoch 00026: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0172 - acc: 0.9958 - val_loss: 0.1928 - val_acc: 0.9488\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9926\n",
      "Epoch 00027: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0268 - acc: 0.9926 - val_loss: 0.1942 - val_acc: 0.9518\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9936\n",
      "Epoch 00028: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0222 - acc: 0.9936 - val_loss: 0.2223 - val_acc: 0.9448\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9942\n",
      "Epoch 00029: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0220 - acc: 0.9942 - val_loss: 0.1754 - val_acc: 0.9546\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9960\n",
      "Epoch 00030: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0169 - acc: 0.9960 - val_loss: 0.1971 - val_acc: 0.9488\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9962\n",
      "Epoch 00031: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0146 - acc: 0.9962 - val_loss: 0.2446 - val_acc: 0.9355\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9954\n",
      "Epoch 00032: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0172 - acc: 0.9954 - val_loss: 0.2377 - val_acc: 0.9413\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9928\n",
      "Epoch 00033: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0233 - acc: 0.9928 - val_loss: 0.2526 - val_acc: 0.9418\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9964\n",
      "Epoch 00034: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0137 - acc: 0.9964 - val_loss: 0.2636 - val_acc: 0.9362\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9955\n",
      "Epoch 00035: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0163 - acc: 0.9955 - val_loss: 0.1762 - val_acc: 0.9585\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9913\n",
      "Epoch 00036: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0291 - acc: 0.9913 - val_loss: 0.1835 - val_acc: 0.9578\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9973\n",
      "Epoch 00037: val_loss did not improve from 0.17174\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0105 - acc: 0.9973 - val_loss: 0.1980 - val_acc: 0.9557\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9916\n",
      "Epoch 00038: val_loss improved from 0.17174 to 0.16631, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/038-0.1663.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0264 - acc: 0.9915 - val_loss: 0.1663 - val_acc: 0.9590\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9951\n",
      "Epoch 00039: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0171 - acc: 0.9951 - val_loss: 0.1760 - val_acc: 0.9569\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9976\n",
      "Epoch 00040: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0109 - acc: 0.9976 - val_loss: 0.1713 - val_acc: 0.9592\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9989\n",
      "Epoch 00041: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0055 - acc: 0.9989 - val_loss: 0.1675 - val_acc: 0.9581\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9984\n",
      "Epoch 00042: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0065 - acc: 0.9984 - val_loss: 0.2244 - val_acc: 0.9450\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9951\n",
      "Epoch 00043: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0162 - acc: 0.9951 - val_loss: 0.1985 - val_acc: 0.9548\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9942\n",
      "Epoch 00044: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0185 - acc: 0.9942 - val_loss: 0.1977 - val_acc: 0.9560\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9975\n",
      "Epoch 00045: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0097 - acc: 0.9975 - val_loss: 0.1868 - val_acc: 0.9548\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9974\n",
      "Epoch 00046: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0090 - acc: 0.9974 - val_loss: 0.2066 - val_acc: 0.9536\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9966\n",
      "Epoch 00047: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0126 - acc: 0.9966 - val_loss: 0.2211 - val_acc: 0.9492\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9968\n",
      "Epoch 00048: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0112 - acc: 0.9968 - val_loss: 0.2473 - val_acc: 0.9432\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9960\n",
      "Epoch 00049: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0146 - acc: 0.9960 - val_loss: 0.2258 - val_acc: 0.9532\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9932\n",
      "Epoch 00050: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0221 - acc: 0.9932 - val_loss: 0.1753 - val_acc: 0.9583\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9986\n",
      "Epoch 00051: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0063 - acc: 0.9986 - val_loss: 0.1725 - val_acc: 0.9583\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9994\n",
      "Epoch 00052: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0041 - acc: 0.9993 - val_loss: 0.1844 - val_acc: 0.9562\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9950\n",
      "Epoch 00053: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0154 - acc: 0.9950 - val_loss: 0.1933 - val_acc: 0.9541\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9989\n",
      "Epoch 00054: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0049 - acc: 0.9989 - val_loss: 0.1741 - val_acc: 0.9602\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9982\n",
      "Epoch 00055: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0070 - acc: 0.9982 - val_loss: 0.2150 - val_acc: 0.9522\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9967\n",
      "Epoch 00056: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0116 - acc: 0.9967 - val_loss: 0.2586 - val_acc: 0.9446\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9977\n",
      "Epoch 00057: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0096 - acc: 0.9976 - val_loss: 0.1847 - val_acc: 0.9539\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9971\n",
      "Epoch 00058: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0108 - acc: 0.9971 - val_loss: 0.1894 - val_acc: 0.9597\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9990\n",
      "Epoch 00059: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0046 - acc: 0.9990 - val_loss: 0.1746 - val_acc: 0.9604\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 00060: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0147 - acc: 0.9954 - val_loss: 0.1967 - val_acc: 0.9555\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9987\n",
      "Epoch 00061: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0055 - acc: 0.9987 - val_loss: 0.1962 - val_acc: 0.9569\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9963\n",
      "Epoch 00062: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0127 - acc: 0.9963 - val_loss: 0.1784 - val_acc: 0.9606\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9975\n",
      "Epoch 00063: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0087 - acc: 0.9975 - val_loss: 0.2167 - val_acc: 0.9532\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9985\n",
      "Epoch 00064: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0062 - acc: 0.9985 - val_loss: 0.1780 - val_acc: 0.9618\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00065: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0032 - acc: 0.9993 - val_loss: 0.1780 - val_acc: 0.9620\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9979\n",
      "Epoch 00066: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0068 - acc: 0.9979 - val_loss: 0.2241 - val_acc: 0.9522\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9940\n",
      "Epoch 00067: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0212 - acc: 0.9940 - val_loss: 0.1806 - val_acc: 0.9618\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9984\n",
      "Epoch 00068: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0056 - acc: 0.9984 - val_loss: 0.1799 - val_acc: 0.9578\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00069: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0037 - acc: 0.9992 - val_loss: 0.1861 - val_acc: 0.9592\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9986\n",
      "Epoch 00070: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0064 - acc: 0.9986 - val_loss: 0.2441 - val_acc: 0.9518\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9947\n",
      "Epoch 00071: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0178 - acc: 0.9947 - val_loss: 0.1664 - val_acc: 0.9604\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9982\n",
      "Epoch 00072: val_loss did not improve from 0.16631\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.1731 - val_acc: 0.9604\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9991\n",
      "Epoch 00073: val_loss improved from 0.16631 to 0.16408, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/073-0.1641.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0030 - acc: 0.9991 - val_loss: 0.1641 - val_acc: 0.9660\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00074: val_loss did not improve from 0.16408\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0042 - acc: 0.9991 - val_loss: 0.1962 - val_acc: 0.9583\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9943\n",
      "Epoch 00075: val_loss did not improve from 0.16408\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0189 - acc: 0.9943 - val_loss: 0.1686 - val_acc: 0.9655\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 00076: val_loss improved from 0.16408 to 0.16135, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/076-0.1614.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0031 - acc: 0.9994 - val_loss: 0.1614 - val_acc: 0.9641\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00077: val_loss did not improve from 0.16135\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0033 - acc: 0.9993 - val_loss: 0.1696 - val_acc: 0.9618\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9973\n",
      "Epoch 00078: val_loss did not improve from 0.16135\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0083 - acc: 0.9973 - val_loss: 0.1734 - val_acc: 0.9637\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9986\n",
      "Epoch 00079: val_loss did not improve from 0.16135\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0057 - acc: 0.9986 - val_loss: 0.2101 - val_acc: 0.9583\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9957\n",
      "Epoch 00080: val_loss did not improve from 0.16135\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0149 - acc: 0.9957 - val_loss: 0.1864 - val_acc: 0.9576\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 00081: val_loss did not improve from 0.16135\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0040 - acc: 0.9990 - val_loss: 0.1832 - val_acc: 0.9604\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00082: val_loss improved from 0.16135 to 0.15827, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/082-0.1583.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1583 - val_acc: 0.9653\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00083: val_loss did not improve from 0.15827\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1807 - val_acc: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9968\n",
      "Epoch 00084: val_loss did not improve from 0.15827\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0109 - acc: 0.9968 - val_loss: 0.2403 - val_acc: 0.9511\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9987\n",
      "Epoch 00085: val_loss did not improve from 0.15827\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0047 - acc: 0.9987 - val_loss: 0.2048 - val_acc: 0.9550\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9974\n",
      "Epoch 00086: val_loss did not improve from 0.15827\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0090 - acc: 0.9974 - val_loss: 0.1820 - val_acc: 0.9590\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9962\n",
      "Epoch 00087: val_loss did not improve from 0.15827\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0123 - acc: 0.9962 - val_loss: 0.1725 - val_acc: 0.9632\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00088: val_loss improved from 0.15827 to 0.15150, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/088-0.1515.hdf5\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1515 - val_acc: 0.9672\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 00089: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0019 - acc: 0.9994 - val_loss: 0.1963 - val_acc: 0.9578\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9968\n",
      "Epoch 00090: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0114 - acc: 0.9968 - val_loss: 0.1828 - val_acc: 0.9618\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9958\n",
      "Epoch 00091: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0147 - acc: 0.9958 - val_loss: 0.1895 - val_acc: 0.9606\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9990\n",
      "Epoch 00092: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0037 - acc: 0.9990 - val_loss: 0.1712 - val_acc: 0.9639\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00093: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1622 - val_acc: 0.9676\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 00094: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.1675 - val_acc: 0.9630\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9971\n",
      "Epoch 00095: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.1861 - val_acc: 0.9583\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9943\n",
      "Epoch 00096: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 4ms/sample - loss: 0.0179 - acc: 0.9943 - val_loss: 0.1725 - val_acc: 0.9634\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9985\n",
      "Epoch 00097: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0057 - acc: 0.9985 - val_loss: 0.1676 - val_acc: 0.9644\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 00098: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1581 - val_acc: 0.9681\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00099: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1740 - val_acc: 0.9627\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 00100: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.1821 - val_acc: 0.9634\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9944\n",
      "Epoch 00101: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0183 - acc: 0.9944 - val_loss: 0.1806 - val_acc: 0.9627\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00102: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0029 - acc: 0.9994 - val_loss: 0.1767 - val_acc: 0.9613\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9968\n",
      "Epoch 00103: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0105 - acc: 0.9968 - val_loss: 0.1717 - val_acc: 0.9630\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 00104: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.1626 - val_acc: 0.9641\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 00105: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0072 - acc: 0.9977 - val_loss: 0.1815 - val_acc: 0.9627\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 00106: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1730 - val_acc: 0.9648\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 00107: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1690 - val_acc: 0.9646\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9970\n",
      "Epoch 00108: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0105 - acc: 0.9970 - val_loss: 0.1898 - val_acc: 0.9620\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9986\n",
      "Epoch 00109: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.1920 - val_acc: 0.9620\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9983\n",
      "Epoch 00110: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0054 - acc: 0.9983 - val_loss: 0.1728 - val_acc: 0.9646\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9966\n",
      "Epoch 00111: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0109 - acc: 0.9966 - val_loss: 0.1673 - val_acc: 0.9651\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00112: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1695 - val_acc: 0.9669\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 00113: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.1700 - val_acc: 0.9681\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9983\n",
      "Epoch 00114: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0062 - acc: 0.9983 - val_loss: 0.1650 - val_acc: 0.9681\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9992\n",
      "Epoch 00115: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0029 - acc: 0.9992 - val_loss: 0.1919 - val_acc: 0.9616\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9988\n",
      "Epoch 00116: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0041 - acc: 0.9988 - val_loss: 0.1790 - val_acc: 0.9648\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9980\n",
      "Epoch 00117: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0070 - acc: 0.9980 - val_loss: 0.2186 - val_acc: 0.9562\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9980\n",
      "Epoch 00118: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0062 - acc: 0.9980 - val_loss: 0.1900 - val_acc: 0.9606\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9967\n",
      "Epoch 00119: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0098 - acc: 0.9967 - val_loss: 0.1569 - val_acc: 0.9667\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00120: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1663 - val_acc: 0.9648\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 00121: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 0.1600 - val_acc: 0.9683\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9972\n",
      "Epoch 00122: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0095 - acc: 0.9972 - val_loss: 0.1714 - val_acc: 0.9639\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9973\n",
      "Epoch 00123: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0081 - acc: 0.9973 - val_loss: 0.1838 - val_acc: 0.9641\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 00124: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1655 - val_acc: 0.9637\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9976\n",
      "Epoch 00125: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0092 - acc: 0.9976 - val_loss: 0.1611 - val_acc: 0.9651\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 00126: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0016 - acc: 0.9996 - val_loss: 0.1721 - val_acc: 0.9658\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9976\n",
      "Epoch 00127: val_loss did not improve from 0.15150\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0079 - acc: 0.9976 - val_loss: 0.1571 - val_acc: 0.9667\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 00128: val_loss improved from 0.15150 to 0.14535, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/128-0.1454.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1454 - val_acc: 0.9688\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 00129: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0034 - acc: 0.9993 - val_loss: 0.1706 - val_acc: 0.9620\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9974\n",
      "Epoch 00130: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0090 - acc: 0.9974 - val_loss: 0.1750 - val_acc: 0.9669\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 00131: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1709 - val_acc: 0.9669\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00132: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0014 - acc: 0.9996 - val_loss: 0.1793 - val_acc: 0.9674\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 00133: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.2011 - val_acc: 0.9599\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9967\n",
      "Epoch 00134: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0102 - acc: 0.9967 - val_loss: 0.1713 - val_acc: 0.9651\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 00135: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0023 - acc: 0.9994 - val_loss: 0.1892 - val_acc: 0.9634\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9957\n",
      "Epoch 00136: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0144 - acc: 0.9957 - val_loss: 0.1532 - val_acc: 0.9683\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 00137: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1735 - val_acc: 0.9676\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9971\n",
      "Epoch 00138: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0092 - acc: 0.9971 - val_loss: 0.1724 - val_acc: 0.9653\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 00139: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1562 - val_acc: 0.9681\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9987\n",
      "Epoch 00140: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0042 - acc: 0.9987 - val_loss: 0.1714 - val_acc: 0.9653\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 00141: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1649 - val_acc: 0.9667\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9953\n",
      "Epoch 00142: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0168 - acc: 0.9953 - val_loss: 0.1653 - val_acc: 0.9669\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9977\n",
      "Epoch 00143: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0074 - acc: 0.9977 - val_loss: 0.1717 - val_acc: 0.9667\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00144: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1568 - val_acc: 0.9676\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.2969e-04 - acc: 1.0000\n",
      "Epoch 00145: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 6.2962e-04 - acc: 1.0000 - val_loss: 0.1519 - val_acc: 0.9690\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.2385e-04 - acc: 0.9999\n",
      "Epoch 00146: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 5.2394e-04 - acc: 0.9999 - val_loss: 0.1518 - val_acc: 0.9697\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00147: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.2920 - val_acc: 0.9495\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9967\n",
      "Epoch 00148: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0097 - acc: 0.9966 - val_loss: 0.1897 - val_acc: 0.9618\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9976\n",
      "Epoch 00149: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0077 - acc: 0.9976 - val_loss: 0.2049 - val_acc: 0.9592\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9988\n",
      "Epoch 00150: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0039 - acc: 0.9988 - val_loss: 0.1561 - val_acc: 0.9683\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 00151: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0016 - acc: 0.9996 - val_loss: 0.1561 - val_acc: 0.9688\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9992\n",
      "Epoch 00152: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1770 - val_acc: 0.9618\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 00153: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0091 - acc: 0.9973 - val_loss: 0.1636 - val_acc: 0.9669\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 00154: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0021 - acc: 0.9994 - val_loss: 0.1545 - val_acc: 0.9688\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00155: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1558 - val_acc: 0.9679\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00156: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1996 - val_acc: 0.9609\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9992\n",
      "Epoch 00157: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1888 - val_acc: 0.9585\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9961\n",
      "Epoch 00158: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0108 - acc: 0.9961 - val_loss: 0.1520 - val_acc: 0.9688\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 00159: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1477 - val_acc: 0.9690\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 00160: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0011 - acc: 0.9997 - val_loss: 0.1637 - val_acc: 0.9681\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00161: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1473 - val_acc: 0.9723\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9987\n",
      "Epoch 00162: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0042 - acc: 0.9987 - val_loss: 0.1800 - val_acc: 0.9646\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9989\n",
      "Epoch 00163: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0039 - acc: 0.9989 - val_loss: 0.1841 - val_acc: 0.9630\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9973\n",
      "Epoch 00164: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0095 - acc: 0.9973 - val_loss: 0.1949 - val_acc: 0.9620\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9982\n",
      "Epoch 00165: val_loss did not improve from 0.14535\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0057 - acc: 0.9982 - val_loss: 0.1522 - val_acc: 0.9688\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9998\n",
      "Epoch 00166: val_loss improved from 0.14535 to 0.13777, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_BN_9_conv_checkpoint/166-0.1378.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1378 - val_acc: 0.9723\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 00167: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0015 - acc: 0.9998 - val_loss: 0.1414 - val_acc: 0.9713\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9994\n",
      "Epoch 00168: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0017 - acc: 0.9994 - val_loss: 0.2204 - val_acc: 0.9604\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9987\n",
      "Epoch 00169: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0044 - acc: 0.9987 - val_loss: 0.1686 - val_acc: 0.9672\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9987\n",
      "Epoch 00170: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0041 - acc: 0.9987 - val_loss: 0.2011 - val_acc: 0.9634\n",
      "Epoch 171/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 00171: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0035 - acc: 0.9989 - val_loss: 0.1671 - val_acc: 0.9665\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9970\n",
      "Epoch 00172: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0105 - acc: 0.9970 - val_loss: 0.1667 - val_acc: 0.9669\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9977\n",
      "Epoch 00173: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0090 - acc: 0.9976 - val_loss: 0.1540 - val_acc: 0.9672\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9986\n",
      "Epoch 00174: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.1467 - val_acc: 0.9660\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00175: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1457 - val_acc: 0.9702\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.7223e-04 - acc: 0.9999\n",
      "Epoch 00176: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 5.7244e-04 - acc: 0.9999 - val_loss: 0.1440 - val_acc: 0.9702\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.3316e-04 - acc: 0.9999\n",
      "Epoch 00177: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 5.4063e-04 - acc: 0.9999 - val_loss: 0.1769 - val_acc: 0.9658\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9976\n",
      "Epoch 00178: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0074 - acc: 0.9976 - val_loss: 0.1977 - val_acc: 0.9609\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9991\n",
      "Epoch 00179: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0032 - acc: 0.9991 - val_loss: 0.1554 - val_acc: 0.9658\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 00180: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 0.1710 - val_acc: 0.9665\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9989\n",
      "Epoch 00181: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0031 - acc: 0.9989 - val_loss: 0.2195 - val_acc: 0.9597\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9978\n",
      "Epoch 00182: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0067 - acc: 0.9978 - val_loss: 0.1911 - val_acc: 0.9639\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 00183: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0016 - acc: 0.9996 - val_loss: 0.1684 - val_acc: 0.9674\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 00184: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0020 - acc: 0.9994 - val_loss: 0.1593 - val_acc: 0.9690\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9987\n",
      "Epoch 00185: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0040 - acc: 0.9988 - val_loss: 0.1872 - val_acc: 0.9653\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9990\n",
      "Epoch 00186: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0037 - acc: 0.9990 - val_loss: 0.1799 - val_acc: 0.9655\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9975\n",
      "Epoch 00187: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0074 - acc: 0.9975 - val_loss: 0.1914 - val_acc: 0.9625\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9985\n",
      "Epoch 00188: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0050 - acc: 0.9985 - val_loss: 0.1656 - val_acc: 0.9681\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9990\n",
      "Epoch 00189: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0035 - acc: 0.9990 - val_loss: 0.1438 - val_acc: 0.9693\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.4602e-04 - acc: 0.9998\n",
      "Epoch 00190: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 7.5075e-04 - acc: 0.9998 - val_loss: 0.1659 - val_acc: 0.9669\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 00191: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 0.1514 - val_acc: 0.9711\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9993\n",
      "Epoch 00192: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0020 - acc: 0.9993 - val_loss: 0.1632 - val_acc: 0.9674\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 00193: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0023 - acc: 0.9993 - val_loss: 0.1989 - val_acc: 0.9611\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9963\n",
      "Epoch 00194: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0121 - acc: 0.9963 - val_loss: 0.1603 - val_acc: 0.9672\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.9225e-04 - acc: 0.9998\n",
      "Epoch 00195: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1539 - val_acc: 0.9686\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9982\n",
      "Epoch 00196: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.1613 - val_acc: 0.9676\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 00197: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0016 - acc: 0.9996 - val_loss: 0.1612 - val_acc: 0.9695\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9982\n",
      "Epoch 00198: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0066 - acc: 0.9982 - val_loss: 0.1502 - val_acc: 0.9695\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.1338e-04 - acc: 1.0000\n",
      "Epoch 00199: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 8.1557e-04 - acc: 1.0000 - val_loss: 0.1450 - val_acc: 0.9713\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 00200: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0019 - acc: 0.9994 - val_loss: 0.2112 - val_acc: 0.9625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9983\n",
      "Epoch 00201: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0062 - acc: 0.9983 - val_loss: 0.1673 - val_acc: 0.9697\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 00202: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.1711 - val_acc: 0.9660\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9984\n",
      "Epoch 00203: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0058 - acc: 0.9984 - val_loss: 0.1576 - val_acc: 0.9690\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.4246e-04 - acc: 0.9998\n",
      "Epoch 00204: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 9.4233e-04 - acc: 0.9998 - val_loss: 0.1493 - val_acc: 0.9716\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 00205: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0014 - acc: 0.9995 - val_loss: 0.1787 - val_acc: 0.9627\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 00206: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0016 - acc: 0.9996 - val_loss: 0.1754 - val_acc: 0.9665\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 00207: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0085 - acc: 0.9976 - val_loss: 0.1574 - val_acc: 0.9686\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9988\n",
      "Epoch 00208: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0046 - acc: 0.9988 - val_loss: 0.1535 - val_acc: 0.9688\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.7042e-04 - acc: 0.9999\n",
      "Epoch 00209: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 7.0895e-04 - acc: 0.9999 - val_loss: 0.1581 - val_acc: 0.9697\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9986\n",
      "Epoch 00210: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0052 - acc: 0.9985 - val_loss: 0.1863 - val_acc: 0.9630\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986\n",
      "Epoch 00211: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.1606 - val_acc: 0.9667\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00212: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1587 - val_acc: 0.9690\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.6459e-04 - acc: 0.9999\n",
      "Epoch 00213: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 6.6452e-04 - acc: 0.9999 - val_loss: 0.1591 - val_acc: 0.9679\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 00214: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1516 - val_acc: 0.9709\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.5338e-04 - acc: 0.9998\n",
      "Epoch 00215: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 166s 5ms/sample - loss: 7.5747e-04 - acc: 0.9998 - val_loss: 0.1582 - val_acc: 0.9716\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9985\n",
      "Epoch 00216: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0051 - acc: 0.9985 - val_loss: 0.1879 - val_acc: 0.9660\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.1917 - acc: 0.9632\n",
      "Loss: 0.1916768369667942 Accuracy: 0.96323985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_BN'\n",
    "\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 1024000)           4096000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 20,493,136\n",
      "Trainable params: 18,444,880\n",
      "Non-trainable params: 2,048,256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 9.6762 - acc: 0.3686\n",
      "Loss: 9.676190913429142 Accuracy: 0.36863968\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 341312)            1365248   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 6,864,592\n",
      "Trainable params: 6,181,456\n",
      "Non-trainable params: 683,136\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 3.7823 - acc: 0.2573\n",
      "Loss: 3.7823052178043195 Accuracy: 0.25732088\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 113728)            454912    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 2,338,128\n",
      "Trainable params: 2,109,904\n",
      "Non-trainable params: 228,224\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.9808 - acc: 0.5047\n",
      "Loss: 1.9808175353617683 Accuracy: 0.5046729\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 37888)             151552    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 846,544\n",
      "Trainable params: 769,744\n",
      "Non-trainable params: 76,800\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.3364 - acc: 0.6432\n",
      "Loss: 1.3364254650916143 Accuracy: 0.6431983\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 25216)             100864    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 668,112\n",
      "Trainable params: 616,144\n",
      "Non-trainable params: 51,968\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.0303 - acc: 0.7092\n",
      "Loss: 1.030256156634195 Accuracy: 0.7092419\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 8320)              33280     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 429,776\n",
      "Trainable params: 411,088\n",
      "Non-trainable params: 18,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.6444 - acc: 0.8258\n",
      "Loss: 0.6444138161366108 Accuracy: 0.82575285\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 2688)              10752     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 416,720\n",
      "Trainable params: 408,784\n",
      "Non-trainable params: 7,936\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.3201 - acc: 0.9128\n",
      "Loss: 0.32009967512307014 Accuracy: 0.9127726\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 480,464\n",
      "Trainable params: 475,600\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.2005 - acc: 0.9435\n",
      "Loss: 0.20054632041289985 Accuracy: 0.9435099\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 770,256\n",
      "Trainable params: 765,136\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.1917 - acc: 0.9632\n",
      "Loss: 0.1916768369667942 Accuracy: 0.96323985\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 1024000)           4096000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 20,493,136\n",
      "Trainable params: 18,444,880\n",
      "Non-trainable params: 2,048,256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 10.5084 - acc: 0.3232\n",
      "Loss: 10.508414896354003 Accuracy: 0.3231568\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 341312)            1365248   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 6,864,592\n",
      "Trainable params: 6,181,456\n",
      "Non-trainable params: 683,136\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 7.8882 - acc: 0.3942\n",
      "Loss: 7.888237498730018 Accuracy: 0.39418483\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 113728)            454912    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 2,338,128\n",
      "Trainable params: 2,109,904\n",
      "Non-trainable params: 228,224\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 4.2380 - acc: 0.5221\n",
      "Loss: 4.238030742979991 Accuracy: 0.5221184\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 37888)             151552    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 846,544\n",
      "Trainable params: 769,744\n",
      "Non-trainable params: 76,800\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 2.3627 - acc: 0.6409\n",
      "Loss: 2.3627194363999093 Accuracy: 0.6409138\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 25216)             100864    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 668,112\n",
      "Trainable params: 616,144\n",
      "Non-trainable params: 51,968\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 1.5612 - acc: 0.7171\n",
      "Loss: 1.5612334081564614 Accuracy: 0.71713394\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 8320)              33280     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 429,776\n",
      "Trainable params: 411,088\n",
      "Non-trainable params: 18,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.7468 - acc: 0.8476\n",
      "Loss: 0.746812808423894 Accuracy: 0.8475597\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 2688)              10752     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 416,720\n",
      "Trainable params: 408,784\n",
      "Non-trainable params: 7,936\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.4135 - acc: 0.9124\n",
      "Loss: 0.41352597511569783 Accuracy: 0.9123572\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 480,464\n",
      "Trainable params: 475,600\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.2275 - acc: 0.9477\n",
      "Loss: 0.22748439204796903 Accuracy: 0.94766355\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 770,256\n",
      "Trainable params: 765,136\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 3ms/sample - loss: 0.2402 - acc: 0.9545\n",
      "Loss: 0.2402077064204377 Accuracy: 0.9545171\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
