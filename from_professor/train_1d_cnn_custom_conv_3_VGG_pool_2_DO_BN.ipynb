{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))    \n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())        \n",
    "        model.add(Activation('relu'))    \n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,397,136\n",
      "Trainable params: 16,396,880\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,230,352\n",
      "Trainable params: 8,229,840\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                4096016   \n",
      "=================================================================\n",
      "Total params: 4,159,568\n",
      "Trainable params: 4,158,800\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,136,784\n",
      "Trainable params: 2,135,760\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,211,792\n",
      "Trainable params: 2,210,256\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,287,376\n",
      "Trainable params: 1,285,328\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 874,960\n",
      "Trainable params: 872,400\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 718,544\n",
      "Trainable params: 715,472\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                253968    \n",
      "=================================================================\n",
      "Total params: 1,013,968\n",
      "Trainable params: 1,009,872\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 1,282,768\n",
      "Trainable params: 1,277,648\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                61456     \n",
      "=================================================================\n",
      "Total params: 1,613,008\n",
      "Trainable params: 1,606,864\n",
      "Non-trainable params: 6,144\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,976,016\n",
      "Trainable params: 1,968,848\n",
      "Non-trainable params: 7,168\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_72 (MaxPooling (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_179 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_180 (Conv1D)          (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_180 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_180 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_181 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_181 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling (None, 3, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                24592     \n",
      "=================================================================\n",
      "Total params: 3,156,688\n",
      "Trainable params: 3,147,472\n",
      "Non-trainable params: 9,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 14):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 11.1846 - acc: 0.1921\n",
      "Epoch 00001: val_loss improved from inf to 10.76626, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/001-10.7663.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 11.1847 - acc: 0.1921 - val_loss: 10.7663 - val_acc: 0.2082\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.5067 - acc: 0.2764\n",
      "Epoch 00002: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 10.5074 - acc: 0.2764 - val_loss: 11.6376 - val_acc: 0.1966\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.2556 - acc: 0.3148\n",
      "Epoch 00003: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 10.2555 - acc: 0.3148 - val_loss: 11.2165 - val_acc: 0.2078\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.1383 - acc: 0.3372\n",
      "Epoch 00004: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 10.1387 - acc: 0.3372 - val_loss: 11.0847 - val_acc: 0.2315\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.0827 - acc: 0.3477\n",
      "Epoch 00005: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 10.0827 - acc: 0.3477 - val_loss: 10.8073 - val_acc: 0.2518\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.0691 - acc: 0.3505\n",
      "Epoch 00006: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 10.0695 - acc: 0.3505 - val_loss: 11.0592 - val_acc: 0.2318\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.0622 - acc: 0.3526\n",
      "Epoch 00007: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 10.0621 - acc: 0.3526 - val_loss: 10.9808 - val_acc: 0.2430\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.0430 - acc: 0.3566\n",
      "Epoch 00008: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 10.0430 - acc: 0.3566 - val_loss: 11.4425 - val_acc: 0.2285\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 10.0253 - acc: 0.3614\n",
      "Epoch 00009: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 10.0252 - acc: 0.3614 - val_loss: 11.3607 - val_acc: 0.2173\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.9809 - acc: 0.3652\n",
      "Epoch 00010: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 9.9805 - acc: 0.3652 - val_loss: 11.3352 - val_acc: 0.2325\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.9417 - acc: 0.3586\n",
      "Epoch 00011: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 9.9412 - acc: 0.3587 - val_loss: 11.9510 - val_acc: 0.1957\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.7041 - acc: 0.3693\n",
      "Epoch 00012: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 9.7041 - acc: 0.3693 - val_loss: 10.8746 - val_acc: 0.2485\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.9769 - acc: 0.3867\n",
      "Epoch 00013: val_loss did not improve from 10.76626\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 8.9774 - acc: 0.3867 - val_loss: 11.9797 - val_acc: 0.1957\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.5366 - acc: 0.4348\n",
      "Epoch 00014: val_loss improved from 10.76626 to 10.17732, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/014-10.1773.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 8.5368 - acc: 0.4348 - val_loss: 10.1773 - val_acc: 0.2597\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.3907 - acc: 0.4534\n",
      "Epoch 00015: val_loss did not improve from 10.17732\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 8.3910 - acc: 0.4533 - val_loss: 10.3419 - val_acc: 0.2385\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.2795 - acc: 0.4632\n",
      "Epoch 00016: val_loss did not improve from 10.17732\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 8.2792 - acc: 0.4632 - val_loss: 10.3056 - val_acc: 0.2595\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.5274 - acc: 0.4903\n",
      "Epoch 00017: val_loss improved from 10.17732 to 9.59488, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/017-9.5949.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 7.5273 - acc: 0.4903 - val_loss: 9.5949 - val_acc: 0.2707\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.2369 - acc: 0.5197\n",
      "Epoch 00018: val_loss did not improve from 9.59488\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 7.2372 - acc: 0.5197 - val_loss: 9.7257 - val_acc: 0.2721\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.1020 - acc: 0.5363\n",
      "Epoch 00019: val_loss did not improve from 9.59488\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 7.1028 - acc: 0.5362 - val_loss: 10.2196 - val_acc: 0.2546\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.8196 - acc: 0.5438\n",
      "Epoch 00020: val_loss did not improve from 9.59488\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.8200 - acc: 0.5438 - val_loss: 10.2527 - val_acc: 0.2378\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.5951 - acc: 0.5634\n",
      "Epoch 00021: val_loss did not improve from 9.59488\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.5942 - acc: 0.5635 - val_loss: 9.6130 - val_acc: 0.2693\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.4730 - acc: 0.5804\n",
      "Epoch 00022: val_loss improved from 9.59488 to 9.28477, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/022-9.2848.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.4721 - acc: 0.5804 - val_loss: 9.2848 - val_acc: 0.2767\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.4286 - acc: 0.5845\n",
      "Epoch 00023: val_loss did not improve from 9.28477\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.4282 - acc: 0.5845 - val_loss: 9.4340 - val_acc: 0.2870\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.3941 - acc: 0.5891\n",
      "Epoch 00024: val_loss improved from 9.28477 to 9.24207, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/024-9.2421.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.3950 - acc: 0.5891 - val_loss: 9.2421 - val_acc: 0.3005\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.3830 - acc: 0.5890\n",
      "Epoch 00025: val_loss did not improve from 9.24207\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.3830 - acc: 0.5891 - val_loss: 10.9369 - val_acc: 0.2180\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.3609 - acc: 0.5924\n",
      "Epoch 00026: val_loss did not improve from 9.24207\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.3614 - acc: 0.5923 - val_loss: 9.6782 - val_acc: 0.2721\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.3532 - acc: 0.5948\n",
      "Epoch 00027: val_loss did not improve from 9.24207\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.3532 - acc: 0.5948 - val_loss: 9.3365 - val_acc: 0.2884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.3369 - acc: 0.5959\n",
      "Epoch 00028: val_loss did not improve from 9.24207\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.3378 - acc: 0.5958 - val_loss: 10.0297 - val_acc: 0.2758\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.3130 - acc: 0.6007\n",
      "Epoch 00029: val_loss did not improve from 9.24207\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.3130 - acc: 0.6007 - val_loss: 9.3766 - val_acc: 0.2944\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.3164 - acc: 0.5999\n",
      "Epoch 00030: val_loss did not improve from 9.24207\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.3159 - acc: 0.6000 - val_loss: 9.9420 - val_acc: 0.2648\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.3184 - acc: 0.5985\n",
      "Epoch 00031: val_loss did not improve from 9.24207\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.3189 - acc: 0.5985 - val_loss: 9.5959 - val_acc: 0.2807\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.2677 - acc: 0.6007\n",
      "Epoch 00032: val_loss did not improve from 9.24207\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.2673 - acc: 0.6007 - val_loss: 9.3968 - val_acc: 0.2826\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.0018 - acc: 0.5979\n",
      "Epoch 00033: val_loss did not improve from 9.24207\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 6.0014 - acc: 0.5980 - val_loss: 9.7713 - val_acc: 0.2497\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.8419 - acc: 0.6197\n",
      "Epoch 00034: val_loss improved from 9.24207 to 9.17387, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/034-9.1739.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.8429 - acc: 0.6197 - val_loss: 9.1739 - val_acc: 0.2737\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.7603 - acc: 0.6324\n",
      "Epoch 00035: val_loss improved from 9.17387 to 9.11663, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/035-9.1166.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.7617 - acc: 0.6323 - val_loss: 9.1166 - val_acc: 0.2732\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.7548 - acc: 0.6325\n",
      "Epoch 00036: val_loss improved from 9.11663 to 8.93083, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/036-8.9308.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.7549 - acc: 0.6325 - val_loss: 8.9308 - val_acc: 0.2902\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.7288 - acc: 0.6375\n",
      "Epoch 00037: val_loss did not improve from 8.93083\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.7294 - acc: 0.6375 - val_loss: 8.9831 - val_acc: 0.2963\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.7224 - acc: 0.6379\n",
      "Epoch 00038: val_loss did not improve from 8.93083\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.7221 - acc: 0.6379 - val_loss: 9.2633 - val_acc: 0.2823\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.3655 - acc: 0.6425\n",
      "Epoch 00039: val_loss did not improve from 8.93083\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 5.3652 - acc: 0.6425 - val_loss: 9.2764 - val_acc: 0.2697\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.7628 - acc: 0.6767\n",
      "Epoch 00040: val_loss improved from 8.93083 to 8.55968, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/040-8.5597.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 4.7630 - acc: 0.6767 - val_loss: 8.5597 - val_acc: 0.2919\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.6285 - acc: 0.6950\n",
      "Epoch 00041: val_loss improved from 8.55968 to 8.21477, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/041-8.2148.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 4.6292 - acc: 0.6949 - val_loss: 8.2148 - val_acc: 0.3194\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.5720 - acc: 0.7048\n",
      "Epoch 00042: val_loss did not improve from 8.21477\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 4.5715 - acc: 0.7048 - val_loss: 8.6201 - val_acc: 0.2702\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.5325 - acc: 0.7090\n",
      "Epoch 00043: val_loss did not improve from 8.21477\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 4.5336 - acc: 0.7089 - val_loss: 9.7643 - val_acc: 0.2048\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.6379 - acc: 0.7335\n",
      "Epoch 00044: val_loss improved from 8.21477 to 7.32634, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/044-7.3263.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.6378 - acc: 0.7335 - val_loss: 7.3263 - val_acc: 0.3128\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3988 - acc: 0.7669\n",
      "Epoch 00045: val_loss did not improve from 7.32634\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.3984 - acc: 0.7669 - val_loss: 7.7427 - val_acc: 0.3249\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3228 - acc: 0.7787\n",
      "Epoch 00046: val_loss did not improve from 7.32634\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.3228 - acc: 0.7787 - val_loss: 8.0522 - val_acc: 0.2961\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2890 - acc: 0.7855\n",
      "Epoch 00047: val_loss did not improve from 7.32634\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2890 - acc: 0.7855 - val_loss: 7.7510 - val_acc: 0.3170\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2745 - acc: 0.7875\n",
      "Epoch 00048: val_loss improved from 7.32634 to 7.08903, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/048-7.0890.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2749 - acc: 0.7875 - val_loss: 7.0890 - val_acc: 0.3485\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2632 - acc: 0.7889\n",
      "Epoch 00049: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2645 - acc: 0.7888 - val_loss: 7.4587 - val_acc: 0.3340\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2511 - acc: 0.7922\n",
      "Epoch 00050: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2516 - acc: 0.7922 - val_loss: 7.3770 - val_acc: 0.3496\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2494 - acc: 0.7915\n",
      "Epoch 00051: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2499 - acc: 0.7914 - val_loss: 7.5662 - val_acc: 0.3364\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2418 - acc: 0.7923\n",
      "Epoch 00052: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2418 - acc: 0.7923 - val_loss: 7.7225 - val_acc: 0.3315\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2421 - acc: 0.7923\n",
      "Epoch 00053: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2421 - acc: 0.7923 - val_loss: 7.3308 - val_acc: 0.3492\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2428 - acc: 0.7919\n",
      "Epoch 00054: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2428 - acc: 0.7919 - val_loss: 8.1790 - val_acc: 0.3142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2371 - acc: 0.7933\n",
      "Epoch 00055: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2379 - acc: 0.7933 - val_loss: 9.7256 - val_acc: 0.2576\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2385 - acc: 0.7929\n",
      "Epoch 00056: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2381 - acc: 0.7930 - val_loss: 7.9789 - val_acc: 0.3191\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2295 - acc: 0.7948\n",
      "Epoch 00057: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2290 - acc: 0.7948 - val_loss: 7.8220 - val_acc: 0.3187\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2269 - acc: 0.7943\n",
      "Epoch 00058: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2269 - acc: 0.7943 - val_loss: 7.7085 - val_acc: 0.3266\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2326 - acc: 0.7940\n",
      "Epoch 00059: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2322 - acc: 0.7941 - val_loss: 7.7758 - val_acc: 0.3354\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2241 - acc: 0.7958\n",
      "Epoch 00060: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2237 - acc: 0.7958 - val_loss: 7.7254 - val_acc: 0.3282\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2141 - acc: 0.7976\n",
      "Epoch 00061: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.2137 - acc: 0.7977 - val_loss: 7.3333 - val_acc: 0.3408\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8762 - acc: 0.8026\n",
      "Epoch 00062: val_loss did not improve from 7.08903\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.8758 - acc: 0.8026 - val_loss: 9.1895 - val_acc: 0.2439\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8685 - acc: 0.8329\n",
      "Epoch 00063: val_loss improved from 7.08903 to 6.63144, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/063-6.6314.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.8683 - acc: 0.8330 - val_loss: 6.6314 - val_acc: 0.2968\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5444 - acc: 0.8835\n",
      "Epoch 00064: val_loss improved from 6.63144 to 6.02458, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/064-6.0246.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.5441 - acc: 0.8835 - val_loss: 6.0246 - val_acc: 0.3194\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4632 - acc: 0.8989\n",
      "Epoch 00065: val_loss improved from 6.02458 to 5.67635, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/065-5.6763.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4632 - acc: 0.8989 - val_loss: 5.6763 - val_acc: 0.3515\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4444 - acc: 0.9028\n",
      "Epoch 00066: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4442 - acc: 0.9028 - val_loss: 5.9933 - val_acc: 0.3454\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4229 - acc: 0.9076\n",
      "Epoch 00067: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4227 - acc: 0.9076 - val_loss: 5.8764 - val_acc: 0.3485\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4116 - acc: 0.9093\n",
      "Epoch 00068: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4114 - acc: 0.9093 - val_loss: 6.1510 - val_acc: 0.3401\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4079 - acc: 0.9101\n",
      "Epoch 00069: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4077 - acc: 0.9101 - val_loss: 6.0463 - val_acc: 0.3438\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4091 - acc: 0.9087\n",
      "Epoch 00070: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4098 - acc: 0.9087 - val_loss: 6.4388 - val_acc: 0.3354\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4074 - acc: 0.9097\n",
      "Epoch 00071: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4073 - acc: 0.9097 - val_loss: 6.5398 - val_acc: 0.3417\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4082 - acc: 0.9090\n",
      "Epoch 00072: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4080 - acc: 0.9091 - val_loss: 6.6310 - val_acc: 0.3280\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4024 - acc: 0.9101\n",
      "Epoch 00073: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4027 - acc: 0.9101 - val_loss: 6.1074 - val_acc: 0.3555\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4058 - acc: 0.9095\n",
      "Epoch 00074: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4056 - acc: 0.9095 - val_loss: 5.9443 - val_acc: 0.3650\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4103 - acc: 0.9081\n",
      "Epoch 00075: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.4110 - acc: 0.9081 - val_loss: 6.1166 - val_acc: 0.3748\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6995 - acc: 0.9254\n",
      "Epoch 00076: val_loss did not improve from 5.67635\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.6994 - acc: 0.9254 - val_loss: 8.4794 - val_acc: 0.2103\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9646\n",
      "Epoch 00077: val_loss improved from 5.67635 to 4.86885, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv_checkpoint/077-4.8689.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.2438 - acc: 0.9647 - val_loss: 4.8689 - val_acc: 0.3378\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9826\n",
      "Epoch 00078: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1838 - acc: 0.9826 - val_loss: 5.2811 - val_acc: 0.3657\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9865\n",
      "Epoch 00079: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1700 - acc: 0.9865 - val_loss: 5.2707 - val_acc: 0.3566\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9856\n",
      "Epoch 00080: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1713 - acc: 0.9856 - val_loss: 5.6472 - val_acc: 0.3408\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9876\n",
      "Epoch 00081: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1646 - acc: 0.9876 - val_loss: 6.1152 - val_acc: 0.3322\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9876\n",
      "Epoch 00082: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1648 - acc: 0.9876 - val_loss: 5.3601 - val_acc: 0.3615\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9861\n",
      "Epoch 00083: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1669 - acc: 0.9861 - val_loss: 6.7071 - val_acc: 0.3005\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9859\n",
      "Epoch 00084: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1679 - acc: 0.9859 - val_loss: 5.6630 - val_acc: 0.3578\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9880\n",
      "Epoch 00085: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1606 - acc: 0.9880 - val_loss: 5.6068 - val_acc: 0.3823\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9870\n",
      "Epoch 00086: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1640 - acc: 0.9870 - val_loss: 5.9928 - val_acc: 0.3461\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9878\n",
      "Epoch 00087: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1584 - acc: 0.9877 - val_loss: 5.7652 - val_acc: 0.3361\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9880\n",
      "Epoch 00088: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1582 - acc: 0.9880 - val_loss: 5.2329 - val_acc: 0.3890\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9890\n",
      "Epoch 00089: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1549 - acc: 0.9890 - val_loss: 5.2909 - val_acc: 0.3767\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9888\n",
      "Epoch 00090: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1541 - acc: 0.9888 - val_loss: 5.7001 - val_acc: 0.3468\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9888\n",
      "Epoch 00091: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1536 - acc: 0.9888 - val_loss: 6.1315 - val_acc: 0.3422\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9887\n",
      "Epoch 00092: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1540 - acc: 0.9887 - val_loss: 5.8312 - val_acc: 0.3562\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9882\n",
      "Epoch 00093: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1554 - acc: 0.9882 - val_loss: 6.7377 - val_acc: 0.3245\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9886\n",
      "Epoch 00094: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1536 - acc: 0.9886 - val_loss: 5.8842 - val_acc: 0.3631\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9883\n",
      "Epoch 00095: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1536 - acc: 0.9883 - val_loss: 5.4452 - val_acc: 0.3855\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9899\n",
      "Epoch 00096: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1484 - acc: 0.9899 - val_loss: 5.6262 - val_acc: 0.3527\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9882\n",
      "Epoch 00097: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1550 - acc: 0.9882 - val_loss: 6.6746 - val_acc: 0.3408\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9893\n",
      "Epoch 00098: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1496 - acc: 0.9893 - val_loss: 5.9618 - val_acc: 0.3669\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9899\n",
      "Epoch 00099: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1489 - acc: 0.9899 - val_loss: 6.3320 - val_acc: 0.3536\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9896\n",
      "Epoch 00100: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1491 - acc: 0.9896 - val_loss: 5.5161 - val_acc: 0.3781\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9886\n",
      "Epoch 00101: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1525 - acc: 0.9886 - val_loss: 5.5509 - val_acc: 0.3846\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9893\n",
      "Epoch 00102: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1500 - acc: 0.9893 - val_loss: 6.0024 - val_acc: 0.3492\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9892\n",
      "Epoch 00103: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1508 - acc: 0.9892 - val_loss: 6.1464 - val_acc: 0.3480\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9886\n",
      "Epoch 00104: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1515 - acc: 0.9886 - val_loss: 6.1485 - val_acc: 0.3669\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9890\n",
      "Epoch 00105: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1518 - acc: 0.9890 - val_loss: 5.7015 - val_acc: 0.3774\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9885\n",
      "Epoch 00106: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1535 - acc: 0.9885 - val_loss: 5.9218 - val_acc: 0.3711\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9901\n",
      "Epoch 00107: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1476 - acc: 0.9901 - val_loss: 5.7703 - val_acc: 0.3792\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9892\n",
      "Epoch 00108: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1504 - acc: 0.9892 - val_loss: 6.0584 - val_acc: 0.3475\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9892\n",
      "Epoch 00109: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1510 - acc: 0.9892 - val_loss: 6.2174 - val_acc: 0.3433\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9888\n",
      "Epoch 00110: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1515 - acc: 0.9888 - val_loss: 5.6350 - val_acc: 0.3783\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9890\n",
      "Epoch 00111: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1508 - acc: 0.9890 - val_loss: 7.0218 - val_acc: 0.3368\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9897\n",
      "Epoch 00112: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1487 - acc: 0.9897 - val_loss: 6.4854 - val_acc: 0.3371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9889\n",
      "Epoch 00113: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1509 - acc: 0.9889 - val_loss: 7.6497 - val_acc: 0.2809\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9902\n",
      "Epoch 00114: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1477 - acc: 0.9902 - val_loss: 5.5733 - val_acc: 0.3867\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9898\n",
      "Epoch 00115: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1476 - acc: 0.9898 - val_loss: 5.8160 - val_acc: 0.3832\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9896\n",
      "Epoch 00116: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1486 - acc: 0.9896 - val_loss: 6.0070 - val_acc: 0.3569\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9896\n",
      "Epoch 00117: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1484 - acc: 0.9896 - val_loss: 6.1965 - val_acc: 0.3601\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9901\n",
      "Epoch 00118: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1455 - acc: 0.9901 - val_loss: 5.7476 - val_acc: 0.3718\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9896\n",
      "Epoch 00119: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1473 - acc: 0.9896 - val_loss: 5.9057 - val_acc: 0.3692\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9892\n",
      "Epoch 00120: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1504 - acc: 0.9892 - val_loss: 5.8631 - val_acc: 0.3562\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9900\n",
      "Epoch 00121: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1461 - acc: 0.9900 - val_loss: 5.9467 - val_acc: 0.3627\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9899\n",
      "Epoch 00122: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1456 - acc: 0.9899 - val_loss: 6.6650 - val_acc: 0.3256\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9901\n",
      "Epoch 00123: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1455 - acc: 0.9901 - val_loss: 5.5636 - val_acc: 0.3869\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9907\n",
      "Epoch 00124: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1432 - acc: 0.9907 - val_loss: 8.0057 - val_acc: 0.2874\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9895\n",
      "Epoch 00125: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1470 - acc: 0.9895 - val_loss: 7.1757 - val_acc: 0.3168\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9897\n",
      "Epoch 00126: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1461 - acc: 0.9897 - val_loss: 5.7619 - val_acc: 0.3767\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9902\n",
      "Epoch 00127: val_loss did not improve from 4.86885\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 0.1448 - acc: 0.9902 - val_loss: 5.6576 - val_acc: 0.3825\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 785us/sample - loss: 5.1888 - acc: 0.3217\n",
      "Loss: 5.188760419253372 Accuracy: 0.32170302\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.1840 - acc: 0.2606\n",
      "Epoch 00001: val_loss improved from inf to 7.05183, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_2_conv_checkpoint/001-7.0518.hdf5\n",
      "36805/36805 [==============================] - 142s 4ms/sample - loss: 7.1844 - acc: 0.2606 - val_loss: 7.0518 - val_acc: 0.2257\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.8991 - acc: 0.3936\n",
      "Epoch 00002: val_loss did not improve from 7.05183\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 6.8996 - acc: 0.3935 - val_loss: 7.4197 - val_acc: 0.2919\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.1446 - acc: 0.4745\n",
      "Epoch 00003: val_loss did not improve from 7.05183\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 6.1455 - acc: 0.4745 - val_loss: 7.3213 - val_acc: 0.2714\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.8856 - acc: 0.5392\n",
      "Epoch 00004: val_loss did not improve from 7.05183\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.8859 - acc: 0.5392 - val_loss: 7.7986 - val_acc: 0.2688\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.6461 - acc: 0.5622\n",
      "Epoch 00005: val_loss did not improve from 7.05183\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.6463 - acc: 0.5622 - val_loss: 7.8006 - val_acc: 0.2628\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.2624 - acc: 0.5993\n",
      "Epoch 00006: val_loss improved from 7.05183 to 6.77828, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_2_conv_checkpoint/006-6.7783.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.2623 - acc: 0.5993 - val_loss: 6.7783 - val_acc: 0.3361\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.1422 - acc: 0.6231\n",
      "Epoch 00007: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.1416 - acc: 0.6231 - val_loss: 7.3690 - val_acc: 0.2895\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.0728 - acc: 0.6402\n",
      "Epoch 00008: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.0730 - acc: 0.6402 - val_loss: 7.9785 - val_acc: 0.2842\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.0334 - acc: 0.6521\n",
      "Epoch 00009: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.0327 - acc: 0.6521 - val_loss: 7.7620 - val_acc: 0.2989\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.0122 - acc: 0.6604\n",
      "Epoch 00010: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.0115 - acc: 0.6605 - val_loss: 7.1617 - val_acc: 0.3336\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.0415 - acc: 0.6523\n",
      "Epoch 00011: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.0421 - acc: 0.6522 - val_loss: 7.4874 - val_acc: 0.3380\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.0364 - acc: 0.6554\n",
      "Epoch 00012: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.0367 - acc: 0.6553 - val_loss: 7.6218 - val_acc: 0.3245\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9918 - acc: 0.6652\n",
      "Epoch 00013: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9925 - acc: 0.6652 - val_loss: 8.0041 - val_acc: 0.3159\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.0116 - acc: 0.6637\n",
      "Epoch 00014: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.0114 - acc: 0.6637 - val_loss: 8.4068 - val_acc: 0.2905\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9845 - acc: 0.6696\n",
      "Epoch 00015: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9843 - acc: 0.6696 - val_loss: 7.7171 - val_acc: 0.3389\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9755 - acc: 0.6724\n",
      "Epoch 00016: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9757 - acc: 0.6724 - val_loss: 7.9523 - val_acc: 0.3217\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9992 - acc: 0.6666\n",
      "Epoch 00017: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9995 - acc: 0.6665 - val_loss: 8.9246 - val_acc: 0.2844\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.0188 - acc: 0.6641\n",
      "Epoch 00018: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 5.0194 - acc: 0.6640 - val_loss: 9.5418 - val_acc: 0.2537\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9449 - acc: 0.6802\n",
      "Epoch 00019: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9447 - acc: 0.6802 - val_loss: 7.7766 - val_acc: 0.3638\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9580 - acc: 0.6773\n",
      "Epoch 00020: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9586 - acc: 0.6773 - val_loss: 8.4467 - val_acc: 0.3140\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9565 - acc: 0.6774\n",
      "Epoch 00021: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9567 - acc: 0.6774 - val_loss: 7.9747 - val_acc: 0.3562\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9553 - acc: 0.6776\n",
      "Epoch 00022: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9551 - acc: 0.6777 - val_loss: 7.8938 - val_acc: 0.3429\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9462 - acc: 0.6801\n",
      "Epoch 00023: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9469 - acc: 0.6800 - val_loss: 8.1058 - val_acc: 0.3410\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9653 - acc: 0.6761\n",
      "Epoch 00024: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9651 - acc: 0.6761 - val_loss: 9.5097 - val_acc: 0.2742\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.9541 - acc: 0.6796\n",
      "Epoch 00025: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.9535 - acc: 0.6797 - val_loss: 7.9539 - val_acc: 0.3438\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.8241 - acc: 0.6763\n",
      "Epoch 00026: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.8240 - acc: 0.6763 - val_loss: 8.3874 - val_acc: 0.3103\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.4768 - acc: 0.6906\n",
      "Epoch 00027: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.4767 - acc: 0.6906 - val_loss: 8.2997 - val_acc: 0.3124\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.3459 - acc: 0.7131\n",
      "Epoch 00028: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.3458 - acc: 0.7131 - val_loss: 7.3470 - val_acc: 0.3569\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.3228 - acc: 0.7171\n",
      "Epoch 00029: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.3235 - acc: 0.7170 - val_loss: 7.2681 - val_acc: 0.3627\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.3267 - acc: 0.7168\n",
      "Epoch 00030: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.3261 - acc: 0.7169 - val_loss: 7.9372 - val_acc: 0.3408\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.3112 - acc: 0.7210\n",
      "Epoch 00031: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.3115 - acc: 0.7209 - val_loss: 7.1901 - val_acc: 0.3748\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.3124 - acc: 0.7203\n",
      "Epoch 00032: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.3123 - acc: 0.7203 - val_loss: 8.8104 - val_acc: 0.2944\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.3104 - acc: 0.7210\n",
      "Epoch 00033: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.3116 - acc: 0.7210 - val_loss: 7.8501 - val_acc: 0.3413\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.2924 - acc: 0.7243\n",
      "Epoch 00034: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.2923 - acc: 0.7244 - val_loss: 7.5916 - val_acc: 0.3622\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.3031 - acc: 0.7228\n",
      "Epoch 00035: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.3034 - acc: 0.7228 - val_loss: 9.4644 - val_acc: 0.2690\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.2964 - acc: 0.7252\n",
      "Epoch 00036: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.2958 - acc: 0.7253 - val_loss: 7.2331 - val_acc: 0.3823\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.3022 - acc: 0.7236\n",
      "Epoch 00037: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.3021 - acc: 0.7237 - val_loss: 7.4850 - val_acc: 0.3480\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.2918 - acc: 0.7245\n",
      "Epoch 00038: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.2912 - acc: 0.7245 - val_loss: 8.3399 - val_acc: 0.3242\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.2861 - acc: 0.7246\n",
      "Epoch 00039: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 4.2864 - acc: 0.7246 - val_loss: 8.7937 - val_acc: 0.2984\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.8358 - acc: 0.7342\n",
      "Epoch 00040: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 3.8352 - acc: 0.7342 - val_loss: 7.7757 - val_acc: 0.3107\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9424 - acc: 0.7719\n",
      "Epoch 00041: val_loss did not improve from 6.77828\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 2.9425 - acc: 0.7719 - val_loss: 7.9781 - val_acc: 0.2914\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5914 - acc: 0.8139\n",
      "Epoch 00042: val_loss improved from 6.77828 to 6.07384, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_2_conv_checkpoint/042-6.0738.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 2.5910 - acc: 0.8139 - val_loss: 6.0738 - val_acc: 0.3846\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5409 - acc: 0.8265\n",
      "Epoch 00043: val_loss improved from 6.07384 to 5.97479, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_2_conv_checkpoint/043-5.9748.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 2.5405 - acc: 0.8265 - val_loss: 5.9748 - val_acc: 0.3969\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5312 - acc: 0.8302\n",
      "Epoch 00044: val_loss did not improve from 5.97479\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 2.5311 - acc: 0.8302 - val_loss: 6.4289 - val_acc: 0.3790\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5188 - acc: 0.8333\n",
      "Epoch 00045: val_loss did not improve from 5.97479\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 2.5189 - acc: 0.8333 - val_loss: 6.0521 - val_acc: 0.3972\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5145 - acc: 0.8346\n",
      "Epoch 00046: val_loss did not improve from 5.97479\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 2.5141 - acc: 0.8346 - val_loss: 6.0135 - val_acc: 0.4107\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5067 - acc: 0.8360\n",
      "Epoch 00047: val_loss improved from 5.97479 to 5.95141, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_2_conv_checkpoint/047-5.9514.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 2.5063 - acc: 0.8360 - val_loss: 5.9514 - val_acc: 0.4137\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5173 - acc: 0.8338\n",
      "Epoch 00048: val_loss did not improve from 5.95141\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 2.5174 - acc: 0.8338 - val_loss: 7.1187 - val_acc: 0.3720\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3581 - acc: 0.8376\n",
      "Epoch 00049: val_loss improved from 5.95141 to 5.55240, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_2_conv_checkpoint/049-5.5524.hdf5\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 2.3579 - acc: 0.8376 - val_loss: 5.5524 - val_acc: 0.3974\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0898 - acc: 0.8487\n",
      "Epoch 00050: val_loss did not improve from 5.55240\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 2.0899 - acc: 0.8487 - val_loss: 5.5924 - val_acc: 0.3885\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0159 - acc: 0.8626\n",
      "Epoch 00051: val_loss improved from 5.55240 to 5.50143, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_2_conv_checkpoint/051-5.5014.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 2.0156 - acc: 0.8626 - val_loss: 5.5014 - val_acc: 0.4076\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9875 - acc: 0.8676\n",
      "Epoch 00052: val_loss did not improve from 5.50143\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 1.9881 - acc: 0.8676 - val_loss: 5.5172 - val_acc: 0.4102\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9865 - acc: 0.8701\n",
      "Epoch 00053: val_loss did not improve from 5.50143\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 1.9864 - acc: 0.8700 - val_loss: 5.5021 - val_acc: 0.4205\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9955 - acc: 0.8673\n",
      "Epoch 00054: val_loss did not improve from 5.50143\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 1.9965 - acc: 0.8672 - val_loss: 5.9354 - val_acc: 0.3972\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9792 - acc: 0.8712\n",
      "Epoch 00055: val_loss did not improve from 5.50143\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 1.9790 - acc: 0.8712 - val_loss: 5.8019 - val_acc: 0.3990\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9787 - acc: 0.8722\n",
      "Epoch 00056: val_loss did not improve from 5.50143\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 1.9789 - acc: 0.8722 - val_loss: 6.6522 - val_acc: 0.3587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5492 - acc: 0.8826\n",
      "Epoch 00057: val_loss improved from 5.50143 to 4.78216, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_2_conv_checkpoint/057-4.7822.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 1.5490 - acc: 0.8826 - val_loss: 4.7822 - val_acc: 0.4074\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9298\n",
      "Epoch 00058: val_loss did not improve from 4.78216\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.2904 - acc: 0.9298 - val_loss: 5.2001 - val_acc: 0.3552\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9748\n",
      "Epoch 00059: val_loss improved from 4.78216 to 3.71484, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_2_conv_checkpoint/059-3.7148.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.1061 - acc: 0.9748 - val_loss: 3.7148 - val_acc: 0.4468\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9851\n",
      "Epoch 00060: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0734 - acc: 0.9851 - val_loss: 3.8709 - val_acc: 0.4498\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9887\n",
      "Epoch 00061: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0588 - acc: 0.9888 - val_loss: 4.0181 - val_acc: 0.4444\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9900\n",
      "Epoch 00062: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0551 - acc: 0.9899 - val_loss: 4.0118 - val_acc: 0.4614\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9886\n",
      "Epoch 00063: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0594 - acc: 0.9886 - val_loss: 4.5765 - val_acc: 0.4298\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9908\n",
      "Epoch 00064: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 0.0488 - acc: 0.9908 - val_loss: 4.1859 - val_acc: 0.4628\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9912\n",
      "Epoch 00065: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 136s 4ms/sample - loss: 0.0500 - acc: 0.9912 - val_loss: 4.4355 - val_acc: 0.4519\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9903\n",
      "Epoch 00066: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 0.0495 - acc: 0.9903 - val_loss: 5.1575 - val_acc: 0.4088\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9902\n",
      "Epoch 00067: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 0.0529 - acc: 0.9902 - val_loss: 4.3094 - val_acc: 0.4584\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9903\n",
      "Epoch 00068: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 137s 4ms/sample - loss: 0.0502 - acc: 0.9903 - val_loss: 4.2106 - val_acc: 0.4621\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9930\n",
      "Epoch 00069: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0428 - acc: 0.9930 - val_loss: 4.4458 - val_acc: 0.4421\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9937\n",
      "Epoch 00070: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0394 - acc: 0.9938 - val_loss: 4.0925 - val_acc: 0.4645\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9915\n",
      "Epoch 00071: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0456 - acc: 0.9916 - val_loss: 4.7854 - val_acc: 0.4267\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9918\n",
      "Epoch 00072: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0453 - acc: 0.9918 - val_loss: 4.2002 - val_acc: 0.4547\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9935\n",
      "Epoch 00073: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0408 - acc: 0.9935 - val_loss: 4.5051 - val_acc: 0.4335\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9903\n",
      "Epoch 00074: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0506 - acc: 0.9903 - val_loss: 4.4495 - val_acc: 0.4430\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9937\n",
      "Epoch 00075: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0375 - acc: 0.9937 - val_loss: 4.3804 - val_acc: 0.4587\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9947\n",
      "Epoch 00076: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0362 - acc: 0.9947 - val_loss: 4.7506 - val_acc: 0.4335\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9931\n",
      "Epoch 00077: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0407 - acc: 0.9931 - val_loss: 4.3785 - val_acc: 0.4519\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9937\n",
      "Epoch 00078: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0384 - acc: 0.9937 - val_loss: 4.2998 - val_acc: 0.4603\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9942\n",
      "Epoch 00079: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0371 - acc: 0.9942 - val_loss: 4.3090 - val_acc: 0.4691\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9935\n",
      "Epoch 00080: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0404 - acc: 0.9935 - val_loss: 4.4151 - val_acc: 0.4566\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9948\n",
      "Epoch 00081: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0366 - acc: 0.9948 - val_loss: 4.3362 - val_acc: 0.4489\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9935\n",
      "Epoch 00082: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0398 - acc: 0.9935 - val_loss: 4.1574 - val_acc: 0.4812\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9952\n",
      "Epoch 00083: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0347 - acc: 0.9952 - val_loss: 4.6239 - val_acc: 0.4491\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9930\n",
      "Epoch 00084: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0440 - acc: 0.9930 - val_loss: 5.0652 - val_acc: 0.4244\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9942\n",
      "Epoch 00085: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0360 - acc: 0.9942 - val_loss: 5.1585 - val_acc: 0.4237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9952\n",
      "Epoch 00086: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0345 - acc: 0.9952 - val_loss: 4.2374 - val_acc: 0.4645\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9951\n",
      "Epoch 00087: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0336 - acc: 0.9951 - val_loss: 4.5267 - val_acc: 0.4561\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9951\n",
      "Epoch 00088: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0350 - acc: 0.9950 - val_loss: 4.1529 - val_acc: 0.4780\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9940\n",
      "Epoch 00089: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0391 - acc: 0.9940 - val_loss: 4.6744 - val_acc: 0.4477\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9955\n",
      "Epoch 00090: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0325 - acc: 0.9955 - val_loss: 4.8869 - val_acc: 0.4384\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9960\n",
      "Epoch 00091: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0309 - acc: 0.9960 - val_loss: 4.7356 - val_acc: 0.4489\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9941\n",
      "Epoch 00092: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0363 - acc: 0.9941 - val_loss: 4.5392 - val_acc: 0.4559\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9959\n",
      "Epoch 00093: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0309 - acc: 0.9959 - val_loss: 4.2097 - val_acc: 0.4780\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9955\n",
      "Epoch 00094: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0322 - acc: 0.9955 - val_loss: 4.7667 - val_acc: 0.4535\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9947\n",
      "Epoch 00095: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0363 - acc: 0.9946 - val_loss: 4.5278 - val_acc: 0.4601\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9952\n",
      "Epoch 00096: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0346 - acc: 0.9952 - val_loss: 4.2724 - val_acc: 0.4577\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9951\n",
      "Epoch 00097: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0348 - acc: 0.9951 - val_loss: 4.5119 - val_acc: 0.4601\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9942\n",
      "Epoch 00098: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0369 - acc: 0.9942 - val_loss: 4.5285 - val_acc: 0.4477\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9946\n",
      "Epoch 00099: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0354 - acc: 0.9946 - val_loss: 4.4936 - val_acc: 0.4691\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9962\n",
      "Epoch 00100: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0303 - acc: 0.9962 - val_loss: 4.5393 - val_acc: 0.4493\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9952\n",
      "Epoch 00101: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0326 - acc: 0.9952 - val_loss: 4.4592 - val_acc: 0.4654\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9961\n",
      "Epoch 00102: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0306 - acc: 0.9961 - val_loss: 4.3370 - val_acc: 0.4710\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9965\n",
      "Epoch 00103: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0296 - acc: 0.9965 - val_loss: 4.7151 - val_acc: 0.4435\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9954\n",
      "Epoch 00104: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0340 - acc: 0.9954 - val_loss: 4.6784 - val_acc: 0.4524\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9954\n",
      "Epoch 00105: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0336 - acc: 0.9954 - val_loss: 5.0261 - val_acc: 0.4272\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9956\n",
      "Epoch 00106: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0335 - acc: 0.9956 - val_loss: 4.9702 - val_acc: 0.4398\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9960\n",
      "Epoch 00107: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0315 - acc: 0.9960 - val_loss: 4.6193 - val_acc: 0.4619\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9953\n",
      "Epoch 00108: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0323 - acc: 0.9953 - val_loss: 5.8173 - val_acc: 0.3927\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9960\n",
      "Epoch 00109: val_loss did not improve from 3.71484\n",
      "36805/36805 [==============================] - 134s 4ms/sample - loss: 0.0314 - acc: 0.9960 - val_loss: 4.2535 - val_acc: 0.4792\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 4.0156 - acc: 0.4160\n",
      "Loss: 4.01559693424625 Accuracy: 0.4159917\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5470 - acc: 0.3350\n",
      "Epoch 00001: val_loss improved from inf to 2.45329, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_3_conv_checkpoint/001-2.4533.hdf5\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 2.5469 - acc: 0.3350 - val_loss: 2.4533 - val_acc: 0.3331\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5037 - acc: 0.5529\n",
      "Epoch 00002: val_loss improved from 2.45329 to 2.04470, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_3_conv_checkpoint/002-2.0447.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 1.5038 - acc: 0.5529 - val_loss: 2.0447 - val_acc: 0.4584\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0598 - acc: 0.6757\n",
      "Epoch 00003: val_loss improved from 2.04470 to 1.78536, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_3_conv_checkpoint/003-1.7854.hdf5\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 1.0599 - acc: 0.6756 - val_loss: 1.7854 - val_acc: 0.5155\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6909 - acc: 0.7834\n",
      "Epoch 00004: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.6910 - acc: 0.7833 - val_loss: 2.1721 - val_acc: 0.4908\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4746 - acc: 0.8505\n",
      "Epoch 00005: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.4745 - acc: 0.8505 - val_loss: 2.1654 - val_acc: 0.4978\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3761 - acc: 0.8809\n",
      "Epoch 00006: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.3761 - acc: 0.8809 - val_loss: 2.4548 - val_acc: 0.4927\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9067\n",
      "Epoch 00007: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2942 - acc: 0.9067 - val_loss: 2.4271 - val_acc: 0.5045\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9230\n",
      "Epoch 00008: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2480 - acc: 0.9230 - val_loss: 2.2173 - val_acc: 0.5418\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9307\n",
      "Epoch 00009: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.2291 - acc: 0.9307 - val_loss: 2.2570 - val_acc: 0.5323\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9459\n",
      "Epoch 00010: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1825 - acc: 0.9458 - val_loss: 2.5511 - val_acc: 0.5392\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9436\n",
      "Epoch 00011: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1825 - acc: 0.9436 - val_loss: 2.6427 - val_acc: 0.5220\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9502\n",
      "Epoch 00012: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1673 - acc: 0.9502 - val_loss: 2.6091 - val_acc: 0.5346\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9580\n",
      "Epoch 00013: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1385 - acc: 0.9580 - val_loss: 2.7508 - val_acc: 0.5383\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9544\n",
      "Epoch 00014: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1542 - acc: 0.9544 - val_loss: 2.8976 - val_acc: 0.5243\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9569\n",
      "Epoch 00015: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1458 - acc: 0.9569 - val_loss: 2.6111 - val_acc: 0.5532\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9616\n",
      "Epoch 00016: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1280 - acc: 0.9616 - val_loss: 2.7266 - val_acc: 0.5481\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9638\n",
      "Epoch 00017: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1213 - acc: 0.9637 - val_loss: 2.9721 - val_acc: 0.5367\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9611\n",
      "Epoch 00018: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1362 - acc: 0.9611 - val_loss: 2.8835 - val_acc: 0.5404\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9664\n",
      "Epoch 00019: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1147 - acc: 0.9663 - val_loss: 3.3431 - val_acc: 0.4997\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9703\n",
      "Epoch 00020: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1056 - acc: 0.9703 - val_loss: 3.0498 - val_acc: 0.5511\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9701\n",
      "Epoch 00021: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1038 - acc: 0.9701 - val_loss: 3.3325 - val_acc: 0.5167\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9708\n",
      "Epoch 00022: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1035 - acc: 0.9708 - val_loss: 2.8762 - val_acc: 0.5565\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9692\n",
      "Epoch 00023: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1089 - acc: 0.9691 - val_loss: 3.6095 - val_acc: 0.5225\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9700\n",
      "Epoch 00024: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.1082 - acc: 0.9700 - val_loss: 3.5583 - val_acc: 0.5083\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9774\n",
      "Epoch 00025: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0822 - acc: 0.9774 - val_loss: 3.3674 - val_acc: 0.5386\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9756\n",
      "Epoch 00026: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0860 - acc: 0.9756 - val_loss: 3.4161 - val_acc: 0.5344\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9773\n",
      "Epoch 00027: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0789 - acc: 0.9772 - val_loss: 3.5117 - val_acc: 0.5290\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9745\n",
      "Epoch 00028: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0941 - acc: 0.9745 - val_loss: 2.9919 - val_acc: 0.5644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9773\n",
      "Epoch 00029: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0841 - acc: 0.9773 - val_loss: 3.0317 - val_acc: 0.5616\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9814\n",
      "Epoch 00030: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0689 - acc: 0.9814 - val_loss: 2.9939 - val_acc: 0.5698\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9799\n",
      "Epoch 00031: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0754 - acc: 0.9799 - val_loss: 3.3813 - val_acc: 0.5355\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9814\n",
      "Epoch 00032: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0701 - acc: 0.9814 - val_loss: 3.3488 - val_acc: 0.5549\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9787\n",
      "Epoch 00033: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0772 - acc: 0.9787 - val_loss: 3.1149 - val_acc: 0.5639\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9820\n",
      "Epoch 00034: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0649 - acc: 0.9820 - val_loss: 3.1834 - val_acc: 0.5504\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9832\n",
      "Epoch 00035: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0620 - acc: 0.9832 - val_loss: 3.3351 - val_acc: 0.5451\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9828\n",
      "Epoch 00036: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0656 - acc: 0.9828 - val_loss: 3.0385 - val_acc: 0.5772\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9812\n",
      "Epoch 00037: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0678 - acc: 0.9813 - val_loss: 2.9611 - val_acc: 0.5693\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9835\n",
      "Epoch 00038: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0631 - acc: 0.9835 - val_loss: 3.1521 - val_acc: 0.5621\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9813\n",
      "Epoch 00039: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0700 - acc: 0.9813 - val_loss: 4.0560 - val_acc: 0.4910\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9845\n",
      "Epoch 00040: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0590 - acc: 0.9845 - val_loss: 3.0818 - val_acc: 0.5807\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9863\n",
      "Epoch 00041: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0528 - acc: 0.9863 - val_loss: 3.2121 - val_acc: 0.5681\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9851\n",
      "Epoch 00042: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0541 - acc: 0.9851 - val_loss: 3.0743 - val_acc: 0.5761\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9855\n",
      "Epoch 00043: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0575 - acc: 0.9855 - val_loss: 3.2066 - val_acc: 0.5563\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9854\n",
      "Epoch 00044: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0559 - acc: 0.9854 - val_loss: 3.2864 - val_acc: 0.5644\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9865\n",
      "Epoch 00045: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0529 - acc: 0.9866 - val_loss: 3.3245 - val_acc: 0.5607\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9858\n",
      "Epoch 00046: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0583 - acc: 0.9858 - val_loss: 3.5790 - val_acc: 0.5316\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9862\n",
      "Epoch 00047: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0545 - acc: 0.9861 - val_loss: 3.0582 - val_acc: 0.5719\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9855\n",
      "Epoch 00048: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0566 - acc: 0.9855 - val_loss: 3.0594 - val_acc: 0.5812\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9882\n",
      "Epoch 00049: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0463 - acc: 0.9882 - val_loss: 3.3893 - val_acc: 0.5604\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9880\n",
      "Epoch 00050: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0494 - acc: 0.9880 - val_loss: 3.2977 - val_acc: 0.5621\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9892\n",
      "Epoch 00051: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0454 - acc: 0.9892 - val_loss: 3.4313 - val_acc: 0.5521\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9878\n",
      "Epoch 00052: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.0492 - acc: 0.9878 - val_loss: 3.1865 - val_acc: 0.5735\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9890\n",
      "Epoch 00053: val_loss did not improve from 1.78536\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0451 - acc: 0.9890 - val_loss: 3.1493 - val_acc: 0.5775\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4VFX6xz9nJpNMekIILRACitIJVRQpglhAsSCiK2tv69pXBde17671t7tiBV27K2JHQUEQRMUGiHQFQwk1vbfJzPn9cWaSSUiZlMmE5P08z3nuzNxzz33nZnK+p7znPUprjSAIgiAAWAJtgCAIgtB6EFEQBEEQKhBREARBECoQURAEQRAqEFEQBEEQKhBREARBECoQURAEQRAqEFEQBEEQKhBREARBECoICrQBDaVjx446KSkp0GYIgiAcVaxbty5Dax1fX76jThSSkpJYu3ZtoM0QBEE4qlBK7fElnwwfCYIgCBWIKAiCIAgViCgIgiAIFRx1cwo14XA42LdvHyUlJYE25ajFbrfTvXt3bDZboE0RBCGAtAlR2LdvH5GRkSQlJaGUCrQ5Rx1aazIzM9m3bx+9evUKtDmCIASQNjF8VFJSQlxcnAhCI1FKERcXJz0tQRDahigAIghNRJ6fIAjQAqKglLIqpX5WSn1aw7kQpdQ7SqmdSqkflFJJ/rZHEIQWZulS2Lkz0FYIPtISPYVbgG21nLsKyNZaHwv8G3isBexpdnJycnjuuecade2UKVPIycnxOf8DDzzAk08+2ah7CUKLozVccAH885+BtkTwEb+KglKqOzAVeKmWLOcAr7lfvwdMUkfhOEZdolBeXl7ntUuWLCEmJsYfZglC4ElPh4IC2LUr0JYIPuLvnsJ/gLsAVy3nE4BUAK11OZALxPnZpmZnzpw5/P777yQnJ3PnnXeyatUqxo4dy7Rp0+jfvz8A5557LsOHD2fAgAHMnz+/4tqkpCQyMjLYvXs3/fr145prrmHAgAGcdtppFBcX13nfDRs2MHr0aAYPHsx5551HdnY2AHPnzqV///4MHjyYiy66CICvvvqK5ORkkpOTGTp0KPn5+X56GoLgRUqKOe7xKcKC0Arwm0uqUuosIE1rvU4pNaGJZV0LXAuQmJhYZ94dO26loGBDU253BBERyfTp859azz/66KNs3ryZDRvMfVetWsX69evZvHlzhYvnyy+/TIcOHSguLmbkyJFMnz6duLiq+rdjxw7efvttXnzxRS688ELef/99Zs2aVet9L730Up5++mnGjx/Pfffdx4MPPsh//vMfHn30UXbt2kVISEjF0NSTTz7Js88+y5gxYygoKMButzf1sQhC/XhEITUVnE6wWgNrj1Av/uwpjAGmKaV2AwuAiUqpN6vl2Q/0AFBKBQHRQGb1grTW87XWI7TWI+Lj6w3y1yoYNWpUFZ//uXPnMmTIEEaPHk1qaio7duw44ppevXqRnJwMwPDhw9m9e3et5efm5pKTk8P48eMBuOyyy1i9ejUAgwcP5pJLLuHNN98kKMjo/pgxY7j99tuZO3cuOTk5FZ8Lgl/xDBuVl8OBA4G1RfAJv9UMWuu7gbsB3D2FO7TW1Zu9i4DLgO+AC4Avtda6Kfetq0XfkoSHh1e8XrVqFcuXL+e7774jLCyMCRMm1LgmICQkpOK11Wqtd/ioNhYvXszq1av55JNP+Mc//sGmTZuYM2cOU6dOZcmSJYwZM4alS5fSt2/fRpUvCD7j6SmAGULq0SNwtgg+0eLrFJRSDymlprnf/heIU0rtBG4H5rS0Pc1BZGRknWP0ubm5xMbGEhYWxvbt2/n++++bfM/o6GhiY2P5+uuvAXjjjTcYP348LpeL1NRUTjnlFB577DFyc3MpKCjg999/Z9CgQcyePZuRI0eyffv2JtsgCPWyaxd4hkllXuGooEXGELTWq4BV7tf3eX1eAsxoCRv8SVxcHGPGjGHgwIGceeaZTJ06tcr5M844gxdeeIF+/fpx/PHHM3r06Ga572uvvcb1119PUVERvXv35pVXXsHpdDJr1ixyc3PRWnPzzTcTExPDvffey8qVK7FYLAwYMIAzzzyzWWwQhDpJSYFx4+DDD0UUjhJUE0drWpwRI0bo6pvsbNu2jX79+gXIoraDPEehWXE4wG6He+6BF16A886DefMCbVW7RSm1Tms9or58bSbMhSAIrYy9e8Hlgt69oWdPqMNxQmg9iCgIguAfPJPMHlGQ4aOjAhEFQRD8g0cUevUyorB3rwl7IbRqRBQEQfAPu3ZBcDB062ZEobjYhL0QWjUiCoIg+IeUFCMGViskJZnPZF6h1SOiIAiCf0hJMfMJYMQBZF7hKEBEIUBEREQ06HNBOOrYtUtE4ShEREEQhOYnNxeysswkM0BMDERFiSgcBYgoNANz5szh2WefrXjv2QinoKCASZMmMWzYMAYNGsTHH3/sc5laa+68804GDhzIoEGDeOeddwA4ePAg48aNIzk5mYEDB/L111/jdDq5/PLLK/L++9//bvbvKAgNwhMIz9NTAHFLPUpoe6Eyb70VNjRv6GySk+E/tQfamzlzJrfeeit//vOfAVi4cCFLly7Fbrfz4YcfEhUVRUZGBqNHj2batGk+7Yf8wQcfsGHDBn755RcyMjIYOXIk48aN43//+x+nn34699xzD06nk6KiIjZs2MD+/fvZvHkzQIN2chMEv+C9RsFDUpJMNB8FtD1RCABDhw4lLS2NAwcOkJ6eTmxsLD169MDhcPDXv/6V1atXY7FY2L9/P4cPH6ZLly71lvnNN99w8cUXY7Va6dy5M+PHj+enn35i5MiRXHnllTgcDs4991ySk5Pp3bs3KSkp3HTTTUydOpXTTjutBb61INSB9xoFDz17wldfBcYewWfanijU0aL3JzNmzOC9997j0KFDzJw5E4C33nqL9PR01q1bh81mIykpqcaQ2Q1h3LhxrF69msWLF3P55Zdz++23c+mll/LLL7+wdOlSXnjhBRYuXMjLL7/cHF9LEBrHrl0QG2vmEjz07Al5eZCTU/VzoVUhcwrNxMyZM1mwYAHvvfceM2aYwK+5ubl06tQJm83GypUr2dOA8dSxY8fyzjvv4HQ6SU9PZ/Xq1YwaNYo9e/bQuXNnrrnmGq6++mrWr19PRkYGLpeL6dOn8/e//53169f762se3aSkwCuvmIpJ8C/e7qgexAPpqKDt9RQCxIABA8jPzychIYGuXbsCcMkll3D22WczaNAgRowY0aBNbc477zy+++47hgwZglKKxx9/nC5duvDaa6/xxBNPYLPZiIiI4PXXX2f//v1cccUVuFxmK+xHHnnEL9/xqOcf/4CXX4abb4YrroAbb4Tjjgu0VW2TlBQYPLjqZ54FbHv2wJAhLW6S4BsSOluooM0/x9GjoajIOA4sWGBCO595phGJ004Di3ScmwWXC0JDjdPHY49Vfp6WBp07w1NPmWfeWigrg5NOgssug5tuCrQ1fiPgobOVUnal1I9KqV+UUluUUg/WkOdypVS6UmqDO13tL3uEdo7WsG0bjB0Lr79ugrM9+CD8/LMRhrPOkmBtzcWBA6ai9Z5kBoiPN2LR2oaP3n4b1q2D998PtCWtAn82jUqBiVrrIUAycIZSqqYtx97RWie700t+tEdozxw8aOYS+vc377t0gfvuMxXUww/DZ5/Ba68F1sa2Qk1rFACUgsTE1iUKLldlb+bHH03vsZ3jN1HQhgL3W5s7SVNMCAxbt5pj9eGx4GD4619hzBi44w7IyGhc+fv2meGSgwebZmdboKY1Ch5a2wK2Tz81PchzzjFRXJt7jdNRiF8HUZVSVqXUBiAN+EJr/UMN2aYrpTYqpd5TSvXwpz1CO2bbNnP09BS8sVjMNpG5uXDnnQ0v+9tvYcQIM1YuvQ0jCp5eQXVa0wI2reHRR41NTz1lPluzJqAmtQb8Kgpaa6fWOhnoDoxSSg2sluUTIElrPRj4AqjxP0opda1Saq1Sam26xGMXGsPWrcY3vnPnms8PGGB6Cq++CqtW+V7uvHlwyikQGQndu0ulAmb4qEcP0wurTs+epjdWWNjydlXnm2/gu+/gL38xdvXsaQS+ndMi7hZa6xxgJXBGtc8ztdal7rcvAcNruX6+1nqE1npEfHy8f40V2ibbtpmho7pCjNx7r5kcvf56KC2tPR+YidTrrjN5Tz0VfvoJJk82otDeJ6xrWqPgwbNWYe/elrOnNh57DDp2hCuvNO9POsmIQjv/+/nT+yheKRXjfh0KTAa2V8vT1evtNGCbv+zxJzk5OTz33HONunbKlCkSq6gl2Lq15qEjb8LC4Lnn4Ndf4fHHa8938KDpHcyfD3ffDZ98YnohY8ZAZib89lvz2n60kZJypOeRh9aygG3TJli82LjGhoWZz046yXhOtZRgvfZa5bBVK8KfPYWuwEql1EbgJ8ycwqdKqYeUUtPceW52u6v+AtwMXO5He/xGXaJQXl5e57VLliwhRpb8+5eMDLMNpC9rMM44Ay680Cx027Gj6rndu82k9ODBZkLynXfgn/80O4uBqVSgfQ9BFBcb0ayvpxBoUXj8cQgPB3cQS8CIOrTMEOBbb8Hll8Ptt7e+RoTW+qhKw4cP19XZunXrEZ+1JDNnztR2u10PGTJE33HHHXrlypX65JNP1meffbbu06eP1lrrc845Rw8bNkz3799fz5s3r+Lanj176vT0dL1r1y7dt29fffXVV+v+/fvryZMn66KioiPutWjRIj1q1CidnJysJ02apA8dOqS11jo/P19ffvnleuDAgXrQoEH6vffe01pr/dlnn+mhQ4fqwYMH64kTJ9b5PQL9HP3G6tVag9ZLlviW/8ABraOitJ40Sevycq0/+UTrqVO1Vkpri0XradO03rjxyOucTq07dND6qqua136ttc7I0HrHDq1druYvuznZutU867feqvl8ebnWQUFaz5nTsnZ5s2uX1lar1rfdVvVzh0Pr8HCtb7yxceWmpmo9e7bWhw/XnW/5cq1tNq3HjNE6NFTryy9v3P0aCLBW+1DHBrySb2iqTxRuuUXr8eObN91yS90Pe9euXXrAgAEV71euXKnDwsJ0SkpKxWeZmZlaa62Lior0gAEDdEZGhta6qihYrVb9888/a621njFjhn7jjTeOuFdWVpZ2uSuGF198Ud9+++1aa63vuusufYuXoVlZWTotLU137969wg6PDbXRZkVh3jzzU9+1y/drnn3WXNOpkzl26aL13/6m9Z49dV83darWffs2ydwaGTfO2NGzp9bXXKP1woVa1/P3DAiffmrsXLOm9jy9eml98cUtZ1N1brrJCNPevUeemzhR66FDG1fujTea756QoPW339acZ8MGrSMjtR44UOvsbFO5WK0N+202El9FQdb1+4lRo0bRy2tcde7cuQwZMoTRo0eTmprKjupDE0CvXr1ITk4GYPjw4eyuwXVv3759nH766QwaNIgnnniCLVu2ALB8+fKK/RwAYmNj+f777xk3blyFHR06dGjOr+gffvwRsrObt8xt28y4cU0ukrVx3XUwbRoMGgTvvmvGmR9+uP4yxoyB7dvN3EJzcfgwfP21sWf4cFi40AxxdewIJ5wAK1Y0372aSm0L17wJ5FqF9HR46SWYNct4SFVnzBj45RcoKDjyXF04HCZ0ysknQ0gIjB9v5gu8J6337DGr56OjzWLJmBjj8Wa1Vg0HEmDaXEC8AEXOPoLw8PCK16tWrWL58uV89913hIWFMWHChBpDaIeEhFS8tlqtFBcXH5Hnpptu4vbbb2fatGmsWrWKBx54wC/2B4S8PPNPdfHFvvn7//47XH21GZ/t1q32fFu3Qt++DYttZLVCA3bKq8Azr/DddyZ0RnPw6aemcnn4YTOfUV5uvJ2WLTMutJdfDjt3msqoMXz8sZkrWbKkcsy/saSkGAHu1Kn2PD17wvLlTbtPY8jNhfvvN/Med91Vc56TTjKrnH/8ESZO9L3sZcvM3NVdd5lQKpddZhYzrlljRMjhMPNVRUVmzql7d3Nd9+4mOOPLL8Pf/gYJCU3/nk1EegrNQGRkJPn5+bWez83NJTY2lrCwMLZv387333/f6Hvl5uaS4P7hvOZVcU6ePLnKlqDZ2dmMHj2a1atXs8vdesvKymr0fVuEr782/zzvvGP2962PJ580awrqq7y3bavf86i5GDkSgoKad7Ly449NRTpokHkfFAQnnmgquBdfNKupX2pkhJjt202reetWs+aiqezaZTyP6nL97dmzMj6Sv0lPh//+F6ZONUL1/PNwySW1Ox2MHm1sb6izwJtvQlwcnH666QF8+KFZGPfeezBqlLl/Sor5Ww4YUPXaOXPA6TS/51aAiEIzEBcXx5gxYxg4cCB31rAi9owzzqC8vJx+/foxZ84cRo+uKQSUbzzwwAPMmDGD4cOH07Fjx4rP//a3v5Gdnc3AgQMZMmQIK1euJD4+nvnz53P++eczZMiQis1/Wi1ffmla6KWl9fcUcnPhjTfM67qGT/LzITXVN8+j5iAsDIYObT4PpKIi+OILE4ahpop20iTTMv3nP00LuCHk58P555sgdSefbFqrTY39U9caBQ9JSabnk5ratHvVxQcfGLfhLl1Mb3LbNhMB9dtvTUDE2oiJMZV2Q0Q9Lw8++ghmzqxcsGexwOzZ5reZnQ0//GB+r+PHH3l9UhL88Y9GlNPSGvQ1/YIvEw+tKbVG76O2QsCfY3Ky1qecovWJJ2p9/PF1e9o89ZSZ1Bs2zHj8OJ015/vxR5Pvww/9Y3NN3Hqr1na71mVlTS/ro4+M/cuX155n5UqT5z//8b1cl0vrGTOMN9WKFVovXmzKcHutNQqXS+uICK1vvrnufCtWmHutWNH4e9VGerrWF15oyj/uOK3vvVfrn39umNfWtddqHR1d+2+qOq+8Uvfk+uHDWn/3Xd1l/Pqr8W6bPdt3OxsI7dX7SGg8AX2O6enm5/jww1q/9pp5vXJlzXmdTvMPf8IJWr/xhsm7fn3NeV991Zzfvt1vph/BwoXmnj/80PSyrrzSVFD1Ccwpp2jdubPWhYW+lfvkk8bGxx8378vLte7RQ+vJkxtvq+dvWJ847dxp8r38cuPvVRMffGC8xWw2rf/xD+Ni2hg8v79Nm3zLP2mS1scc03R34YsuMqLqJ68yX0VBho+E1oEn3tDEiTBjhtnf94UXas67YoVZ8HPjjZWTgbUNIW3bBjYbHHNMs5tcK821CMrpNKulp0wx36EuHnzQeCk9/3z95a5aZYY2zj/feL+AGba7+mozVOWJctpQ6oqO6k2PHmYorLk8kLKyzLzI+eebidp168zEeVAj/Wg8zgK+/P327zfDnrNm1T2P4gv33GO8nubObVo5TUREQWgdfPklRESYidrQUOO98cEHNY+xPvus2bBlxgzjddS3b+2isHWr2XKzsRVEY+jWrXmCq/3wg5koPeec+vOOHWtiLz32WN3ulPv2mbHvPn3MftXeFdlVV5mx8BdfbJy9HlGoLcSFh+Bg84xqipb6ww9mTsZX76Rvv4WBA41zwoMPmus9E/KN5ZhjzO/Ll7/f22+b+ZFLLmnaPcF8j/POM66sAdxHXERBaB18+SWMG1fZIr7uOjPp+corVfPt2WNaz9dcU+mCOXGi8VyqyZvFEwivpRkzpunB8T7+2DyPM86oPy+YSjE93YhmTWRmGiEtKjKCGxVV9XxCgnGjffnlxnkGedYo1CcKUPNahW3bTK9owwaYPh02b667jI0bTf6ICONCet999feofEGpyr9ffbz5plkr0qdP0+8LpreQkwN/+pPp8TTl99NIRBSEwLN/vwlC5+0X3rcvTJhgPDJcrsrPPUNK111X+dmkSSYU848/Vi23pMS0XlvKHdUbT3C1pgyRfPyxeQbR0b7lP/FEszjq8certjQdDnj6aVNx/fSTEdrahPK660zvbNGihtubkmJCk3ut0amV6qKQmmr2ybbZYOVK48U1dSocOlTz9Xv3mu8aGWl6FUOHNtzeujjpJLP2oy5voE2bzEK3WbOa777Dh5th0XfeMXt09OxpgvZ9+WWL7QonoiAEnpUrzbH6YqHrrjOtzy++MO9LSszQxjnnVF1ZPGGCad1VH0L67TcjKIHqKUDj5xV+/dWkadPqz+vNgw+aMfannzbvv/gCkpNNxTJsmGmFX3BB7deffrp5tvPnN+y+WpsK0pdeApjKLjXVzJtkZpr75uXB55+bv+enn5rFYGeffeTeC578hYUmf0NWqvuKL3+/t94yczHN7er99NNmfujVV83f7MUXTcOnc+eWWZ3ry2x0a0ptxfsoPDw80CYcQcCe4xVXaB0be6QLYEmJ1vHxWp93nnnv8QqpyT1z2DATH8ibBQtM/l9+8Y/ddeFwGE+SG25o3PWPP25sry/WUk2cfbbWMTEmcB9o3bu3cW311TvmoYfMdTt3+n7Pt9821zz9tG/5n3++0its9GitQ0KO9Db7+GPjpnnOOcY7SmvjXXXiiSb/qlW+29dQiou1Dg7W+s47az7vdGrdvbuJdeVvCgqMS/Wll2r9v/81uhjEJbV1I6LgxuXSOjFR6/PPr/n87NkmYNi+fVqPHGmCzdVUud15p3FFLCio/Oy++4wffnGxf2yvj1NPNWsvGsPJJzf+2vXrzb92RITWjz1mxLUh7NtnnrmvkUyzs4077IgRlZV3fXz2mbGxTx/zN/rgg5rzedaj3HabEdpp04xQNGU9ha+ceKLWJ51U87kvvzR2LVjgfzuaCRGFFmT27Nn6mWeeqXh///336yeeeELn5+friRMn6qFDh+qBAwfqjz76qCJPbaJQW4jtmkJg1xYuu7EE5Dn+/rv5GXo9vyp4fNo9rd7aWqKff27OL11a+dmMGVofe2zz2+wrHlHKy2vYdWlp5rr772/8vb/5RuuDBxt//TnnGJ//0tL68/7pT8bedet8L98TYhu0nj+/7rw332zyjRxZ92+lubnjDtNbqElUr7zSRDv1dV1IK8BXUWhzAfFu/fxWNhza0KxlJndJ5j9n1D6WN3PmTG699daKKKULFy5k6dKl2O12PvzwQ6KiosjIyGD06NFMmzYNVYc/88svv0yHDh0oLi5m5MiRTJ8+HZfLxTXXXMPq1avp1atXRQyjhx9+mOjoaDZt2gSYeEdHHV9+aY6TJtV8/phjzATkokXGy+TSS2vOd/LJZpJyxQqTH4w7aiDmEzyMGWPmNH74wWzZ6SuLF5vrGjqfUP3eTeHaa81E96JFdc9BfP+9mfz3zFn4Sq9eZj3DtdcaT7K6+Ne/zNzSJ5+Y9QfeG+P4k5NOMvGI/v1vs7bC5TJzIE6niWk0fXrlrm1tCL+JglLKDqwGQtz3eU9rfX+1PCHA65i9mTOBmVrr3f6yyV8MHTqUtLQ0Dhw4QHp6OrGxsfTo0QOHw8Ff//pXVq9ejcViYf/+/Rw+fJguXbrUWtbcuXP58MMPASpCbKenp9cYAnv58uUsWLCg4trY2Fg/fks/sWIFdO0Kxx9fe57rrzdRKC+99Eg3Sg/h4SaYmWeyubzcTDQ3V6TSxnDCCWYCfM2ahonCxx+b6JnN7VHTEDwTzs8/byq/mhoyDodxBujWzURwbQh2u4ly6wtWq/HGWbOmYZFLm8rJJ5s1FXffXfN5z97ObQx/9hRKgYla6wKllA34Rin1mdbaO0ToVUC21vpYpdRFwGNAk6by62rR+5MZM2bw3nvvcejQoYrAc2+99Rbp6emsW7cOm81GUlJSjSGzPfgaYrvNoLXpKUyeXPdq0LPPNhEnL7us7vImTTLeN9nZxl/f4QhsTyE62iyk8l4ElZJi9mdYuNBEKJ082azEPess6NDBBLVbtsyEw27qCtmmYLXCDTeYCJ5TpxrX4Or7Dzz1lFkr8MEHxjXUn4SG1t6b9Bfx8WaBXU6OeR4WizlaraaH4BWQsi3hN5dU9zCWZ2mlzZ2qr8Q4B/CEw3wPmKTqGltpxcycOZMFCxbw3nvvMWPGDMCEue7UqRM2m42VK1eypx6f9dpCbNcWArumcNlHFVu3Gj/w+lp/QUEmLEMdPSzAlKO1CeOwdav5LBBrFLw56SSzt8ITT5jV2sccYypamw3+8AezQOmyy0xY51NPhb/8xSwu82UVs7+54w7jAvnVVyZy6AsvVK4Z2bPHhO4++2w499zA2ulPunY1DYvjjoNjjzXDXomJbVYQwM/rFJRSVqXUBiAN+EJr/UO1LAlAKoDWuhzIBeJqKOdapdRapdTa9PR0f5rcaAYMGEB+fj4JCQl07doVgEsuuYS1a9cyaNAgXn/9dfr27VtnGbWF2K4tBHZN4bJbHU5n1cVn3njmE5prSOCEE0wLbsUKszoWzCK4QHLyySZE9V13mZb/E0+Y8fHvvzf+53v3mkV3d95pQlA8/7zpYdQUYrmlsVrhllvMyuJRo8wq24kTYccOs8AKjE/90dmOE2rDl9nopiYgBlgJDKz2+Wagu9f734GOdZXVGr2P2gp+eY5/+IPZV7imsMLnnmv2621OzjhD6379tJ41y/iRB5qyMq3ffFNrr/2662TrVq23bPGvTY3B5dL6pZdMxFabzXgBPfFEoK0SGgCtKUqq1jrHLQrVg7jsB3oAKKWCgGjMhLPQFti4Ef73PxOqYOxYE37B02twOs0wT3NPHE6caHoJX30V+KEjMMNEl1zi+0rffv1ah93VUcoEzNu61QxtTZ5sehFCm8NvoqCUildKxbhfhwKTge3Vsi0CPLOHFwBfuhVNaAv8859mAnLbNjPuPHu2mVDNyDDhFnJymn/y0FNeS+621p7o1s1MlC9b1jzB54RWhz97Cl2BlUqpjcBPmDmFT5VSDymlPA7Y/wXilFI7gduBOY29mWhJ0zji+TkcsHRp7fMB9fHrr8bD5s9/Nq3kd9+FZ54x4/3JyfB//2fynXJK0wyvTnKy2YsBWmeLWxBaOf70PtqotR6qtR6stR6otX7I/fl9WutF7tclWusZWutjtdajtNaN2t3DbreTmZkpwtBItNZkZmZit9srP7zpJhOy+eWXG1foo48aX/TbbjPvlTIC8d135vO33zaVdn0eRQ3FYqkUGukpCEKDaRMrmrt3785CQModAAAgAElEQVS+fftorZ5JRwN2u53u3bubN/PmmRQcbGLzX3VVwzxMdu82m5TfdJNxtfRm2DBYv97EjT/hhGazvwrnnmuGNwYO9E/5gtCGUUdb63rEiBF67dq1gTaj7fLNN6alPXmyWbR0442mde92j/WJP/3J9DBSUszGLS2N1iasckREy99bEFopSql1WusR9eWT/RSESlJTTUiDXr2M19Cll5qJ4uee872M/fuNIFxxRWAEAUyvRgRBEBqFiIJgKC42+8MWF5vYOzExRhAuvdTEncnI8K2cJ5807qazZ/vXXkEQ/IKIgmCGW665xoz1v/VW1QnaG24w+/X6MuGclmbmImbN8t0vXxCEVoWIgmBCE7/1Fjz0kIll403//mZ7xBdeMD2Auvj3v82WmbVFlRQEodUjotDe+ewzE5fngguMR1BN3HCDidfz+ee1l5OdbTyVLryw7jDYgiC0akQU2jNbtphNxwcPhldeqd3t9NxzzXqC2iacXS4T3TM/32yCIgjCUYuIQnslPd0MFYWHV+5qVhs2m9kh67PPjJupN06nmY945RUjCIMH+9duQRD8iohCe6S01GzscvCg8TSqvnlKTVxzjVktPG9e5Wfl5WYvgJdfNrH1//53/9ksCEKLIKLQ3tDabKH4zTfw6qsmTr4vdO9uomP+979mMtnhMJvEvPUW/OMf8MADEldfENoAIgptjexs03o//XTjVbR1qxECD088Aa+9Zlr2Mxu48+kNN0BmJrz5pplQfvddsy5B5hEEoc0gYS7aEuvWGS+i/fvNto/b3ZHKExNNcLuePeFvfzMV+ttvN7xlr7VZw7Bjh5lcnjvXxDcSBKHVI2Eu2hNam60dx4wxE79ff232MNizx8wBDB9uROCee2DEiLo9jepCKbj1ViMI8+aJIAhCG0R6Ckc7RUVmWOe11+C008wYf02bijscsHataenHxDT+flobz6Xq0U8FQWjVBLynoJTqoZRaqZTaqpTaopQ6Yu8+pdQEpVSuUmqDO93nL3uczhKKin7F5XL46xYtT0oKnHgivP463HcfLFlSsyCAcSs98cSmCQKY3oIIgiC0Wfy5n0I58Bet9XqlVCSwTin1hdZ6a7V8X2utz/KjHQBkZHzAtm2XMHLkVsLD28jmK1deCXv3GjE4o/r214IgCA3HnzuvHdRar3e/zge2AQGKpQwhIcYXv7Q0NVAmNC+ezennzBFBEASh2WiRiWalVBIwFPihhtMnKqV+UUp9ppQa4C8b2pwozJ9vhoSuuCLQlgiC0Ibw+3acSqkI4H3gVq11XrXT64GeWusCpdQU4COgTw1lXAtcC5CYmNgoO0JCEgBFSUkbEIXiYjOxfP75Mr4vCEKz4teeglLKhhGEt7TWH1Q/r7XO01oXuF8vAWxKqSNmSrXW87XWI7TWI+Lj4xtli2Xrrxz3XChlebsadX2jcLnMBvbPPAMbNtQfetpX3n3XLFK77rrmKU8QBMGN33oKSikF/BfYprX+Vy15ugCHtdZaKTUKI1KZfjFo7166vVtEyimbYIhf7nAky5dX3VsgMtJ4AI0ZY/ZBHju2ceXOmwfHHWf2ORAEQWhG/NlTGAP8EZjo5XI6RSl1vVLqeneeC4DNSqlfgLnARdpfCydOOQVXiIWwVS3YU3j+eeMi+ttvZv3ArFlw6JCJEzRunFlE1lA2bYI1a0wvQWINCYLQzLSrxWuFE3pj2bkb+95ylMXPc+z79pmwEnfcAY89VvVcTg5MmWLWGezYYXoQvnLjjfDSSyaURVxc89osCEKbJeCL11ojZZOHEbpfU76tBVZEv/hiZUTS6sTEmK0rDx82cw6+UlgIb7wBM2aIIAiC4BfalSg4TzsFANcnC/17I4fDiMLpp0Pv3jXnOeEEE3r6//7PxCjyhQULIC9PJpgFQfAb7UoUgo8fQWFPUJ+v8O+NFi0yG9j86U9153vkETMvMGeOb+XOmwcDBpiJakEQBD/QrkQhJKQHmaPBtmYTFBT470bPP292M5s6te58iYlmzmHBAvjuu7rzrl8PP/0kE8yCIPiVdiUKwcFdyB5tRTmcxl3UH/z2G6xYYSpvq7X+/LNnQ9eucNttZl1DbcybB6Gh8Mc/Np+tgiAI1WhXoqCUhZIR3XGGB5kgcv7ghRcgKAiuusq3/BERZjvLH34wPYaayMszLq0XXdT0KKeCIAh10K5EASA4PJG80TFGFJrbHbe42Ox7fP750KWL79dddhkMHWrmFoqLKz8vKTHzE3/4g/E8kglmQRD8TLsThZCQHmSdgPHz37ixeQt/5x0TfqK+CebqWCzGRTU11bioLlpkhok6dYJzzjHzDXPmwKhRzWuvIAhCNfweEK+1ERLSg7ThORwDprcwpBljXjz/vNnZbPz4hl87fjycdx489JB5Hxtr9lKeMQMmTjQRUQVBEPxMuxMFu70HpR3KcQ0dhGXx4qqxiZrC+vXw44/w1FON9w56+mk49liYNEmEQBCEgOCTKLi30nwFyAdewuyNMEdrvcyPtvmFkBATetsxeSQhT74KWVnQoYPvBbz+OnzxBURHQ1RU5XHxYggLg0svbbxxCQnw+OONv14QBKGJ+NpTuFJr/ZRS6nQgFhPo7g3gKBQFs9lO0YRjCXncBUuXwsUX+3ZxeblxHS0vNx5GublVw2Fff714BwmCcFTj60SzZzxkCvCG1nqL12dHFXa7EYWCfiEmgmlDXFPXrDE9i//+FzIzTTiLwkI4cAB+/dUMHQmCIBzF+NpTWKeUWgb0Au5WSkUCday0ar0EBXXAYgmltHy/2dv4889Na9+XhWYffwzBwSamEZi5g7Awk7p29a/hgiAILYCvPYWrgDnASK11EWADjsrNgZVShIT0MHs1T5kCGRkmfER9aG1EYeLEhoW6FgRBOIrwVRROBH7VWucopWYBfwNy/WeWf6kQhdNPN2sEfBlC2rYNfv/drBsQBEFoo/gqCs8DRUqpIcBfgN+B1+u6QCnVQym1Uim1VSm1xe3BVD2PUkrNVUrtVEptVEoNa/A3aAR2ew9KSvYar6MTTzSeQ/Xx8cfmePbZ/jVOEAQhgPgqCuXubTLPAZ7RWj8L1DeGUg78RWvdHxgN/Fkp1b9anjOBPu50LUZ8/E5ISCJlZQdxuRymkl+/HrZurfuijz+GESOM26ggCEIbxVdRyFdK3Y1xRV2slLJg5hVqRWt9UGu93v06H9gGVK9RzwFe14bvgRillN9nbI1bqqas7IAJXBcaaja7qY1Dh0zAOhk6EgShjeOrKMwESjHrFQ4B3YEnfL2JUioJs+Dth2qnEoBUr/f7OFI4mh2PW2pJSapxS73iCrPN5YEDNV/wySfmOG2av00TBEEIKD6JglsI3gKilVJnASVa6zrnFDwopSKA94FbtdZ5jTFSKXWtUmqtUmptenp6Y4qogmcBW2mpW49uv924pc6dW/MFH38MSUkwaFCT7y0IgtCa8UkUlFIXAj8CM4ALgR+UUhf4cJ0NIwhvaa0/qCHLfqCH1/vu7s+qoLWer7UeobUeER8f74vJdXKEKBxzDEyfbvZCyM+vmrmw0GzIc845suOZIAhtHl+Hj+7BrFG4TGt9KTAKuLeuC5RSCvgvsE1r/a9asi0CLnV7IY0GcrXWB320qdEEBUVitUZXigLAnXeasBUvvlg187JlUFoqQ0eCILQLfBUFi9Y6zet9pg/XjsFMTE9USm1wpylKqeuVUte78ywBUoCdwIvADQ2wvUlUuKV6GDnShK/+979N+AoPixaZeEZjx7aUaYIgCAHD1zAXnyullgJvu9/PxFTotaK1/oZ64iO53Vz/7KMNzUpISGLVngKY3sJZZ5ltMf/4RzPP8OmnMHWqhLEWBKFd4OtE853AfGCwO83XWs/2p2H+pmJVszdnngkDBsATT5iwFmvWmDAYMnQkCEI7wedNdrTW72MmjdsEdnsPHI4MnM5irNZQ86HFAnfcYVxUly0zE8w2mwmcJwiC0A6os6eglMpXSuXVkPKVUo1yL20tVHog7at64g9/gG7dTG/BEwAvKioAFgqCILQ8dYqC1jpSax1VQ4rUWh/VNeURbqkegoPhlltgxQrYsUOGjgRBaFf46n3U5vCIQhUPJA/XXVcZHltEQRCEdoTPcwptjZCQ7kANPQUw+y4/+CBs2ADdu7ewZYIgCIGj3YqC1WrHZutUsyiA2YtZEAShndFuh4+gFrdUQRCEdky7FgWzqllEQRAEwUO7FgXpKQiCIFSl3YuC05lHeflRu920IAhCs9LuRQGQISRBEAQ37VoUPDuwyRCSIAiCoV2LQkhIIiCiIAiC4KFdi0JwcFfAIqIgCILgpl2LgsUSREhIN5lTEARBcOM3UVBKvayUSlNKba7l/ASlVK7Xrmz3+cuWuhC3VEEQhEr82VN4FahvI4KvtdbJ7vSQH22pFSMKNQTFEwRBaIf4TRS01quBLH+V31yEhJhVzWZnUEEQhPZNoOcUTlRK/aKU+kwpNaC2TEqpa5VSa5VSa9PT05vVgPDwAWhdSmFhjaNcgiAI7YpAisJ6oKfWegjwNPBRbRm11vO11iO01iPi4+Ob1YjY2FMByM5e3qzlCoIgHI0ETBS01nla6wL36yWATSnVsaXtsNt7EBp6PNnZX7T0rQVBEFodARMFpVQXpZRyvx7ltiUzELbExp5KTs5XuFxlgbi9IAhCq8GfLqlvA98Bxyul9imlrlJKXa+Uut6d5QJgs1LqF2AucJEO0Gxvhw6TcbmKyMv7LhC3FwRBaDX4bec1rfXF9Zx/BnjGX/dvCDExEwAr2dnLiYkZH2hzBEEQAkagvY9aBUFB0URFjSIrS+YVBEFo34gouImNPZX8/J9wOHICbYogCELAEFFwExs7GXCRk7My0KYIgiAEDBEFN1FRJ2CxhMt6BUEQ2jUiCm4slmBiYsbLegVBENo1IgpexMZOprh4ByUlewJtiiAIQkAQUfCiMuTFigBbIgiCEBhEFLwIDx9AcHAXGUISBKHdIqLghVKK2NhTyc5egdauQJsjCILQ4ogoVCM2djIORzoFBRsDbYogCEKLI6JQjdjYSYCE0hYEoX0iolCNkJAEwsL6y7yCIAjtEhGFGoiNPZXc3K9xOksCbYogCEKLIqJQA7Gxk3G5isnLWxNoUwRBEFoUEYUaiIkZj1JBMq8gCEK7w5+b7LyslEpTSm2u5bxSSs1VSu1USm1USg3zly0NJSgokqio0WRlLQu0KYIgCC2KP3sKrwJn1HH+TKCPO10LPO9HWxpMhw5TKChYR0nJvkCbIgiC0GL4TRS01quBrDqynAO8rg3fAzFKqa7+sqehxMefD0BGxocBtkQQBKHl8Nt2nD6QAKR6vd/n/uxgYMypSljY8YSFDSAj4wO6d78p0OYIQrPgdFYml6vy6HIv4FeqarJYwGo1KSjIvPegNZSXg8NRmZQy+Wy2yqNSJq/DAaWllamszHzmdJpyPMnpNPfx3Nv7qHVl8thQG1pX/Y6e7w2V5Xu+o6dsz3Xe9/Dk934mShnby8qqpvLyqjYoVfXZV3/mWld9vp7X1fN7XvfvD8nJDf+7N4RAioLPKKWuxQwxkZiY2GL3jY8/nz17/kFZWTrBwfEtdt+jFc8/t8NR9QimcggOrjxarSZ/YWFlKigwx+JiKCmpPHqS9z+p9z+b55/f++j5ZwsKqvxn81Rqnn9I71RSAjk5R6by8soyvJOnQvRUYuXlphxPRRgcXJmCgmquQEpLzXf0/r7FxSZv9UrCajXf2XM/TyovN+e872ezmVReXrUSLi2trBSbgnel5Wv+5rivALNnt21R2A/08Hrf3f3ZEWit5wPzAUaMGFFH26B5iY+fzp49D5OR8THdul3dUrf1iaIi2LPHVFzerTLPa4cDcnMhL69qysmB7OzK5Kn8rFYICQG7vTIFBx/ZEvRUaEVFRyaHw3f7Pa3H1oTFAtHREBNjUnQ0hIZWVv6lpZWvLZaqYmOzmdfl5aZiz82tfFYOR2WFHRJS+ToiwpTvnex2U5Z3pe95rXVVofC8drmOFByHw5wPCTkyecTRuxXuEVnvVrJ3S7u6PXDk785mM597/148vx/P78s7eQTMW2w9303rI1vWTueRPRmo2kCoTvWehkfQPN/N04jwLttTpid5Pwvv19WF2NMA8H6WHrx7BN72QM3P17uX5t1b69Ch6b/z+gikKCwCblRKLQBOAHK11q1i6MhDePhg7PbeZGR80CKi4HTCwYNHVtrZ2ZCWBrt3w65dJh0+3Pj7REZCbKxJMTGQlGR+tJ4WeWamOZaWmh+5d+vTZoOoKOjaFcLCTAoPN0dPhVZ9+MAzdOARFc/RZjPXVk9hYZUVpN1uXoeEVP4TVf9n8x4G8Bw9z9N7aMLz3vOP5j2EYLebStrit1k2QTg68JsoKKXeBiYAHZVS+4D7ARuA1voFYAkwBdgJFAFX+MuWxqKUIj7+fPbtewqHIwebLaZZyj10CNatgx07YOdO+P13k3bvrr21bbVCYiL06gVnnWUq8l69IC7uyNa8p2UaFWVau1FRlSk62lTWgiAINeG36kFrfXE95zXwZ3/dv7no2HE6qalPkpW1mM6dL2nw9SUlsH49/PADfP+9Oe7x2tgtOhqOOcaME06fbir7uLjKVnxMjHkdHV3Z9RUEQfAX0mash6ioUQQHdyM9/X2fRKG01FT8K1fCl18aISgrM+cSE2H0aLj5Zhg1Cvr2NQJQ15ioIAhCSyKiUA9KWYiPP5+DB/+L01mI1Rp+RJ7SUpg/HxYtgm+/NRONSsGwYUYAxoyBE04w4/CCIAitGREFH+jY8Xz273+GrKylFYvawExyvv8+3HWXmfwdOBCuuQYmToRx48ywjyAIwtGEiIIPREePxWbrSHr6+xWisHYt3H47fP21EYNly2Dy5AAbKgiC0ETEAc8HLJYg4uLOITPzU/btK+Wyy2DkSPj1V5g3D37+WQRBEIS2gfQUfCQ+fjpfffUz559vFoHNng1//atx8xQEQWgriCj4yLp1k7jttpOJjc1nw4YQ+vULtEWCIAjNjwwf+cA778BZZwXTo0c2zz03geOPL6//IkEQhKMQEYV6ePppuPhis75g8eJfiI7eQm7u14E2SxAEwS+IKNSC1nDPPWadwbRpsHQp9Oo1EYsllPT09wJtniAIgl8QUagBreHPf4Z//tOsO3jvPROUzWoNJz5+OocOvUJp6YFAmykIgtDsiChUQ2u47TZ4/nm4807jcuodQC4p6UG0LmfXrvsCZ6QgCIKfEFHwQmu4+2546im45RZ47LEj4xKFhvYmIeFGDh16hYKCzYExVBAEwU+IKHjx0ENGCK6/Hv7979oD1fXseQ9WayQpKXNa1kBBEAQ/I6Lg5rHH4IEH4PLL4dln645carPF0bPnPWRlLSY7e2VLmSgILYrWmiJHEdpPW+T9nvU7v2b8SkFZgV/Kb25Ky0s5kH+AjYc38u3ebzmYf9Bvz8YbrTUl5SVkF2e3yLOSxWuY4aI5c+Cii+Cll3zbfSsh4Sb273+G33+/k+HDf0Qp0dfWhNYaFeCY5FprNqVt4tu936KUItgaXCUBZBdnk12STVZxFlnFWWSXZBMaFMrpx5zOacecRmxo/VEVy13lZBVnkVGUQUZRBplFmWQVZxFtjyYxOpEeUT3oHNEZiw+/0YyiDFakrOCLlC9Y9vsyUvNSCbIEEWOPIdYeS2xobMUxJiSGaHs0MfYYokPMcVTCKI7pcEy99t775b08+u2jFZ9FhUSREJlAQlQCncM7U+QoIrsku+L5ZBdnU1JeQp+4PgyIH0D/+P4MiB/AgE4DSIhMILc01zy/4uyK51hSXoI9yE5oUCj2IHtFSopJonds7zp/H6m5qSzcspBFvy0iNTeVjKIM8svyj8gXFxrHoM6DGBg/kEGdB9GnQx+UUpS7ynG6nOaonViVlfjweDqFdyI+LJ7w4Mpoy4cLDrPx8EZ+OfwLGw9vZOPhjaQVplFcXkxJeQkl5SUVee8++W7+Oemf9f4dm4Lyp9Ippc4AngKswEta60ernb8ceILKvZmf0Vq/VFeZI0aM0GvXrm02GxcuhJkz4bzzzCI1zz6zvnDo0Jts3/5H+vV7i86d/9BkW9IK01h7YC3RIdF0DOtIfHg8MfaYKv/MntZbbmkuuSW5BFuD6RnTkyCL7/pe7iqntLy04gdX6iwlPiyeyJDIWvP/fPBnVu9Zzeq9q9mStoWQoBDCbeGEB4dXHKNDoukS0YUuEV3oGtGVrpFd6RLRhYjgiIp7eafesb3pFtmtTlvLnGW8u+VdFmxZQJ8OfZjSZwrjeo6rqFS9SclO4d0t77Jw60I2p21mbOJYzjz2TM7scyb9OvZrsEhkFGWQW5JLqC2U0KBQQm2hhFhD6iynpLyElbtW8slvn/Dpb5+Smpfq070sylJR2WYWZZJdko1FWTipx0mceeyZTOkzhc7hndmavpVtGdvYmr614nVaYVq95dssNhKiEuge1Z3okGgigiMIt4UTERxBRHAEZc4yVu5eyfqD69FoYuwxTOw1keFdh1NQVlBZOXtV1LklueSU5OBwOarc57bRt3Hv+HuJCI44wo79efu5+P2L+Xrv11w99GrGJ41nf95+9ue7U95+DhceJiI4oqoI2WOxWW38mvkrW9K2kJKdgqbxdVdCZALjk8YzLnEc45PGc3zc8RwqOMS7W9/lnS3vsCZ1DQDJXZIZED+AjmEdzf9kWDwdwzoSZgtjZ9ZONqVtYlPaJjanbW5QKz7MFkan8E4UOYqq/P0SIhMY3HkwCZEJFb87e5CdUJs5jkoYxUk9TmrUd1ZKrdNaj6g3n79EQSllBX4DJgP7gJ+Ai7XWW73yXA6M0Frf6Gu5zSkKxcVw3HHQuTOsWWO2sPSmvtam1i7WrRuBw5HFCSf8isUS0mAbUrJT+HDbh3z060d8u/fbI37oVmUlLiyOMFsYeaV55JXmUe6quqI6yBJEr5heHBd3HH069KFPXB+sysq+vH1V/tkO5B8gvyz/iOs9xNhjSIxOpGd0TxKjE4mxx7D2wFq+Tf224gffp0MfhnUdRrmrnEJHIYVlhRXH7JJs0gvTff5nVSjG9RzHRQMvYnq/6cSHx1ecSytMY97aeTy/9nkOFhykR1QP0grTKHWWEhEcwam9T2Vqn6kM7zqcL1K+4N2t77L2gPldnJBwAiO6jeCrPV+xOc04A/SM7smZx57JhKQJ9IrtRWJ0Ip3DO1f5+2YVZ/HV7q9YuXslX+76ki3pW2q0OdQWWiGE3pWrUoo1qWsochQRZgvjtGNO46w+ZzGp9yTsQXbKnGWUlpdS5iyjzFmGRhNrj6VDaAciQyIrxN/pcvLD/h/4bMdnLNm5hPUH1x9hR3RINP3j+9OvYz8SoxPpGNaRuLC4isor1h5Ldkk2qbmppOalkpqbyt68vezP209+WT4FZQUUlhVSUFZAQVkBSilGdx/Nab1PY/IxkxnRbYRPDQ3P0EZOSQ4ZRRn86/t/8eqGV0mITOBfp/+LGf1nVDzjZb8vY9YHsyh0FDLvrHnMGjzLp99JTRQ7itmesZ0t6Vs4VHCIGHsMHUI7VDzP2NBYQqwhlDpLqzREih3FbE3fyld7vuKrPV9xqOAQYFr8WcVZaDSDOg1i5oCZXDjgQvrE9fHJHq01e3L3sCt7F0opgixBWJUVq8VKkCUIh9NBelE66YXppBWmkV5kjjaLjcGdB1ekuLC4Rj+T+mgNonAi8IDW+nT3+7sBtNaPeOW5nACKwuOPm8B2K1fChAnmM5d28eG2D3nkm0f4NfNXhncdzshuIxmZMJJRCaPoGd2zSkWSnrmMNetPp2PC3XTvdl1Fy9keZK/I59IuMosyOVhwkAP5BziYf5CdWTv55LdP2JS2CYAhnYdwbt9zmdhrIsWOYjKKMkgvSjfHwnQKHYVEhUQRHRJNtD26orteUl7CjqwdJmWaY5GjCDCtzy4RXSq65QmRCcTYY6p0pe1BdoIsQaQVprE3d2+VlF2SzcBOAytaU2MTx9I1su6dgspd5aQXpnOw4CCHCg5xMP8gRY6iipaOJwVbg/l+3/e8vflttmdsx6qsnNr7VM7rex7f7/+e/236H2XOMs449gxuOeEWTjvmNIodxXy560uW7FjC4h2Lq7TCRyWM4sL+F3JB/wvoGdOz4vPU3FQ+2/kZn+38jOUpy6u05kKsISRGJ5IYnUhWcRYbDm1AowkNCmVsz7GcknQK3SK7Uewopri8uMqx0GHE0FOpFpYVUlJewujuoznruLOYkDQBe5C9Sb9PD4cKDvH5zs8pKCugX8d+9I/vT5eILs02PKa1xqVdWC3Ns9/rmtQ1/HnJn9lwaAMTe03kqTOeYuGWhfx99d/pH9+fd2e8S7/4wAcP01qzM2snX+35ijWpa0iMTmTmgJmtwjZ/0BpE4QLgDK311e73fwRO8BYAtyg8AqRjehW3aa2P6G8rpa4FrgVITEwcvsd7k+NGkplp9kY++WT49FNTmb296W0e+eYRtmVso0+HPkzsNZGfD/3MhkMbKHOaPTU9XUjP8E2ho7DG8i3KQpgtjNCg0CO62J7zJyeezLnHn8s5fc+hd2zvJn8nMD/0gwVmAqxzROcGDStVx+F0YLM2YDytEXjG3RdsXsCCzQvYlbOLcFs4lw25jJtOuIm+HfvWet3W9K2sO7iOsYlj6RXbq957lTnL2J6xnT05e9ibu5c9uZXH0KBQJiRNYGKviYxKGFXj8JTgO06Xk3nr5nHPl/eQU5IDwBXJV/DMlGcIs4UF2Lr2ydEiCnFAgda6VCl1HTBTaz2xrnKbq6dw++1mgnntz2X8WPYKj337GLtydjGo0yD+OvavzOg/o6LlVOYsY9PhTfy4/0d+OvATeaV5FS31aHs0dlVI5oHHCQ9PJjp+FsXljophlSJHETH2GLpFdqNbZPA4izYAAA84SURBVDe6RnQ1x8iuzdaSbCtordmSvoXuUd2JsccE2hyhGUgvTOeRbx5hWNdhTRouEppOaxCFeoePquW3Alla6+i6ym0OUdi1C/r2hYtnlZEx6XwW71jMqIRR3DP2Hs467iyfvDSqs2/fXHbuvJWIiKEMHPgxdnv3JtkoCILQnPgqCv70o/wJ6KOU6qWUCgYuAhZ5Z1BKeQ9QTwO2+dGeCv72N7AEOckc/0cW71jMs1Oe5furvmfa8dMaJQgA3bvfzKBBn1BcvIP160eSl/dDM1stCILgf/wmClrrcuBGYCmmsl+otd6ilHpIKTXNne1mpdQWpdQvwM3A5f6yx8P69fC/t10c95dr+HTXQp6c/CQ3jLyhWSbt4uKmMmzYd1gsYfz883gOH36rGSwWBEFoOfy6TsEfNGX4SGs4dbJmTeStlCTP5f7x9/PAhAea10CgrCyDLVsuIDf3KxIT76ZXr7/L4jZBEAJKaxg+anUsWwZfuu6jJHkut42+jfvH3++X+wQHd2TIkGV07Xote/c+wi+/nEpJyV6/3EsQBKE5aTei4HTC5S8+DuP/zpVDrub/Tvs/v4ZBsFiCOe64Fzj++P+Sn/8TP/00iEOHXm+RWCmCIAiNpd2Iwk3z3+bQoNmcFHUx86e90CJxcZRSdO16JSNG/EJExGC2b7+MLVsuoKws3e/3FgRBaAztRhTuOv90TrXfzZc3vtZsKzd9JTS0N8nJq+jd+3EyMz/lp58GkZHxSYvaIAiC4AvtaqK5NVBQsJFt2/5IYeFGIiNH0LXr1XTqdBFBQXUuzxAEQWgSMtHcSomIGMzw4T9y7LFzcblK+e2361mzpivbtl1OTs7XMucgCEJAkZ5CANFak5+/loMHXyIt7W2cznzs9l5ERAwjPLwfYWH9CAvrS1jY8Vit4fUXKAiCUAu+9hRkk50AopQiKmokUVEjOfbYf5GW9i4ZGR9RWLiRjIwPAVdF3rCwfnTufCldulxGSEjdkUoFQRAai/QUWikuVynFxTspLNxGUdE2srOXk5u7GrDSsePZdO16DR06nI4JGSUIglA30lM4yrFYQggPH0B4+AAAkpLupajoNw4efIlDh14lI+MjQkK6Exs7uUZhsFqjCQ3thd3uSUlYraEt/TUEQTjKkJ7CUYjLVUZm5iccPPgSBQUba8ihcTiy0Lq0yqfBwV2IiBhKdPQ4YmLGERk5AovlyH0DtNY4HGmUlu7Hao3AZosjKChGeiWCcBQjPYU2jMUSTHz8dOLjp9eaR2sXZWWHKSnZRUnJLoqLd1FSkkJe3g9kZd3tLieUqKjRREWdhMtVSHFxCiUlKRQXp+ByFR1RZlBQDEFBHbDZ4gkL60No6HGEhR1HaOjxhIX18Wky3OkspqRkDyUlu7FaI4iIGCTuuILQipCeQjukrCyd3NxvyM1dTU7OagoKNmCxhBIa2hu7vbf72IuQkO64XEU4HJk4HFmUl2fhcGRRVnaI4uIdlJZWjedks8UTFBSD1RpFUFA0QUFRWK1RaF1GScluiot34XAcPsIeuz2J8PDBREQMJjx8MHZ7IkFBcdhsHQkKim6R1eeC0NYJ+CY7/kJEoflxuUpRKrjBla/TWURx8U6Kin6juPg3Skr24HTmUV6eS3l5Hk6nOSoVhN2eVDG3YVJPnM58Cgp+obBwIwUFGykq+hVwVrmHUkHu3kkcFosdpawoFVSRwOq22zsBuHA6C92poCKZITUFWNzXWQBFcHBnd6/JpIiIITUOrTXsuZZRUrKXsrJD2Gyx2Gzx2GxxMgwnBAQZPhJ8xmIJadR1VmsYERGmhd9Y4uKmVLx2OksoKtpGWdlBHI4Md8qsOGpdhtbl7uR0H8sADWivhX8aUO75kHis1gis1nCs1gj3d/XkdblfOykp2UtOzirS0v4HgFIhREYOJzi4Ey5XSUVyOovRuhSLxY7VGu3uEcUQFBSN1RpOaenBiiG70tL9eLsVGxQ2W0dstniCg7t4iWSvCseAoKBYnM58t7DmVRy1dnl9l/CK1y5XGeXluW4RznW/zsNqjSYkJIGQkG4EB3cLiKOB1hqnsxCLJQSLxb/7fQvNg19FQSl1BvAUYAVe0lo/Wu18CPA6MBzIxOzRvNufNgmtF6vVTmTkUGBowGwoKdlHXt73Fam4OAWLxe4WgShstk5YLCG4XMWUl+dSUrLbXRHn4HQWEBLSFbs9iZiYCRWeXyEhXSkvz6WsLA2HIx2HI42ysjTKyg6QlbWEsrJDLfLdgoJi3eIQhlLBWCzB7mMISllxucqqCKDLVYLWpbhcDrR2uEXYvAaFxRLmFqcw9+swtHZ6CZPpKRphVNhscQQHd6lINltnt6BGYbVGVhyt1gg8Yq21E3BVNAJcrlJcrlK3XaVuG8srepCm5xjkfm+r8h09r7V24nQW4HJ59yQLUcpS8bf2JFNF6WrfvxytXdhscdhsnQgO7oTN1qnGoU6tXbhcZYDLXV7dQSS01u6GjkIpW0CGTv0mCu49l58FJgP7gJ+UUou01lu9sl0FZGutj1VKXQQ8Bsz0l02CUB92e3fs9gvo1OmCFrtn5eT7rgqR8czHBAVFuo9RgKoYEjMVmqcys7l7LNHuOZ1ogoIiKS/PobR0P6WlB/j/9u42Rq6qjuP497e77bJuDZVaibSVFiGpNcESSUXBpNZoqhLLC/AJCPENvsAEokbB+NiEt6KJJkKEWLUqCFQbQ6K1NNW+EFigyqOxEoxt6G4NtPRh2+ne+fvinLk73Ra6lp2HvfP7JJO59+zt9Px379z/PefMnFOr7eHYsT3Uanup18ep12tE1CiKw2ULTBosL4YDA2fn7cF8cRrIF9m0HRHU6+O5LkcoiiPU64dzV+Gysj6NuhfFEY4fH6VW20uttpcDB3ZQq41Sr4+37ffcatJcBgbml8kr4hhpAcoTj0m/16EySUwm4XHq9aNNR/fR1zdEf/9QPn6I8877AkuWfKmlcbSypbAK2BURLwBI+jWwDmhOCuuA7+Tt+4EfSlLMtoEOszegv3+I4eHlDA8vn9HXHRxcVH7PpVvV67XcVXaQoniVojhIURwi3Sn3k+78+5H6cmIazHf9jcdZOUkVTY8JoMgtnFq+QNfKVoY0cEL3W+pWHAbipJZSvX606f+eTI4AExMvU6uN5hbgGLXaKBMT+6e0TgbzeN3kxb8oxsskkFoQQ02tk6GyizMdN/koinHmzj235X+TViaFRcB/mvZ3A+97rWMiYkLSAWAB8N8W1svMukRf31z6+hYwZ86CTlcF4P8cd1nSsnp00qyYJVXSjZJGJI3s2+cFaszMWqWVSWEPJ6bSxbnslMcojRKdTRpwPkFE3BURl0bEpQsXLmxRdc3MrJVJ4THgIknLJM0FPgNsnnLMZuCGvH018LDHE8zMOqdlYwp5jOCLwB9IH0m9JyKekbQeGImIzcDdwM8l7QJeJiUOMzPrkJZ+TyEiHgIemlL2rabto8A1rayDmZlN36wYaDYzs/ZwUjAzs5KTgpmZlWbdLKmS9gH/PsN//lZ654txvRJrr8QJjrWK2hnn+RFx2s/0z7qk8EZIGpnO1LFV0Cux9kqc4FirqBvjdPeRmZmVnBTMzKzUa0nhrk5XoI16JdZeiRMcaxV1XZw9NaZgZmavr9daCmZm9jp6JilIWivpH5J2Sbq10/WZSZLukTQm6emmsnMkbZH0z/z8lk7WcSZIWiJpm6RnJT0j6eZcXqlYJZ0l6VFJf8txfjeXL5P0SD6H780TTVaCpH5JT0r6fd6vZKySXpT0lKSdkkZyWVedvz2RFJqWBv0YsAL4rKQVna3VjPopsHZK2a3A1oi4CNia92e7CeDLEbECuAy4Kf8dqxbrMWBNRLwHWAmslXQZabnaOyLiQuAV0nK2VXEz8FzTfpVj/VBErGz6KGpXnb89kRRoWho00qrYjaVBKyEi/kyaZbbZOmBD3t4AXNXWSrVARLwUEU/k7YOki8giKhZrJIfy7pz8CGANadlaqECcDZIWA58AfpL3RUVjfQ1ddf72SlI41dKgizpUl3Y5NyJeytt7gdYv7tpGkpYClwCPUMFYc3fKTmAM2AL8C9gfkyvBV+kc/j7wVaCe9xdQ3VgD+KOkxyXdmMu66vxt6dTZ1h0iIiRV5mNmkuYBDwC3RMSr6cYyqUqsEVEAKyXNBzYByztcpZaQdCUwFhGPS1rd6fq0wRURsUfS24Atkp5v/mE3nL+90lKYztKgVTMq6e0A+Xmsw/WZEZLmkBLCxoh4MBdXMlaAiNgPbAPeD8zPy9ZCdc7hy4FPSnqR1K27BvgB1YyViNiTn8dIyX4VXXb+9kpSmM7SoFXTvNTpDcDvOliXGZH7mu8GnouI7zX9qFKxSlqYWwhIGgI+Qho/2UZathYqECdARNwWEYsjYinpfflwRFxLBWOVNCzpzY1t4KPA03TZ+dszX16T9HFS32VjadDbO1ylGSPpV8Bq0oyLo8C3gd8C9wHvIM0q+6mImDoYPatIugL4C/AUk/3PXyeNK1QmVkkXkwYc+0k3bvdFxHpJF5Dups8BngSui4hjnavpzMrdR1+JiCurGGuOaVPeHQB+GRG3S1pAF52/PZMUzMzs9Hql+8jMzKbBScHMzEpOCmZmVnJSMDOzkpOCmZmVnBTM2kjS6sZMoGbdyEnBzMxKTgpmpyDpurymwU5Jd+YJ6g5JuiOvcbBV0sJ87EpJf5X0d0mbGvPhS7pQ0p/yughPSHpnfvl5ku6X9LykjWqevMmsw5wUzKaQ9C7g08DlEbESKIBrgWFgJCLeDWwnfXMc4GfA1yLiYtK3rRvlG4Ef5XURPgA0ZsK8BLiFtLbHBaT5f8y6gmdJNTvZh4H3Ao/lm/gh0iRldeDefMwvgAclnQ3Mj4jtuXwD8Js8x82iiNgEEBFHAfLrPRoRu/P+TmApsKP1YZmdnpOC2ckEbIiI204olL455bgznSOmeQ6fAr8PrYu4+8jsZFuBq/Oc9401dM8nvV8aM3d+DtgREQeAVyR9MJdfD2zPK8PtlnRVfo1BSW9qaxRmZ8B3KGZTRMSzkr5BWiGrDzgO3AQcBlbln42Rxh0gTXf843zRfwH4fC6/HrhT0vr8Gte0MQyzM+JZUs2mSdKhiJjX6XqYtZK7j8zMrOSWgpmZldxSMDOzkpOCmZmVnBTMzKzkpGBmZiUnBTMzKzkpmJlZ6X+WOBS1WxeQowAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.8901 - acc: 0.4876\n",
      "Loss: 1.8901062693551323 Accuracy: 0.4876428\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2435 - acc: 0.3654\n",
      "Epoch 00001: val_loss improved from inf to 1.93407, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_4_conv_checkpoint/001-1.9341.hdf5\n",
      "36805/36805 [==============================] - 193s 5ms/sample - loss: 2.2434 - acc: 0.3654 - val_loss: 1.9341 - val_acc: 0.3920\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4593 - acc: 0.5630\n",
      "Epoch 00002: val_loss improved from 1.93407 to 1.73732, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_4_conv_checkpoint/002-1.7373.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 1.4593 - acc: 0.5630 - val_loss: 1.7373 - val_acc: 0.4987\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1168 - acc: 0.6603\n",
      "Epoch 00003: val_loss did not improve from 1.73732\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 1.1169 - acc: 0.6602 - val_loss: 1.8191 - val_acc: 0.5246\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8679 - acc: 0.7336\n",
      "Epoch 00004: val_loss improved from 1.73732 to 1.44377, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_4_conv_checkpoint/004-1.4438.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.8680 - acc: 0.7336 - val_loss: 1.4438 - val_acc: 0.5982\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6775 - acc: 0.7887\n",
      "Epoch 00005: val_loss improved from 1.44377 to 1.27273, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_4_conv_checkpoint/005-1.2727.hdf5\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.6775 - acc: 0.7888 - val_loss: 1.2727 - val_acc: 0.6539\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.8344\n",
      "Epoch 00006: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.5282 - acc: 0.8343 - val_loss: 1.6528 - val_acc: 0.5949\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8660\n",
      "Epoch 00007: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.4223 - acc: 0.8659 - val_loss: 1.4604 - val_acc: 0.6441\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.8913\n",
      "Epoch 00008: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3462 - acc: 0.8913 - val_loss: 1.7785 - val_acc: 0.5928\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.9011\n",
      "Epoch 00009: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.3143 - acc: 0.9011 - val_loss: 1.3506 - val_acc: 0.6573\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9248\n",
      "Epoch 00010: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2415 - acc: 0.9248 - val_loss: 1.5576 - val_acc: 0.6508\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9343\n",
      "Epoch 00011: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2136 - acc: 0.9342 - val_loss: 1.7995 - val_acc: 0.6294\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9344\n",
      "Epoch 00012: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.2077 - acc: 0.9344 - val_loss: 1.6684 - val_acc: 0.6406\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9420\n",
      "Epoch 00013: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1856 - acc: 0.9420 - val_loss: 1.9722 - val_acc: 0.6075\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9449\n",
      "Epoch 00014: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1755 - acc: 0.9449 - val_loss: 1.8200 - val_acc: 0.6497\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9529\n",
      "Epoch 00015: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1532 - acc: 0.9528 - val_loss: 1.9222 - val_acc: 0.6403\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9520\n",
      "Epoch 00016: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1587 - acc: 0.9520 - val_loss: 1.7623 - val_acc: 0.6646\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9563\n",
      "Epoch 00017: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1443 - acc: 0.9563 - val_loss: 1.7708 - val_acc: 0.6655\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9549\n",
      "Epoch 00018: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1503 - acc: 0.9549 - val_loss: 1.6532 - val_acc: 0.6846\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9645\n",
      "Epoch 00019: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1197 - acc: 0.9645 - val_loss: 1.7961 - val_acc: 0.6732\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9634\n",
      "Epoch 00020: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1263 - acc: 0.9634 - val_loss: 1.9412 - val_acc: 0.6527\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9657\n",
      "Epoch 00021: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1166 - acc: 0.9657 - val_loss: 1.7905 - val_acc: 0.6760\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9677\n",
      "Epoch 00022: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1100 - acc: 0.9677 - val_loss: 1.7425 - val_acc: 0.6769\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9651\n",
      "Epoch 00023: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1159 - acc: 0.9651 - val_loss: 1.7939 - val_acc: 0.6746\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9682\n",
      "Epoch 00024: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1122 - acc: 0.9682 - val_loss: 1.8348 - val_acc: 0.6713\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9720\n",
      "Epoch 00025: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0982 - acc: 0.9720 - val_loss: 1.8254 - val_acc: 0.6825\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9710\n",
      "Epoch 00026: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1040 - acc: 0.9710 - val_loss: 2.0004 - val_acc: 0.6690\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9703\n",
      "Epoch 00027: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1036 - acc: 0.9703 - val_loss: 1.9090 - val_acc: 0.6753\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9742\n",
      "Epoch 00028: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0910 - acc: 0.9742 - val_loss: 1.8328 - val_acc: 0.6876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9730\n",
      "Epoch 00029: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0949 - acc: 0.9730 - val_loss: 1.8917 - val_acc: 0.6874\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9761\n",
      "Epoch 00030: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0861 - acc: 0.9760 - val_loss: 1.8995 - val_acc: 0.6781\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9703\n",
      "Epoch 00031: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.1061 - acc: 0.9702 - val_loss: 2.1124 - val_acc: 0.6734\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9735\n",
      "Epoch 00032: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0969 - acc: 0.9734 - val_loss: 1.9138 - val_acc: 0.6897\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9752\n",
      "Epoch 00033: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0904 - acc: 0.9752 - val_loss: 1.9539 - val_acc: 0.6867\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9782\n",
      "Epoch 00034: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0794 - acc: 0.9782 - val_loss: 1.8822 - val_acc: 0.6951\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9799\n",
      "Epoch 00035: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0716 - acc: 0.9799 - val_loss: 2.1773 - val_acc: 0.6466\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9800\n",
      "Epoch 00036: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0748 - acc: 0.9800 - val_loss: 2.0504 - val_acc: 0.6713\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9779\n",
      "Epoch 00037: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0792 - acc: 0.9779 - val_loss: 2.0638 - val_acc: 0.6855\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9783\n",
      "Epoch 00038: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0788 - acc: 0.9783 - val_loss: 2.0517 - val_acc: 0.6725\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9772\n",
      "Epoch 00039: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0824 - acc: 0.9772 - val_loss: 2.0939 - val_acc: 0.6723\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9792\n",
      "Epoch 00040: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0762 - acc: 0.9792 - val_loss: 1.9907 - val_acc: 0.6858\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9775\n",
      "Epoch 00041: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0816 - acc: 0.9775 - val_loss: 2.0917 - val_acc: 0.6879\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9817\n",
      "Epoch 00042: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0696 - acc: 0.9817 - val_loss: 1.9886 - val_acc: 0.7009\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9825\n",
      "Epoch 00043: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0675 - acc: 0.9824 - val_loss: 1.9929 - val_acc: 0.6853\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9802\n",
      "Epoch 00044: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0708 - acc: 0.9802 - val_loss: 1.9108 - val_acc: 0.6962\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9818\n",
      "Epoch 00045: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0682 - acc: 0.9818 - val_loss: 1.9474 - val_acc: 0.7058\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9821\n",
      "Epoch 00046: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0671 - acc: 0.9821 - val_loss: 2.1215 - val_acc: 0.6632\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9816\n",
      "Epoch 00047: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0696 - acc: 0.9816 - val_loss: 2.1254 - val_acc: 0.6771\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9821\n",
      "Epoch 00048: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0672 - acc: 0.9821 - val_loss: 1.9987 - val_acc: 0.6935\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9833\n",
      "Epoch 00049: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0628 - acc: 0.9833 - val_loss: 1.9642 - val_acc: 0.6993\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9844\n",
      "Epoch 00050: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0619 - acc: 0.9844 - val_loss: 1.9097 - val_acc: 0.6981\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9855\n",
      "Epoch 00051: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0576 - acc: 0.9855 - val_loss: 2.1091 - val_acc: 0.6881\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9827\n",
      "Epoch 00052: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0659 - acc: 0.9827 - val_loss: 1.9814 - val_acc: 0.7018\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9856\n",
      "Epoch 00053: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0572 - acc: 0.9856 - val_loss: 2.2033 - val_acc: 0.6713\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9828\n",
      "Epoch 00054: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0661 - acc: 0.9828 - val_loss: 2.1320 - val_acc: 0.6958\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9869\n",
      "Epoch 00055: val_loss did not improve from 1.27273\n",
      "36805/36805 [==============================] - 179s 5ms/sample - loss: 0.0503 - acc: 0.9869 - val_loss: 2.1180 - val_acc: 0.6823\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4lFX2x783U9IbCQRICL2GFAglCAgIIk1QEEFF1HXRda0/lRU71rWtiyDqIqiACOuCqFjAAiyutNBrqAmmkN4mmUmmnd8fNzOZJJNkksxkUs7nee7zZua9773nfZPc895zzj1XEBEYhmEYBgA83C0AwzAM03JgpcAwDMNYYaXAMAzDWGGlwDAMw1hhpcAwDMNYYaXAMAzDWGGlwDAMw1hhpcAwDMNYYaXAMAzDWFG6W4CGEhoaSj169HC3GAzDMK2Kw4cP5xJRx/rqtTql0KNHDxw6dMjdYjAMw7QqhBBXHKnH5iOGYRjGCisFhmEYxgorBYZhGMZKq/Mp2MNgMCAtLQ1lZWXuFqXV4uXlhYiICKhUKneLwjCMG2kTSiEtLQ3+/v7o0aMHhBDuFqfVQUTIy8tDWloaevbs6W5xGIZxI23CfFRWVoaQkBBWCI1ECIGQkBCeaTEM0zaUAgBWCE2Enx/DMEAbUgr1YTJpUV6eDrPZ6G5RGIZhWiztRimYzeXQ66+CSO/0tgsLC/HBBx806tpp06ahsLDQ4fpLly7FO++806i+GIZh6qPdKAUhpE+dyPkzhbqUgtFYd38//PADgoKCnC4TwzBMY2iHSsHg9LaXLFmCS5cuIS4uDosXL8bu3bsxduxYzJw5E4MGDQIA3HTTTYiPj0dUVBRWrVplvbZHjx7Izc1FSkoKBg4ciEWLFiEqKgqTJ0+GTqers99jx44hISEBMTExuPnmm1FQUAAAWL58OQYNGoSYmBjMnz8fAPDf//4XcXFxiIuLw5AhQ6DRaJz+HBiGaf20iZBUWy5ceAwlJcfsnCGYTCXw8PCEEOoGtennF4e+fZfVev6NN97AqVOncOyY7Hf37t04cuQITp06ZQ3x/OSTT9ChQwfodDoMHz4cc+bMQUhISDXZL2Djxo34+OOPceutt2LLli1YsGBBrf0uXLgQK1aswLhx4/DCCy/gpZdewrJly/DGG28gOTkZnp6eVtPUO++8g5UrV2L06NEoKSmBl5dXg54BwzBOJjUVWLAAeP11YPRod0tjpd3MFABLdA01S28jRoyoEvO/fPlyxMbGIiEhAampqbhw4UKNa3r27Im4uDgAQHx8PFJSUmptv6ioCIWFhRg3bhwA4K677sKePXsAADExMbjjjjvw+eefQ6mUen/06NF4/PHHsXz5chQWFlq/ZxjGTfz738CePcDMmcD58+6WxkqbGxnqeqMvKTkGpTIYXl7dXS6Hr6+v9efdu3fjl19+wb59++Dj44Px48fbXRPg6elp/VmhUNRrPqqN77//Hnv27MG2bdvw2muv4eTJk1iyZAmmT5+OH374AaNHj8aOHTswYMCARrXPtEL27gUKC4Fp09wtCWNh+3age3dAqwWmTgX27QM6dXK3VO1ppiD9Cq5wNPv7+9dpoy8qKkJwcDB8fHyQlJSE/fv3N7nPwMBABAcH47fffgMArF+/HuPGjYPZbEZqaiomTJiAN998E0VFRSgpKcGlS5cQHR2Np556CsOHD0dSUlKTZWBaEU88AcydC+TkuFsSBgBKSoDffpO/k23bgKtXgRtvBEpL3S0ZKwVnEBISgtGjR2Pw4MFYvHhxjfNTpkyB0WjEwIEDsWTJEiQkJDil37Vr12Lx4sWIiYnBsWPH8MILL8BkMmHBggWIjo7GkCFD8MgjjyAoKAjLli3D4MGDERMTA5VKhalTpzpFBqYVoNcDR4/KN9J333W3NAwA7N4tfy9TpwIjRwIbNwKJicDttwMmk1tFE0TNY2N3FsOGDaPqm+ycPXsWAwcOrPdane4izOZy+PpGuUq8Vo2jz5FpZRw6BAwfDoSFyTfRlBSgWpAD08w89BDw2WdAXh5gMRu//z7w8MPAgw8CK1YATs4yIIQ4TETD6qvHMwWGaescPCiPn3wilcI//+leeRjpT7juukqFAEhF8eSTwMqVwFNPASdOuGXW0OYczXUhhApEBhAR5/ph2g8HD0oH5tSpwC23AMuXSx9DcLC7JXM9e/cCX30lney2paREKkd3mFEvXAAuXQIef7zmuTfflP6Ft9+Wxd8fGDECGDVKloQEoEMHl4rX7mYKAEDkXpsdw9iloAB44AEgP9+57R44IAcWIYDnngM0GuC995zbR0uECLj3XqkEt28HkpKAsjKgc2f5DBYvBsxm5/aZmyvDTOti+3Z5nDKl5jkPD2D9euDiRXm880759/D3vwPTpwOvvOJcee3QTpUCm5CYFsi//w189JEcDJxFUZEcDEeOlJ9jYoCbbwaWLZPnnEFL9UseOSLvfeVKIC0NOHUK+N//gO++k2/hp08DW7c6t89nnwXGj5f91sb27UDfvkCvXvbPCwH07i0Xtq1cKe+jsBDYtQtYtMi58tqBlQLDtBS+/14ev/7aeW1agjJGjKj87vnnpUJYsaJpbZtMMpopNNSxwZVIzoTWrGlav47y+eeAWi1NZtWZNw/o1w94+WXnzRZMJvm7I5KrlO1RViYHd3uzhLrw85PKpiJtjithpcAwLQGdDvj1V8DbW5ofcnOd067FyTzMJuhkyBAZE//uu9KM0hguXpSD1BNPAOXlMmKmuLjua9aulTOhBx+UNnVXYjTKMM8ZM+z7ThQKaUo7cQL49lvn9HngAJCdDQwYAGzYIJ9RdX77Tf6uG6oUmpF2qhScnxSvofj5+TXoe6aNs2uXHCyef16+uW7b5px2Dx6UporqzskXXpA+jJUrq35vNkvTx88/A+npNU1DZrOcYcTEACdPyoF+504gMxNYurR2OfLyZGTN0KHy7f3hh11rdvr1VyArS5pgauO224A+feRswRmybN0KqFRytqBWSz9AdbZvlxFH48c3vT9XQUStqsTHx1N1zpw5U+M7e5jNRiouTqSysgyH6rsSX1/fBn3fHDj6HBkX8Ne/Evn4EGm1RJGRRDNnNr1Ns5moSxeiBQvsn582jSgkhGjrVqJnnyW6/nqiwEAiOUTK0rEj0Q03EC1ZQrRhA9GECfL7qVOJ0tIq27rvPiKFgujECft9/elPlefffVe28dVXTb/H2liwgCgoiKisrO56n34qZfn226b1ZzYT9e4tnxUR0cMPEymVRMnJVesNGkQ0eXLT+mokAA6RA2Os2wf5hpamKAUiouLiw6TT/eFwfUd46qmn6P3337d+fvHFF+ntt98mjUZD1113HQ0ZMoQGDx5MX3/9tbVOfUrBbDbTk08+SVFRUTR48GDatGkTERFlZGTQ2LFjKTY2lqKiomjPnj1kNBrprrvustZ99913G3UfrBTchNlM1L17pSJ45BEiLy+ikpKmtZuaKv/Fly+3f37//srBX6EgGjKE6P77iT75hGjnTqIVK4juuYcoLk4OcACRvz/R6tVSZltyc6WCGTOm5rk9e+S1ixfLzwYDUXQ0UbduTb9He2g0UsHed1/9dfV6op49iYYNqyl3Qzh1St7jhx/Kz6mpRGq1fJ4WrlyRdRr5/9lU2q9SePRRonHjai3GMcPIOHZknXVqlEcfrfNhHzlyhK699lrr54EDB9Iff/xBBoOBioqKiIgoJyeHevfuTeaKP7z6lMLmzZtp0qRJZDQaKTMzk7p160YZGRn0zjvv0KuvvkpEREajkYqLi+nQoUM0adIkaxsFBQV1ylsbrBTchGVA+de/5Oddu+TnzZub1u6WLbKd/ftrr7NjB9FvvxGVltbdVlkZ0ZEjRFlZtddZvVr299lnld+Vl8u348jIqgrgt99k3SVLHLuXhvD557LtPXscq2+R+4cfGt/nq6/KNtLTK7/7y1+kYkhNlZ//9S9Zx03/Z44qBZf5FIQQ3YQQu4QQZ4QQp4UQj9qpI4QQy4UQF4UQJ4QQQ10lj02vcHb67CFDhiA7OxsZGRk4fvw4goOD0a1bNxARnnnmGcTExGDSpElIT09HVlaWQ23+73//w2233QaFQoGwsDCMGzcOiYmJGD58OD799FMsXboUJ0+ehL+/P3r16oXLly/j4Ycfxvbt2xEQEODU+2NcjCXqyJLBdMwYmYaiqVFIBw9KG3dsbO11Jk+W/fn41N2Wp6d0UNeVxfOee+QCq8WLpb8CkM7sM2dkCgebzMEYMwa46y7gH/8Azp51/J4c4fPPZfZRR/couPNOWf+llxrvW9i6VYb9du1a+d2SJdIH89Zb8vP27UBkpHREt2Qc0RyNKQC6ABha8bM/gPMABlWrMw3Aj5AjdQKAA/W121TzUWnpOSopcb6mfv755+m9996jp59+mt577z0iIvr000/p1ltvJb1eT0RE3bt3p+QKG2N9M4XHHnuM1qxZY/1+wYIF9M033xARUXp6Oq1atYpiY2Np7dq1RESk0Who8+bNNGvWLLrnnnsadQ88U3ATY8cSxcZW/e7uu6VNvOJvp1GMH080fHjTZGsoR48SeXhIH8nly0Te3kQ332y/blaWvMcJE5pmurElM1P2//TTDbvO8ha/Y0fD+/zjD3ntG2/UPHfvvUSentJ05O/vmEnLRaClmY8AfAPg+mrf/QvAbTafzwHoUlc7TVUKWu0l0mhqcYY1gVOnTtGoUaOob9++lJEhHdnLli2jhx56iIiIdu7cSQAcVgpbtmyhyZMnk9FopOzsbIqMjKSrV69SSkoKGY1GIiJasWIFPfroo5STk2M1U508eZJiqw8wDsJKoRrJydIsYDC4ro/8fGnPf+aZqt9/84389/zpp8a1azQS+fkRPfhg02VsKI88QiSE9FH4+VWaT+zxwQfyPr/4wjl9v/eebO/06YZdV14ufRwJCUQ6XcOuXbFC9pmUVPPcxYvy9ztqFLncuV4PLUopAOgB4A8AAdW+/w7AGJvPvwIYZuf6+wAcAnAoMjKyxs02ZDDT6a5QcfFhh+s3hMGDB9P48eOtn3NycighIYEGDx5Md999Nw0YMMBhpVCbo/mzzz6jqKgoiouLozFjxtDly5fp2LFjNGTIEIqNjaXY2Fj6oZG2UVYK1bjvPvkv8vrrrutj40bZx969Vb/XaqWz9K9/bVy7Fj9FxUyyWSksJOrcmRxyqhqN0skbFibfqm++Wc5wYmLkID11qnwWjjJ8uFRGjeGTT6TMkZHSL2EyOXbdddcRDRhQ+/k775TtKpVEFS9v7qDFKAUAfgAOA5ht55xDSsG2NHWmUFaWTsXFiWQ2O/gLb8tUm7KzUrChrEyaNlQqWY4dc00/CxYQhYbKwbE6s2cTde3q+OBki2WAO3u26TI2hp9/JnrgAcdmWYcOydDXrl2JoqKkOW3WLKL58+U9LFrkWJ9JSbL+P/7ReLl37iQaOlS2M3So/FwXeXlyJlCXuSopSc6cxo1rvFxOoEUoBQAqADsAPF7L+WY3H5WXZ1NxcSKZTOUOX9MmSUmpMd1lpWDD1q3y3+Pzz+Vbb3R0/THvDcVolGGcta0jWL+e6o0eqo3775drDhqjUFoSS5bIZ7BuXf11n39e+hNsI4Aag8kkn31kpOx7xgyic+fs1123TtY5cKDuNtesIfrf/5omVxNxVCm4MvpIAFgD4CwR1bbd07cAFlZEISUAKCKiq66SScrFqS4AyNw3Gk2L2P6vRfL55zLSZt48YPVquXq3rhW7jeHgQbnSd/p0++enTweUSvtRSEQyaikjo/a2hw+XWTdbM6+8AowbB/zlLzKhXW0Qyd/ZxIlVI4Aag4eHXAmdlAS88YZMOzJ8OPDLLzXrfv217G9YPXvX/OlPjkdDuRlX/sWMBnAngOuEEMcqyjQhxF+EEH+pqPMDgMsALgL4GMBfXSgPAFYKAACDQW4FCLTcPXtPn3Zf9s3CQplJc948OShPnw78+c8ytHDvXuf18913MgfPDTfYPx8cLNMhVE82d+kSMGmSzOszfrxULLbodDKnj20SvNaKUilzGPn7y8R29nI1WRLzJSfXndaioXh7y81uTp2SIatTp8rd0izodDLMdNas1q98bXFkOtGSSqPNRxoN0YULZNSXUHFxIun1efVf01YpKCBKTJTOyMOHrXbfFmM++vprOSXfssU9/VsWM9maBIqLiXr0kKkMnLUKNzaWyGbRo13ef5+sC54MBqK33pJhnv7+0o7t6Uk0enTViJnff5fX2Kygb/Xs3ClNQ/PnV/WF/fe/8jkCRBMn1r8Ir7EUFhJNmiT7eeEFKUNTI8SaGbQEn4IrSqOVQlERUWIimfJyqLg4kcrL61iZ2dZJT5dKobhYHjMziaiO5/jHHzLUz1mx5PUxfrz805wwoXn6q86ECUR9+tS83927pcOwsRFBtlhSULz5Zt310tJkvbvvrnSAzpxZmXdo0yb53a23VvoPLLmFMtyf48upvPaavK+VK+Xf5Lx5ZI0W+s9/XP/3qdfL3wNAtHAh0R13SL9NeevwT7JSqI7ZTHT0KJkvXapIipdW/zVtlfPn5SyBSL6BnjxJZDbbf45mc2UStF27XC/b8eOyrz59Kt+Qm5PUVDnwL11q//zjj1OjFznZYlksZfk91MWIEbJuWJj9we/NN+X5v/1Nfp4/X4ZztjVMJpnET6WSsyUvL6IXX3Td7MAeZjPRyy+TNWfU7bc3X99NhJWCPZKTiY4cIU3REdLpUhy7xgEKCgpo5cqVjbp26tSpjc5V1CgqlCNdviw/5+TI2UJRkf3nuGGD/DMRQr4ZuZp775Xx+efOybwxFYv/mo233pL3e+GC/fM6nczl07GjXKXqKGYz0dWrRPv2ybUJI0bIJHiOvN3+9JMc8PNqMXmazTLPjiUhW69eRHPmOC5bayI3VybomztXRtC5i3XrpAmvlZiOiFgp2KfClq7NPE5a7UXHrnGA5ORkioqKsnvO4MrVsI2hrEwqAUtiM5NJJjq7eLHmcywokG+nw4fLQcfTU67AdRU5OfLtz5JZ8o475D+eRuO6PqsTG0s0cmTddZKSpNkgLq5+/8KaNUT9+8tnZ5uS2mKbdhYGA9H06dLu7ohZimk6rSzc11Gl0IZc5g4QEAB4eEBZQnBm9NGSJUtw6dIlxMXFYfHixdi9ezfGjh2LmTNnYlDF9nk33XQT4uPjERUVhVWrVlmv7dGjB3Jzc5GSkoKBAwdi0aJFiIqKwuTJk6HT6Wr0tW3bNowcORJDhgzBpEmTrAn2SkpKcM899yA6OhoxMTHYsmULAGD79u0YOnQoYmNjMXHiRECrlQ1ZkpN5eMjtFAsLZRSHLc89J6OTPvwQuP9+ucPW55877bnV4OOP5XaFDz8sPz/4oIw22bDBdX3acuoUcPw4cMcdddfr319GxJw4Adx9d+1RUm+/LTeODwoCHnlEbmjz3XeyH41GJmBzFkolsGkTEBcnP7eFyKOWTluKOLLFEc3Rkkp9M4V6MmfTuBFaGhdfTGPGlDgrc3aNmcKuXbvIx8eHLltMNESUVzH112q1FBUVRbm5uUQkk+Tl5ORQcnIyKRQKOnr0KBERzZ07l9avX1+jr/z8fGv67Y8//pgef/xxIiL629/+Ro/aCJqfn0/Z2dkUERFhlSMvL0/azA8dqvqWo9MRJSbSGdtUC4mJ0mRka76Jj5eLuFzh0NPriSIiZASJBbNZvo3HxDSPk3vJErk6ta700La8/bZ8K3/55arfm81Ezz0nz82b17Skdg0lM5PonXfsr5Bm2jXgmUItqJQAAcLs2hj4ESNGoGfPntbPy5cvR2xsLBISEpCamooLFy7UuKZnz56Iq3jTi4+PR0pKSo06aWlpuOGGGxAdHY23334bp0+fBgD88ssvePDBB631goODsX//flx77bVWOTp06CAXq3l7V33L8fKSs6iSErm3rckkN1gPCwNefbWy3p//LBdxWTaDdyZbtwJpacCjNhnWhQD++lf5Ru7M9QH2MJvljGTy5LrTQ9vyxBMy7fILL1SuJTCb5T28+qp8Xhs2yPTVzUVYmJRLoWi+Ppk2hdLdAjibZcvqqWDyAB07B0MQoOoVD7nw2vn42uSO3717N3755Rfs27cPPj4+GD9+PMrKympc4+npaf1ZoVDYNR89/PDDePzxxzFz5kzs3r0bSxuyypZImo+q79cLAB07Apcvy1Wy6ely4P/iCyAwsLLO7bfLAWf1arnC05ksXw706lW5p4Btn4sXS9OLK1eE/u9/QGqqXMHqKEIAq1YB585J5fDbb/I+PvsMePxx4J13ZB2GaUW0v5mCQgHy84KyBCAy1V/fAfz9/aGxt9KygqKiIgQHB8PHxwdJSUnYv39/o/sqKipCeHg4AGDt2rXW76+//nqstNmEvaCgAAkJCdizZw+Sk5MBAPlXr8pZgL0NVYKC5Nvl3/8OPPOMTBcwf37VOgEBwK23SmVRUtLoe6jB4cPA778DDz1U8w3X11fa7TdvlhuxNwWjEfjnP2XKgb//HfjqK2nfLyuTvhJfX7k6tSF4eclZQmCg3GTls8+kr4AVAtNKaX9KAYA50A8eBgDaOga2sjJpCnCAkJAQjB49GoMHD8bixYtrnJ8yZQqMRiMGDhyIJUuWICEhoZGSA0uXLsXcuXMRHx+P0NBQ6/fPPfccCgoKMHjwYMTGxmLXrl3o2LEjVq1ahdmzZyM2Nhbzbr9dVrbdAcuCEICfH3DggFy+v3Kl/UHtz3+WCuE//2n0PdRg+XIp0z332D//wAMyNceaNY3v48QJICFBvsFv2yYV35w5QHS07HvNGuCmm+w/m/ro2lXmwOnUSU5VX3iBFQLTahFUW+REC2XYsGF0qJpN++zZsxg4cKDDbRh1eVCcTgZ1DoVHRI+aFYqKgAsX5NZ5jtqXWwOpqTKaaMgQu4PW2VOnMHDkSGmuqc0sRQQMGiRNUL//3nSZsrLkc160SG7ZWBuTJgHnz8v8Ng2xl5eXA6+9JmcGHTpIZWfJoXP+vDT9nD8PXLkiTWODBzf+XohYGTAtFiHEYSKqJ3NfG/QpOIJQe8HkDSgKi4GIaif1ejnwAJXhm62FggJpGrLxTVShtFSer23gUiikszcoqPY+hJCzhSeflEnroqKaJvOqVfKZP/RQ3fUefBCYPVv6PGbOdKztgwelqej0aZkobdkyufcxIBOsxcfL4ixYITBtgHZpPhJCCaM/IMr00kxkwWyWGSjNZmkrtuPobbEYjVL2K1fsn7c4mevboD04uP7B7c47ZURNU8w5gFQGH34os4TWt5n5jTcC4eEyU2l9vxciWW/UqMqMp+vXVyoEhmFqpf0qBb+KD4WFlSfS0+XbdI8e0qmq0zVf+ua8vKbtbWBxdBcX229Hp5PKrjE28+p06iQdsuvWSfNMY/nyS+DqVeCxx+qvq1RKk9bvvwPXXCMVoD10Oqm0nnpKmolOn659vwKGYWrQLpUC4AFSCZi9lNLkAgD5+dK+3amTtD17eclB1LLvgCshkm/4V5uwv5BGI9ceKBT226m+krmp/PnPUpH9858ycuell4CFC4ExY4DevWV4Zl0QyWsHDJBrAxzt87vv5LOKj6+5+Ux6utyQZcMGuU5g06aqIbUMw9RL+/QpCAEhVDAFKOGRXSoH1JQUOWBGVDgZvL3lsaysdhu9s7BEOjXlrbu4WEYP+fnJ3bh0usp7AOTsQaFw3r1cf73ceOTpp+VnIeSz69VLRif93/8BiYm1m6J+/x04ckSajxqSLmD6dHnd3LnAzTdL38brrwNHj8rooeJiGSJ6001Nv0eGaYe005mCNCGZ/CuiWM6flwNTr16VA5RlQG0Ov4LlLb6srHHmKn2FbyQgQM50PDxqzhbqczI3FA8PYMcO+eZ+9qy8hz/+AHbvlvb8w4eBivxLdlm2TPov7ryz4X336CEXmz3wgFwPMHIkcO21UuHt28cKgWGaQPtWCuoKhzIR0LNn1bdopVI6U+2sPHYGfn5+lR8sSoGoceYqiz/B31/K3bGjNIdZZh5ms1RuzjIdWejfX765Dxggn6OFBQtkVNKzz0oHeHVSUuTb/H33NV4mT0/ggw+k6ercObkGITFRrjtgGKbRtGulQGQEunWTCsGe7bm5IpC02so3+MaYkDQaaRqyRBaFhcn2MjPlZ4vDvL7II2ehUEiTzvnzwKef1jz//vtSvvrCUB3hjjvkfe7cKbO9MgzTJFgpBAbWHqro7e1QBNKSJUuqpJhYunQp3nnnHZSUlGDixIkYOnQooqOj8c0339S82BIqWqGUbpo3z26K7RopsGGTLvv66xFz223Y8tVXsrJaLQfI3Fw587BEIzl7plAXN94oQ0KXLq2qWDUamTtp7txK/01T8fdvu2mMGaaZaXOO5se2P4ZjmcfqrWc260FUDoXCv/ZKBgNQVoa4q9dg2fQVtVabN28eHnvsMWuW0i+//BI7duyAl5cXtm7dioCAAOTm5iIhIQEzZ86smoSvvFzmIwoMBIqL8clbb6FDdDR0Oh2GDx+OOXPmwGw2Y9GiRdizZw969uyJ/Px8AMArr7yCQD8/nNy4EYiMRIFtNs7OneXq5aws2b5SKZVFcyGETC43bpycGVjSf6xdK1eMOxKGyjBMs9PmlIKjCCEqJgAEoBbnq+Xt055d3IYhQ4YgOzsbGRkZyMnJQXBwMLp16waDwYBnnnkGe/bsgYeHB9LT05GVlYXOnTtXXmwbKurpieUffYSte/YAgDXFdk5OTs0U2JDpsjd98IG83t8fwbbRRp6eMrQ2J0f6Rnx9m3/F7bXXAlOnyhQTixZJR/h770n7/8iRzSsLwzAO0eaUwrIp9eXOlhgM+Sgruwwfn0FQKGqxtRuNwLFjDpk55s6di82bNyMzMxPz5s0DAGzYsAE5OTk4fPgwVCoVevToUTNltsWf4OWF3ceO4Zfff683xXYVSkqkqcjW0WuhS5dKh7O9dNnNweuvy1xLb70lF51dvFh1jwaGYVoU7dYQK4Q0tdS5LWcDIpDmzZuHTZs2YfPmzZg7dy4Amea6U6dOUKlU2LVrF67YS0Gh1Vo3vSkqL0ewnx98vL2rpNiukQK0z3dHAAAgAElEQVS7wnx0/aRJWPnJJ9KmLgQKLAvxLHh7V+Yxak5/gi1xccBtt8kQ1Jdekgp29mz3yMIwTL20Y6UgJ0n17tXsYARSVFQUNBoNwsPD0aVLFwDAHbNn49ChQ4iOjsa6deswoHp+H6LK9QMAptxwg90U2zVSYFfMRJ578kkUFBVh8PTp1nTZNQgPl/4K2xDY5ubll6V/5tAhuf9yc+5ExjBMg2iXqbMBwGw2oLT0ODw9I6FW15Ee+48/ZBRPLemma6WsTG7g0qNH7aGS5eVye0tLiu6SEiApCejb17H0DNnZUr7oaKesVG7Mc3SYRx6RTubkZPeZshimHeNo6myeKTgyU2hMDiSLA7nC1FNnHYtpxzKwO7pgrrhYRhS5Og2HM3j3XelPYIXAMC2adqwUBABF/UrBNgdSQ7DU12hqj16yKAVLH0qlXPjlSF9Esu2AgIbJ5S4sK60ZhmnRtBml0BgzmFzAZqi7UmNzIOl00txEVDU9ty2lpVYnc4VAcmbiyKpmrVauP/CvY51FA2htZkSGYVxDm1AKXl5eyMvLa/DAJoSq/plCY3MgWRLUqdWV6bltsaxkrh4V5OnpWF+2+Y6aCBEhLy8PXvbCWhmGaVe0iXUKERERSEtLQ05OToOu0+uzQWSEp6e57ooFBXLvAEdnC0RyP+SAAPnz1avSJ2GbisFolDl79Pqq7RYWyhW/SmXdju3s7Mrd1pyAl5cXIpyVdoJhmFZLm1AKKpXKutq3ISQl/QP5+T8iLi697ooffAB89pl07DoSgXTunFzJu3atTMc9ZQrwxRcyXt/Ct9/K3ct+/x2wjfj54guZ5O306arf26LXA8OHA3ffXfdm9wzDMA2kTZiPGotKFQqDIad+s1NUlAwXTU11rOEzZ+Rx0CC5irdz55p7Cxw5ImcOsbFVv+/bVx7Pn6+9/cRE6Y+47jrH5GEYhnGQdq0U1OqOIDLAZNLUXTEqSh5Pn3asYUu9AQPkwH/zzcCPP1ZGGwFyE5oBA2r6FCxK4cKF2tv/9Vc5Yxk3zjF5GIZhHKRdKwWVSi4qMxjq8UUMGiSPlhlAfZw5I7eqtKwivuUWqRC2b6+sc+QIMHRozWuDgmToZl0zhR9/lHsU15bym2EYppGwUgBgMOTWXTEkRG5c4+hM4cyZSkUCyGyhISHA5s3yc2am3Ec5Pt7+9X371j5TyMoCDhyQ+xUwDMM4GZcpBSHEJ0KIbCHEqVrOjxdCFAkhjlWUF1wlS22oVHIxlV7vQNTSoEGOKQWTSaaqsJicABlJdNNNcj/j8nI5SwDszxQAoF+/2mcK338vI5pmzqxfFoZhmAbiypnCZwCm1FPnNyKKqygvu1AWuzg8UwDkIH/mTL27sCE5WQ78tjMFAJgzR64t+PnnSqUQF2e/jb59ZRhrSUnNc9u2yS1EqzuoGYZhnIDLlAIR7QFQR+If92OZKTisFByJQLKNPLJl4kSZ5G7LFqkU+vWrPUVFv37yePFi1e/LyoCffgJmzGj+DXMYhmkXuNunMEoIcVwI8aMQIqq2SkKI+4QQh4QQhxq6QK0uFAo/CKGu39EMVA7y9ZmQLEqh+hoDtVqafL75Bjh4sHbTEVB7WOquXdJhzf4EhmFchDuVwhEA3YkoFsAKAF/XVpGIVhHRMCIa1tGJSdWEEFCpOjo+UwDqj0A6c0ZuJGNvFjBnjlwdnZ5et1Lo00ceqzubv/1WhrBOmFC/vAzDMI3AbUqBiIqJqKTi5x8AqIQQtWw84DosC9jqxdEIpOqRR7ZMnly5LqG2yCNA1gkPrzpTIJKO6smT7W+9yTAM4wTcphSEEJ2FzF8NIcSIClnymlsOh5UCUH8EktkMnD1bu1Lw9gamT5c/DxlSd1/Vw1KPHQPS0th0xDCMS3FlSOpGAPsA9BdCpAkh7hVC/EUI8ZeKKrcAOCWEOA5gOYD55Ib8zd7ePaHVXnAsw2p9EUhXrkibf21KAZCb1n/yCRAcXHdffftWnSls2yadyxalwjAM4wJclhCPiG6r5/z7ANyezc3XNxZXr66GXp8BT8/wuisPGSIT0B09at8nYPE3RNXqM5eDvcWRXBf9+snMrAUFUoF8+y2QkCC37WQYhnER7o4+cjt+fjEAgJKS4/VXvukmubfC55/bP19b5FFjsM2BlJEhcyWx6YhhGBfT7pWCr28DlEKHDnJg3rDB/habZ84AXbrUbxpyBMtahfPnpYMZ4FXMDMO4nHavFFSqIHh6dndMKQDAnXfKDW5++qnmuboijxpKr14yw+qFC9J01LOn89pmGIaphXavFADAzy8WpaUnHKs8bZqcMaxfX/V7IucqBU9PIDJSRh39+qucofAqZoZhXAwrBUi/glZ7DiaTA9ttqtXA/PnA11/LbTMtpKXJNBjOfJvv108mwCsrY9MRwzDNAisFyAgkwIzSUgdTYy9cKAdqSypsoHL9gjOVQt++MutqQAAwdqzz2mUYhqkFVgqQ5iMAKC110K8wYoQcsG1NSLUlwmsKFmfzlClyhsIwDONiWCkA8PbuDQ8PH5SUOOhXEELOFv77XyAlRX535oxcQxDqxEwd/fvLI4eiMgzTTLBSACCEB3x9ox2PQAKABQvkccMGeXSmk9nCpElyTcT8+c5tl2EYphZYKVQgI5COO5buAgB69JDbbK5b5/zIIwsKBXDHHXLnNoZhmGaAlUIFfn6xMBoLUV5ezyY6tixcKBeXWSKReB0BwzCtHFYKFViczQ77FQDglltkGutnn5WfWSkwDNPKYaVQga9vNIAGRCABcnvNWbNkumyAlQLDMK0eVgoVKJUB8PLq2TBnMyBNSIBc5cwZTBmGaeWwB9MGP7/YhiuFyZOlMujfn9NQMAzT6mGlYIOvbyxyc7+ByaSFQuHj2EVKpXQ0e3u7VjiGYZhmgJWCDXJvBUJp6SkEBIxw/MJRo1wmE8MwTHPCPgUbKiOQGmhCYhiGaSOwUrDBy6snFAo/VgoMw7RbHFIKQohHhRABQrJGCHFECDHZ1cI1NzLdRYzjeyswDMO0MRydKfyJiIoBTAYQDOBOAG+4TCo34ucXg5KSE46nu2AYhmlDOKoULLGW0wCsJ6LTNt+1KXx9Y2EyFaGs7Iq7RWEYhml2HFUKh4UQP0EqhR1CCH8AZteJ5T4avLcCwzBMG8JRpXAvgCUAhhORFoAKwD0uk8qNWNJdNCgHEsMwTBvBUaUwCsA5IioUQiwA8ByAonquaZUolX7w8urNEUgMw7RLHFUKHwLQCiFiATwB4BKAdS6Tys1Y9lZgGIZpbziqFIwkw3FmAXifiFYC8HedWO7Fzy8WOt0lGI0l7haFYRimWXFUKWiEEE9DhqJ+L4TwgPQrtEmks1mmu2AYhmlPOKoU5gEoh1yvkAkgAsDbLpPKzfj6xgAASkqOulkShmGY5sUhpVChCDYACBRCzABQRkRt1qfg5dUDanVnFBX9z92iMAzDNCuOprm4FcBBAHMB3ArggBDiFlcK5k6EEAgKGo/Cwt28splhmHaFo6mzn4Vco5ANAEKIjgB+AbDZVYK5m6Cg8cjO3gSd7gJ8fPq5WxyGYZhmwVGfgodFIVSQ14BrWyVBQeMBAIWFu90qB8MwTHPi6MC+XQixQwhxtxDibgDfA/jBdWK5H2/vflCru7BSYBimXeGQ+YiIFgsh5gAYXfHVKiLa6jqx3E91v4Lg/ZcZhmkHOLwdJxFtAbDFhbK0OKRfYSN0uvPw8envbnEYhmFcTp3mIyGERghRbKdohBDF9Vz7iRAiWwhhdwVYxYY9y4UQF4UQJ4QQQ5tyI66A/QoMw7Q36lQKRORPRAF2ij8RBdTT9mcAptRxfiqAvhXlPsj8Si0Kb+++UKu7slJgGKbd4LIIIiLaAyC/jiqzAKwjyX4AQUKILq6SpzFY/AoFBbt4vQLDMO0Ch30KLiAcQKrN57SK765WryiEuA9yNoHIyMhmEc6C9Ct8Aa32HHx9BzRr3wzDtDyIALMZMBoBg6Hq0WiUdXx8AD8/QK0GqseomM2ATgeUlMijl5es6+MDeHhU7UerBQoKKkuXLkDfvq69P3cqBYcholUAVgHAsGHDmvWVPShoAgDpV2ClwNiDSP5zFxXJAUClqlqI5ABQUgJoNJU/+/gAYWFA586At3ft7ZtMQGkpUF4OlJXJo+Xn0tLK9iyltLSynl5fWd9olLJYZLagVsv+bYtaXTn42RYPD0CprHp/SqXsR6eT/ZaVyZ9NJtmOWg14elYWna7qQFdQIJ+dwSCvsS1C1JTN21vKZrk/yz0aDFJGi9xElfepUEjZbYulnslUeX8mk2zHtuj1lQO+RQE4ikIhB3xfX9m25fdTGz4+sq4Q8rlU7+upp4A33nC8/8bgTqWQDqCbzeeIiu9aFN7evaFWh6OwcDfCw//ibnHaFFotcOGCPFoGGqVSFoWicnDRauXRUmwHHsvPKlXlP5Svr/zZ01NeaxmILcfiYqCwUJaiosojUNm/bVEoapby8so2CgsbNlDYIyBAKoiQEHk/Gk1l0eka16ZKVXUwVior31otR6KaA7oz8PKSz8kyqFZHoQCCgytLUJCUt/pzNpulTJa/gYICefTwkPdkUTh+fvJ6Dw95b7YFqKncTKaqCsJWaVRX6iqV7Mf2b8Iiq+Vv1vZv1/KGb1HYpaWyWBSEbfHykvdnW6+kRMpo+3wspU8f5/x+6sKdSuFbAA8JITYBGAmgiIhqmI7cTaVf4Zd2uV6BCMjJAbKy5D+Y7UCpVFZ968vPl8fCQlnX8paoVst/Gp0OOH8eSEoCzp0D/vjDOTJ6eMh/Ikfw9gb8/eUgFBQEBAYC3brJQVkI+SZoMlV9K6z+9moyyTb69KlsJyhItkFU9S3TYk6wDAL+/pU/a7VAZqYsWVnymJsLhIbKerbF11cOIF5eVQf66oOMxQzh6VnVFOEoZnPlG3j1N2shat6f5R49PavKZ/tvYlE8lhmLxVzSzv6VWg0uUwpCiI0AxgMIFUKkAXgRFXswENFHkCuipwG4CECLFrznc3DwBGRnb4BWmwRf34HuFqfJlJfLATknp6ZZo7gYSEsDUlKA5GR5bOybqj38/IABA4CxY4H+/WUJDLRvm/XykgOct3fl0cur0oRg+VmplAO15e3Mciwvl4Op7cCqbBUGU/fh4VH5fGvD07NhbQpRqcSYlo/L/kWI6LZ6zhOAB13VvzOxXa/Q0pSCTgdcvCjfvC2mmOp24/JyIDUVuHJFlszMutsMDgZ69JCD95Qp8ucuXWR7ljdl24G7Q4fK6W2HDnKQt7xR6vWVR7Vamkhc8YaoUFQO/gzDNB5+b3IAL69e8PSMqPArPOBWWTIygHXrgF27Kk0wtk5Di8nA1p6qUgEREUD37sC0afLYvbscoC0DaXXzgzPw8nJOOwzDNB+sFBxA+hUmID9/h1v8CuXlwLZtwCefADt2SLtvbCxwzTXAPfdIE0y/frL4+TWraAzDtDFYKThIUNB4ZGWth1Z7Fr6+g1zaV1GRdMYmJQEHDwKbNkknbng4sGQJcPfdro9VZhimfcJKwUGq+hWcqxQuXADWrJEK4OzZqjZ/T0/gppvkjGDSJGk7ZxiGcRWsFBzEy6snPD27VfgV/trk9gwG4JtvgI8+An79VUbFDBsGTJ0qHbyW0rOn9AkwDMM0B6wUHKTSr/Bjk/wKGRnABx/ImUFmJhAZCbz6KvCnP8kIH4ZhGHfCSqEBBAdPRFbWOmg0iQgIGNGga69eBd58U84MDAZg+nTg/vtlyCebhBiGaSmwUmgAISE3QgglcnK2OKwUsrKkMvjwQ6kM7roLePZZoFcvFwvLMAzTCFyWOrstolIFIzh4EnJyNtebSru0VCav6tkTeO89YP58ua5gzRpWCAzDtFxYKTSQjh1vQVnZZZSUHKu1zvHj0mn89tvALbfI0NJPPwV6925GQRmGYRoBK4UGEhIyC4ACOTmba5wjAt5/Hxg5Uq41+OUXufqY1xQwDNNaYKXQQNTqUAQHT0BOzn+qmJDy8uR6gocflusJjh8HrrvOjYIyDMM0AlYKjaBjx1ug011AaekpAMCePTLtxPbtwLJlMiVFx45uFpJhGKYRsFJoBKGhNwHwQE7OZqxfD0ycKNMy798PPPoo54lnGKb1wkqhEajVYQgMvBZvvx2EhQvl3gAHDwJDhrhbMoZhmKbB6xQagdEI/POfK7B+/WDMm1eIdeuCoFa7WyqGYZimwzOFBlJaCsyeDaxfPxi33/4GXn99OSsEhmHaDDxTaAC5uTI9xaFDwMqVwKhRPyAvrxC9er3gbtEYhmGcAs8UHESnA268EThxAvjqK+Cvf5VRSKWlJ6HVnnO3eAzDME6BlYIDmM1yP4P9+4ENG4BZs+T3oaGzAQA5OVvcKB3DMIzzYPORA7z4IvDvf8vEdrNnV37v5RWBgIBRyMnZjO7dn3GfgAzTSsnV5iIxPRHn884jJiwGCREJ8FZ513mNzqDDhfwLOJd7DufyKkruOWj0Grw0/iXcGnVrk+UymAzILs1GVmkWMksyUWYsg9JDCZWHCiqFCioPFTyVnugR1ANhvmG1ptIv0BXgYPpBJGYkol9IP8wZOAcKj5adFpmVQj2sWyf3O7j3XmDx4prnO3a8BZcuPQGd7hK8vTm5UXslpzQHp7JPQW/SQwgBAWEdKHxVvujToQ9CfUIbvA9HnjYPu1N2I6UwBX5qP2vx9/SHr8oX2aXZuFxwGZcKLuFywWVcLrgMrUGLf97wT9w88OZG3QsRodRQCj914zf8NpqNSC9Oh86og86gQ5mxzPpzUm4SDmYcRGJ6IpILk6tcp1aokRCRgPHdx2N8j/Ho6t8Vp3NO42TWSZzMluVi/kWYyWy9JiIgAv1D+kNv0mPe5nnYmrQV7099HyE+IQ7JelVzFTuTd2Jn8k4kZiTiaslV5GpzHb7XIK8gDAwdiEEdB2Fg6ED4qn1xMP0g9qXtQ1JuUpW6/UP645mxz+D26Nuh9Kg6/Obr8rHhxAasProaKYUpiO8Sj4SIBIwMH4mRESPR2a+zwzI1BVFfts+WxrBhw+jQoUPN0teePTJlxdixcrWyvR3QysquYP/+HujV601ERv6tWeRqLZjJjMMZh/HtuW/x7flvcSn/EgZ1HITYsFjEhMUgtrM8BnkFOdRega4Ap3NOw2g2VikmswkqhQo+Kh94K73lUSWP/mp/+Kp94SGqWkpzSnNwIusETmafxImsEzibexbdArpZ/wmHdhlq94213FiODE0GzuScwZGrR3D46mEcuXoEqcWp9cof7BWMfiH9rCXcPxyhPqEI8QlBiHcIQnxCoFao8fsfv2Nn8k78mvwrjmUeA6H+/1E/tR96B/dGr+BeSC5MxrHMY/jbNX/DaxNfqzH4VEdr0OJQxiHsTd1rLXm6PPQK7oXhXYdjeNfhGBE+AkO7DIWv2tduG2nFaTiQdgD70/bjQPoBHL56GFqDttY+uwd2x/Dw4RjRdQSGhw9Hv5B+OHr1KHan7MbuK7tx5OqRKgO/gEDvDr0R3Ska0Z2iMbDjQPQP6Y9+If2sMhnNRrz5vzfx0n9fQohPCFbfuBrT+02v0XdWSRb2pu6ViiBlJ87knLH+fq7pdg26BXRDZ7/O1hLmFwYflQ8MJgMMZoP1qDPocDH/Is7mnpUl5yxytDkAgFCfUCREJCAhPAEJEQmI7xqPny/9jFf2vIKT2SfRK7gXnh7zNO6MuRN7U/di9dHV2HJmC8pN5RjWdRiGdh6Kw1cP43jWcRjNRgBAZGAknhz1JB4e+XCdv8/aEEIcJqJh9dZjpWCfCxeAhASgUydg714gOLj2uocPj4TJVIrhw082eke21kh2aTZSClNgMpuqDNLF5cX4+fLP2HZ+GzI0GfAQHhgTOQYxnWJwJvcMjmceR54uz9pOXOc43DLwFswZNAcDQgdU6cNoNmL7xe1Ye3wtvj33LfQmfYPlFBDWt2t/tT+KyouQWVK5EXaYbxgGhA5ASmEKrhRdAQAoPZSIDYtF/9D+yNXmIkOTgQxNBvJ1+VXa7hfSD/Fd4jG0y1DEhsXCV+0LIrIO5ESE4vJiXMi/gPN553E+7zzO5Z1DWnFanTKrFWpc0+0aTOw5Edf1vA6DOg6CzqCDRq9Bib7EWkK8Q9AruFeVWUi5sRz/t+P/8OGhDzGu+zhsumVTjbdMTbkGG09txNrja3Ew/aB14Okf0h/XdLsGPYN64njWcSRmJOKPoj8AAB7CAx28O1hnQZajwWSw/j7VCjWGdB6CkeEjER0WDV+VL7xV3vBWesNL6QVvlTd6BPVAJ99Odd5/UVkRfvvjN+RqcxHVMQqDOg6qVSFV51jmMSzcuhAns0/i3iH3YtHQRTiUcQj70vZhX9o+XC64DADwUflgbORYTOw5ERN7TURsWGyTTTt52jxo9Bp0D+xudywwkxnbzm3DK3teweGrh+Gp8ES5qRxBXkFYEL0A9w69F3Gd46z1dQYdjlw9ggPpUuHO6DcDC2MXNko2VgpNoKQEGDoUKCiQzuX6Ul6fSl6GlEv/hzHDdiMoaJzT5CgsK8SHiR9CrVCjq39XhAeEy6N/eL12V1dhNBvxw4UfsOboGnx//nuYyGS3np/aD1P6TMHMfjMxre+0KlN5IkKGJgMnsk7gaOZRfH/he+xN3QsAiOoYhVsG3YKxkWPxw4UfsOHkBmSVZiHUJxR3RN+BKX2mwEvpBaWH0loUQgG9SQ+dUQetQQutQQudQYdSQyk05Rpo9BpoyjUoLi+GRq+Br9oX0Z2iERMWg+hO0QjzC7PKllmSiQNpB3AgXZZL+ZcQ5heGLn5d0NW/K7r6d0UXvy7oG9IXcZ3jEOAZ0KjnqDVokVmSiTxtHvJ0edZjqb4Uw8OH45pu18BH5dOoti2sP74e9393P4K8gvCfuf/BNd2uwcH0g/j4yMfYdGoTSg2lGNxpMGb0nYHRkaOREJGAUJ/QGu1klWQhMSMRiemJyNHmWJWe5eghPDCo4yCMDB+JuM5x8FR6NkluZ1BuLMdL/30Jb/7+pnXG0dW/K0ZFjJKl2ygM6zoMaoV7FhkREbZf3I7NZzZjQs8JmDNwjsv/p1kpNIFHHwVWrAB27QLG1TPG703dixs33giluQCrx03GjaO211k/V5uLC3kXMKrbqDrraco1uH799TiQfsDu+e6B3TF30FzMHzwfQ7sMrXOGYiZzDfOJPfal7sNTvzwFhYcC/Tr0q2LqMJMZa4+vxdrja5FZkonOfp1xV+xdGBs5FiqFyjo4Kz2U8FR6IrpTdIMGh/TidGxN2orNZzZjz5U9IBBUHirM6DcDd8fdjal9pkKlsGO/Y+rkRNYJzPlyDlIKU9C3Q1+czT0LH5UP5kfNx6L4RRgZPrJNz26PZR7D+bzzGBk+EpGBkW36XuuDlUIj2bcPGD1arkN4//26636d9DVu23IbIgIiUKjNgNGkxfY7dmBk98l26+9P2485X85BhiYDi69ZjL9P/Lvd6WqpvhRTN0zF3tS92HzrZkzoMQHpmnRkaDKQXpyOdE069qbuxY5LO2A0G9GnQx/Mj5qPW6NuhRACJ7JOVCm52lzcH38/nr32WbvT9jJjGV7c9SLe2fcOwv3DEREQgfN556uYeABAIRSY1nca7h1yL6b1neayQTqrJAv70/ZjdORou2+uTMMoKivCwz8+jEsFl7AwZiFui76t0TMcpvXCSqER6PUyqV1xMXDmDODvX3vdDxM/xEM/PoThXYdj223bcLXwOK5ffz10Zk9sv3Mnrul2jbUuEeGjQx/h0e2PoltgN4yNHIu1x9diWt9p+GL2Fwj0CrTW1Rl0mLFxBnan7MbGORvrDK/L1+Vj69mt2HR6E3Ym76zimFN5qDCw40DEhMUAADae3AgvpRceH/U4nhj1hLXPxPRE3PX1XTibexb3Db0Pb09+2zpg5GnzrLZwTbkGNw+8GV39uzbq2TIM414cVQrSLtiKSnx8PLmKpUuJAKLvvqu9jtlspmd+eYawFDTjixlUqi+1ntu+bzx1e0tBPq/50E8XfyIiIq1eS3dtvYuwFDRtwzTK1+YTEdGHiR+S8mUlDXh/AJ3PPU9ERGWGMpr6+VQSSwWtP76+QbJnajJp9eHVtOHEBjqZdZL0Rn2V80k5STT3y7mEpaAOb3agt39/m5799VlSvKSg8H+E0/YL2xvUH8MwrQsAh8iBMdbtg3xDi6uUwqlTRCoV0W231V5Hb9TTwq0LCUtBi75dRAaTocr53NzvaMsO0KAVkaR+RU0fJn5IQz4aQlgKenHXi2Qym6rU3528m0LeDKGgN4Lou3Pf0ayNswhLQR8f/tgVt0hERIfSD9EN628gLAVhKejur++mAl2By/pjGKZl4KhSYPMRAJMJGDNGhqGePWt/1zQiwl1f34X1J9bj5fEv47lrn6vhtCIy4cCBvigXYVhySvoQAj0D8fnszzGj3wy7fScXJGPWplk4mX0SAPD+1Pfx4IgHnXp/9tibuhdEhNGRo13eF8Mw7sdR8xGvaAbwwQcy9HTdutq30Xztt9esCuH5cc/brSOEAl27PoDLl/+GrTfvx5pTv2De4Hno06FPrX33DO6JvffuxeKfFiOucxzuH3a/M26pXmx9HgzDMBba/UzhyhUgKkrOFH780f5Wml+e/hLzNs/DgpgFWHfTujrD2gyGPOzbF4GwsLvQv/9HTpOTYRimKTg6U2j3WVKffloe//Uv+wphf9p+LNy6EGMix2D1javrjXNWqULQqdN8ZGWth8FQ6AKJGYZhXEe7Vgo6HfDNN8DChUD37jXPpxSmYNamWQgPCMfWeVsdXowVHv4QzGYtsrLWOllihmEY19KulcLPPwNaLXCznWSSxeXFuHHjjdCb9Pj+9u8btIjK3z8e/v4jkZ7+Achm7QDDMExLx6VKQQgxRQhxTghxUQixxM75u4UQOUKIYxXlz66Upzpbt9W20LUAABL+SURBVAJBQcD48VW/T8pNwux/z0ZSbhI2z91cI0mbI4SHPwid7jzy8390jrAMwzDNgMuUghBCAWAlgKkABgG4TQgxyE7VfxNRXEVZ7Sp5qmM0Atu2ATNmyJTYBpMBm89sxnVrr8PAlQOx58oerJqxChN7TWxU+5063Qovr964dOlJmM0Nz+zJMAzjDlw5UxgB4CIRXSYiPYBNAGa5sL8G8dtvQF4ecN2NOVi6eym6L+uOuf+Zi0sFl/D6da8j7fE03DPknka37+HhiT59lkGrTUJ6+gonSs4wDOM6XLlOIRyA7c4jaQBG2qk3RwhxLYDzAP6PiGrsViKEuA/AfQAQGRnpFOG2bgU8vU14L+8GnDh7DFP6TMGq4aswtc9Up22XFxo6Ax06TENKykvo1Ol2eHp2cUq7DMMwrsLdjuZtAHoQUQyAnwHYDdcholVENIyIhnWsbXVZAyACvv4aGHDbahzPPooNszfghzt+wIx+M5y+f2qfPstgNpfj8uUaLhWGYZgWhyuVQjqAbjafIyq+s0JEeURUXvFxNYB4F8pj5fBhIDU3H5d6PItx3cdh/uD5LuvLx6cvunV7AllZ61BUtNdl/TAMwzgDVyqFRAB9hRA9hRBqAPMBfGtbQQhha0+ZCeCsC+WxsnUrIK57AVoqwPKpy12+8UZk5DNQq8Nx4cJDoFp2KmMYhmkJuEwpEJERwEMAdkAO9l8S0WkhxMtCiJkV1R4RQpwWQhwH8AiAu10ljy0bd54ADfsQDwx7wLrfgCtRKv3Qu/c7KCk5iqtXmy3AimEYpsG0u9xHSUmEgW9OgE/Pk0hdfAEdvDs4UbraISIcOzYBpaWnMHLkeahUzdMvwzAMwLmPauXlLf8BevwXz416rdkUAgAIIdC373IYjYVITn6u2fplGIZpCO1KKZTqS7FF8yS8i2Lxt4mLmr1/P78YhIc/iIyMD5Gb+02z988wDFMf7UopPLv9Tei9U7EgZIXTQ08dpVevN+DvPxxnzy5ASclJt8jAMAxTG+1GKSQXJGPl0beAk7fh8Tlj3SaHQuGNwYO3QqHwx6lTs6DX57pNFoZhmOq0G6VwKvsUPMo7oPfltzCg4fntnIqnZzgGD/4a5eUZOHNmLsxmg3sFYhiGqaDdKIXRHW+E8Z0U3Dolwt2iAAACAkagf//VKCzcjYsXH3W3OAzDMADa0R7N330HmA1qu3snuIvOnRegtPQkUlPfgq9vNMLDH3C3SAzDtHPajVKYMwcIDASG1Rul27z06vU6SktP4+LFR6BWd0Jo6GyXr7BmGIapjXZjPvL1BWbNsr8PszsRQoFBg76Aj88gnD59C44eHYuCgp3uFothmHZKu1EKLRmlMgDx8Yno2/cDlJWl4PjxiTh2bAIKC39zt2gMw7QzWCm0EDw81AgPfwAjR15Enz7vobT0LI4duxbHj0+GXp/lbvEYhmknsFJoYSgUXoiIeAQJCZfRu/c7KCr6HSdPzoTJpHW3aAzDtANYKbRQFAofdOv2BAYN+gIaTSLOnr0TRGZ3i8UwTBuHlUILJzR0Fnr3fhe5uV/x7m0Mw7icdhOS2pqJiHgUOt1FpKa+DW/v3uja9X53i8QwTBuFlUIrQAiBPn2WoawsGefPPwgvrx7o0OEGd4vFMEwbhM1HrQQPDyUGDdoEX9/BOH16LmdYZRjGJbBSaEUolf6Ijv4OCoU/jh4di0uXnkJZWZq7xWIYpg3BSqGV4eUVgbi4XejQ4Qakpr6DAwd64syZO6DRHHa3aAzDtAHYp9AK8fHph6iof0OnS0F6+gpcvfoxsrO/QGDgtQgOngi1urNN6QK1OgweHmp3i80wTCtAEJG7ZWgQw4YNo0OHDrlbjBaF0ViMq1fXID19JcrKLtU4L4QaISHT0KnT7QgJmQGFwtsNUjIM406EEIeJqN6UoKwU2hhmsx56fTb0+kxrKS09hZycL6HXX4VC4Y/Q0NkIC7sDwcHXQQj3bEvKMEzz4qhSYPNRG8PDQw0vrwh4eVXdTKhPn3+gsHA3srI2ICdnC7Ky1sLDwwc+Pv3h4zMQPj4D4OMzEL6+g+DjMwBCsLuJYdojPFNoh5hMZcjP/x5FRb9Dqz2L0tKzKC+/Yj3v6RmJsLA7EBZ2B3x9o9woKcMwzoLNR0yDMBpLoNOdQ0nJceTk/Af5+T8DMMHPLw5hYQvQocMUmEwl0OuzrMVgyIZSGYTAwNEICEiAUhno7ttgGKYWWCkwTUKvz0J29r+RlbUBGs1Bu3UUikCYTBoAZgACvr7RCAwcDX//ESAqh16fifLyq1bfhoeHJyIiHkNo6Cw2TzFMM8NKgXEaWu0FFBfvh0rVASpVGNTqMKjVneDh4QmjUYPi4gMoLv4dRUW/o7h4f4WikKhUHa2hsTrdJZSVXYKv72B07/48Onacw45uhmkmWCkwboHIBJ3uIhQKP6hUneDhobKeM5uNyMn5N65ceRVabRJ8fAage/fn4Oc3FOXl6SgvT4Nen47y8nQYDHnw8oqscIDLolKFuPHOGKZ1w0qBabEQmZCTswVXrryK0tKaOZyUyg5QqTqgrCwVROXW71WqUHh794FaHQ5Pz67w9Ay3/mw266qE4er1mTAYCqBQ+EGpDKwoQVAqA+Hp2R0BAQk1IrQYpi3DIalMi0UIBTp1uhUdO96C/PwdMBoL4ekZXjHId7UuriMyoazsCrTaJGvR6S5Dqz2NgoKfYTIV221fqQyCWt0ZSmUwDIYcGI2FMBqLKupXvgR5ekYgIGBURRkJIjMMhqwKpSKPRmNBRW2PCj+IPKrVneHvPwx+fvHw9u4NIUSjn4fZbERR0R7k5HyFoqI9UKs7w9u7L3x8+sHbux98fPrB07M7PDz435VxPTxTYFotRmNJhbkpAwqFD9TqzlCpwqBQeNmtT2SGyaSBVnsexcX7UFS0F8XF+1Be/oed2h5QqTpCpepgubpi5zsziEwoL08HkR6AdLj7+8fDzy8WAFUoocpCZIa3dy94e/exFi+vXigtPY3c3K+Qm/sNjMZ8eHh4IzBwLIzGfGi156spPY8KX07XCgXaFWp1OHx8+sHXNxre3n1ZaTB1wuYjhnGQ8vIMaDSH4eHhWTHwdoZKFVqnE9xs1v9/e3cbI1dVx3H8+7t3dne2s1u225YWS+kDJfKgtSghKBgLREVFIQYUBSTGhDeYQNQo+BhJeOEb0RckQkRFRQWRKjEkiqWivBBoaaWUB+kT2lLZQrtlZ3dmd2bu3xf37DC7Xdp1t9PZmf1/ksm998zdu+ff3p3/3HPPPYfBwW0MDGxiYGAj+fwm8vmtRFF7aKZ68wVGobCTQmHHmOYwSBPKggUfZ8GCT9Lb+2HieA4AZkaptJ9C4SWGhv5FsbiL4eFXqklweHgv5fLr1eNIHeRyZ5DLrSaXO4tsdiXZ7HKy2eW0tc2f8ErGzEiSIuXyQUqlA5TLB6rraUISoHCFJCAiSYpUKocol9+oXn1VKnkymV46Ok4KY22lrzjuolR6jVKpr9qFeWRkP21t8+nqOpvu7nczZ87bvbPBceJJwbkZxixheHgPhcJ2CoUdZLOn0NNz4ZQHK6xUCgwNvcjg4FYGB58hn9/K4OBWRkZeGbNfFOXIZpcTx11UKgPVV7k8AFSm9LujKBfu08wlinKUywcYGdlHkhTf8mekDtrbF1IqvVbdL4o6yeVW09W1mkymhyjKEkWd1WUa5wCVyhuUywMhCQ0AMXGcI47nEEWjyzlEURtS7SsDJFQqQyTJIJXKYFgfQmqv/tybyxxtbb3V+1qZTC+ZzAnT6kJdKh2kWNxFFHWSza54yyvZNNYhCoUdQEIu945jmjA9KTg3S5VK/RSLuxkefplicTfF4m4KhV0kySBx3E0czyWT6Q7r3WQy8yb4IOwGFJrMjLTZLCGKssRx94RNVWZGuXyIkZF9jIzso1LJhy7JJ9LWdiJx3IUkkqTM0NAL5PObyeefZmBgM0ND26hU8kdIKgp1nUscd4WmwEGSZIhKZbDalDcZUjtR1IlZiSQZmsxPEEWdSBmkuGbZFhJj75h/PxDF4i4KhZ0Uizspl/vHHKujYwnZ7Kl0dq6io2NJzReF7WMSevpg6Afo6VnLvHkXksu9c1rJyZOCc67ppE1awyRJISQII47nEsdzjviBmCRlkqSAWSl82JcwK2NWIk0oOeI4F64mMuN+XzEklyEqlfyY5rR0+TqVSgGohGNWqsculw+N2/cgZmWy2eV0dq4km10ZlitIkkL1KjF9badU6gsdC1aNeSVJif7+v9Lfv4FicSeQ9spbtuwbLF36pSn923rvI+dc05FEHGeP2MQykSjKEEXdU/x9ncRx5zF7DsbMJt0bLUnKb9lBYPHiawAoFv9dTRDt7W87JnU8krqONSDpEkkvStou6eYJ3u+QdF94/wlJy+tZH+ecq7f/p3vyZHqMZbOnsHjx5zj99J+yaNFV06na5OpUrwMrvUNyB/AR4EzgM5LOHLfbF4CDZrYKuB34Xr3q45xz7ujqeaVwLrDdzHZaehfoN8Bl4/a5DLgnrD8AXKzpPAXknHNuWuqZFJYA/6nZ3hPKJtzHzMrAIeCwhj1J10vaKGnj/v3761Rd55xzTTF+sZndZWbnmNk5CxcubHR1nHOuZdUzKewFltZsnxzKJtxH6VMmJwCv45xzriHqmRSeAk6TtEJSO3AV8NC4fR4CrgvrVwCPWrM9OOGccy2kbs8pmFlZ0heBPwEx8BMz2ybpVmCjmT0E3A38QtJ24ABp4nDOOdcgdX14zcweBh4eV/btmvUicGU96+Ccc27ymm6YC0n7gZen+OMLgNeOYXVmolaPsdXjg9aP0eNrjGVmdtSeOk2XFKZD0sbJjP3RzFo9xlaPD1o/Ro9vZmuKLqnOOeeOD08KzjnnqmZbUrir0RU4Dlo9xlaPD1o/Ro9vBptV9xScc84d2Wy7UnDOOXcEsyYpHG1uh2Yk6SeS+iQ9W1PWK+kRSS+F5bxG1nE6JC2VtEHSc5K2SboxlLdEjJKykp6U9M8Q33dD+Yowv8j2MN/I1CZxniEkxZI2S/pj2G61+HZL2ippi6SNoaxpz9FZkRQmObdDM/oZcMm4spuB9WZ2GrA+bDerMvBlMzsTOA+4Ify/tUqMw8BFZvYuYA1wiaTzSOcVuT3MM3KQdN6RZnYj8HzNdqvFB3Chma2p6YratOforEgKTG5uh6ZjZn8jHR6kVu0cFfcAlx/XSh1DZrbPzJ4O6wOkHyxLaJEYLZUPm23hZcBFpPOLQBPHByDpZOBjwI/Dtmih+I6gac/R2ZIUJjO3Q6tYZGb7wvp/gUWNrMyxEqZqPRt4ghaKMTStbAH6gEeAHUB/mF8Emv9c/QHwVSAJ2/NprfggTeR/lrRJ0vWhrGnP0bqOfeQay8xMUtN3L5PUBfwOuMnM3qidnK/ZYzSzCrBGUg+wDji9wVU6ZiRdCvSZ2SZJaxtdnzq6wMz2SjoReETSC7VvNts5OluuFCYzt0OreFXSSQBh2dfg+kyLpDbShHCvmT0YilsqRgAz6wc2AO8FesL8ItDc5+r5wCck7SZtsr0I+CGtEx8AZrY3LPtIE/u5NPE5OluSwmTmdmgVtXNUXAf8oYF1mZbQ/nw38LyZfb/mrZaIUdLCcIWApE7gg6T3TTaQzi8CTRyfmd1iZieb2XLSv7lHzexqWiQ+AEk5Sd2j68CHgGdp4nN01jy8JumjpO2bo3M73NbgKk2bpF8Da0lHZXwV+A7we+B+4BTS0WQ/ZWbjb0Y3BUkXAH8HtvJmm/TXSe8rNH2MklaT3oSMSb+g3W9mt0paSfrNuhfYDFxjZsONq+n0heajr5jZpa0UX4hlXdjMAL8ys9skzadJz9FZkxScc84d3WxpPnLOOTcJnhScc85VeVJwzjlX5UnBOedclScF55xzVZ4UnDuOJK0dHS3UuZnIk4JzzrkqTwrOTUDSNWGugy2S7gwD1+Ul3R7mPlgvaWHYd42kf0h6RtK60bHzJa2S9JcwX8LTkk4Nh++S9ICkFyTdq9rBnJxrME8Kzo0j6Qzg08D5ZrYGqABXAzlgo5mdBTxG+gQ5wM+Br5nZatKnr0fL7wXuCPMlvA8YHTXzbOAm0rk9VpKOEeTcjOCjpDp3uIuB9wBPhS/xnaQDmiXAfWGfXwIPSjoB6DGzx0L5PcBvw3g4S8xsHYCZFQHC8Z40sz1hewuwHHi8/mE5d3SeFJw7nIB7zOyWMYXSt8btN9UxYmrH+angf4duBvHmI+cOtx64IoyPPzrf7jLSv5fR0T0/CzxuZoeAg5LeH8qvBR4LM8XtkXR5OEaHpDnHNQrnpsC/oTg3jpk9J+mbpLNpRUAJuAEYBM4N7/WR3neAdGjkH4UP/Z3A50P5tcCdkm4Nx7jyOIbh3JT4KKnOTZKkvJl1NboeztWTNx8555yr8isF55xzVX6l4JxzrsqTgnPOuSpPCs4556o8KTjnnKvypOCcc67Kk4Jzzrmq/wHJCoTV4c+hmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.4117 - acc: 0.6154\n",
      "Loss: 1.4116695821470933 Accuracy: 0.61536866\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2321 - acc: 0.3649\n",
      "Epoch 00001: val_loss improved from inf to 1.74084, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_5_conv_checkpoint/001-1.7408.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 2.2321 - acc: 0.3649 - val_loss: 1.7408 - val_acc: 0.4396\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3856 - acc: 0.5782\n",
      "Epoch 00002: val_loss improved from 1.74084 to 1.24133, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_5_conv_checkpoint/002-1.2413.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 1.3856 - acc: 0.5782 - val_loss: 1.2413 - val_acc: 0.6198\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0669 - acc: 0.6745\n",
      "Epoch 00003: val_loss improved from 1.24133 to 1.13597, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_5_conv_checkpoint/003-1.1360.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 1.0669 - acc: 0.6745 - val_loss: 1.1360 - val_acc: 0.6536\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8478 - acc: 0.7399\n",
      "Epoch 00004: val_loss did not improve from 1.13597\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.8477 - acc: 0.7399 - val_loss: 1.3040 - val_acc: 0.6273\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6738 - acc: 0.7901\n",
      "Epoch 00005: val_loss improved from 1.13597 to 1.12194, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_5_conv_checkpoint/005-1.1219.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.6738 - acc: 0.7901 - val_loss: 1.1219 - val_acc: 0.6855\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8307\n",
      "Epoch 00006: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.5382 - acc: 0.8307 - val_loss: 1.5310 - val_acc: 0.6133\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4392 - acc: 0.8598\n",
      "Epoch 00007: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.4392 - acc: 0.8598 - val_loss: 1.2502 - val_acc: 0.6797\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.8906\n",
      "Epoch 00008: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.3468 - acc: 0.8905 - val_loss: 1.4073 - val_acc: 0.6415\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9007\n",
      "Epoch 00009: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.3057 - acc: 0.9007 - val_loss: 1.6838 - val_acc: 0.6271\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9213\n",
      "Epoch 00010: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.2518 - acc: 0.9213 - val_loss: 1.5668 - val_acc: 0.6625\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9292\n",
      "Epoch 00011: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.2248 - acc: 0.9292 - val_loss: 1.3460 - val_acc: 0.6962\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9343\n",
      "Epoch 00012: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.2058 - acc: 0.9343 - val_loss: 1.3822 - val_acc: 0.7000\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9413\n",
      "Epoch 00013: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1849 - acc: 0.9413 - val_loss: 1.5095 - val_acc: 0.6816\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9431\n",
      "Epoch 00014: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1812 - acc: 0.9431 - val_loss: 1.6308 - val_acc: 0.6727\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9525\n",
      "Epoch 00015: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1500 - acc: 0.9525 - val_loss: 1.4637 - val_acc: 0.7023\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9527\n",
      "Epoch 00016: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1533 - acc: 0.9527 - val_loss: 1.4691 - val_acc: 0.6960\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9561\n",
      "Epoch 00017: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1414 - acc: 0.9561 - val_loss: 1.4672 - val_acc: 0.7107\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9573\n",
      "Epoch 00018: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1388 - acc: 0.9573 - val_loss: 1.6208 - val_acc: 0.6874\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9610\n",
      "Epoch 00019: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1271 - acc: 0.9609 - val_loss: 1.6222 - val_acc: 0.6942\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9627\n",
      "Epoch 00020: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1231 - acc: 0.9627 - val_loss: 1.5477 - val_acc: 0.7098\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9622\n",
      "Epoch 00021: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1219 - acc: 0.9621 - val_loss: 1.5881 - val_acc: 0.7107\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9629\n",
      "Epoch 00022: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1224 - acc: 0.9629 - val_loss: 1.5426 - val_acc: 0.7154\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9678\n",
      "Epoch 00023: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1103 - acc: 0.9678 - val_loss: 1.5045 - val_acc: 0.7135\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9673\n",
      "Epoch 00024: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1107 - acc: 0.9673 - val_loss: 1.7724 - val_acc: 0.7023\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9669\n",
      "Epoch 00025: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1113 - acc: 0.9669 - val_loss: 1.7128 - val_acc: 0.7060\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9693\n",
      "Epoch 00026: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.1031 - acc: 0.9693 - val_loss: 1.8672 - val_acc: 0.7007\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9733\n",
      "Epoch 00027: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0981 - acc: 0.9733 - val_loss: 1.5455 - val_acc: 0.7352\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9748\n",
      "Epoch 00028: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0904 - acc: 0.9748 - val_loss: 1.7568 - val_acc: 0.7158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9747\n",
      "Epoch 00029: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0865 - acc: 0.9747 - val_loss: 1.6108 - val_acc: 0.7384\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9720\n",
      "Epoch 00030: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0973 - acc: 0.9720 - val_loss: 1.9222 - val_acc: 0.6979\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9728\n",
      "Epoch 00031: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0920 - acc: 0.9728 - val_loss: 1.8618 - val_acc: 0.7018\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9760\n",
      "Epoch 00032: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0860 - acc: 0.9760 - val_loss: 1.6331 - val_acc: 0.7338\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9734\n",
      "Epoch 00033: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0919 - acc: 0.9734 - val_loss: 1.6403 - val_acc: 0.7435\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9769\n",
      "Epoch 00034: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0808 - acc: 0.9769 - val_loss: 1.7403 - val_acc: 0.7205\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9802\n",
      "Epoch 00035: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0703 - acc: 0.9802 - val_loss: 1.8803 - val_acc: 0.7112\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9750\n",
      "Epoch 00036: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0896 - acc: 0.9750 - val_loss: 1.6574 - val_acc: 0.7326\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9792\n",
      "Epoch 00037: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0738 - acc: 0.9792 - val_loss: 1.5655 - val_acc: 0.7442\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9777\n",
      "Epoch 00038: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0777 - acc: 0.9777 - val_loss: 1.5931 - val_acc: 0.7428\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9791\n",
      "Epoch 00039: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0767 - acc: 0.9791 - val_loss: 1.7052 - val_acc: 0.7188\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9767\n",
      "Epoch 00040: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0851 - acc: 0.9767 - val_loss: 1.6845 - val_acc: 0.7319\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9820\n",
      "Epoch 00041: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0658 - acc: 0.9820 - val_loss: 1.5656 - val_acc: 0.7526\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9782\n",
      "Epoch 00042: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0804 - acc: 0.9782 - val_loss: 1.8220 - val_acc: 0.7265\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9806\n",
      "Epoch 00043: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0706 - acc: 0.9806 - val_loss: 2.0190 - val_acc: 0.7000\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9809\n",
      "Epoch 00044: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0676 - acc: 0.9809 - val_loss: 1.7262 - val_acc: 0.7370\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9842\n",
      "Epoch 00045: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0579 - acc: 0.9842 - val_loss: 1.7381 - val_acc: 0.7300\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9812\n",
      "Epoch 00046: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0686 - acc: 0.9812 - val_loss: 1.8028 - val_acc: 0.7293\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9820\n",
      "Epoch 00047: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0650 - acc: 0.9820 - val_loss: 1.7460 - val_acc: 0.7261\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9807\n",
      "Epoch 00048: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0703 - acc: 0.9807 - val_loss: 1.7991 - val_acc: 0.7386\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9840\n",
      "Epoch 00049: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0607 - acc: 0.9840 - val_loss: 1.8523 - val_acc: 0.7128\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9846\n",
      "Epoch 00050: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0559 - acc: 0.9846 - val_loss: 1.7442 - val_acc: 0.7342\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9837\n",
      "Epoch 00051: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0599 - acc: 0.9837 - val_loss: 1.6937 - val_acc: 0.7403\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9831\n",
      "Epoch 00052: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0631 - acc: 0.9831 - val_loss: 1.7237 - val_acc: 0.7375\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9822\n",
      "Epoch 00053: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0635 - acc: 0.9822 - val_loss: 1.6477 - val_acc: 0.7417\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9871\n",
      "Epoch 00054: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0518 - acc: 0.9871 - val_loss: 1.6219 - val_acc: 0.7533\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9864\n",
      "Epoch 00055: val_loss did not improve from 1.12194\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 0.0517 - acc: 0.9863 - val_loss: 1.8071 - val_acc: 0.7305\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.2342 - acc: 0.6503\n",
      "Loss: 1.2341685036880081 Accuracy: 0.6502596\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1150 - acc: 0.3818\n",
      "Epoch 00001: val_loss improved from inf to 1.77099, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_6_conv_checkpoint/001-1.7710.hdf5\n",
      "36805/36805 [==============================] - 231s 6ms/sample - loss: 2.1148 - acc: 0.3819 - val_loss: 1.7710 - val_acc: 0.4207\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2656 - acc: 0.6124\n",
      "Epoch 00002: val_loss improved from 1.77099 to 1.29125, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_6_conv_checkpoint/002-1.2912.hdf5\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 1.2656 - acc: 0.6124 - val_loss: 1.2912 - val_acc: 0.6245\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9968 - acc: 0.7033\n",
      "Epoch 00003: val_loss improved from 1.29125 to 0.91551, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_6_conv_checkpoint/003-0.9155.hdf5\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.9971 - acc: 0.7033 - val_loss: 0.9155 - val_acc: 0.7293\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8397 - acc: 0.7489\n",
      "Epoch 00004: val_loss improved from 0.91551 to 0.90867, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_6_conv_checkpoint/004-0.9087.hdf5\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.8397 - acc: 0.7489 - val_loss: 0.9087 - val_acc: 0.7275\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7264 - acc: 0.7809\n",
      "Epoch 00005: val_loss improved from 0.90867 to 0.81273, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_6_conv_checkpoint/005-0.8127.hdf5\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.7264 - acc: 0.7809 - val_loss: 0.8127 - val_acc: 0.7640\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6351 - acc: 0.8091\n",
      "Epoch 00006: val_loss did not improve from 0.81273\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.6355 - acc: 0.8090 - val_loss: 0.8697 - val_acc: 0.7489\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5632 - acc: 0.8301\n",
      "Epoch 00007: val_loss did not improve from 0.81273\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.5632 - acc: 0.8301 - val_loss: 0.8235 - val_acc: 0.7775\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4861 - acc: 0.8517\n",
      "Epoch 00008: val_loss did not improve from 0.81273\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.4863 - acc: 0.8517 - val_loss: 0.8849 - val_acc: 0.7573\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4306 - acc: 0.8674\n",
      "Epoch 00009: val_loss did not improve from 0.81273\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.4306 - acc: 0.8674 - val_loss: 0.9889 - val_acc: 0.7538\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8863\n",
      "Epoch 00010: val_loss did not improve from 0.81273\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.3742 - acc: 0.8862 - val_loss: 0.9444 - val_acc: 0.7577\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3336 - acc: 0.8945\n",
      "Epoch 00011: val_loss improved from 0.81273 to 0.80841, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_6_conv_checkpoint/011-0.8084.hdf5\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.3337 - acc: 0.8945 - val_loss: 0.8084 - val_acc: 0.8036\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.9049\n",
      "Epoch 00012: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.3012 - acc: 0.9049 - val_loss: 0.9718 - val_acc: 0.7633\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.9200\n",
      "Epoch 00013: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.2537 - acc: 0.9200 - val_loss: 0.8944 - val_acc: 0.7824\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9241\n",
      "Epoch 00014: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.2406 - acc: 0.9241 - val_loss: 0.8266 - val_acc: 0.8015\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9333\n",
      "Epoch 00015: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.2109 - acc: 0.9333 - val_loss: 0.8377 - val_acc: 0.7978\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9381\n",
      "Epoch 00016: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1981 - acc: 0.9381 - val_loss: 0.8892 - val_acc: 0.7952\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9425\n",
      "Epoch 00017: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1844 - acc: 0.9425 - val_loss: 1.0534 - val_acc: 0.7678\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9465\n",
      "Epoch 00018: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1705 - acc: 0.9464 - val_loss: 1.0426 - val_acc: 0.7750\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9441\n",
      "Epoch 00019: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.1751 - acc: 0.9441 - val_loss: 0.9070 - val_acc: 0.7992\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9554\n",
      "Epoch 00020: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1424 - acc: 0.9554 - val_loss: 0.9731 - val_acc: 0.7908\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9571\n",
      "Epoch 00021: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1391 - acc: 0.9571 - val_loss: 0.9907 - val_acc: 0.7939\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9554\n",
      "Epoch 00022: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.1477 - acc: 0.9553 - val_loss: 0.9567 - val_acc: 0.8022\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9544\n",
      "Epoch 00023: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1465 - acc: 0.9544 - val_loss: 1.0022 - val_acc: 0.7952\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9633\n",
      "Epoch 00024: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.1196 - acc: 0.9633 - val_loss: 1.0084 - val_acc: 0.7922\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9641\n",
      "Epoch 00025: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.1186 - acc: 0.9641 - val_loss: 0.9833 - val_acc: 0.8011\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9620\n",
      "Epoch 00026: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1225 - acc: 0.9620 - val_loss: 0.9482 - val_acc: 0.8118\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9671\n",
      "Epoch 00027: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.1071 - acc: 0.9671 - val_loss: 1.1731 - val_acc: 0.7810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9631\n",
      "Epoch 00028: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.1217 - acc: 0.9631 - val_loss: 1.0317 - val_acc: 0.7973\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9613\n",
      "Epoch 00029: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.1290 - acc: 0.9613 - val_loss: 1.2146 - val_acc: 0.7759\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9688\n",
      "Epoch 00030: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.1004 - acc: 0.9688 - val_loss: 0.9877 - val_acc: 0.8053\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9713\n",
      "Epoch 00031: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0954 - acc: 0.9713 - val_loss: 1.0158 - val_acc: 0.8036\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9731\n",
      "Epoch 00032: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0913 - acc: 0.9731 - val_loss: 1.0687 - val_acc: 0.8050\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9728\n",
      "Epoch 00033: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0914 - acc: 0.9728 - val_loss: 1.0592 - val_acc: 0.8060\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9732\n",
      "Epoch 00034: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0892 - acc: 0.9732 - val_loss: 1.0125 - val_acc: 0.8041\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9709\n",
      "Epoch 00035: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0944 - acc: 0.9709 - val_loss: 1.0436 - val_acc: 0.8090\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9722\n",
      "Epoch 00036: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0942 - acc: 0.9722 - val_loss: 1.0816 - val_acc: 0.8004\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9738\n",
      "Epoch 00037: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0856 - acc: 0.9738 - val_loss: 1.0681 - val_acc: 0.8055\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9714\n",
      "Epoch 00038: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0946 - acc: 0.9714 - val_loss: 1.0334 - val_acc: 0.8062\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9779\n",
      "Epoch 00039: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0785 - acc: 0.9779 - val_loss: 0.9594 - val_acc: 0.8176\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9774\n",
      "Epoch 00040: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0766 - acc: 0.9774 - val_loss: 1.0276 - val_acc: 0.8164\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9788\n",
      "Epoch 00041: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0750 - acc: 0.9788 - val_loss: 1.1215 - val_acc: 0.7957\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9744\n",
      "Epoch 00042: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0865 - acc: 0.9744 - val_loss: 1.0979 - val_acc: 0.8046\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9776\n",
      "Epoch 00043: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0788 - acc: 0.9776 - val_loss: 1.1147 - val_acc: 0.7987\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9771\n",
      "Epoch 00044: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0805 - acc: 0.9771 - val_loss: 1.0271 - val_acc: 0.8185\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9798\n",
      "Epoch 00045: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0712 - acc: 0.9798 - val_loss: 1.1913 - val_acc: 0.8036\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9781\n",
      "Epoch 00046: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0748 - acc: 0.9781 - val_loss: 1.0949 - val_acc: 0.8067\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9779\n",
      "Epoch 00047: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0763 - acc: 0.9779 - val_loss: 1.1952 - val_acc: 0.7980\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9799\n",
      "Epoch 00048: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0694 - acc: 0.9799 - val_loss: 1.1328 - val_acc: 0.8020\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9800\n",
      "Epoch 00049: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0694 - acc: 0.9799 - val_loss: 1.1035 - val_acc: 0.8076\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9778\n",
      "Epoch 00050: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0760 - acc: 0.9778 - val_loss: 1.0925 - val_acc: 0.8083\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9827\n",
      "Epoch 00051: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0619 - acc: 0.9827 - val_loss: 1.1622 - val_acc: 0.8123\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9797\n",
      "Epoch 00052: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0699 - acc: 0.9796 - val_loss: 1.1813 - val_acc: 0.7959\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9801\n",
      "Epoch 00053: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0696 - acc: 0.9801 - val_loss: 1.1572 - val_acc: 0.8055\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9810\n",
      "Epoch 00054: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0681 - acc: 0.9810 - val_loss: 1.1436 - val_acc: 0.8078\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9808\n",
      "Epoch 00055: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0669 - acc: 0.9808 - val_loss: 1.1764 - val_acc: 0.8004\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9829\n",
      "Epoch 00056: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0623 - acc: 0.9829 - val_loss: 1.0848 - val_acc: 0.8169\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9822\n",
      "Epoch 00057: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0660 - acc: 0.9822 - val_loss: 1.1189 - val_acc: 0.8143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9839\n",
      "Epoch 00058: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0574 - acc: 0.9839 - val_loss: 1.0558 - val_acc: 0.8157\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9810\n",
      "Epoch 00059: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0659 - acc: 0.9810 - val_loss: 1.0395 - val_acc: 0.8223\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9832\n",
      "Epoch 00060: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0631 - acc: 0.9832 - val_loss: 1.0063 - val_acc: 0.8269\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9832\n",
      "Epoch 00061: val_loss did not improve from 0.80841\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0616 - acc: 0.9832 - val_loss: 1.1928 - val_acc: 0.8083\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX5wPHvnTX7QggQ1oQdwr6JIqCigFgRF8QFFRdsrbX1Z2vrVouodbdWa2vRulPRgrsoVllVsLKvYUuAJASy75NklvP742SF7JlJQub9PM95JjNz7zlnbmbue+85555rKKUQQgghAExtXQEhhBDthwQFIYQQlSQoCCGEqCRBQQghRCUJCkIIISpJUBBCCFFJgoIQQohKEhSEEEJUkqAghBCikqWtK9BUnTt3VrGxsW1dDSGEOKNs2bIlUykV3dByZ1xQiI2NZfPmzW1dDSGEOKMYhnG0MctJ85EQQohKEhSEEEJUkqAghBCi0hnXp1Abp9NJSkoKJSUlbV2VM1ZAQAA9e/bEarW2dVWEEG2oQwSFlJQUQkNDiY2NxTCMtq7OGUcpRVZWFikpKcTFxbV1dYQQbahDNB+VlJQQFRUlAaGZDMMgKipKzrSEEB0jKAASEFpItp8QAjpQUGiI2+2gtDQVj8fZ1lURQoh2y2+CgsdTQllZGkp5Pyjk5uby97//vVnrzpo1i9zc3EYvv2jRIp599tlmlSWEEA3xm6BgGGYAlHJ7Pe/6goLL5ap33ZUrVxIREeH1OgkhRHP4UVDQA618ERTuu+8+Dh8+zKhRo7j33ntZu3YtkydPZvbs2QwdOhSAOXPmMHbsWOLj41myZEnlurGxsWRmZnLkyBGGDBnCwoULiY+PZ/r06TgcjnrL3b59OxMnTmTEiBFcfvnl5OTkAPDiiy8ydOhQRowYwTXXXAPAunXrGDVqFKNGjWL06NEUFBR4fTsIIc58HWJIanUHD95NYeH2Wt7x4HYXYTIFYBhNG4sfEjKKAQNeqPP9J598kt27d7N9uy537dq1bN26ld27d1cO8Xz99dfp1KkTDoeD8ePHc+WVVxIVFXVK3Q/y3nvv8eqrr3L11VezYsUK5s+fX2e5N954Iy+99BJTp07l4Ycf5pFHHuGFF17gySefJCkpCbvdXtk09eyzz/Lyyy8zadIkCgsLCQgIaNI2EEL4B785U4CK0TWqVUqbMGFCjTH/L774IiNHjmTixIkkJydz8ODB09aJi4tj1KhRAIwdO5YjR47UmX9eXh65ublMnToVgJtuuon169cDMGLECK6//nreffddLBYd9ydNmsQ999zDiy++SG5ubuXrQghRXYfbM9R1RK+UorBwCzZbd+z27j6vR3BwcOXfa9eu5ZtvvmHjxo0EBQVx3nnn1XpNgN1ur/zbbDY32HxUly+++IL169fz2Wef8fjjj7Nr1y7uu+8+LrnkElauXMmkSZNYtWoVgwcPblb+QoiOy2/OFPQ4fJNP+hRCQ0PrbaPPy8sjMjKSoKAgEhIS2LRpU4vLDA8PJzIykg0bNgDwzjvvMHXqVDweD8nJyZx//vk89dRT5OXlUVhYyOHDhxk+fDh/+MMfGD9+PAkJCS2ugxCi4+lwZwr1MQwLStU/Gqg5oqKimDRpEsOGDePiiy/mkksuqfH+zJkzeeWVVxgyZAiDBg1i4sSJXin3rbfe4he/+AXFxcX07duXN954A7fbzfz588nLy0Mpxa9//WsiIiL44x//yJo1azCZTMTHx3PxxRd7pQ5CiI7FUMo3beyGYfQC3ga6ohvylyil/nrKMgbwV2AWUAwsUEptrS/fcePGqVNvsrNv3z6GDBnSYJ2KivZgMtkJDOzflI/iNxq7HYUQZx7DMLYopcY1tJwvzxRcwG+VUlsNwwgFthiG8V+l1N5qy1wMDChPZwH/KH/0CcMw+6T5SAghOgqf9SkopdIqjvqVUgXAPqDHKYtdBryttE1AhGEYMb6qE0hQEEKI+rRKR7NhGLHAaODHU97qASRXe57C6YEDwzBuNwxjs2EYmzMyMlpQD4sEBSGEqIfPg4JhGCHACuBupVR+c/JQSi1RSo1TSo2Ljo5uQV3MPuloFkKIjsKnQcHQlw6vAJYqpT6sZZFUoFe15z3LX/NRfcyAG191rgshxJnOZ0GhfGTRv4B9Sqnn61jsU+BGQ5sI5Cml0nxXJ3P5Xx5fFSGEEGc0X44+mgTcAOwyDKNiMqIHgN4ASqlXgJXo4aiH0ENSb/ZhfYCKmVJd1QJE2wgJCaGwsLDRrwshRGvwWVBQSn1H1YRDdS2jgDt9VYdT+XKmVCGE6Aj8ZpoL8N09Fe677z5efvnlyucVN8IpLCxk2rRpjBkzhuHDh/PJJ580Ok+lFPfeey/Dhg1j+PDhvP/++wCkpaUxZcoURo0axbBhw9iwYQNut5sFCxZULvuXv/zFq59PCOE/Ot40F3ffDdtrmzobzMpNoKcYkykQjCZ89FGj4IW6p86eN28ed999N3feqU96PvjgA1atWkVAQAAfffQRYWFhZGZmMnHiRGbPnt2o+yF/+OGHbN++nR07dpCZmcn48eOZMmUK//73v5kxYwYPPvggbreb4uJitm/fTmpqKrt37wZo0p3chBCiuo4XFOpj+Gb67NGjR5Oens7x48fJyMggMjKSXr164XQ6eeCBB1i/fj0mk4nU1FROnjxJt27dGszzu+++49prr8VsNtO1a1emTp3KTz/9xPjx47nllltwOp3MmTOHUaNG0bdvXxITE7nrrru45JJLmD59ulc/nxDCf3S8oFDPEb3yuHAUbcdu74XN1tWrxc6dO5fly5dz4sQJ5s2bB8DSpUvJyMhgy5YtWK1WYmNja50yuymmTJnC+vXr+eKLL1iwYAH33HMPN954Izt27GDVqlW88sorfPDBB7z++uve+FhCCD8jfQpeMm/ePJYtW8by5cuZO3cuoKfM7tKlC1arlTVr1nD06NFG5zd58mTef/993G43GRkZrF+/ngkTJnD06FG6du3KwoULue2229i6dSuZmZl4PB6uvPJKHnvsMbZurXdOQSGEqFPHO1Oohy/vqRAfH09BQQE9evQgJkZP33T99ddz6aWXMnz4cMaNG9ekm9pcfvnlbNy4kZEjR2IYBk8//TTdunXjrbfe4plnnsFqtRISEsLbb79NamoqN998Mx6Pvv7iiSee8PrnE0L4B59Nne0rLZk6G6CwcCdmcxiBgbE+qN2ZTabOFqLjauzU2X7VfAQVTUgy/5EQQtTG74KCTJ8thBB187ugIDfaEUKIuklQEEIIUckPg4LcaEcIIerih0FBdzSfaaOuhBCiNfhdUKiYPtub91TIzc3l73//e7PWnTVrlsxVJIRoN/wuKPjiqub6goLLVf/w15UrVxIREeG1ugghREv4T1BwOOD4cQyPnhTPm/dqvu+++zh8+DCjRo3i3nvvZe3atUyePJnZs2czdOhQAObMmcPYsWOJj49nyZIllevGxsaSmZnJkSNHGDJkCAsXLiQ+Pp7p06fjcDhOK+uzzz7jrLPOYvTo0Vx44YWcPHkSgMLCQm6++WaGDx/OiBEjWLFiBQBfffUVY8aMYeTIkUybNs1rn1kI0TF1uGku6pw522UBRygqyI6HQZhMNhoxgzXQ4MzZPPnkk+zevZvt5QWvXbuWrVu3snv3buLi4gB4/fXX6dSpEw6Hg/Hjx3PllVcSFRVVI5+DBw/y3nvv8eqrr3L11VezYsUK5s+fX2OZc889l02bNmEYBq+99hpPP/00zz33HI8++ijh4eHs2rULgJycHDIyMli4cCHr168nLi6O7Ozsxn1gIYTf6nBBoU5G+UmRooH7wXnHhAkTKgMCwIsvvshHH30EQHJyMgcPHjwtKMTFxTFq1CgAxo4dy5EjR07LNyUlhXnz5pGWlkZZWVllGd988w3Lli2rXC4yMpLPPvuMKVOmVC7TqVMnr35GIUTH0+GCQp1H9E4P7NiPp1d3ioKOExAQh9UaVcfCLRccHFz599q1a/nmm2/YuHEjQUFBnHfeebVOoW232yv/NpvNtTYf3XXXXdxzzz3Mnj2btWvXsmjRIp/UXwjhn/ynT8FiAcPAcOq+BG92NIeGhlJQUFDn+3l5eURGRhIUFERCQgKbNm1qdll5eXn06NEDgLfeeqvy9YsuuqjGLUFzcnKYOHEi69evJykpCUCaj4QQDfKfoGAYYLVCWUVQ8F5Hc1RUFJMmTWLYsGHce++9p70/c+ZMXC4XQ4YM4b777mPixInNLmvRokXMnTuXsWPH0rlz58rXH3roIXJychg2bBgjR45kzZo1REdHs2TJEq644gpGjhxZefMfIYSoi39NnZ2QAIZBQfcirNZoAgJ6+aiWZyaZOluIjkumzq6N1QplZTL/kRBC1MG/goLNpoMCZkCCghBCnMr/goJSGB6zV/sUhBCio/C/oACYXIY0HwkhRC38KyhYrQAYEhSEEKJW/hUUKs8UlAQFIYSohX8FhYozBSe09T0VQkJC2qxsIYSoi38FBcMAmw3DVXEvBe/dU0EIIToC/woKAFYrhlMHA281Id133301pphYtGgRzz77LIWFhUybNo0xY8YwfPhwPvnkkwbzqmuK7dqmwK5rumwhhGiuDjch3t1f3c32E7XNnV3O4QCPG3eAwmQKxjAajoujuo3ihZl1z509b9487r77bu68804APvjgA1atWkVAQAAfffQRYWFhZGZmMnHiRGbPno1Rz5zdtU2x7fF4ap0Cu7bpsoUQoiU6XFBokMkE7oprFLzTpzB69GjS09M5fvw4GRkZREZG0qtXL5xOJw888ADr16/HZDKRmprKyZMn6datW5151TbFdkZGRq1TYNc2XbYQQrREhwsK9R3RA3DiBKSkUNAfAkP6Y7F451aYc+fOZfny5Zw4caJy4rmlS5eSkZHBli1bsFqtxMbG1jpldoXGTrEthBC+4n99CpXDUr07ffa8efNYtmwZy5cvZ+7cuYCe5rpLly5YrVbWrFnD0aNH682jrim265oCu7bpsoUQoiX8NigYTu8Ghfj4eAoKCujRowcxMTEAXH/99WzevJnhw4fz9ttvM3jw4HrzqGuK7bqmwK5tumwhhGgJ/5o6G6CsDHbupKQrGF16YLfH+KCWZyaZOluIjkumzq6LRXej6KkuZFI8IYSozv+CgskEVisml4FMny2EEDX5LCgYhvG6YRjphmHsruP98wzDyDMMY3t5ergl5TWpGcxmw/ByR/OZ7kxrRhRC+IYvzxTeBGY2sMwGpdSo8rS4uQUFBASQlZXV+B2bzeb10UdnMqUUWVlZBAQEtHVVhBBtzGfXKSil1huGEeur/Kvr2bMnKSkpZGRkNG6F7GxUYQFOTy42mwQG0IG1Z8+ebV0NIUQba+uL1842DGMHcBz4nVJqT20LGYZxO3A7QO/evU9732q1Vl7t2yjPPgv33svmb/sy8oLDzam3EEJ0SG3Z0bwV6KOUGgm8BHxc14JKqSVKqXFKqXHR0dEtL7n8iNiUKhd7CSFEdW0WFJRS+UqpwvK/VwJWwzA6t0rh5UHBciJfOliFEKKaNgsKhmF0M8qnCzUMY0J5XbJapfDyoGBLd+PxOFqlSCGEOBP4rE/BMIz3gPOAzoZhpAB/AqwASqlXgKuAOwzDcAEO4BrVWoft3bujDAN7psLlysVsDmqVYoUQor3z5eijaxt4/2/A33xVfr1sNjzR4dgzcnG5crHbu7dJNYQQor3xvyuay6nu0dgzwOXKbeuqCCFEu+G/QaFndwLSJSgIIUR1fhsU6NlTzhSEEOIUfhsUjF6xWIrAnXuirasihBDtht8GBVPv/gColOQ2rokQQrQffhwU+gJgpKa2cU2EEKL98NugUHEBm5Fyso0rIoQQ7Yf/BoXu+toEc1rrXEQthBBnAv8NCgEBOCMtmNPy2romQgjRbvhvUACc3YKwnCho62oIIUS74ddBwdUtBMtJmRBPCCEq+HVQcMdEYEt3tnU1hBCi3fDroOCJ6Yw1X6GKitq6KkII0S74dVBQPboC4Dkmt+QUQgjw96BQfq2CO2lfG9dECCHaB78OCvTXVzWrQxIUhBAC/DwomHrG4bYBhw60dVWEEKJd8OugEBg8gJLuoA4mtHVVhBCiXfDvoBDYH0dPC0bi0bauihBCtAt+HRQMw4Q7rhvWo7ng8bR1dYQQos35dVAAMPoPxFTqwZMq91UQQgi/DwqWweMBKNmzuo1rItqlLVsgJgaOHGnrmgjRKvw+KAQMmwaAc+93bVwT0S699x6cOAFff93WNfEPhYWgVFvXon1yts6UPH4fFAIHTsVjAff+HW1dFdEerVypH7+Tg4ZmKyqCefNg5876l0tPh9hYuO466eM7lccD/frB4sU+L8rvg4JhsVHWIwAj8UhbV0W0N0lJsG8fWCzw/fdtXZsz12efwQcfwJ131n8W8Oc/Q1YWLFsGixZ5vx5uN6xaBTfcAFdcAd980/Szkrw82LjR+3VryI4dkJwMffr4vCi/DwoA7tiuWI/koJQcnfiN5cv1Dyw3t+5lvvxSP952GyQmwvHjrVO3jubDD/Xjd9/Bp5/WvszRo/CPf8Ctt+r06KPw7rstL1sp2L4dfvtbfQvemTPh8891kL/oIhg7Fv7974abZpTSgW3wYDjnHPjvf1tet6b46iv9OGOG78tSSp1RaezYscrbCm+9UDkDUYUFe72et2inrr1WKVDqn/+se5lLLlGqf3+lfvxRL/vBB61Xv46iuFip4GClbr1VqUGDdHI6T1/u5puVstmUOnZMqdJSpc4/Xz//7rvml+3xKHXddfp/Z7UqNWeOUitWKOVw6PTaa0oNHqzf791bqSeeUGrbNqXc7pr5JCYqdfHFerkxY5SKjVVq6NDaP4evTJmi1OjRLcoC2KwasY9t1I4Y+A0QBhjAv4CtwPTGrOvt5IugUPLMH5QClb7r717PW7RTvXrpr//ZZ9f+fnGxUoGBSv3610qVlem/f/Ob1q1jR/Dxx3o7f/21Uh99pP9+5ZWay+zdq5TJpNTdd1e9lpWl1IABSnXurNThw80r+y9/0eXde69SmZm1L+N2K/Xpp0pNnqyXBaU6dVLqyiuVevllpf78Z/2/DwlR6oUXdCD48EO93EsvNa9eTZWbq5TZrNQDD7QoG28HhR3ljzOAD4F4YGtj1vV28kVQcH/+qVKgkpdd4/W8RTt09Kj+6vfvrx/37z99mS+/1O999ZV+ft55Svngu9fh3XijUhEROrB6PEqde65SXbsqVVBQtcxVV+mdbnp6zXUPHFAqMlKpIUOUyslpWrkbNyplsSh12WW63MZISVHqnXeUWrCg6qABlLr8cqWSk6uW83iUuuACXbe6go03rVih67F+fYuy8XZQ2Fn++Ffg8vK/tzVmXW8nXwQFdeCAUqCOPDLY+3mL9mfpUv3V/+ILfYRa2xHYXXfpI0SHQz9/6CF9tFZ9Z9aRuVwtz6OsTAeEG2+sem3jRr3tFy3Szzdv1s8ffrj2PNas0U0/sbFKvfde43bwmZl6px4Xp1R2dvPq7vEodeiQrl9tdu7U351f/ap5+TfFbbcpFR7e4uYqbweFN4CvgYNAEBAKbGnMut5OPgkKpaXKYzbU0RttyuNxN7y8OLPdcYdSoaH6R3bxxUr17FlzJ+jxKNW3r+5TqFBx5vDNN61f39b2yiu6Pf/mm/XOry4HDij1/vt176y+/lpvs08+qfn6VVfpfoa0NKWmT9fNNbm5dZezdq1SI0bovM46q/5+BrdbqVmzdP1/+qnu5bzhjjv0gcLu3TVf93h0/9O0aae/11Qej/5+Xnlly/JR3g8KJmAMEFH+vBMwojHrejv5JCgopZx9OqsTF6CKiw/5JH/RjgwfrndGSumdGij13/9Wvb9/v37t5ZerXsvNVcowlHrkkdata2vLytLNInFxSgUF6e0wfbpuRnO7ldq6Vak//lGp+HhV2bzy3HO15/Xzn+udf3FxzdcPHtRNOxMn6vWfeabherlcSr3+ulLdu+t1rrxSqQ0blCoqqrncE0+c/r/zlYwMfSZ00UVVZzCpqbpDG/T3JS7u9Gaxpti1S+f12mstrq63g8IkILj87/nA80Cfxqzr7eSzoHDBRJU3CHXypIww6dCys/WPdfFi/dzh0D/s+fOrlqnooExMrLnuiBF6B9CR3X23bhbZuVMHiMcfV6pbN709wsL0o8mk1NSpuuN1+nTdH5CaWjMfl0v3HcydW3s5d92l8+rR4/SgUZ/CQh2Yg4P1+maz/r/cdptSjz2mn8+b1/h+hJb66191PT7+WKlXX9XNPAEBSj39tFLff6//PvdcpUpKmpf/M8/o/Kv3aTST1/sUykcejQS2AXcC6xqzrreTr4KC546fK2cw6tDB3/skf9FOfPGF/tqvXl312i9+ofsP8vL084su0p2bp/rlL/UOsDWHIp4qI0OpI0d8k/f+/foIfuHCmq+XlCj15ptK3XCDPmKtfuR76JBSdrse4lvdhg16Oy9bVntZ6el6Z/7++82ra1aWHjX00ENKzZihz25AqYEDq/6PraGsTH9XLBZd/nnn6TOhCu+9p1+/+ebmBaoLLtBntl7g7aCwtfzxYeDW6q+1dvJVUFDPP68UqF1rp/omf9E+3H+//gFXb3bYtElVnqIXFOj26N/+9vR1//1vvdzWra1X3woej1Jvv62DEuiO11tuUerdd08/Sm+uOXN0/mlpTVtv0SJdp2+/rXrt7rv1dszP907dGlLRMdzcjuWWWL1aj2RbsuT0axyU0p3ojW0mq66gQHey33uvV6rp7aCwDri/vKO5W3kfw67GrOvt5LOg8KkelrrtlTDlaa1TT9H6Jk/WnZXVeTz6oqrJk3Wn6Kk7uAoVQ1lffLFxZf30k9559+unhzX+6U96eOHBg007aszNrboIa/Jk3WRx+eVVR8egL27at6/xeZ5qzRqdz+OPN33d4mLdMT9kiL7wzOPRF4P97GfNr09H4nbrZjTD0PuZxqr4LlY/q20BbweFbsA9wOTy572BGxuzrreTz4LC3r1KgdrzAMrhOOKbMs50Tqc+o3r3XaUSEmo/KmrPSkp0U0dtZwEVHZQXXaSPlktLa8+jVy/dZt2QzZt1X0WfPnq0zcCBeqdQsROfN69quGt9Nm7UnZVms+4HqT5KyuXSZy1PPKEDhN2u1JNPNr15y+3WV8v26tW09v3qPv9cf66nn64aZvrGG83LqyMqKtLXuYSE6DOHLVsaPjC44476v4tN5NWgoPOjK/Cz8tSlset5O/ksKDgcymMYKnEBKj39Q9+Ucab77LOqnRroYZ3nnafUH/5Q/5DC9uL773W9P/ro9PdSUnQHasXFSnW55hrdOVrfD7oiIMTGKpWUVPV6UZFS//ufbgevOLqvq7nD4dDNMmazzuf77+v/bGlpSl1xhc537Nj6h5Ke6s039XpLlzZ+ndrMnq07gG+8Ude7NS7sOpOkpuo+gorvWa9eSt15px75duoBlsej/++XXea14r19pnA1cBR4C3gbSAKuasy63k4+CwpKKU/vXirtIkMlJj7kszLOaAsW6NEVW7fq4YF33KHU+PH6CPiee9q6dg176in9la9riOD06fr9V1+tO4+//U0vU1dnb10B4VTvvafb3IcMqZmXx6ObDfr21eVcd13TAu5//qNUdLRui65rqGh1hYV6mOeECS0/80tK0qNtQI/RF7VLT9e/nzlz9AAH0MNzqwfyhAT9+j/+4bVivT7NRfWzAyC6YuqLetZ5HUgHdtfxvgG8CBwqH900pjF18WVQUBdcoAqGBaodOy72XRlnqrIyfZFR9aGbFa66Ss9R46XTXJ+59FLdd1CXzz7TO/Tjx+teZts2/bN5993T39uyRTfjNBQQKqxZo4NsTIzONyFBqZkzdf5DhtS8dqIpMjL02Q4otXJl/cv+9rd6uZZMPFfdo4+qVrtOoCMoLtbNbJ076wEQf/iDPqOsGBbdmO9RI3k7KOw65XmDHc3AlPIL3uoKCrOAL8uDw0Tgx8bUxadB4fbblTPSrr77rot0Np/qm2/01+XDWprWKq72/c9/Wq8+DofusD1xonHLu906qN16a8vKdbl0s9kdd1S9VlKiO5/DwxsfECrs3q2vWA0O1kf3YWG636asrGX1dDj0kM8uXereRhVz6vzyly0rq7rSUn1FdHP7JvxVZqYetgq6Dyk+Xs/g6kXeDgrPAKuABeXpS+CpRqwXW09Q+CdwbbXn+4GYhvL0aVB4+mmlQG34DFVS4qVhfh3FHXfoK1xPvYJUKb2j7NVLjxf3laVL9RnJhAn6oqiKfg2LRf+YEhLqX3/PHuW1zs8ZM/TYcZdLDxONjdV5n39+864hSEnRfTO33NL4INcYu3fr5pxZs07vA9m/Xwe3CROaf2GV8L41a/SgBKg5a6wX+KKj+cryK5mfp3xSvEasU19Q+Bw4t9rzb4FxdSx7O7AZ2Ny7d2+vbqgayqfE/ekVVHr6Ct+VU5djx3x3YVJLuN36qtb65l95+GHdt3D0aOPzTU3VY7sbmsvmd79TlWPzp0/XV68uXqx38HfdpXd8hqGDxpYttefzyis6j+oXFjXX4sW6vGHDdJ6jRyu1alXrXUXbFC+9pOtYfZrnwkJd96iopv2/ROtwOHSfQ0umx6hFY4OChUZSSq0AVjR2eW9SSi0BlgCMGzdO+ayg/v0BCDkRTEbGh0RHX+Gzok7jdMJ558HJk7B0KVx2WeuV3ZBNm/TN66+oZ3vcfLO+W9Ybb8Cf/lT3cidOwIoV+i5WGzbo432AhQvh6achIqJqWYdD3zpxxQr45S/hr3/Vt8asbsECeOgh/d7f/qbvqHbppfDyy9CrV9Vy330H3brp+9y21AUXwMMPQ0kJvP8+XHUVmNrpTQzvvBO18kuyf/tnjkTPJCuiH6ZnXsK8OxrTM69jPtYbjkFpqU5lZfpRKejcGaKjoUsX/bfVqu9omZ+vb1iXmwsFBWAY+uObzTqBvi1zQUFVKirS/7qAgJrJbNbrV0+lpfpfX1xc9eh263xVtV+/1QrBwRAUVPVYWqq/YtVTWRmEh0NYmH4MD9dlK6VvfVyR3G7x9ZDlAAAgAElEQVT9M3S5qh4ryq2oW0UdKupVkUpK9DawWPRnslj089JS/V5FKivT9QwLg9BQ/RgSossrKqrIL4Di4ptx//P0z3zjjfqupr5kKFX3PtYwjAKgtgUMQCmlwurN3DBigc+VUsNqee+fwFql1Hvlz/cD5yml0urLc9y4cWrz5s31LdJ8RUUQEkLG3WNJuGI/55yTjtkc6JuyTvXaa3rHGBcHR47AU0/B735X9U1syA8/wBdf6B2zt3dQv/2t3uFmZOhvcV1mzICEBH3ryoq9Q4XSUn1D9o8+0t/y+Hi4+mqYPVsHweefh65d4e9/hzlzdHC87DL43//guefg7rsb3hZ5eToY/PnPehs8/7y+taNh6O06bhzqg/+QkwNpaXpnFRmpd3iRkQ1vNo9H3yb30CFIWp9MXnB3Ch1mCgupTA6H/vE7HFV/K6XzrtixmEyn75A8HrDZ9M4tJEQ/Bgfr5St21GVlVX9X7GQq/jYMvV71VFqqv0pHkjwUFrX8OxEUpHdaZwLD0P/Xbt30ds3P11+P/Hy9vepjteqdutVa9Z2o2E0qpfMOCqqZ7PaqwFIRTNzu04Og1aq3YUGBrkt+vv7e2GxVeQUHQ2BgzeOfiq/+3Llwyy3N3SbGFqXUuAaXqy8otFQDQeES4FfoDuezgBeVUhMaytOnQQGgRw9Kpw5j4+1fEx//EdHRc3xXVoWyMhgwQH+D16zRR90ffKD/+//4h/7GNOTii/V9XF96CX71K+/VTSno2xeGDtVBpz4ffADz5umbo0+fXjOPhQvJ/ddysm5/gJJZV1DSs3/ljs1kgoCkfQQ++ScCD+4gYOb52HdvwZJ5Astrr2CdcwkWi45Je/bA7t36cc8efSToctVMhnITVJpDUFkuwSEmgvpEo/bs4UTkUE4Uh1FaenrVTSYdGDp10j/MgAD9wwwM1D/II0fg8GFqXTcwsGpHfuq6AQF6/YogUPFoMtVMFUfIRUVVqbBQ52+3669ARQoI0K9V39l4PFXrVCSLBWJjy5NjH32WPEA0GaiJ5+B+/EncylR5JGy310wAWVmQnq63e3q63qmGhuqTuYoUGqqXdburdooej94eoaFVKThYv1cRMCv+92539QtfdLLb9Xas2IaBgXpnWqFie1Y/uq7YZjYbxMToM5zq61RXVlb1vTs1VZy5dERtHhQMw3gPOA/oDJwE/gRYAZRSrxiGYQB/A2YCxcDNSqkG9/Y+DwpTp6I8br5/IoFOnaYzdOi/fVdWhVdegTvu0Dv1GTP0r+qRR2DxYpg6VTefREXVvX5urj7Hr/hmb98OAwd6p27btsGYMfpM5tZbK18uK9M7jawsyMkpP8q1lhE8bSIhU0YT8O6/2LcPfvwR/vfuAX78HxzES3VCn1TEx+sWoooju4rk8YCjWFG0/SDF2/ZT7LahMOg2ayzd4qOIidE7jrAwXffMTP05MjMhO7vqKL8iud16x9q/v47dAwboOBkZqXd2p54UtVsPPqgD++rVOvoJv9LmQcFXfB4Ubr0VVq5k/9rZnDy5lEmTMnzbhFRSovcyvXvrdu/qhylLl+r6jBmjm4fq8u67VW3vt90Ggwbp9vpT29/r4XbrneLJkzVT+or1ZGw8TPpF15ORZyM9XS9XcRTbGDEc56yuR5hw10R69DJVHkFXHPFWtNFWNrukZuPEijMgtEb7bni4DgTx8bppoFGSkvQ2OXxYt/s0YZt0SBXtH8LvNDYo+PkvpBb9+8OJE3QJupQ0zxKys78iOvpy35X32muQkgJvvnn6j/X66/Uh7G9+o4/YR4+uPY8VK6BHD90WX9F2/8wzcP/9NRZTSp9qHz+um2B274Zdu/TjgQNVnWrV2YyJdLEPJjrTRpcu+gQkKkrvlKOidIqM1DvtwkIo2p9C0R+foPhn8+g7exhnPXghPcIKMDb/BBGNbdf24lFsXBx8+21Vm42/k4AgGiBB4VTlI5DCt7qwh3QmI+MD3wUFh0N3ik6Zoke01OaGG+D3v4d//Ut39p6qsFA3Oy1cqHd611yDa8UnbPrjKr5MvIX/HelKRkZV80j1TjbD0M0gw4bpeNKjh26WqUy5+wk7azDGs03pp+gJX2yFw2vg9XAoPgBrfqw5qqgtSEAQolEkKJxqmO4TN825nLOBsk7vowYkYgwaDI89VnOYY0v98596GMx779V9BBcZCVdeqZuInnlG97pV99VXqJISkiZex4a3YOVKg6+/+Te5bhPm11yMHuuhd28TY8ZUHd1XtMcPHarbxOv0+HL9eHkTg+Ktt+ogBXqIaHx809YXQrQZ6VOozY4dsHcvjr3fkLv1daIKhmH76RBcdBF8+ql3yigqqjpM//bb+pddvRqmTYN336X0qus5dEg3+2zZAlvf3s3WjJ7kKn0k3q2bHog0q8tmLnzqQiIeuBMef7x5dRw7Vvcgb9zYtPUKCnQ/yI03wh//2LyyhRBeJX0KLTFyJIwcid0zl8SNn5MdMZT4z67XbfTr1ukRQS31j3/ocX6PPFLnImVlumXouw3nkxD0X/YtHEzijbp5HMBmU4xwlTJv0A7G/N9UJkyAESMqWkrGQfoV8OSTumlq2rSm1W/HDti6VZ+dNFVoqO6kkPZrIc44cqbQgAMHfsmJE28xacxRzEPH6LaXH39sWRv19u1w7rkwaZIe01+NxwPff68HHn3wgR4yabfDwIiTDD65jsG/upDBZ3di6FCIT/oc6xWXwpdfwsyZp5dTWAgTJujOhK1boWfPxtVv1y4dRMxmvV5MTPM/qxCiXWjsmYL0vjUgOvpqPJ5isorX6D6FzZv11Aa1ycrSF209/njNa9OrS0mBSy7R48TfeAPQI3fWrYN779WDZaZMgXfegVmzYOVK3Rqzc4uLD0zXsjjsOa67DkaNAuunK/Q4zbo6qUNC4MMPdYf23Ln61KMh27fD+efrZqN16yQgCOFvGjNBUntKPp0ltRYej0t9911XtXv3VXpytlGj9MRsp84sWVCgZ5ysuDDzpptOv79Afr5ePzRUZW/YrZYuVeraa/UU/qBnTr74Yj1Vf0FBLZWZNUvfEMXp1FMrR0YqdcMNDX+I//xHF3DnnfUv99NPOs/evfVN0IUQHQbeniW1vaTWDgpKKbV//51q3bpA5XIV6hufQM27WpWU6DtNmc1Kffyxvo0iKHXhhUrl5ellnE7luXiW2mQ6W910YUrlDaqio/UNzZYv1zGjXuWzuKrPPlPq66/13x9/3LgPUXEzlXfeqf39H37Qc/nHxXn1xh5CiPZBgoIX5eSsU2vWoNLSyneoM2boI+rsbD2n/pVX6k355ptVK73xhp7rf8QIVZCQov553r/VaLYo0Pfi/sUv9D3Zm3QHxLIyfdOUOXN0BsHBjb+ZidOp1NSp+vZ/O3bos5ifftJ3yLrpJl2p/v319N1CiA6nsUFBRh81Qnj4uQQFDSYl5Tm6dr0e4+mndaP+Y4/pWcJWrNAzct50U9VKCxawzTGYJb/ZzdLBoRRwLSOij/OPxfpC5YqJxJrEatXDPF94QfclzJp1+nULdbFYYNkyPVR0yhTdz1DRx9Cli+4Leekl6N69GRUTQjRVbkkuR3KPcDT3KCcKT3Cy6CTpRemcLDpJVnEW47uP5/oR1zO8y3CMVhzJJ6OPGikt7XX277+VESNW0anTdD2DaXlHMQ89pKesRg/4WbZMX5e2eTME2D1cbfmQn5+9k7O/WoRhbmHffkICDBmi/162TM9K2hQ//qiHwQ4bpkcmjR+v513qoMNHXR4XezP24nQ7Gdt9bFtXp0lcHhcmw4TJaP53Jq0gjR9Tf+THlB/ZlLqJY3nHsJqs2Mw2rGb92DW4K/NHzGf2oNnYzI2YkbcWHuVhw9ENvLnjTb469BUjuo5gZr+ZzOw/k8GdB7faTq3MXUZWcRZZjqzKx06BnZjUaxJWcx3TpjaT2+Pm++Tv+TjhY35I/gGb2UaoPZQQWwihtlCCrEE43U7K3GWUukspdZdS7CwmJT+FI7lHyC/NPy3PyIBIugR3IcwexrYT23B5XMRHx3Pd8Ou4bvh1xEbENru+MiGel3k8pWza1I+goEGMGvUtpKbqs4Vrr4W//pVih8FLL+nLAnJz9T739tth/nyIDHNXzY/sBT/OGsn+9L3csDoLo777G9QioyiDTSmbCLWH0imwU2UKtAR69YdbVFaE3WLHYmr8yWiJq4QDWQdIyEzAZJjoG9mXvpF9iQho3BQZJa4S9mfuZ8fJHWw+vpnNxzez7cQ2Slx6bo9LB17K8zOep3+n/s36TBV+SP6Bv//0d1weFwGWgMpkM9tweVyUukordwKlrlKcHicujwunWz+6PC6sZiuBlkACrYH60RJIQVkBJwpPVB41ZhZnEmwNZkj0EIZ0HsLQ6KEMjR6K3WznROEJ0grTSCtII60wjfzSfNzKjdvjxuVx4VZuUvJTOJZ3DACrycrIbiMZGDUQt8dNmbsMp0fvsPZm7CUlP4XooGhuGnkTt425jUGdB+H2uDmcc5g96XvYk7GHlPwUugZ3pWdYT3qG9aRXeC8sJgvv736ft3a8RVJuEqG2UGb0n8Ge9D3sy9wHQJ/wPlzY90JsZhv5pfnkleaRV5JHYVkhPcJ6MLSz/lwVnxMgszizcseeWZxJWmEaqfmppBaUp/xUCssKcavyz+tx41b6c9UmIiCCSwZcwmWDLmNm/5mE2kPJK8ljV/oudp7cyc6TO0nKTaKwrJCisiKKnEUUlulZH/tG9qV/p/4M6DSA/p36YzPb+OLAF3x64FMyizOxm+1M7DkRwzAoKC2gsKxQ5+MswmqyYrfYsZlt2M12AiwB9AzrSWxELH3C++jHiD7EhMQQHRxdIyhnFGWwfO9ylu5ayvfJ3wPwwLkP8Pi05l2MKkHBB5KTn+Pw4d8xZsz/CAsbD04nTqy88YY++D5+XI82feABOPts3xx8/3vXv7n54wWUeZzMGTyHNy57o8GdZrGzmE/3f8q7O9/lq0Nf4Vanz3xnM9sItZUf5ZQf7XQO6szCMQu5dOCljQ4Ye9L38OzGZ1m6cykAA6IGMLjzYIZ0HsLgzoOxmCx6x1CSV7lzSMpNIiEzgaTcJDzKc1qekQGRxEXG0T20O+H2cMLsYZWPRc4i9mTsYU/6Hg7nHK5cP9gazJiYMYzrPo5x3ceRnJfMYxseo8xdxv9N/D8enPwgoXbdhpdelM7nBz7nk/2fsDdjL1cMvoKfj/s5fSP71qjH4ezD3PftfSzfu5yowCg6B3WmxFVSmUrdpVhMFuxmO3aLHbtZ7wxsZhsWkwWr2YrVZMVsMuN0O3G4HDicjsrHUHso3UK60TW4a+VjTkkOezP2sjdjL6kFqadtm1BbKDGhMYTbw7GYLJhNZv1omIkOjuasHmdxVo+zGB0zmgBLQK3/M7fHzdeHv+a1ba/x6f5PcXlc9IvsR0p+CqXuqhtIRAVGke3IRp1y3y0Dg2l9p7Fg5AIuH3I5QdYgAI7mHmXV4VV8eehL1h1Zh8kwER4QXvm/C7YFk5yXzP6s/XXuzKsLsgbRM6wnPUJ70COsB2G2sBqf12wyE2wNpnNQZ6KCoogKjCIqKIqknCQ+2f8Jnx/4nCxHVuXZUXJ+co3v2ICoAYTaQgm2BRNsDSbEFoLb4yYxN5FD2YdIyU+pXD7cHs4lAy/h8sGXM6PfjMrvkq8cyT3Cst3LGN99PNP6NvFC1HISFHzA5Spg06beRERMIz5+OcuX6ynqDx6Ec87RZwmTJ/umbKUUT3z3BA+ufpCpfaYya8AsHlz9IL3De/Ofuf9hTMyYGsu7PW7WHV3HOzvfYcXeFRSUFdAzrCfXD7+enw38GU63k2xHdmXKKcnRRznOQgpKCygoK+BA1gGO5R3jrB5n8fgFj9f5ZVRKseHYBp7+/mm+OPgFgZZAbh51M2H2MPZl7iMhM4FD2YdOC0Ymw0SYPYxeYb0YEj2EwVGD9WPnwRgYJOYkVqXcRE4WniSvNK8yqDg9TsyGmf6d+hPfJZ746HiGdRnG8C7DGRg1ELOp5o0O0grSuP/b+3lrx1vEhMRw08ib2HBsAz8k/4BC0SusF4M7D2Z10mrcys2MfjP4xbhfcE6vc3hiwxO8/NPL2Mw2fj/p9/z27N8SbKtv4ijvyyvJIyEzgTJ3GTGhMcSExHi9DicKT/D2jrf5IfkHvV2j44nvEs/Q6KGE2EJwup2kFaaRkp9CSn4KuSW5zOw/k97hvZtdpsvjIjEnkb0Ze0nITMBsmE/bsXcL6Ua4PbxFZ7Muj4sfkn/gk4RPSCtMY3iX4YzoOoKR3UbSI7RHg3kXO4tJzEkkrySP8T3GN7upra1IUPCRxMSH2LDhQ9544yfWrQsmPh6eeAJ+9rPTzwxcHhdvbHuDxzc8TogthFtG38L8EfPpEtylSWU63U5++cUveW3ba1w//Hr+Nftf2C12NiZv5OrlV5NRlMGLF7/IwjEL2ZOxh3d2vMPSXUtJLUgl1BbKVUOv4oYRNzA1dmqT2qedbidv7XiLxesWk5yfzPmx5/Pw1IcJsARwOPswh3N02n5iOztP7qRzUGfumnAXvxz/SzoH1bzhQZm7jMScRJRS+kg/IJxga3Czf+RKKUpcJZgME3aLvUnr/pjyI7/+6tf8L/V/jOo2issGXcZlgy5jVLdRGIZBan4q/9r2L17d+mrl0aHJMHHr6Ft55LxHiAmVC/rEmUeCgg8UFcGiRUW88IKVgK6JjLzrOc4ZE8XPBs7i7J5nV3ZkKaX4dP+n3P/t/ezL3MfEnhMB2JSyCYvJwqUDL+WW0bdwfuz5BFmD6twxuj1uThad5JZPbmHV4VU8NPkhFp+/uMbymcWZzP9wPqsOr6JPeB+O5h3FYrIws/9M5g/XnYeB1pbdJKjEVcKSLUt4fMPjpBel13ivR2gP+nfqz7z4edw06qbKpoP2zqM85Jfm19v05vK4WHlwJeuPruemkTcxvOvwVqyhEN4lQcHLPv4Yfv1rSE5WjLtlEXtjH8Vjsld2HIbZw5jebzpT+0xl2e5lfJ/8PYOiBvHEtCeYM3gOhmGwN2Mvb2x7g7d3vl25c7WarEQGRhIZEElkYCQe5akcNZFbkguAxWThnz/7J7eMrv2O3R7l4anvnmL1kdXMHjiba4ZdQ3RwtNe3QVFZER8nfEx4QDh9I/sSFxHX4oAjhGgdEhS8xOPRHcdPPQVDx2XR5ZZfsDZ9OcPC4MXzFzJ2yLN8k/gNXx78kpWHVnK84DjdQrqxaOoibh1za62jb5xuJ18e+pJ9GfvILcklpyRHJ0cOgG5PLW9LjQqM4pxe55xxwymFEO2LBIVm8igPK/auoMRVglkF8o8Xg/huTSAXzjnJ7h7/R5Yji0fPf5RZkdvIzV7JxInHsFp1E4RSikPZh+gR1uOMaUYRQvgHuZ9CM72/+32u+/C6qhf6AAvgG2BY8DC+nP8lo7qNorBwB5sz3icl5S/Exel7IhiGwYCoAW1RbSGE8AoJCtW4PW4Wr1/MwPDhON5cwclsB4sed3D2ZAduj5tJvSdVjvUOCRlJdPRckpOfISbmVgICmj8kTwgh2gsJCtUs272MhMwEgr9YTnDaANZ/CmedVffy/fo9Q1bWZxw+/Hvi45e1XkWFEMJH5CY75VweF4vXLybMMQK193K+/77+gAAQENCHXr3+QEbG++Tmrm+digohhA9JUCj33q73OJB1gPxPFvHgAyb6N3J6nN69f4/d3ouDB3+NqmX6CCGEOJNIUKD8LGHdYgJyRhFbOod77mn8umZzEP36PUNR0Q7S0v7lu0oKIUQrkKAALN25lEM5hyj5ahHPP2cQUPu8YXWKjr6a8PApJCU9iNOZ45tKCiFEK/D7oODyuHhk7aOYT47hgp6zmTOn6XkYhkH//n/F6cziyJFHvF9JIYRoJX4fFN7Z8Q5JeYdRaxbx1xeMZk93HRo6ipiYhaSm/o2ior3eraQQQrQSvw4KTreTP37zKKSO444Lf8awYS3LLy7uMSyWMBISbsbjcXqnkkII0Yr8Oih8efArUouTCNn6RxY/0vI74ths0QwcuISCgv+RlPRHL9RQCCFal18HhXe+/xacgTy6YAadOnknzy5driIm5naSk58iO/u/3slUCCFaiV8HhQ0pq+HYuSyY37SbtDSkf/+/EBQUz759N1BWdtKreQshhC/5bVBIL0rnJLvoWnwBEY27L3yjmc1BDB26DLc7j337bkLVct9hIYRoj/w2KHybuAaAc2KadxPshoSEDKNfv7+Qk7OK5OTnfVKGEEJ4m98GhQ+3roaScGaPH+2zMrp3/zmdO19JUtL95Of/5LNyhBDCW/w2KKxL/haOTGXyJN9NFGsYBoMGvYrNFsO+ffNxu4t9VpYQQniDXwaFo7lHyXAfJjTzAvr29W1ZVmskgwe/hcNxgMTE+3xbmBBCtJBfBoU1R3R/woToac2+grkpIiPPp0eP35Ca+hLZ2d/4vkAhhGgmnwYFwzBmGoax3zCMQ4ZhnHaYbBjGAsMwMgzD2F6ebvNlfSp8vvdbKIpm5pj41igOgL59nyAoaDD799+M05nbauUKIURT+CwoGIZhBl4GLgaGAtcahjG0lkXfV0qNKk+v+ao+FZRSrElaDUkXcO65rXCaUM5sDmTw4LcpLU3j0KHftFq5QgjRFL48U5gAHFJKJSqlyoBlwGU+LK9RDmQdINt1HEvyBYwZ07plh4WNp0+fhzh58m0yMj5s3cKFEKIRfBkUegDJ1Z6nlL92qisNw9hpGMZywzB61ZaRYRi3G4ax2TCMzRkZGS2q1LdJ3wIwMmwaNluLsmqWPn0eJCRkLAcO/FyudhZCtDtt3dH8GRCrlBoB/Bd4q7aFlFJLlFLjlFLjoqOjW1Tgfw+thrzeTBvj42FHdTCZrAwZ8jYuVwF7916H213SJvUQQoja+DIopALVj/x7lr9WSSmVpZQqLX/6GjDWh/XBozysTlwDiRcw6ZzW6084VXDwUAYNepXc3NXs3TtPptkWQrQbvgwKPwEDDMOIMwzDBlwDfFp9AcMwYqo9nQ3s82F92HFiB/mubEiaxjnn+LKkhnXrdgMDBrxMVtanJCTchFLutq2QEEIAPrucVynlMgzjV8AqwAy8rpTaYxjGYmCzUupT4NeGYcwGXEA2sMBX9QFYnbQagL6m8+nc2ZclNU6PHr/E7S4gMfE+zOZgBg5cgtEaF04IIUQdfDfHA6CUWgmsPOW1h6v9fT9wvy/rUN3qpNWYcgYxdXRt/d1to3fvP+ByFXDs2OOYzaH06/ecBAYhRJvxaVBoT5xuJ2uPrMdz6AYmXd7WtakpLu5R3O4CUlL+gtkcSlzcI21dJSGEn/KboPDT8Z8odhVC4jQmTWrr2tRkGAb9+/8Ft7uAo0cXY7FE0qvX3W1dLSGEH/KboFBUVkSnslF48qYycGBb1+Z0hmFi4MAluFz5HD78f1gs4cTE3NzW1RJC+Bm/CQoX9buILh9uo/8YMLX11Rl1MJksDB26lF27Cti//zYsljCio69s62oJIfxIO909el9WFiQk0OZDURtiMtkZNuxDwsImsnfvtWRnf93WVRJC+BG/CQobN+rH9tafUBuzOZjhwz8nKGgIu3dfTl7eD21dJSGEn/CboNC3L/z+9zB+fFvXpHGs1khGjvwau707O3ZcyIkTb7d1lYQQfsBvgsLQofDUUxAY2NY1aTybrSujR39HWNhZJCTcxP79t8tcSUIIn/KboHCmstm6MmLEf+nd+37S0l5l27ZzcDgS27paQogOSoLCGcBkstC3758ZNuwzSkqS2LJlLBkZH7d1tYQQHZAEhTNI584/Y+zYrQQE9GPPnsvZvfsqSkpS2rpaQogORILCGSYwMI4xY34gLu7PZGd/wU8/DSE5+S94PK62rpoQogOQoHAGMpls9OlzP+PH7yU8fAqHD9/Dli3jyMvb1NZVE0Kc4SQonMECA+MYPvxz4uNX4HJlsX37ZE6ceKetqyWEOINJUDjDGYZBdPQVjB+/m/DwKSQk3MixY0+jlGrrqgkhzkASFDoIiyWcESNW0qXLNSQm/oHDh+9BKU9bV0sIcYbxmwnx/IHJZGfIkKXYbN1ISXmBsrITDB78JiaTva2rJoQ4Q0hQ6GAMw0S/fs9js3UnMfH3lJQcpWvX+URETCUoaKjc1U0IUS8JCh2QYRj07n0vdnt3EhPv4+DBOwGwWqMJD59Cp04X0bXrTZjNAW1cUyFEe2OcaR2S48aNU5s3b27rapwxlFKUlCSRm7uW3Nx15OaupbT0GHZ7H/r2fZIuXebJ2YMQfsAwjC1KqXENLScdzR2cYRgEBvYlJuYWhgx5i7PPPsrIkd9isUSwb9+1bNt2Dnl5G9u6mkKIdkKCgh+KjLyAceO2MGjQ65SUHGXbtnPYvfsqTp58j9LS1LaunhCiDUmfgp8yDDMxMTcTHT2X5ORnSEl5gczMFQAEBPQlImIK4eFTiIg4n8DA2LatrBCi1UifggDA43FRVLST3Nz15OWtJy9vA05nJqCDRGTkBURETCMy8nxstq5tXFshRFM1tk9BgoKolVKK4uK95OSsJjd3NTk5a3C78wAIDh5OZOQ0IiMvJDx8ChZLaI113e5iXK4cbLYYDENaKIVoDyQoCK9Syk1BwVZycr4lN/dbcnM3oFQphmEhNHQcYFBWdhKnMx23uxCA0NAJDBz4CqGho9u28kIICQrCt9xuB/n5P5CT8y15ed9hMtmxWrtgs3XFau2CYZhJTn4WpzOTnj1/Q2zsI6edUQghWo8EBdHmnM4ckpIe4Pjxf2K396B//xfp3HlOrddFKOWhtDSZoqJ9FBfvxe0uwDCsGIYNk8mKYVgJCRlJePikNvgkQpz5GhsUZPSR8BmrNZKBA/9B1643ceDAz9mz5wpMpiAslohqKRynM4Oion14PEUN5hkRMY24uMWEh5/TChVoldUAAAwXSURBVJ9ACP8jZwqiVXg8Lk6efIuion24XLnVUg5WaxRBQUMIDh5KUNBQgoKGYLV2QiknHo8TpcrweEpJT3+fY8eewOlMp1OnmcTGPkJY2IQGy3a7iwEwm4MasWwRDsfh8nQIh+MwLlcu3bv/gsjI81q6GYRoM9J8JDokt7uI1NS/c+zYU7hcWQQHD8NkCixvarJgGFbAjdOZhdOZhcuVjcdTApgJDz+XqKhLiIqaVTk5oMdTRn7+RrKzV5GdvYrCwq01yrNYOmEYJpzOTDp1upi+fZ8kJGREm3x2IVpCgoLo0FyuAlJT/0Z+/kaUcqKUq/yswolhmLBYorBao7BaO2GxROF255GV9SVFRTsAsNv7EBQ0mPz873G7CzEMC2FhZxMZOY2goMEEBvYnIKAfVmsEbreD1NS/cezYn3G58uja9Ubi4hYTENC7jbeCEI0nQUGIWpSUpJCdvZKsrJU4HIeIiJhMZOQMIiMvwGIJq3ddpzOHY8eeICXlRZRyYbf3wGbrVp5iyh+jsVo7VyZ9pmEFKn5n+tFksmMyBWMy2Ss73pVSuFx5lJWlUVZ2grKyE3g8jvL1DMDAMAwsliiCg4cSEBB72nUgSrlxOBIpKtqDYRhEREzDYgnx3gYUZywJCkL4SEnJMdLSXqWk5FiNHbjTmdGM3EyYzUGYTIG43QXlTV2NXNMUQFDQYIKChgAmiov3UFycUCMPw7ARGTmNqKjZdO58KXZ7jybVzu0upqBgK/n5m3A4DhEQ0JvAwAHlqX+NgKOUQikXSrkwmwObVI7wPQkKQrQyj8eJy5Vd3p+RWZ6yUMoNUGMorsdTittdjMdThNtdjNtdhNkcgt0eU37Woc88zOZgQJXfc1unsrK08qG7evhuUdFeQBEcHE9QUDzBwTq53UVkZX1GZuanlJQcBvSUJWZzCCZTIGZzICZTRQqolux4PA7y8/9HUdFOlHIBYLFE4nLl1PjMVmtnlPLg8ZSUByN9C1ibrRvBwcOrpaEo5cLpzMblyilPubjdxShVWr5+KR5PGVZrZwID4wgI6EtgYF8CAuJOGySgz6pyKSk5Up6OUlJyBIslnMjICwkLOwuTyVbr/0kphcdTWv7/qEqGYe7Q08hLUBBCABVTluwjM/NTiop24HY78Hiqkn5eUm3nrDvmQ0PHEhZ2FmFhEwkLOwubrSsuV2H5qKyDOBwHKSk5hslkrRFUwKC4+ABFRbsoLt5b79mPYdjLm9Ls5U1pVsrK0msZnmw+9VNREYAqlzCHlI8082AyBRMRMZXIyAux2brhcByguHg/xcUHcDgO4HYX1FEfW43gqOtkwzCsldfLmEyBWK3R2GxdKy/WtFgicLvzcblyygNfNm53EQEBceWj6oYQGDgQszkAj8eJw3GoPKjvo7j4IIZhwWIJxWwOw2wOxWIJKw/WFXWwYzIFEBAQ2+wJKiUoCCHanO7jOERx8X5MJjsWSyQWSyRWayfM5nBMptMvlVJK4XRmUFKShMORSElJUuWw4uoslojyM4o4AgJiy89k8sjNXUtOzjfk5HyDw7G/fGkDu703QUH/397dxchV1nEc//66a7e7Xe0LVFLbTVta0lKTUpBUEDTYRlOJES5qfEFCDAk3NYHERGh8i1zpjdULohBBqzZKqBSbXoiwkCZc0LJAgb5YWaGmS6hbtS+0sm/Tvxfn2el0unZn187OnJnfJ5nsOc85O3n+7Zn9z3nOOf9nOR0dy5k+fX5qP1s8C8tuVhgsO3MZOO/W6IhhCoX/MDzcz9BQP4XCqQv6lf2Bn8O0ae0MDvZxLnlNo63tIwwNHS2efQFMn74ACAqFU8USMf9LV9f9LF36w3H/3cfipGBmTW9g4AgjIydob19WleschcL7DA/3MzJyktbWWbS2zqWlpbM4DFUoDKSzlIOcOXOAgYG3aWtbSEfH1em1ouy6zFkKhdMUCu9RKLyfEtRgMUG1tXXR0bFsUn31E81m1vRmzOgCuqr2/i0t7bS0LLrI9hl0dq6q+NmW7HbqD417J1w1VbWusaT1kg5J6pX0wBjb2yQ9nrbvlrS4mv0xM7OLq1pSkNQCPAR8DlgJfEXSyrLd7gaOR8QyYDPwo2r1x8zMxlfNM4U1QG9EvBURQ8DvgdvK9rkN2JKWtwHr1Mj3hJmZ1blqJoUFwJGS9b7UNuY+kV2OPwlcVv5Gku6R1COp59ixyTwgZGZmlcjFXIkR8UhEXB8R18+bN6/W3TEza1jVTArvcP5l/4Wpbcx9JLUCs4B/VbFPZmZ2EdVMCi8BV0laImk68GVgR9k+O4C70vIG4LnI24MTZmYNpGrPKUTEiKRvAE+TPaP+WETsl/Qg0BMRO4BHgd9I6gX+TZY4zMysRnL3RLOkY8DfJ/nrlwP/vITdqSXHUp8aJZZGiQMcy6hFETHuRdncJYX/h6SeSh7zzgPHUp8aJZZGiQMcy0Tl4u4jMzObGk4KZmZW1GxJ4ZFad+ASciz1qVFiaZQ4wLFMSFNdUzAzs4trtjMFMzO7iKZJCuOV8a5nkh6T1C9pX0nbXEnPSHoz/ZxTyz5WQlKXpOclHZC0X9K9qT2PscyQtEfSaymWH6T2JakMfG8qCz/2RMF1SFKLpFcl7UzruYxF0mFJb0jaK6knteXxGJstaZukv0g6KOnGqYijKZJChWW869mvgPVlbQ8A3RFxFdCd1uvdCPDNiFgJ3ABsTP8PeYxlEFgbEdcAq4H1km4gK/++OZWDP05WHj4v7gUOlqznOZZPR8Tqkts383iM/RT4U0SsAK4h+7+pfhwR0fAv4Ebg6ZL1TcCmWvdrgjEsBvaVrB8C5qfl+cChWvdxEjH9EfhM3mMBOoBXgI+TPVjUmtrPO+7q+UVWm6wbWAvsBJTjWA4Dl5e15eoYI6sD9zbpuu9UxtEUZwpUVsY7b66IiHfT8lHgilp2ZqLSLHvXArvJaSxpuGUv0A88A/wNOBHnZmXP03H2E+BbnJtl/jLyG0sAf5b0sqR7UlvejrElwDHgl2lI7xeSZjIFcTRLUmhokX1tyM1tZJI6gT8A90XEqdJteYolIgoRsZrsW/YaYEWNuzQpkj4P9EfEy7XuyyVyc0RcRzZcvFHSp0o35uQYawWuA34WEdcCZygbKqpWHM2SFCop4503/5A0HyD97K9xfyoi6QNkCWFrRDyZmnMZy6iIOAE8TzbEMjuVgYf8HGc3AV+QdJhshsS1ZOPZeYyFiHgn/ewHtpMl7LwdY31AX0TsTuvbyJJE1eNolqRQSRnvvCktO34X2fh8XUtTrT4KHIyIH5dsymMs8yTNTsvtZNdGDpIlhw1pt1zEEhGbImJhRCwm+2w8FxF3kMNYJM2U9MHRZeCzwD5ydoxFxFHgiKTlqWkdcICpiKPWF1Sm8MLNrcBfycZ9v13r/kyw778D3gWGyb5B3E025tsNvAk8C8ytdT8riONmstPd14G96XVrTmNZBbyaYtkHfC+1XwnsAXqBJ4C2Wvd1gnHdAuzMayypz6+l1/7Rz3pOj7HVQE86xp4C5kxFHH6i2czMippl+MjMzCrgpGBmZkVOCmZmVuSkYGZmRU4KZmZW5KRgNoUk3TJahdSsHjkpmJlZkZOC2RgkfS3Nl7BX0sOp+N1pSZvT/AndkualfVdLelHS65K2j9a4l7RM0rNpzoVXJC1Nb99ZUid/a3rS26wuOCmYlZF0NfAl4KbICt4VgDuAmUBPRHwU2AV8P/3Kr4H7I2IV8EZJ+1bgocjmXPgE2VPpkFWHvY9sbo8ryWoPmdWF1vF3MWs664CPAS+lL/HtZIXHzgKPp31+CzwpaRYwOyJ2pfYtwBOp/s6CiNgOEBEDAOn99kREX1rfSzZXxgvVD8tsfE4KZhcSsCUiNp3XKH23bL/J1ogZLFku4M+h1REPH5ldqBvYIOnDUJzfdxHZ52W0auhXgRci4iRwXNInU/udwK6IeA/ok3R7eo82SR1TGoXZJPgbilmZiDgg6Ttks3dNI6tOu5FsopM1aVs/2XUHyEoY/zz90X8L+HpqvxN4WNKD6T2+OIVhmE2Kq6SaVUjS6YjorHU/zKrJw0dmZlbkMwUzMyvymYKZmRU5KZiZWZGTgpmZFTkpmJlZkZOCmZkVOSmYmVnRfwHs40XX9hA0KAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.9189 - acc: 0.7697\n",
      "Loss: 0.9188817410825569 Accuracy: 0.7696781\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2099 - acc: 0.3437\n",
      "Epoch 00001: val_loss improved from inf to 1.82124, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv_checkpoint/001-1.8212.hdf5\n",
      "36805/36805 [==============================] - 247s 7ms/sample - loss: 2.2098 - acc: 0.3437 - val_loss: 1.8212 - val_acc: 0.4298\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3053 - acc: 0.5942\n",
      "Epoch 00002: val_loss improved from 1.82124 to 0.99661, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv_checkpoint/002-0.9966.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 1.3052 - acc: 0.5942 - val_loss: 0.9966 - val_acc: 0.7116\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9897 - acc: 0.6999\n",
      "Epoch 00003: val_loss improved from 0.99661 to 0.89248, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv_checkpoint/003-0.8925.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.9896 - acc: 0.7000 - val_loss: 0.8925 - val_acc: 0.7298\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8332 - acc: 0.7510\n",
      "Epoch 00004: val_loss improved from 0.89248 to 0.77262, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv_checkpoint/004-0.7726.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.8331 - acc: 0.7510 - val_loss: 0.7726 - val_acc: 0.7764\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7263 - acc: 0.7817\n",
      "Epoch 00005: val_loss improved from 0.77262 to 0.68957, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv_checkpoint/005-0.6896.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.7262 - acc: 0.7817 - val_loss: 0.6896 - val_acc: 0.8029\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6499 - acc: 0.8077\n",
      "Epoch 00006: val_loss improved from 0.68957 to 0.65017, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv_checkpoint/006-0.6502.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.6498 - acc: 0.8077 - val_loss: 0.6502 - val_acc: 0.8134\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5890 - acc: 0.8232\n",
      "Epoch 00007: val_loss did not improve from 0.65017\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.5890 - acc: 0.8232 - val_loss: 0.7710 - val_acc: 0.7820\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.8369\n",
      "Epoch 00008: val_loss improved from 0.65017 to 0.59080, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv_checkpoint/008-0.5908.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.5427 - acc: 0.8369 - val_loss: 0.5908 - val_acc: 0.8344\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4923 - acc: 0.8536\n",
      "Epoch 00009: val_loss improved from 0.59080 to 0.58457, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv_checkpoint/009-0.5846.hdf5\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.4925 - acc: 0.8536 - val_loss: 0.5846 - val_acc: 0.8402\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4548 - acc: 0.8636\n",
      "Epoch 00010: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.4550 - acc: 0.8635 - val_loss: 0.6900 - val_acc: 0.8143\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4246 - acc: 0.8721\n",
      "Epoch 00011: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.4246 - acc: 0.8721 - val_loss: 0.6619 - val_acc: 0.8185\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8861\n",
      "Epoch 00012: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.3789 - acc: 0.8861 - val_loss: 0.7712 - val_acc: 0.7959\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3550 - acc: 0.8909\n",
      "Epoch 00013: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.3550 - acc: 0.8909 - val_loss: 0.6988 - val_acc: 0.8272\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3159 - acc: 0.9021\n",
      "Epoch 00014: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.3158 - acc: 0.9021 - val_loss: 0.6482 - val_acc: 0.8355\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9108\n",
      "Epoch 00015: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.2882 - acc: 0.9107 - val_loss: 0.7037 - val_acc: 0.8272\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9153\n",
      "Epoch 00016: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.2710 - acc: 0.9152 - val_loss: 0.6857 - val_acc: 0.8258\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9215\n",
      "Epoch 00017: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.2526 - acc: 0.9215 - val_loss: 0.6493 - val_acc: 0.8330\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9299\n",
      "Epoch 00018: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.2230 - acc: 0.9299 - val_loss: 0.6547 - val_acc: 0.8374\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9295\n",
      "Epoch 00019: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.2201 - acc: 0.9295 - val_loss: 0.6987 - val_acc: 0.8386\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9383\n",
      "Epoch 00020: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1990 - acc: 0.9383 - val_loss: 0.6016 - val_acc: 0.8500\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9389\n",
      "Epoch 00021: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1907 - acc: 0.9389 - val_loss: 0.8306 - val_acc: 0.8202\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9465\n",
      "Epoch 00022: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1701 - acc: 0.9465 - val_loss: 0.6585 - val_acc: 0.8421\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9490\n",
      "Epoch 00023: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1595 - acc: 0.9489 - val_loss: 0.6162 - val_acc: 0.8560\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9508\n",
      "Epoch 00024: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1548 - acc: 0.9508 - val_loss: 0.8553 - val_acc: 0.8178\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9525\n",
      "Epoch 00025: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1466 - acc: 0.9525 - val_loss: 0.5983 - val_acc: 0.8602\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9559\n",
      "Epoch 00026: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1364 - acc: 0.9559 - val_loss: 0.6964 - val_acc: 0.8488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9576\n",
      "Epoch 00027: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1283 - acc: 0.9576 - val_loss: 0.7019 - val_acc: 0.8418\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9595\n",
      "Epoch 00028: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1252 - acc: 0.9595 - val_loss: 0.7135 - val_acc: 0.8477\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9648\n",
      "Epoch 00029: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1140 - acc: 0.9648 - val_loss: 0.6629 - val_acc: 0.8556\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9615\n",
      "Epoch 00030: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1217 - acc: 0.9616 - val_loss: 0.7031 - val_acc: 0.8446\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9654\n",
      "Epoch 00031: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1087 - acc: 0.9654 - val_loss: 0.8026 - val_acc: 0.8251\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9654\n",
      "Epoch 00032: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1094 - acc: 0.9653 - val_loss: 0.7393 - val_acc: 0.8416\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9663\n",
      "Epoch 00033: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1069 - acc: 0.9663 - val_loss: 0.7074 - val_acc: 0.8498\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9689\n",
      "Epoch 00034: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0993 - acc: 0.9689 - val_loss: 0.6328 - val_acc: 0.8623\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9710\n",
      "Epoch 00035: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0926 - acc: 0.9710 - val_loss: 0.6928 - val_acc: 0.8444\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9719\n",
      "Epoch 00036: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0924 - acc: 0.9719 - val_loss: 0.8678 - val_acc: 0.8337\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9727\n",
      "Epoch 00037: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0857 - acc: 0.9727 - val_loss: 0.8088 - val_acc: 0.8428\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9702\n",
      "Epoch 00038: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0940 - acc: 0.9702 - val_loss: 0.6811 - val_acc: 0.8665\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9748\n",
      "Epoch 00039: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0785 - acc: 0.9748 - val_loss: 0.7287 - val_acc: 0.8567\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9759\n",
      "Epoch 00040: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0770 - acc: 0.9759 - val_loss: 0.7114 - val_acc: 0.8505\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9747\n",
      "Epoch 00041: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0804 - acc: 0.9747 - val_loss: 0.6976 - val_acc: 0.8577\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9770\n",
      "Epoch 00042: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0749 - acc: 0.9770 - val_loss: 0.7063 - val_acc: 0.8581\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9752\n",
      "Epoch 00043: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0777 - acc: 0.9751 - val_loss: 0.7356 - val_acc: 0.8584\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9718\n",
      "Epoch 00044: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0901 - acc: 0.9718 - val_loss: 0.7500 - val_acc: 0.8484\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9802\n",
      "Epoch 00045: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0645 - acc: 0.9802 - val_loss: 0.6819 - val_acc: 0.8640\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9807\n",
      "Epoch 00046: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0657 - acc: 0.9807 - val_loss: 0.7920 - val_acc: 0.8449\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9746\n",
      "Epoch 00047: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0825 - acc: 0.9745 - val_loss: 0.8959 - val_acc: 0.8418\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9772\n",
      "Epoch 00048: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0711 - acc: 0.9771 - val_loss: 0.7563 - val_acc: 0.8600\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9769\n",
      "Epoch 00049: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0752 - acc: 0.9768 - val_loss: 0.7803 - val_acc: 0.8500\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9773\n",
      "Epoch 00050: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0718 - acc: 0.9773 - val_loss: 0.6998 - val_acc: 0.8635\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9805\n",
      "Epoch 00051: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 210s 6ms/sample - loss: 0.0624 - acc: 0.9805 - val_loss: 0.6934 - val_acc: 0.8698\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9811\n",
      "Epoch 00052: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0597 - acc: 0.9811 - val_loss: 0.8815 - val_acc: 0.8323\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9797\n",
      "Epoch 00053: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0647 - acc: 0.9797 - val_loss: 0.7286 - val_acc: 0.8630\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9812\n",
      "Epoch 00054: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0626 - acc: 0.9812 - val_loss: 0.7691 - val_acc: 0.8693\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9828\n",
      "Epoch 00055: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0564 - acc: 0.9828 - val_loss: 0.7292 - val_acc: 0.8649\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9778\n",
      "Epoch 00056: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0727 - acc: 0.9778 - val_loss: 0.7399 - val_acc: 0.8637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9738\n",
      "Epoch 00057: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0867 - acc: 0.9738 - val_loss: 0.8217 - val_acc: 0.8612\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9829\n",
      "Epoch 00058: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0543 - acc: 0.9829 - val_loss: 0.7618 - val_acc: 0.8630\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9843\n",
      "Epoch 00059: val_loss did not improve from 0.58457\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0519 - acc: 0.9843 - val_loss: 0.7129 - val_acc: 0.8707\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.6970 - acc: 0.7992\n",
      "Loss: 0.6969671004657805 Accuracy: 0.79916924\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2898 - acc: 0.3079\n",
      "Epoch 00001: val_loss improved from inf to 1.64585, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/001-1.6459.hdf5\n",
      "36805/36805 [==============================] - 253s 7ms/sample - loss: 2.2897 - acc: 0.3079 - val_loss: 1.6459 - val_acc: 0.4610\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3210 - acc: 0.5845\n",
      "Epoch 00002: val_loss improved from 1.64585 to 1.00206, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/002-1.0021.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 1.3210 - acc: 0.5845 - val_loss: 1.0021 - val_acc: 0.6951\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0078 - acc: 0.6894\n",
      "Epoch 00003: val_loss improved from 1.00206 to 0.87066, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/003-0.8707.hdf5\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 1.0081 - acc: 0.6893 - val_loss: 0.8707 - val_acc: 0.7445\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8425 - acc: 0.7424\n",
      "Epoch 00004: val_loss improved from 0.87066 to 0.76934, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/004-0.7693.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.8425 - acc: 0.7423 - val_loss: 0.7693 - val_acc: 0.7780\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7322 - acc: 0.7824\n",
      "Epoch 00005: val_loss improved from 0.76934 to 0.64319, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/005-0.6432.hdf5\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.7322 - acc: 0.7824 - val_loss: 0.6432 - val_acc: 0.8120\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6447 - acc: 0.8095\n",
      "Epoch 00006: val_loss did not improve from 0.64319\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.6447 - acc: 0.8095 - val_loss: 0.7518 - val_acc: 0.7892\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5855 - acc: 0.8248\n",
      "Epoch 00007: val_loss improved from 0.64319 to 0.62076, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/007-0.6208.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.5856 - acc: 0.8248 - val_loss: 0.6208 - val_acc: 0.8239\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5199 - acc: 0.8439\n",
      "Epoch 00008: val_loss improved from 0.62076 to 0.55194, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/008-0.5519.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.5199 - acc: 0.8439 - val_loss: 0.5519 - val_acc: 0.8449\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4847 - acc: 0.8551\n",
      "Epoch 00009: val_loss improved from 0.55194 to 0.54913, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/009-0.5491.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.4847 - acc: 0.8551 - val_loss: 0.5491 - val_acc: 0.8423\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8702\n",
      "Epoch 00010: val_loss did not improve from 0.54913\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.4344 - acc: 0.8702 - val_loss: 0.5837 - val_acc: 0.8372\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8813\n",
      "Epoch 00011: val_loss improved from 0.54913 to 0.49260, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/011-0.4926.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.3949 - acc: 0.8813 - val_loss: 0.4926 - val_acc: 0.8728\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8933\n",
      "Epoch 00012: val_loss did not improve from 0.49260\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.3552 - acc: 0.8932 - val_loss: 0.6872 - val_acc: 0.8213\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.9005\n",
      "Epoch 00013: val_loss did not improve from 0.49260\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.3241 - acc: 0.9005 - val_loss: 0.5903 - val_acc: 0.8442\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3051 - acc: 0.9057\n",
      "Epoch 00014: val_loss did not improve from 0.49260\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.3051 - acc: 0.9057 - val_loss: 0.5407 - val_acc: 0.8537\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9171\n",
      "Epoch 00015: val_loss did not improve from 0.49260\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.2738 - acc: 0.9171 - val_loss: 0.6838 - val_acc: 0.8339\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9204\n",
      "Epoch 00016: val_loss did not improve from 0.49260\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.2516 - acc: 0.9204 - val_loss: 0.5406 - val_acc: 0.8691\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9234\n",
      "Epoch 00017: val_loss improved from 0.49260 to 0.48656, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/017-0.4866.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.2413 - acc: 0.9234 - val_loss: 0.4866 - val_acc: 0.8814\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9334\n",
      "Epoch 00018: val_loss improved from 0.48656 to 0.46413, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/018-0.4641.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.2110 - acc: 0.9334 - val_loss: 0.4641 - val_acc: 0.8861\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9365\n",
      "Epoch 00019: val_loss improved from 0.46413 to 0.42376, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/019-0.4238.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.2014 - acc: 0.9365 - val_loss: 0.4238 - val_acc: 0.8866\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9416\n",
      "Epoch 00020: val_loss did not improve from 0.42376\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1833 - acc: 0.9415 - val_loss: 0.4605 - val_acc: 0.8831\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9450\n",
      "Epoch 00021: val_loss did not improve from 0.42376\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1743 - acc: 0.9450 - val_loss: 0.6100 - val_acc: 0.8526\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9470\n",
      "Epoch 00022: val_loss did not improve from 0.42376\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1665 - acc: 0.9470 - val_loss: 0.4785 - val_acc: 0.8798\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9511\n",
      "Epoch 00023: val_loss did not improve from 0.42376\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1535 - acc: 0.9511 - val_loss: 0.4798 - val_acc: 0.8775\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9566\n",
      "Epoch 00024: val_loss did not improve from 0.42376\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1388 - acc: 0.9566 - val_loss: 0.6163 - val_acc: 0.8609\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9573\n",
      "Epoch 00025: val_loss did not improve from 0.42376\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1340 - acc: 0.9572 - val_loss: 0.4804 - val_acc: 0.8789\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9570\n",
      "Epoch 00026: val_loss did not improve from 0.42376\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1327 - acc: 0.9570 - val_loss: 0.5078 - val_acc: 0.8877\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9570\n",
      "Epoch 00027: val_loss did not improve from 0.42376\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1404 - acc: 0.9570 - val_loss: 0.5297 - val_acc: 0.8828\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9657\n",
      "Epoch 00028: val_loss did not improve from 0.42376\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1054 - acc: 0.9657 - val_loss: 0.5577 - val_acc: 0.8793\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9633\n",
      "Epoch 00029: val_loss improved from 0.42376 to 0.41323, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv_checkpoint/029-0.4132.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1133 - acc: 0.9633 - val_loss: 0.4132 - val_acc: 0.8991\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9684\n",
      "Epoch 00030: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1013 - acc: 0.9684 - val_loss: 0.5129 - val_acc: 0.8901\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9674\n",
      "Epoch 00031: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1003 - acc: 0.9673 - val_loss: 0.5859 - val_acc: 0.8772\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9660\n",
      "Epoch 00032: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.1050 - acc: 0.9660 - val_loss: 0.5289 - val_acc: 0.8887\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9728\n",
      "Epoch 00033: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.0838 - acc: 0.9728 - val_loss: 0.4500 - val_acc: 0.9015\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9675\n",
      "Epoch 00034: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.0996 - acc: 0.9675 - val_loss: 0.4220 - val_acc: 0.9061\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9711\n",
      "Epoch 00035: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.0901 - acc: 0.9711 - val_loss: 0.4791 - val_acc: 0.8880\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9785\n",
      "Epoch 00036: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0671 - acc: 0.9785 - val_loss: 0.4617 - val_acc: 0.9010\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9749\n",
      "Epoch 00037: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0803 - acc: 0.9749 - val_loss: 0.5971 - val_acc: 0.8796\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9720\n",
      "Epoch 00038: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0873 - acc: 0.9720 - val_loss: 0.5193 - val_acc: 0.8896\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9762\n",
      "Epoch 00039: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0754 - acc: 0.9762 - val_loss: 0.4870 - val_acc: 0.9008\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9778\n",
      "Epoch 00040: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0721 - acc: 0.9778 - val_loss: 0.5914 - val_acc: 0.8803\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9780\n",
      "Epoch 00041: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0704 - acc: 0.9780 - val_loss: 0.5895 - val_acc: 0.8791\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9800\n",
      "Epoch 00042: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0658 - acc: 0.9800 - val_loss: 0.4758 - val_acc: 0.9038\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9783\n",
      "Epoch 00043: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.0699 - acc: 0.9783 - val_loss: 0.4992 - val_acc: 0.8954\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9802\n",
      "Epoch 00044: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0615 - acc: 0.9802 - val_loss: 0.4540 - val_acc: 0.9012\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9830\n",
      "Epoch 00045: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0557 - acc: 0.9830 - val_loss: 0.5360 - val_acc: 0.8919\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9816\n",
      "Epoch 00046: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 204s 6ms/sample - loss: 0.0576 - acc: 0.9816 - val_loss: 0.7663 - val_acc: 0.8682\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9780\n",
      "Epoch 00047: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0698 - acc: 0.9780 - val_loss: 0.4896 - val_acc: 0.8996\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9810\n",
      "Epoch 00048: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 6ms/sample - loss: 0.0596 - acc: 0.9810 - val_loss: 0.4737 - val_acc: 0.9066\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9834\n",
      "Epoch 00049: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0528 - acc: 0.9834 - val_loss: 0.4672 - val_acc: 0.9110\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9842\n",
      "Epoch 00050: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 6ms/sample - loss: 0.0517 - acc: 0.9842 - val_loss: 0.5446 - val_acc: 0.8842\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9840\n",
      "Epoch 00051: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0503 - acc: 0.9839 - val_loss: 0.5296 - val_acc: 0.8961\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9798\n",
      "Epoch 00052: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0629 - acc: 0.9798 - val_loss: 0.4990 - val_acc: 0.9022\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9850\n",
      "Epoch 00053: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 6ms/sample - loss: 0.0485 - acc: 0.9849 - val_loss: 0.4536 - val_acc: 0.9092\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9760\n",
      "Epoch 00054: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0761 - acc: 0.9760 - val_loss: 0.5243 - val_acc: 0.8996\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9869\n",
      "Epoch 00055: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0439 - acc: 0.9869 - val_loss: 0.4849 - val_acc: 0.9089\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9868\n",
      "Epoch 00056: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0429 - acc: 0.9868 - val_loss: 0.4854 - val_acc: 0.9064\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9875\n",
      "Epoch 00057: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0408 - acc: 0.9875 - val_loss: 0.7641 - val_acc: 0.8735\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9830\n",
      "Epoch 00058: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0549 - acc: 0.9829 - val_loss: 0.5910 - val_acc: 0.8908\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9829\n",
      "Epoch 00059: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0558 - acc: 0.9829 - val_loss: 0.5223 - val_acc: 0.8998\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9872\n",
      "Epoch 00060: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0419 - acc: 0.9871 - val_loss: 0.5206 - val_acc: 0.9059\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9844\n",
      "Epoch 00061: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0514 - acc: 0.9844 - val_loss: 0.5840 - val_acc: 0.8898\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9882\n",
      "Epoch 00062: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0369 - acc: 0.9881 - val_loss: 0.5498 - val_acc: 0.9061\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9825\n",
      "Epoch 00063: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0561 - acc: 0.9824 - val_loss: 0.5665 - val_acc: 0.8926\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9818\n",
      "Epoch 00064: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0600 - acc: 0.9818 - val_loss: 0.5206 - val_acc: 0.9071\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9820\n",
      "Epoch 00065: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0592 - acc: 0.9820 - val_loss: 0.5417 - val_acc: 0.9047\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9873\n",
      "Epoch 00066: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.0422 - acc: 0.9873 - val_loss: 0.4918 - val_acc: 0.9136\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9874\n",
      "Epoch 00067: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.0418 - acc: 0.9874 - val_loss: 0.5055 - val_acc: 0.9133\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9900\n",
      "Epoch 00068: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0328 - acc: 0.9900 - val_loss: 0.5124 - val_acc: 0.9045\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9885\n",
      "Epoch 00069: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 6ms/sample - loss: 0.0370 - acc: 0.9885 - val_loss: 0.4908 - val_acc: 0.9115\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9883\n",
      "Epoch 00070: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.0382 - acc: 0.9883 - val_loss: 0.5155 - val_acc: 0.9029\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9897\n",
      "Epoch 00071: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0353 - acc: 0.9897 - val_loss: 0.6060 - val_acc: 0.8933\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9860\n",
      "Epoch 00072: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 6ms/sample - loss: 0.0433 - acc: 0.9860 - val_loss: 0.5503 - val_acc: 0.9038\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9864\n",
      "Epoch 00073: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.0426 - acc: 0.9864 - val_loss: 0.5948 - val_acc: 0.8926\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9890\n",
      "Epoch 00074: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 6ms/sample - loss: 0.0345 - acc: 0.9890 - val_loss: 0.5823 - val_acc: 0.9113\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9854\n",
      "Epoch 00075: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 6ms/sample - loss: 0.0455 - acc: 0.9854 - val_loss: 0.6105 - val_acc: 0.8980\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9898\n",
      "Epoch 00076: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.0336 - acc: 0.9898 - val_loss: 0.6608 - val_acc: 0.8933\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9856\n",
      "Epoch 00077: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0474 - acc: 0.9856 - val_loss: 0.4921 - val_acc: 0.9133\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9903\n",
      "Epoch 00078: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 0.0309 - acc: 0.9903 - val_loss: 0.7863 - val_acc: 0.8703\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9852\n",
      "Epoch 00079: val_loss did not improve from 0.41323\n",
      "36805/36805 [==============================] - 202s 5ms/sample - loss: 0.0476 - acc: 0.9852 - val_loss: 1.0555 - val_acc: 0.8493\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4VVXat+990nsHAgESek+ogkgRVLDAoIhgGXsbHR1fLIPOq4M6M2J77Y69Dop8oCgjSlEiirQk9E6oKYT0Xk5Z3x8rJ72R5HBC8tzXta9zzt5rr/XsffZev7We1QylFIIgCIIAYHK2AYIgCELbQURBEARBqEBEQRAEQahAREEQBEGoQERBEARBqEBEQRAEQahAREEQBEGoQERBEARBqEBEQRAEQajA1dkGnC2hoaEqMjLS2WYIgiCcV8THx2copcIaC3feiUJkZCRxcXHONkMQBOG8wjCME00JJ+4jQRAEoQIRBUEQBKECEQVBEAShgvOuTaEuzGYzSUlJlJSUONuU8xZPT08iIiJwc3NztimCIDiRdiEKSUlJ+Pn5ERkZiWEYzjbnvEMpRWZmJklJSURFRTnbHEEQnEi7cB+VlJQQEhIigtBMDMMgJCREalqCILQPUQBEEFqI3D9BEKAdiUJjWK1FlJYmY7OZnW2KIAhCm6XDiILNVkpZWSpKtb4o5OTk8Pbbbzfr3CuuuIKcnJwmh1+4cCEvvfRSs9ISBEFojA4jCoahL1UpW6vH3ZAoWCyWBs9dtWoVgYGBrW6TIAhCc+gwolB5qa0vCgsWLCAxMZGYmBgeffRRYmNjmTBhAjNnzmTQoEEAzJo1i5EjRzJ48GDee++9inMjIyPJyMjg+PHjDBw4kLvuuovBgwdz2WWXUVxc3GC6O3bsYOzYsQwbNoyrr76a7OxsAF5//XUGDRrEsGHDmDdvHgC//PILMTExxMTEMHz4cPLz81v9PgiCcP7TLrqkVuXw4YcoKNhRxxEbVmshJpMXhnF2l+3rG0Pfvq/We3zRokXs2bOHHTt0urGxsSQkJLBnz56KLp4fffQRwcHBFBcXM3r0aGbPnk1ISEgN2w/z5Zdf8v7773PdddexfPlybrrppnrTvfnmm3njjTeYNGkSTz31FE8//TSvvvoqixYt4tixY3h4eFS4pl566SXeeustxo8fT0FBAZ6enmd1DwRB6Bh0oJqCHXVOUhkzZky1Pv+vv/460dHRjB07llOnTnH48OFa50RFRRETEwPAyJEjOX78eL3x5+bmkpOTw6RJkwC45ZZb2LBhAwDDhg3jxhtv5D//+Q+urloAx48fz/z583n99dfJycmp2C8IglCVdpcz1Feit9nMFBbuxMOjB+7unRxuh4+PT8X32NhY1q1bx6ZNm/D29mby5Ml1jgnw8PCo+O7i4tKo+6g+vv/+ezZs2MDKlSv55z//ye7du1mwYAFXXnklq1atYvz48axevZoBAwY0K35BENovHaam4MiGZj8/vwZ99Lm5uQQFBeHt7c2BAwfYvHlzi9MMCAggKCiIX3/9FYDPP/+cSZMmYbPZOHXqFBdffDHPP/88ubm5FBQUkJiYyNChQ/nrX//K6NGjOXDgQIttEASh/dHuagr147iG5pCQEMaPH8+QIUO4/PLLufLKK6sdnz59Ou+88w4DBw6kf//+jB07tlXS/fTTT7n33nspKiqiV69efPzxx1itVm666SZyc3NRSvHggw8SGBjIk08+yfr16zGZTAwePJjLL7+8VWwQBKF9YSh1bnzsrcWoUaNUzUV29u/fz8CBAxs9Nz8/ATe3MDw9uzvKvPOapt5HQRDOPwzDiFdKjWosXIdxH4HdhdT6NQVBEIT2QocSBXBBKauzjRAEQWizdChRkJqCIAhCw3QoUQCTQ3ofCYIgtBc6lCgYhriPBEEQGqKDiYK4jwRBEBqiQ4lCW2po9vX1Pav9giAI54IOJQpSUxAEQWiYDiUKjmpoXrBgAW+99VbFb/tCOAUFBUydOpURI0YwdOhQvv322ybHqZTi0UcfZciQIQwdOpSvvvoKgNTUVCZOnEhMTAxDhgzh119/xWq1cuutt1aEfeWVV1r9GgVB6Bi0v2kuHnoIdtQ1dTa428pwVaUoFz/OakXimBh4tf6ps+fOnctDDz3E/fffD8DSpUtZvXo1np6efPPNN/j7+5ORkcHYsWOZOXNmk9ZD/vrrr9mxYwc7d+4kIyOD0aNHM3HiRL744gumTZvG3/72N6xWK0VFRezYsYPk5GT27NkDcFYruQmCIFSl/YlCQxiUz5ytyn+0DsOHD+fMmTOkpKSQnp5OUFAQ3bt3x2w288QTT7BhwwZMJhPJycmkpaXRpUuXRuP87bffuP7663FxcaFz585MmjSJbdu2MXr0aG6//XbMZjOzZs0iJiaGXr16cfToUR544AGuvPJKLrvssla7NkEQOhbtTxQaKNFbytIpLT2Bj88wDJN7qyY7Z84cli1bxunTp5k7dy4AixcvJj09nfj4eNzc3IiMjKxzyuyzYeLEiWzYsIHvv/+eW2+9lfnz53PzzTezc+dOVq9ezTvvvMPSpUv56KOPWuOyBEHoYHSoNgVHTp89d+5clixZwrJly5gzZw6gp8zu1KkTbm5urF+/nhMnTjQ5vgkTJvDVV19htVpJT09nw4YNjBkzhhMnTtC5c2fuuusu7rzzThISEsjIyMBmszF79mz+8Y9/kJCQ0OrXJwhCx8BhNQXDMLoDnwGd0f6a95RSr9UIYwCvAVcARcCtSikH5miOmz578ODB5Ofn061bN8LDwwG48cYbmTFjBkOHDmXUqFFntajN1VdfzaZNm4iOjsYwDF544QW6dOnCp59+yosvvoibmxu+vr589tlnJCcnc9ttt2Gz6et67rnnWv36BEHoGDhs6mzDMMKBcKVUgmEYfkA8MEspta9KmCuAB9CicAHwmlLqgobibcnU2RZLHsXFh/Dy6o+rq99ZX1N7R6bOFoT2i9OnzlZKpdpL/UqpfGA/0K1GsD8AnynNZiCwXEwchONqCoIgCO2Bc9KmYBhGJDAc2FLjUDfgVJXfSdQWjla0w96m0DZGNQuCILQ1HC4KhmH4AsuBh5RSec2M427DMOIMw4hLT09vgS0ugGMamgVBENoDDhUFwzDc0IKwWCn1dR1BkoGqa2NGlO+rhlLqPaXUKKXUqLCwsBZYJO4jQRCEhnCYKJT3LPoQ2K+U+r96gn0H3GxoxgK5SqlUx9lkrymI+0gQBKEuHDl4bTzwR2C3YRj2eSeeAHoAKKXeAVahex4dQXdJvc2B9lA5illqCoIgCHXhMFFQSv1GI3NJKN0f9n5H2VATXXlp/UnxcnJy+OKLL7jvvvvO+twrrriCL774gsDAwFa1SRAEoTl0qBHNYHchta77KCcnh7fffrvOYxaLpcFzV61aJYIgCEKbocOJgiNqCgsWLCAxMZGYmBgeffRRYmNjmTBhAjNnzmTQoEEAzJo1i5EjRzJ48GDee++9inMjIyPJyMjg+PHjDBw4kLvuuovBgwdz2WWXUVxcXCutlStXcsEFFzB8+HAuueQS0tLSACgoKOC2225j6NChDBs2jOXLlwPw448/MmLECKKjo5k6dWqrXrcgCO2PdjchXgMzZwNgtfbCMAxMZyGHjcyczaJFi9izZw87yhOOjY0lISGBPXv2EBUVBcBHH31EcHAwxcXFjB49mtmzZxMSElItnsOHD/Pll1/y/vvvc91117F8+XJuuummamEuuugiNm/ejGEYfPDBB7zwwgu8/PLLPPvsswQEBLB7924AsrOzSU9P56677mLDhg1ERUWRlZXV9IsWBKFD0u5EoTGaspZBazBmzJgKQQB4/fXX+eabbwA4deoUhw8friUKUVFRxMTEADBy5EiOHz9eK96kpCTmzp1LamoqZWVlFWmsW7eOJUuWVIQLCgpi5cqVTJw4sSJMcHBwq16jIAjtj3YnCg2V6AGKipJQyoqPj2Pn+PHx8an4Hhsby7p169i0aRPe3t5Mnjy5zim0PTw8Kr67uLjU6T564IEHmD9/PjNnziQ2NpaFCxc6xH5BEDomHa5NwRENzX5+fuTn59d7PDc3l6CgILy9vTlw4ACbN29udlq5ubl066ZnAvn0008r9l966aXVlgTNzs5m7NixbNiwgWPHjgGI+0gQhEbpcKLgiIbmkJAQxo8fz5AhQ3j00UdrHZ8+fToWi4WBAweyYMECxo4d2+y0Fi5cyJw5cxg5ciShoaEV+//3f/+X7OxshgwZQnR0NOvXrycsLIz33nuPa665hujo6IrFfwRBEOrDYVNnO4qWTJ0NUFJyErM5Cz+/GEeYd14jU2cLQvvF6VNnt11MtLb7SBAEob3Q4URBT5+tZKZUQRCEOuiAoiDTZwuCINRHhxMFmT5bEAShfjqcKMj02YIgCPXT4URBagqCIAj10+FEoXKdZueKgq+vr1PTFwRBqIsOKAriPhIEQaiPDicKjnAfLViwoNoUEwsXLuSll16ioKCAqVOnMmLECIYOHcq3337baFz1TbFd1xTY9U2XLQiC0Fza3YR4D/34EDtONzB3NgqrtQCTyRPDcGtSnDFdYnh1ev0z7c2dO5eHHnqI++/Xi8gtXbqU1atX4+npyTfffIO/vz8ZGRmMHTuWmTNnNjhTa11TbNtstjqnwK5rumxBEISW0O5Eoem03vQew4cP58yZM6SkpJCenk5QUBDdu3fHbDbzxBNPsGHDBkwmE8nJyaSlpdGlS5d646priu309PQ6p8Cua7psQRCEltDuRKGhEj3oBuaCggTc3bvh4RHeaunOmTOHZcuWcfr06YqJ5xYvXkx6ejrx8fG4ubkRGRlZ55TZdpo6xbYgCIKj6HBtCrr3kUFrz380d+5clixZwrJly5gzZw6gp7nu1KkTbm5urF+/nhMnTjQYR31TbNc3BXZd02ULgiC0hA4nCprWnz578ODB5Ofn061bN8LDdQ3kxhtvJC4ujqFDh/LZZ58xYMCABuOob4rt+qbArmu6bEEQhJbQ4abOBigo2IWLiz9eXpGtbN35jUydLQjtF5k6uwG0C0nGKQiCINSk44iC1QrFxWCz4Qj3kSAIQnug3YhCo26w3FzYuxdKSx2yTvP5zvnmRhQEwTG0C1Hw9PQkMzOz4YzNRU9vgdWK1BSqo5QiMzMTT09PZ5siCIKTaRfjFCIiIkhKSiI9Pb3+QKWlkJEBBw9idivEZivDw6P+kcUdDU9PTyIiIpxthiAITqZdiIKbm1vFaN96SUyE4cPh0085MGYDWVk/EBOTfG4MFARBOE9oF+6jJlE+NQRZWbi4+GK1FjjXHkEQhDZIxxGFgAAwjCqiUCiNq4IgCDXoOKJgMkFgYIUogBWbrdTZVgmCILQpOo4ogHYhZWfj4uIDIC4kQRCEGnQ8UaioKYgoCIIg1EREQRAEQajAYaJgGMZHhmGcMQxjTz3HJxuGkWsYxo7y7SlH2VJBUFA1UbDZCh2epCAIwvmEI8cpfAK8CXzWQJhflVJXOdCG6kibgiAIQoM4rKaglNoAZDkq/mZhFwXDGxBREARBqImz2xTGGYax0zCMHwzDGFxfIMMw7jYMI84wjLgGp7JojOBgsNlwKfcaiSgIgiBUx5mikAD0VEpFA28AK+oLqJR6Tyk1Sik1KiwsrPkpli9s75JrBsBqlTYFQRCEqjhNFJRSeUqpgvLvqwA3wzBCHZpo+VQXLnllgNQUBEEQauI0UTAMo4thGEb59zHltmQ6NFG7KOQWAyIKgiAINXFY7yPDML4EJgOhhmEkAX8H3ACUUu8A1wJ/MgzDAhQD85SjJyMqdx8Z2bmYwj1FFARBEGrgMFFQSl3fyPE30V1Wzx1VZ0qNkJlSBUEQauLs3kfnlvKagh6r4CsNzYIgCDXoWKLg6Qne3rKmgiAIQj10LFGAiqkuTCYfEQVBEIQadDxRqDIpnoiCIAhCdTqmKEibgiAIQp10TFGQmoIgCEKddDxRqJg+W9oUBEEQatLxRKGa+0hEQRAEoSodUxSKi3E1e2CzFaKUzdkWCYIgtBk6niiUD2BzK9CXbrUWOdMaQRCENkXHE4XyqS48Cr0AKCtLdaY1giAIbYoOKwqexbrGUFx8xJnWCIIgtCk6rCh4FOp1mouLDzvTGkEQhDZFxxOF8jYF1zwrLi5+UlMQBEGoQscThfKagpGdjZdXX6kpCIIgVKHjiYK/P7i4QHY2Xl59pKYgCIJQhY4nCoZRMapZ1xSOYbOZnW2VIAhCm6BJomAYxl8Mw/A3NB8ahpFgGMZljjbOYVSIQh/ASknJCWdbJAiC0CZoak3hdqVUHnAZEAT8EVjkMKscTfmkeN7efQHpgSQIgmCnqaJglH9eAXyulNpbZd/5R/n8R7qmIGMVBEEQ7DRVFOINw1iDFoXVhmH4AefvpEHl7iM3t07l3VKlpiAIggDg2sRwdwAxwFGlVJFhGMHAbY4zy8GUu48Mw5AeSIIgCFVoak1hHHBQKZVjGMZNwP8CuY4zy8EEB0NODlitMlZBEAShCk0VhX8DRYZhRAMPA4nAZw6zytGUD2AjNxcvrz6UlByXbqmCIAg0XRQsSikF/AF4Uyn1FuDnOLMcTPlUF/axCkpZpFuqIAgCTReFfMMwHkd3Rf3eMAwT4OY4sxyMvaZQMVZBeiAJgiBA00VhLlCKHq9wGogAXnSYVY6miijIWAVBEIRKmiQK5UKwGAgwDOMqoEQpdf63KWRnl3dL9ZWagiAIAk2f5uI6YCswB7gO2GIYxrWONMyhVGlT0N1SpQeSIAgCNH2cwt+A0UqpMwCGYYQB64BljjLMoVQRBQAvrz4UFOxwokGCIAhtg6a2KZjsglBO5lmc2/ZwdwdfX8jOBsDLqy8lJcew2SxONkwQBMG5NLWm8KNhGKuBL8t/zwVWOcakc0T5VBegawpKWSgtPYGXV28nGyYIguA8mtrQ/CjwHjCsfHtPKfVXRxrmcMqnugBdUwDplioIQhvmn/+En392eDJNdgEppZYrpeaXb9840qhzQjVR0GMVioqksVkQhDZIcTE8+SRs2ODwpBoUBcMw8g3DyKtjyzcMI6+Rcz8yDOOMYRh76jluGIbxumEYRwzD2GUYxoiWXMhZUz59NoC7e2fplioIQtvl4EFQCgYNcnhSDYqCUspPKeVfx+anlPJvJO5PgOkNHL8c6Fu+3Y2eX+ncUaVNoXK2VKkpCILQBtm3T38OHuzwpBzWg0gptQHIaiDIH4DPlGYzEGgYRrij7KmF3X2kFIBMoS0IQttl715wdYW+fR2elDO7lXYDTlX5nVS+79wQHAxlZVBQAIC39wCKixOxWArOmQmCIAhNYt8+LQju7g5PqqldUp2KYRh3o11M9OjRo3UijY7Wn+vXw8yZBAZO5sSJf5Cb+wshIVe2ThqCcJbYbJCfr5f7KCsDT0+9eXmBtzeYGijGmc363Lw8vZWV6X1mM1gsYBj6fJMJXFwgLAy6dNFDdgxDh0lNhZMndTzDhkHXrtXTOHFCt3UePVq5zzB0hdtiqdx8ffW59s3HpzK8UrqSfvq0Ti89XV9jYKDe7EOI0tPhzBltS9euEBWlt27ddN7o4qILzyaTjtO+5efrazh5Utubk1M97eJiyM2t3Ayj8h57eel4DaNyKy2FkhK9lZVpz3OnTnqzz5hjterNbNb33h632QwhIfpeh4WBm1vldaem6uNdulRuViskJ+stJQU8PKB7d+i+YQo9oi5l9G4YOrTlz1lDOFMUkoHuVX5HlO+rhVLqPXSXWEaNGqVaJfWpU/U/+tVXMHMm/v7jMZk8ycpaK6LQDjCbdcaVkQERETojca3ytNtsOrM4cULXzPfu1YWxkpLKFzgsTD8iQUGVGVZBgX6pT5+GtDSdARUX662kRL/U9szJZoOiIn1OYaE+7umpM3dvb21P1eP5+TojUfU84SZTZQYTGqrDZWdXbkVFzbtX3t7g768zYFuNRXa7dIFRo/S1//qrvl/1YTLpa3Jx0fejNXB11SJRNWM/Wzw8dOZux9MTAgIqN8OAzMzK/7Hqf6iUPt8uzq6uOsM+c6aiSbIWLi6Vcbu56WewZtiwMAgP12F37tTPktVaaV+3bloIc3Nh927F6awHIAsWfAHPPdf8e9EUnCkK3wF/NgxjCXABkKuUSj1nqbu5wTXXwJIlUFyMi5cXAQETyc5ee85MEDTFxbB7N8TH6xfEatWZlJ+fLmFmZMCpU3pLTdUvsbu7flnd3fVLZH9xrVY4cgQSE3WJ1Y7JpF80f38dX0ZG5UsI+mXv109nQIcP61JqQSOeRA8PbaM9k/f0rCxl2tP08dHp+vrq8KWlOvMuKtLC1amTLv36+uqtqgC5uVWWUu2l2/T0ys1kgj599DlBQZUZkf3e2TMxN7dKQbTZdEZnNus47AKXk6Mzqe7doUcPXWLesUP/J/HxOlMbPx4efhgmTdLtnfYSetV7bKesTGd09hJvSUn1excUVFk6DgvT4XNy9Jafr4+Hhen7YBhaNI8f11tKirbfXjq3WquX7L29oWdPfR09elSvpbQmZrO21zD0/26vuXh5VRch0M9iVpa+zk6danuBbDYtTC4u+tqrnb9rN2XRo0h+/Ws8r73KMRdTBUPVVyxpacSG8SUwGQgF0oC/U74Gg1LqHcMwDOBNdA+lIuA2pVRcY/GOGjVKxcU1GqxprFsHl14Ky5bB7NmcPPkSR48+yrhxSXh4nLvmjfONsjJdUrKXllNS9Mt64oT+TE+vdCNYrfqBr+q6qPrA22z6fHsGHRSkM7O8PJ0RgM7UIiL0C253Z5SV6Qyz6lZSojOp3r1hwADo319nLCkpla6E/HxdyrbXBCIidC+/vn11OlUpKdEvsj2zysnRGYw9M/P3r/3yC0Kr8+WXcMMNsGtXi3xHhmHEK6VGNRbOYTUFpdT1jRxXwP2OSr9JTJ6sc4avvoLZswkOvpSjRyE7ex1dutziVNPOFRaLdj0kJ+uSeFKS3tLTdWk6M1NvBQXV3SA1cXHRpczISIiJqSydurpW+pyVql46txMRASNHwogROuO3Z7RWq07P17dhX7qj8PSs9IkLgtPYt0+/YP36nZPkzouGZofh6grXXguffAIFBfj4DMXNrRNZWWvbnSikpMDvv8OmTbBli/6dlaVdEjWxN0KGhmofdr9+ulTs46MzaD8/6Ny5cgsP1xmnays/TS4uOl1B6NDs26f9hB4e5yS5ji0KAHPnwr//Df/9L8a8eQQFXUJ29jqUUhjngW+gsBA2boRDhypL9VlZlVt2duV+0M/VqFHaPxwcXLl17Vrey6G7zuidUTIXhNbGbDWTW5qLgYFhGJgME37ufriYXJxtWi1sSrfym4waL9/evedkJLMdEYWLLtJF3aVLoVwUzpz5gsLC3fj6DnO2dRXYbNqlY+9qt3On7k27ZYtu8LITEKBL90FBOrOPjNTfBwyAceO0a+ccdHVuMxSUFZCcl0xyfjLJecl4unoyo/8MPF09z6kdVpuVAxkHiAyMxMe9dVo+lVIk5SVxIOMABzIOcDL3JKkFqaQWpJJRlMHMfjN5fMLjeLt5NxpPakFqRTwHMg6QWZxJTOcYLoi4gJHhI/F28yYxO5FtydvYlrKN0wWn8ffwx9/DnwCPAC7qcRGTIic12XarzcqpvFMkZiWSnJ/MuIhx9A2pPTArMSuR30/9zrju4+gT3KfasRJLCWsS17Dz9E583H3w9/DHz92PgrIC4lPjiU+NZ+fpnZRaS6ud5+3mTUyXGEaGj2RE+AjMVrO+7swDHM48TCefTgzvMpzh4cMZET6CwWGDcXOpvSR9kbmIxKxE+oX0w8O1eik+LiWOt7a9xa60XUyJnMJV/a5ifI/xuJpqZ7nZxdm8G/8ub2x9g+7+3Vl14yqCvcr7upaW6p4Tc+Y0+d62FIc1NDuKVm1otvOXv8C778KZM5S457F5c3d6936J7t0fbt10mkhysi7979tXuR05op8POyaT9sNffDFMmaIz+5CQ1nfhOIsyaxlf7fmKT3Z+QohXCKO7jmZU11GMCB9BgGdAveeZrWY2ntrIqsOr+OHID+w5U3vqrWCvYG6LuY17Rt5D94DuxB6P5ftD3/Nj4o8EeATw2PjHmD1wdq3SpE3Z2HNmD7HHY4k9HsvW5K1c2P1CHr3wUUZ3G12vTauPrOaRtY+w58weTIaJfiH9iOkSQ3f/7pzKO8XR7KMcyz5GqbWUEeEjGNN1DKO7jWZiz4l08ulUK76U/BTu+/4+1h1dR6G5sGK/h4sH4X7hdPXriruLO7HHY+kZ0JPXpr/GzP4zMQyD3JJcfjnxCxtObOBw1mESsxI5mn2UYktlH1Jfd18CPQNJyksCqChd55ZqX6Onqyfd/LqRX5ZPXmkeJRbdyPTwuIf519R/4e5SvdRRZC5ie+p24lLiiEuNIz4lniNZRzDbzNXCXdDtAm4adhPT+0zn52M/89nOz9h4amPF8YGhA5nRbwaDOw1m1eFVfH/4ewrK6u4i5u/hz4jwEYwMH0nPgJ4oFEopbMrGidwTxKfGsz11e8X983L1on9of/oE9+F0wWl2nN5REbeXqxcjwkdwQbcLGNJpCPvS9/Hbqd+IT4nHbDPj6erJhd0v5OLIi+nk04kPt3/I1uSt+Lj5ENMlhq3JWzHbzAR6BjKx50S6+3enq19Xuvl1Iz41no+2f0ShuZCJPSeyOWkzA0MHsvaPawnzCdPd8oYNgy++gOsbbKZtlKY2NIsogHa2jx8Pn38ON93E1q0D8fDoSXT0j62bTj3k5ekBQWvX6m3/fr3fMKBXL11z7N+/eje7Xr2a5m+3KRs7T+/kUOYhkvOTSclPISU/hWJLMTZlw2qzolD4e/gT6hVKqHco4X7hzBsyD3+P2gkopUgrTMPDxQM/D786Sz41wydmJxKfoktuCakJlFpLifCPIMIvggj/CLr4diHUW6ft4+7D0r1LeXPrm6QWpNI3uC8Wm4VjOcf0PcFgdLfRXN7ncq7oewUjwkew58we1h9bz8/Hf+aX47+QX5aPm8mNCT0ncHHkxUQGRtLNrxvd/LtxMvck78a/y4oDK7DYLHi6elJiKcElpAhQAAAgAElEQVTL1YspUVM4knWEg5kH6R/SnwUXLaBHQA9+P/U7m5I2senUJrJL9CSKUYFRjOw6krWJa8ktzeXiyIuZP24+A0IH4O7ijoeLB0l5STz+0+OsPbqWXkG9mD92PhlFGexI28GO0ztIykuiR0APegX1IiowCleTK3EpcexM20mZtQwPFw8eGPMAj094vKLkuHzfcu7+792UWEq4PeZ2hnQawoDQAfQP7U9nn87VXJ6/HP+F+1fdz970vUyJmkKxuZityVuxKiuerp70Ce5D76De9ArqRe+g3gwIHcCA0AF09euKYRikF6azNXkrW5O3cqbwDCPCRzC62+haJefCskL+uu6vvLXtLUZ3Hc2Sa5cQFRjFxlMb+XD7hyzdu5Qisx5E0dWvKyPDRzIobBC9g3rTO7g3nXw6sfrIaj7f9Tk703ZWxDswdCA3R9/Mpb0uZeOpjaw8tJLY47FYbBbCvMO4esDVzB40m4k9J1JqKa0QKQ8XD6KComq7YWpgtVk5knUED1cPegT0qBbepmwcyTpCQmoCW5O3siV5CwmpCZRYSvBw8WBMtzFc1OMiBoYOJCE1gdgTsew8vROFon9If+4bfR+3RN9CgGcA+aX5rD26lpWHVrIteRsp+SkVz5GbyY0bht7A/HHzGdZ5GKuPrGbWV7PoFdSLdX9cR/iqDTBvnnYNDGuZ50JE4Wyw2bSfJToaVq7k8OEHSU39gPHjs3BxaX03Q0mJdv2sXw+xsRC3/wxqwHI89t/GpPGeXHKJrgEMHqz7PJ8tmUWZfH/4e1YnrmZt4lrSi9Irjnm6etLVrys+bj64mFwwGSYMDHJLc8koyiCnRI8SigyM5JM/fFLNJXA48zB//uHPrElcUy2+CP8I5g2ex83RN1e4ANIK0vh4x8e8n/A+R7P18Fd3F3eGdhqKr7svyfnJJOUlVZQya3JZ78uYP3Y+l/W+DMMwyCjKID4lnk1Jm1iduJotSVtQKFxNrljKV8zrG9yXKVFTmN5nOlOjpuLn4VfvPUrNT+Wj7R+RXpTOtN7TmBw5GS83L6w2K1/v/5p//fYvdpyuXKJ1UNggxkWMY2LPiUzqOYmegT0ByCvN4/3493ll8ysk59ceexnkGcSTE5/kvtH31XIx1NduVWopZWfaTt7e9jaf7fwMfw9/Hhv/GEeyjvDxjo8Z1XUUi69ZTL+QxnujmK1m3tz6Ji/8/gI9A3pyaa9LuaTXJYyNGFvLnpby9f6vueO7O7ApG+G+4RzMPIivuy/zBs9jRv8ZjOo6iq5+DXfl2p22m5+O/cSEHhMYET6i1v3JLcklMTuR6M7R57xdwGw1czT7KD0De9bpfswqzuJk7kmiO0c32h5ZZC4iJT+FAI8AXSOoQuzxWK764iq6+nVlTcblRP7jTd146NmyvEhE4Wx59FF49VU4cYIM93j27JlJdPRPBAVNaZXoS0t1LWDpUvj2W107cHeHwZfGkTjqavKMJOYMnMeSOYsbLeE0xLqj65i3bB6ZxZmEeYdxWe/LmNZ7GsPDh9PNrxuBnoENPrBmq5nNSZu5/bvbScxK5KGxD/HkxCd5dfOrLNq4CE9XTx4Z9wh+Hn7kleaRX5rPrjO7WHd0HTZl48LuFxLuG863B7/FYrMwqeckbhh6A6O7jmZwp8HVXAtKKTKLMzlTeIaMogwyijLIKs5ibMRYhnQa0uB1ZhRlsCZxDXEpcQzvMpyLoy4mwj+i2fetJkop1h9fT6mllLERYwnyCmowfJm1jLWJa8kuyabUUkqptRQXw4U5g+dU+oebwZ4ze/jbz3/ju4PfYTJMPH7R4/x90t/r9HG3BU7knOBP3/+JQnMht0bfypzBc/B193W2Wecdv5/6ncsXX05JcQE3H/XlsVfj6mxzORuaKgoopc6rbeTIkcohHDmilGEo9b//q8zmPBUb66oSExc0O7qi0lL1+5ZS9eKLSl15pVJ+frqnflCQUrffrtQPPyj17paPlcezHqrnKz3Vn7//s2Ih6vF1j9cZX3phuvrvwf+qp35+Sk37fJqa9vk0tWL/CmWxWpRSStlsNvX8b88r09MmNeitQWrTqU3KarM22/6C0gJ133/vUyxEuT7jqliIumH5DSo1P7XO8Em5Ser5355Xg94apEJfCFXzf5yv9qfvb3b6QnW2JG1RcclxzjZDOIccyz6m/nRToPJ4yqSMhYaas3SOSkhJaHZ8QJxqQh4rNYWqzJqlW3hPnmT7gcuwWgsZNSrhrKLYvP8kD3zxMnHqA3ArArMnLlZ/fF0D6RnUlaE9u9MzsDspBSl8suMTpkZNZcm1SwjxCuGe/97D+wnv8/6M97lzxJ0AbEvexlOxT/HjEd2+YTJMDOk0hJySHE7mnqRPcB/+csFf+OXELyzbt4w5g+bw0R8+arXS2ZrENby97W0evOBBpkS1Tq1JEIQmUFYGPj6kPXYfr03x4a1tb/HAmAf4x5R/NCs6cR81h9hY7cx//31OTcsnMXE+o0btbLRrqs0Gi1cf5MnVz3HCfzEA3XNvYFzffoR0y8Xqmkt2STYp+SmczD1JSn4KVmXl4XEPs+iSRRWNtWarmRlfzmDd0XW8cfkb/HDkB1YeWkmIVwgPjHmAyZGTGdl1JL7uvlhsFr7Z/w0vb3qZLclbMBkmnr/keR4e9/B5Mb5CEIRG2LsXhgyBxYvhhhvILdG9vxrqfdcQIgrNQSk914LZjDnhF37f1I3w8Dvo1+8t8krzsNqsFb5lpWDbNvhyieKTXe+TM+4BUK6M5C5emzef8UPqn+LbYrNQZC6qs3dPXmkeEz6ewK60XQR6BvLIuEd48IIHG2w03ZKkRaGhbpGCIJxn/L//B9ddB9u36z7nLcTpcx+dlxgG/M//wC234BYbT6fu15GW9jkprjO54ZvbyCvN44ExDzDe9Ah/+58Qdu0rxjTjPmwTPmGY92V8e+vnRIbV7ldeE1eTa52CALp/9eqbVrPiwArmDZlHoGdgo/FdEHHBWV+qIAhtnL179YCk/v3PabJSU6hJaakeEDBiBNlfPM6C7yby4XETvYJ6Myh4BN8eXgplvvgf+DMBo37glHkHT058kr9P+nubHDovCMJ5ir2WcLh11o6XmkJz8fCA++8n+19PccvSYlamwtTwAOa4x7Hgz/64ev8vve9YyMHo5zC5BLLy2pVc1c/xc5wLgtDB2LfvnM55ZEdEoQ62XT2G67IgKWUDC8fO5thXA7n3E38mToR33x3CgAHL2Je+jwCPALr5y7oLwjmgsFA3ZPlKn/8OgdWqawhXnvtVIEUUqqCU4o2tb/DImkcI9/Uh9j+KT09+yqef+DBr1u8sXXphxUIsg8LOvYILHZgbbtAzH65a5WxLGuemm/Ryc88/72xLzl+OH9ddUs9xewKIKFSQW5LLHd/dwfL9y5nRbwb/9r+bP//DzIpEH+6663tuvHEuhpEClDcQ33abnoDoySedarfQAVBKT45lX/+yLXc5Vgq++07XaBYtatu2tmUOHtSfThAFmTUfPaHXtP9M49uD3/LyZS+z/NpvuevNK/mWP/Da4Pd46aVOKFVIWpoeg8C+fXphnlde0WouCI7k+HG9FmhWlp4/vS2TkqLXPE1NrczYhLNHRMF5mK1mrlt2HdtStrH02qXMHzefRx4x+OFHg3cu/ZoHD96PX1kUvr4jSE5+HaWs8M47+uTsbL3OsyA4koQqo+rtU+i2Vara9/PPzrPjfOfgQb0gSmjoOU+6Q4uCUoq7Vt7FqsOrePuKt7l64NW88w68/roernD3C33AYsFYvpwePRZQVHSAM8c+hU8/1ct4BgTo9Z0FwZFs3175/XwRhcBAEYWWcOjQOVuTuSYdWhSe+OkJPt35KX+f9HfuGXUP69bBn/+sG/xffBE9lfbAgbB4MWFhs/HxGUbhB4/rKU7/8he45hpYsaLulewFobVISNDTHfj6atdlW2b/fi0If/iDnhveZnO2RecnBw86xXUEHVgUvj3wLYs2LuLuEXfz90l/5+BBXfgfNAi+/FIvGo9hwI03wq+/YpxKIrLnQsKWncE8IEIvyjN3rhaIH1t5MR6LpdUGrAjtgO3b9TJ7AwacHzWFgQNh6lTdBrJrl7MtOv/Iz9dtMyIK55Y1iWvwc/fjrSvfwjAMFizQnTtWrgS/qtMM2ZfA+/JLQo+G43cYkmaUYFMWvQ5mSEjru5A++EBnAE19oR55BG6+uXVtENoGqalw+jQMH64z2/NFFC6+WP8WF9LZc+iQ/hRROLdsP72d6C7RuJpcOXFC96K79149w0U1evXSK95/8QXGO++gfLxImpzB6dOfgpsbzJ6tlaSoqPWM+/lnXe1+6aXGw5aWwnvv6ZkUMzNbzwahbWBvTxgxQldjk5N17bQtkp0NaWlaFCIitE9cROHscWLPI+igomC1WdmZtpPhXYYD8Pbbev+f/lTPCTfcoEvtixfDTTfj3fkCTpx4FputVLuQCgvh++9bxzil9JoOJpP2Y5082XD42Fhd3bTZ4L//bR0bhLaDveeRvX0L2m5twW6X3c4pU+CXX/SgO6E6Npsuhd5xR+1jBw9q13Xv3ufeLjqoKBzJOkKRuYjhXYZTXKy9NbNmQffu9Zxw3XW6kcFiwbjvPqKinqG09CSpqR/ApEnQuXPruZBOntT+xIcf1gLx6qsNh1+xAnx8IDxcr/MptC8SEqBvX/D3Pz9FoaAA4uOdZ1Nb5ZFH4N13dU/GmjW/Q4f0mvEtXJO5uXRIUdh+WlfJh4cP58svdXvYAw80cEKnTrqn0aWXwrBhBAVdSmDgZI4efZyi0qO6hfr773WJvaVs3Kg/r79eb++9p6vldWGzab/X9Olw9dWwejUUF7fcBqHtsH27bk8A7cp0d2/bouDpWemDnTxZf4oLqTqvvKK3SZP0HEe//FL9uBN7HkFHFYXU7biZ3BgYOog33tC9/SZNauSkJUvghx8AMAyDAQM+wzDc2Lt3DtY5V+tuqStXtty4jRt118OhQ3VporCwcrBcTeLidK1i1iy9FRXJYLr2RFaWHs08YoT+7eqq/fRtWRT69y/vugeEhcGwYSIKVVm6FObP122Rq1aBl1f1d1YpXVMQUTi37EjbweBOg9m22Z0dO3QtodEpWkymyocd8PTszsCBn1NYuJMjnb6CqCj45z91w29L2LgRLrhAZwDR0TBtGrz2Wt1jIVas0DZdcYVWNX9/va81KS7W1dym+oUXLtSCJv3TW86OHfrTLgqgXTNtdazC/v2611xVpkzRz3R7GcvTEm/Af/8Lf/wjTJgA//kPeHvr7z/9VBkmOVkXBEUUzh1KKbanbmd4l+G88YYeZ3Pjjc2LKyTkCnr0WEBq2vtkPztbv6zPPdd84/LyYPduPQbCzmOP6R4dn39eO/yKFVoMgoO1W+HKK3VtxWptvg01+ewz3SD29deNh7XZ4MMPYc8e2Lq19WxwFjt2NN7Q70jsjcx29xFoUTh2rO1lssXFulZjb0+wM2WKtnXzZqeY1aps2qS7oL/8ct3HldLvas2Fy9LTtRjMmKFFc8WKyvaCqVP1Cmupqfq3veeRk0YzQwcUhZT8FNKL0onyGs7y5brx38en+fFFRj5LQMAEdnf/N5a5V8G//qUz9sZITq69b8sWnbFeeGHlvosv1gOXnn9eN9rZOXRIl8xmzarcN2uWfgA3bWr+BdXE3qOpKQ3p27ZBUpL+vmxZ69lQHxYLrFmjP1sbsxkuuUQLbVPiP3NGZ979++uS/YQJup3nzJnm25CQoHs/VJ3/ZuBA/YzY+7K3FQ4e1JlhTVGYNEnXZs+md96xY22zprlwoX4uHnsM1q6tfsxq1ZlJly66O+4dd+g1lj/6SAvBV1/BU0/pdzw4uPK8Sy7Rn/bagpO7owK65Hw+bSNHjlQtYeXBlYqFqAde/FWBUkeOtCg6pZRSJSVJ6rffQtX2tUOVLTRUqdGjlbJY6g5cVKTU7bcrBUp99VX1Y3//u1KGoVROTvX9a9cqZTIpNW2aUmVlet8LL+g4jh+vDJebq5Sbm1IPP9zyi1JKqcJCpTw9dZweHjr+hnjkER32oouU6tlTKZutdeyoj6ef1vdg/vzWj3v1ah03KPXvfzce/plndNhrr1XqyiuVmjxZ/37mmebbMGCAUjNnVt+3c6eOd8mS5sfrCL74Qtu1a1ftY9ddp5+fAwcaj+fTT3U8M2cqlZXV+nY2l02btF1PPaXUkCFKBQUplZioj1ksSt16qz5+5536egMDK5+f8eOV2ru37nitVqWCg/X5Sin1l78o5ePjkHcHiFNNyGOdnsmf7dZSUXj2l2cVC1F/vDNXhYW1KKpqnDmzXK1fj0p7bba+rS+/XDvQ0aNKjRihjwcHKzVokH4o7Fx6qVLDhtWdwPvv6/P++Ef9wFx4oVLDh9cON326Ur17t85DtXKlTnPhQv35+ef1h7XZlIqMVOryy5X6+GMdftu2lttQH0eO6IwmJKRugW0pd92llK+vfqFDQ5XKzq4/bFmZUuHhWrSrMnWqFseq/3FTyc/XBYSFC6vvLy7WBYSnnjr7OB3JU09pu0pKah9LTdWZ5PjxDd+Lo0eV8vNTqm9fXbiIilIqPr7yeFqaUq+8ojPecy0YV1yhn7X8fP3sBQYqNXSoUnl5lYJQ9b8ym5X6/Xf9DjX2/197rVIREfodmj697ve6FRBRqIdrvrpG9Xm9j7rwQqUmTmxRVLXYt+8mFbveRZmnT1DKy0upd95RavlypX76SZekgoKUCghQ6rvvlPryS337/9//0ydbLPqFuPfe+hOwl0bvvFNnGE8/XTvMO+/oMHv2tPyC7rlHZ4zFxUp166bUjBn1h42P1+l++KFSmZlKuboq9de/nl16q1crNXKkUosXNxzO/vL4+ema0oUX6tJVfaWxs8Vs1kJw/fVKJSToe91QbcT+X37/ffX9S5bo/atXn70NGzfqc7/9tvax3r2VmjPn7ON0JHPmKNWnT/3H7QWFN9+s+7jFokXD31+pY8d0ybx7dy38Tz2law6urjoOw1Dqggsar7m2Ftu26XT/9a/KfT/+qEUwPFwfq+tdbCr2d/bgQS2E8+a13OY6EFGoh6hXo9ScpXNUYGDD+W9zKCvLUhs3dlUJ3/VTts6dVUX10b7FxFT6qywWpfr31zUDq1Wp7dubVhq/997K+HbsqB0mJUUf+8c/WnYxNpsWgtmz9e//+R9dequvxPz440q5uCiVkaF/T5vW9BrL6dNK3XBD5XVdeGHD4Zct0+FefVX/Tk5WqnNnpfr1a52M4qefdPzLl+vfd96pM6SDB+sOf+GFOkOsWSIsKdGly+Zk4G+8oW04dar2sRkztAujqWRn60z1sceUiotzjFtvyJCGCw02m1KXXaYLGVVdnnb++c/az396uj4HlOrSRalHH9WFnRUr9P9x0UVKFRRUhrdatYj+5z+td11K6XsXFFT72Xr++ZYLglI6T7B7FwxDu5EdQJsQBWA6cBA4Aiyo4/itQDqwo3y7s7E4WyIK2cXZioWox1f9S4FSr73W7KjqJSPjB7V+PSpx73z98O/YoVRsrFI//KDbE6ry2Wf6L1ixQqm33tLfjx5tOAGLRWegY8fW/3JPmqQf4kOHmn8hCQnano8/1r83b67+uyo2m67yX3JJ5T67u2v79vrTKC3V/vqgIKXc3fXL8MQT+sU4fbruc/LytFjFxOgSvZ3YWC1KV1/d8kzv3nuV8vbWbSpKaVv8/JS66qraYe01pFdeqTsuu5ieOXN2NsyZo2srdV3LY4/p+1X1+hvioYf0PbWXtHv10iKemXl2NtWH2azteeyxhsMdO6ZrdNOnV7+ubdu0bXPn1r5ei0Wp/ftrX+vSpbqkPmWKfiY+/1y7Y+0Fi7feapVLq3gP6msbSk5ueRp212u/fjqtxmrKzcTpogC4AIlAL8Ad2AkMqhHmVuDNs4m3JaIQeyxWsRD13LJVCpRas6bZUTXIgQN3qfXrDZWZ+WPDAc1mXZoeOVJn9F26ND1DayhcYqIuoQ4YULvRuqk884zOSNLSKtOLjNQvdE127dKP0jvvVO5LT9eZ9N/+Vjt8SYkO27OnPm/SJP3iK6VFFJT64IO67Xr4YX1806bax156SbW4fcFiUapTJ91YWBV7qfDrr6vvv/VWndHVd5/37tXnvfRS023YsUPf+0cfrfu43RVTX82lKvv36wz37ru1CHz4oa7Fubho18cPPzQex/PP6+c0IaHu44cO1V9gqMnrr+uwgwYpNW6cbnfp2lX71M+2neDzz/V98vLScQ4ZomsJM2fq/TX/q+Zw9dXa5dtQm1JrcMcdlYIWF+eQJNqCKIwDVlf5/TjweI0w51QUXtn0imIh6l9vpNZbM28NzOY8tWXLILV+vYs6efIVZWsoA//wQ/03uLtXumpag9hYnRlMn15/T6iGGD1a10aq8thjOk67i8jOU0/VXbqfOlWXfuzXb7Eo9e67OgMAHf8PP1QXOJtNi0VdrohDh3RmdtddddtssSgVHa3Fq7j4rC63gthYbdvSpdX3l5RoV59haKErK9Olfw8Ppe67r+E4L7xQuwqbKvjTp+vaU32ZpL3WtmJF43FdcYX209vF3U5CglKDB+t47r1XN6DWxd69uqZjGNr1U1f7yLff6ng2b27cHqtV378//EF3rBg/Xm8bNzZ+bl189pkWuW+/rXTfFRbqZ8vDQ6lff60MW1amXYPPP6/Ubbfp/yU0VKmLL1Zq3brq/8+ePfregcPcOdWwt0uBrvk4gLYgCtcCH1T5/ceaAlAuCqnALmAZ0L2xeFsiCjd/c7Pq8lIXdf/92hvgyB6TZnOu2r17llq/HrV37w3KYimoO2BZWWWJ+f/+r3WNePddHe/ZdlFNTVV1tkvYXSXvv199/+DBurRfk3//W4ffvVufO3q0quiit3Zt/X/Agw/qrrAFNe7Zn/6kX/T6XEtK6ZcblFq0qNHLrJM//1mXPOvKJPPzdWZiF7T77tPf9+1rOM6PPtLh7BlUXp6uiV11Ve1rsdv/4ov1x5ebq8M891zD6a5a1XAtpbhYdyM2DF0TqNk/22rV/1VwsK69REfrQkHNGsGiRTqd5tZKHUF6ui6QBAbq5/D663WJ357xdumiuw3ffntlY/G4cbq96u67tWsqIEB3/bZ3A3ckaWnahq5dHZbE+SIKIYBH+fd7gJ/rietuIA6I69GjR7NvyrB/D1OX/+dyNWWKUmPGNDuaJmOzWdXx4/9U69cbauvWoaq4+GTdAe2Zd33V85bwwAM67iefbHoJxF572bmz+n6bTWceU6fq9pGsrMr+22+8UTue06d1hjN0qH7JOnfW/tLG1Nje0PvNN5X7MjJ0Zn3HHY3bP2OGVv2GxMN+PVUbh61WnUFcc03D5y1ZokvfoEu7jVFQoO2ZN08Lf2ioPtfNTbtR7KV4q1V3We7Ro/GaTkSEDvf880qdrOO5KivT7sO+fXXbTUP88ot2N3bvXr1Ny94rxi4Cubm63Qi0u/O++3QPtUGDHJqZNZtjx3TmD0qFhWlBX7GitiuouFipt9/W9xO08D34oBaWc8nIkXW7Z1uJtiAKjbqPaoR3AXIbi7e5NYUSc4lyfcZVPb7ucRUertQttzQrmmaRmfmj2rDBX23ZMkiVldXhErDZGi9tNhezWWdGoF/8RYvqdxXYmTVLZxB1Zd5PPFFZ2rJvhqFUUlLdcV18sT5+//1N98uWlekSnn1Aj1K61tLUrrYHD+oX+5576g9jterrDA7WDbG7d+uSPOiqfGMcParUTTc1fSzGPfdU3q9LLlFqyxal1q/XQjdkiHZF2QeAffZZ4/H98IOurdjv/+TJup/8F19on7S99L5yZdPs275du6x69tQdJFJSdEn54ourPwelpbokHRSkn6dOnbSQOmIAYWtw4oQuuDTFhVpWpruLt6SDRktITta1dAfRFkTBFTgKRFVpaB5cI0x4le9XA5sbi7e5ohCXHKdYiPp469IWeReaS1bWzyo21l0lJExUFksz/d0tYcsWPbAMdEm1ps/cTlGRbjj905/qPp6ervtrL1qke9y8/XbDLfapqU0byVqTG2/Udlos2p/fpcvZlaL+8hddO6lrhK1SlWM+JkzQJXbQAuHh4RifbmKivqaffqq+/6eftKts2DDdFhIdfXaD3Q4f1l0iBw7U4lBVrKdNOzsfaXy8FuOoKP2seHg0rTFbOC9wuihoG7gCOFTeC+lv5fueAWaWf38O2FsuGOuBAY3F2VxRWLxrsWIhaum6wwp0geBcc/r0l2r9etSePXOUzdaMUa6twaZNeuCPYeieIFVJS9N9v0GXYp3J0qXajg0bKnvbnE13scxMXZqdMqW2K+aHH/T133yzzjTPnNFunaFD6xdDR7Jmjc6AmzvQzU5Rka7xfP217vPenJ4UW7dWusaefbb5tghtjjYhCo7YWtLQnFWUpd7/wKpAF7CcwcmTL6n161GHDz/kHAOU0pnHrFn673/8cZ0x7tihXQeenk1znziavDzdI2v+fJ1ZDx169j0D7D7xfv2U+vlnve/oUS0W0dGV4xDaAr/8UvfUKM5g61bd06yxtgjhvEJEoR4eeUQXyprTS7M1sNls6tChv6j161FHjjzivBqD2ax9w6AncPPx0YPCHDlf0dkyfXplH/Sm9IGvizVrdOM46DaKESO0r7w1ZkIUhPOIpoqCax0Tp7Zrai4Oda4xDIM+ff4PpSycOvUSpaVJDBjwCSaTx7k1xNVVr+jWpQs88wyMGaPneQ8PP7d2NMQf/gA//qhtvP765sVx6aV6KvNnn4UXX9TTYK9c6bRF0QWhrdPh1lPYt6/2lO/nGsMw0bfvG/Tq9Txnzixh585pmM05zjAEnn4adu6EDRvaliAAzJypFw966CHwaIFoennpdS527tQic9VVrWejILQzOlRNwb441C23ONsSXWPo0eMxPJFTGegAABMMSURBVDwiOHDgVrZvv4hhw37A07P7uTdm2LBzn2ZT6NoVEhP1Z2swaJDeBEGolw5VU6hvcShn0rnzDQwb9iOlpadISBhHQcEeZ5vUtoiI0OtjC4JwTuhQb9v+/fqzrRUWg4KmMHz4r4Bi+/aLyMn5xdkmCYLQQelQorBvny509u3rbEtq4+s7jBEjNuHh0ZWdOy/jzJmlzjZJEIQOSIcShf37daeTlrRZOhJPzx4MH/4bfn6j2bdvLocPP4jVWuxsswRB6EB0OFFoS+0JdeHmFkx09Dq6dXuQ5OQ3iI8fQX5+grPNEgShg9BhRMFshsOH2157Ql24uHjSt+9rDBu2Boslj4SECzh2bCFmc7azTRMEoZ3TYUQhMVELQ1uvKVQlOPhSRo/eTWjoNZw48TSbNkVw8OA90kNJEASH0WFEwd7z6HwSBdDupMGDv2LkyAQ6dbqetLTPiIsbyq5dl1NSctLZ5gmC0M7oMKIwZIie5eB8EwU7fn7DGTDgA8aNSyIq6jlyc39j27ZhpKV96WzTBEFoRxh6nqTzh1GjRqm4uDhnm+F0iosT2b//j+TlbaJTpxvo2/ct3NwCnW2WIAhtFMMw4pVSoxoL12FqCu0NL6/exMRsIDLyGc6c+YqtW/tx8uSLWK2FzjZNEITzGBGF8xiTyZXIyCcZOXILvr7DOXr0MTZvjuLkyRexWHKdbZ4gCOch4j5qR+Tm/s7x40+Tnb0GMOHrOwx//wsJCBhPSMiVuLoGONtEQRCcRFPdRyIK7ZC8vK1kZq4iL28jeXmbsVoLcHfvSv/+HxAScrmzzRMEwQk0VRQ61NTZHQV//zH4+48BwGazkJe3iUOH/sTu3VcQHn4nvXu/jKurv5OtFAShLSJtCu0ck8mVwMAJjBoVT/fufyU19SO2bRvGyZMvkZPzK1ZrkbNNFAShDSE1hQ6CyeRB796LCA2dyaFD93L06KPlR1zw9Y0mLGwO4eG34e7e2al2CoLgXKRNoYNSVpZGXt5W8vK2kJMTS17eRgzDjdDQq+na9W4CAiZgMrk720xBEFoJaVMQGsTdvTOhoTMIDZ0BQGHhAVJT3+P06U9IT1+KYbjj4zMUP79R+PmNxNd3GN7eg3F19XWy5YIgOBKpKQjVsFqLycpaRV7eVvLz48jPj8dqrRzz4OnZC1/f4QQHX0Zw8BV4ekY40VpBEJqK1BSEZuHi4kVY2GzCwmYDoJSNkpJjFBTsprBwN4WFu8jL20JGxnIAfHyGERJyFWFhs/H1HY5hGM40XxCEFiI1BeGsUUpRVLSPzMxVZGWtIifnV8CKp2cUYWHXEhw8DQ+Pnnh4dMPFxavOOKzWYoqKDlBUtB8/v9F4e7fBNVIFoR0hg9eEc4bZnElGxrekpy8jO3stSlkqjrm6huDmForJ5IHJ5InJ5EFZWRrFxUcAGwAmkzf9+39I587znHQFgtD+EfeRcM5wcwshPPx2wsNvx2zOJj8/nrKyZEpLkygtTcJszsRmK8VmK0WpUnx8BtOp0zx8fIbg6dmTxMSH2b//evLzt9Kr1/OYTG4VcStlxTBcnHh1gtCxEFEQWhU3tyCCgy85q3Oio38mMfERkpJeIT8/nqCgqRQW7qagYDfFxYfx8upDUNClBAVdQlDQxbXmcLLZyigtTaGsLBmbrYzAwIkiJILQTMR9JLQZ0tIWc/DgXdhsJeW9nIbi5dWPwsI95OT8gs2mpwU3DDdMJg8Mwx0wsFgyq8Xj5dWHHj0W0LnzHzGZ3LHZzGRnryEtbTHFxUfx8xuJv/8Y/PwuwNu7H4YhA/uF9o+0KQjnJRZLHobhgouLT7X9NlsZeXmbyc39Das1H5utDKXKUMqKu3sXPDy64e7eDas1l5MnX6CgIAEPjwgCA6eSlfU9ZnMGrq7B+PgMpqBgB1ZrPgCenpH07PlUuYBUrziXlWVQWLirvEH8AEVFh/D07E5Q0CUEBk7B3T3snN0XQWgpIgpCh0UpRXb2Wk6c+Cf5+XGEhMygc+cbCQ6ehsnkjlJWiooOkJe3mZSUd8jPj8PLqy+RkU/j7T2QrKzvycz8L3l5WwD9fri4+OHl1Yfi4qMV4zZ8fKLx8uqDm1sIbm4huLoG4+Lii4uLNy4uPphMPuWC1RU3t1AMw4RSNszmTMrKTmOzleLjM7CWADbl+oqLEzGZ3PHw6O7QbsA2W1m5Wy+O3r1fxtOzh8PSEhyLiIIgNAGlFJmZ33Hs2JMUFu6u2O/nN5qQkKsICBiPt/dA3N3DMQwDm81CQUE82dnryMmJpbQ0GbM5C4sls1qvq5oYhiuuroGYzdmAtcoRE97e/fD1jSlveO+Fp2cUXl5RuLoGYbUWYrUWYrMVUlCwi+zsNWRlraG09CQA7u7dCAjQa2b4+ETj7d0fd/cuTRaKkpKTZGX9SGHhHgIDJxMcPK1CpHJyNnDo0J8oKtqHYbjj4uJNv37v06nTtWd9n6uilGqRkFks+ZSVpWE2///27j44jvq+4/j7c9Ld6fRgWZJl2cYeG4JjQhIwiUtwSdME4sZkMiHpkAJJGPIwzXRCJ6HTThtPG9rknz5MpzSZMm0yaVJIGJIJMQlxM6WJIZ46JIANxhgbY4hdsIssyZZOjyfdw7d/7E+XQwhZyJZ3jb6vmRvd7q1On7vd0/f2t7u/Xw8TEz2Uy3mkeqQ0Uj3Z7CpaWjb4NTNTeFFw7jUwq9DXdz+l0gDt7ZvJZpe9xt83yuXh6j/wcnmUcnmIiYnucBD8/ygWT5JOLyGT6SKT6UKqZ2TkKYaGnmB4eA/j4/97yr9TV7eItraraWvbhFmZwcFfkM8/XC0Sk8vkcmtJp9uQMqRSGaRsOC04RyrVgFmRgYGHGB09AICUwWyCVCpHe/tmUqlGenrupqFhDWvX/gu53DoOHPgoQ0OPsXz5H3LhhbfPuIdjViaf/yV9ffdx4sSPmZh4iUqliFkRqNDcvJ4lSz5ER8e1NDdfitkEw8NPMjj4KKOjz9DaupGOjg9UTyowK9PX92OOHfsqAwMPnfJ9yuXWsXz5J+nquolsdsUplz8bKpUilcr4tF3FRNf+HCSVypLLnT/t4/n8TjKZpTQ2rpvT3/ei4Nw5plweoVA4QqFwhLGxw5TLeVKpJurqmqira6Sh4QJaWn7rFcc+AMbHjzEy8jSjo88yNnaQ0dFDlMtDmE1UTweObgUqlQJQZtGijbS3X0N7+2ZyubXk8/9DX99Wenu3Uiz2sGrVn7F69Repq2sEoqakw4dv48UX/yEc7M+RSqVD4WkITWdR89nw8F6KxR6kDG1t76Wx8aLqt3kwBgZ2MDj4MGBkMssoFk9iNgFAKtVApVIIv7uJlpbLOH78OxQKR8hmV7Fs2afI5d5AJrOUdHop9fWtmJUwK2JWYmhoN93d3yKf30m0J/am8LfrkFJkMitoaXl7tV8vKcvERHf1Vi4PYzZ5CnWRVKqJdLqN+vo2UqlGxsYOMTKyl+HhvYyNHQIsPH89qVQTTU0X09R0Cc3Nl1Jf30o+v5OBgZ+Tzz9MpTJGS8uGcCbdJqQ6+vp+RF/fDykUngdg0aIrWbbsZpYu/QOKxX6OH7+L7u67KBSeZ8WKz/LGN94xp+0rEUVB0mbgK0Ad8A0z+7spj2eBu4C3AyeA683syEzP6UXBufllVqFSGXvVPYGBgR2cOPGf1YP9lcoElcpYaOoaplwepqFhDZ2dH6a9/ZpXHdBpYuI4J05so79/O9nsqnBG2OVks+cxOPgrent/QG/vvYyPv0Br67tYufJzdHRcO21RnM7o6CG6u+9kdHQ/ZmWgglmJQuEIo6MHmTxeNBd1dS00Nb21WuyiolSiVMozMvIUhcKvX7Z8U9NbaG39XdLpNvr7HwzHq6JmxKj4XUVHx7WUy/mQ+UB17w3E4sXvYdmyT9DZ+fuv+RjUpNiLgqITxZ8FNgFHgceAG81sf80ynwUuMbM/knQD8GEzu36m5/Wi4NzCYWaUSidJpzvO6POWSkMMDz/B0NBuJvdWolsXdXUt1SvwpTTl8gilUj+lUn8oeBfQ0LB6xlOZS6UhRkb2USqdpKXlHWQyS6Y8nmdg4OeYlWhr2/SywmlmDA3toqfnu6TT7XR1fZyGhtWn/ZqTUBQ2An9jZu8L01sAzOxva5Z5ICzzS0n1QDfQaTOE8qLgnHOv3WyLwnxetXMe8GLN9NEwb9plLDp1Iw+c2a8EzjnnZu2cuJRT0mck7ZK0q7e3N+44zjn3ujWfReEYsKpmemWYN+0yofmoleiA88uY2dfNbIOZbejs9KtInXNuvsxnUXgMWCvpfEWd1NwA3D9lmfuBm8P964AHZzqe4Jxzbn7NWy+pZlaS9MfAA0SnpH7TzJ6W9GVgl5ndD/w78G1JzwEniQqHc865mMxr19lm9hPgJ1Pm3VZzvwB8ZD4zOOecm71z4kCzc865s8OLgnPOuapzru8jSb3AqXsOm94SoO8MxjmTPNvcJDkbJDufZ5ubczXbajM75emb51xROB2Sds3mir44eLa5SXI2SHY+zzY3r/ds3nzknHOuyouCc865qoVWFL4ed4AZeLa5SXI2SHY+zzY3r+tsC+qYgnPOuZkttD0F55xzM1gwRUHSZkkHJT0n6QsxZ/mmpB5J+2rmtUv6qaRD4WdbTNlWSXpI0n5JT0v6fFLySWqQ9KikJ0O2L4X550t6JKzb74W+tmIhqU7SE5K2JSmbpCOSnpK0R9KuMC/2dRpyLJZ0r6RnJB2QtDEJ2SStC+/X5G1Q0q1JyBby/Un4HOyTdE/4fJz29rYgikIYBe4O4BrgYuBGSRfHGOk/gM1T5n0B2G5ma4HtYToOJeBPzexi4ArglvBeJSHfOHCVmV0KrAc2S7oC+HvgdjO7EOgHPh1DtkmfBw7UTCcp23vMbH3NKYtJWKcQDdn7X2Z2EXAp0fsXezYzOxjer/VEQwaPAvclIZuk84DPARvM7C1E/cvdwJnY3szsdX8DNgIP1ExvAbbEnGkNsK9m+iCwPNxfDhyM+30LWX5ENKRqovIBjcDjwDuILtapn25dn+VMK4n+SVwFbAOUoGxHgCVT5sW+Tom6yz9MOL6ZpGxT8vwe8IukZOM3A5S1E/Vhtw1435nY3hbEngKzGwUubl1m9lK43w10xRkGQNIa4DLgERKSLzTP7AF6gJ8CzwMDFo3cB/Gu238G/hyohOkOkpPNgP+WtFvSZ8K8JKzT84Fe4Fuh2e0bkpoSkq3WDcA94X7s2czsGPCPwAvAS0SjVu7mDGxvC6UonFMsKvOxnhYmqRn4AXCrmQ3WPhZnPjMrW7Q7vxK4HLgojhxTSfoA0GNmu+PO8ireaWZvI2pCvUXSu2ofjHGd1gNvA/7VzC4DRpjSHBP35yG0y38Q+P7Ux+LKFo5jXEtUVFcATbyySXpOFkpRmM0ocHE7Lmk5QPjZE1cQSWmignC3mW1NWj4AMxsAHiLaRV4cRu6D+NbtlcAHJR0BvkvUhPSVhGSb/GaJmfUQtYtfTjLW6VHgqJk9EqbvJSoSScg26RrgcTM7HqaTkO29wGEz6zWzIrCVaBs87e1toRSF2YwCF7faUehuJmrLP+skiWjwowNm9k81D8WeT1KnpMXhfo7oWMcBouJwXZzZzGyLma00szVE29eDZvaxJGST1CSpZfI+Ufv4PhKwTs2sG3hR0row62pgfxKy1biR3zQdQTKyvQBcIakxfGYn37fT397iPHhzlg/MvB94lqgN+i9jznIPUTtgkeib0qeJ2p+3A4eAnwHtMWV7J9Hu8F5gT7i9Pwn5gEuAJ0K2fcBtYf4FwKPAc0S7+NmY1++7gW1JyRYyPBluT09u/0lYpyHHemBXWK8/BNoSlK2JaNz41pp5Scn2JeCZ8Fn4NpA9E9ubX9HsnHOuaqE0HznnnJsFLwrOOeeqvCg455yr8qLgnHOuyouCc865Ki8Kzp1Fkt492YOqc0nkRcE551yVFwXnpiHp42Hshj2SvhY64huWdHvow367pM6w7HpJv5K0V9J9k/3rS7pQ0s/C+A+PS3pDePrmmvED7g5XpDqXCF4UnJtC0puA64ErLep8rwx8jOjq1l1m9mZgB/DX4VfuAv7CzC4BnqqZfzdwh0XjP/w20VXsEPU8eyvR2B4XEPVZ41wi1J96EecWnKuJBlV5LHyJzxF1elYBvheW+Q6wVVIrsNjMdoT5dwLfD30NnWdm9wGYWQEgPN+jZnY0TO8hGltj5/y/LOdOzYuCc68k4E4z2/KymdIXpyw31z5ixmvul/HPoUsQbz5y7pW2A9dJWgrVsYxXE31eJnug/Ciw08zyQL+k3wnzbwJ2mNkQcFTSh8JzZCU1ntVX4dwc+DcU56Yws/2S/opopLIUUW+2txANAHN5eKyH6LgDRF0U/1v4p/9r4JNh/k3A1yR9OTzHR87iy3BuTryXVOdmSdKwmTXHncO5+eTNR84556p8T8E551yV7yk455yr8qLgnHOuyouCc865Ki8KzjnnqrwoOOecq/Ki4Jxzrur/AQo6olFSJWWeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.4885 - acc: 0.8773\n",
      "Loss: 0.48854290922854177 Accuracy: 0.87725854\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2058 - acc: 0.3310\n",
      "Epoch 00001: val_loss improved from inf to 1.74293, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/001-1.7429.hdf5\n",
      "36805/36805 [==============================] - 270s 7ms/sample - loss: 2.2057 - acc: 0.3310 - val_loss: 1.7429 - val_acc: 0.4421\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2359 - acc: 0.6133\n",
      "Epoch 00002: val_loss improved from 1.74293 to 1.10657, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/002-1.1066.hdf5\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 1.2358 - acc: 0.6133 - val_loss: 1.1066 - val_acc: 0.6804\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8964 - acc: 0.7312\n",
      "Epoch 00003: val_loss improved from 1.10657 to 0.69263, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/003-0.6926.hdf5\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.8963 - acc: 0.7312 - val_loss: 0.6926 - val_acc: 0.7955\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6994 - acc: 0.7927\n",
      "Epoch 00004: val_loss improved from 0.69263 to 0.66685, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/004-0.6668.hdf5\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.6993 - acc: 0.7927 - val_loss: 0.6668 - val_acc: 0.8174\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5623 - acc: 0.8321\n",
      "Epoch 00005: val_loss improved from 0.66685 to 0.58061, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/005-0.5806.hdf5\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.5623 - acc: 0.8321 - val_loss: 0.5806 - val_acc: 0.8341\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4823 - acc: 0.8559\n",
      "Epoch 00006: val_loss improved from 0.58061 to 0.52942, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/006-0.5294.hdf5\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.4823 - acc: 0.8559 - val_loss: 0.5294 - val_acc: 0.8612\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8760\n",
      "Epoch 00007: val_loss improved from 0.52942 to 0.44921, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/007-0.4492.hdf5\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.4081 - acc: 0.8760 - val_loss: 0.4492 - val_acc: 0.8733\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8916\n",
      "Epoch 00008: val_loss improved from 0.44921 to 0.42178, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/008-0.4218.hdf5\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.3581 - acc: 0.8915 - val_loss: 0.4218 - val_acc: 0.8768\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3200 - acc: 0.9006\n",
      "Epoch 00009: val_loss improved from 0.42178 to 0.42082, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/009-0.4208.hdf5\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.3203 - acc: 0.9005 - val_loss: 0.4208 - val_acc: 0.8817\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9101\n",
      "Epoch 00010: val_loss did not improve from 0.42082\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.2918 - acc: 0.9100 - val_loss: 0.5513 - val_acc: 0.8528\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9165\n",
      "Epoch 00011: val_loss did not improve from 0.42082\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.2696 - acc: 0.9165 - val_loss: 0.5433 - val_acc: 0.8414\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9257\n",
      "Epoch 00012: val_loss improved from 0.42082 to 0.35995, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/012-0.3600.hdf5\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.2365 - acc: 0.9257 - val_loss: 0.3600 - val_acc: 0.8996\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9349\n",
      "Epoch 00013: val_loss improved from 0.35995 to 0.30548, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/013-0.3055.hdf5\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.2048 - acc: 0.9349 - val_loss: 0.3055 - val_acc: 0.9252\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9406\n",
      "Epoch 00014: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.1899 - acc: 0.9405 - val_loss: 0.3842 - val_acc: 0.9008\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9427\n",
      "Epoch 00015: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.1835 - acc: 0.9427 - val_loss: 0.3820 - val_acc: 0.8901\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9477\n",
      "Epoch 00016: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.1610 - acc: 0.9477 - val_loss: 0.3468 - val_acc: 0.9075\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9524\n",
      "Epoch 00017: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.1474 - acc: 0.9524 - val_loss: 0.4859 - val_acc: 0.8740\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9531\n",
      "Epoch 00018: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.1428 - acc: 0.9530 - val_loss: 0.3779 - val_acc: 0.9138\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9600\n",
      "Epoch 00019: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.1249 - acc: 0.9598 - val_loss: 0.3406 - val_acc: 0.9213\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9560\n",
      "Epoch 00020: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.1368 - acc: 0.9560 - val_loss: 0.3308 - val_acc: 0.9241\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9615\n",
      "Epoch 00021: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.1162 - acc: 0.9615 - val_loss: 0.3197 - val_acc: 0.9180\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9680\n",
      "Epoch 00022: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0992 - acc: 0.9680 - val_loss: 0.4113 - val_acc: 0.9094\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9712\n",
      "Epoch 00023: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0919 - acc: 0.9712 - val_loss: 0.3300 - val_acc: 0.9262\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9711\n",
      "Epoch 00024: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0913 - acc: 0.9711 - val_loss: 0.3780 - val_acc: 0.9213\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9724\n",
      "Epoch 00025: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0855 - acc: 0.9724 - val_loss: 0.3268 - val_acc: 0.9227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9688\n",
      "Epoch 00026: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0942 - acc: 0.9687 - val_loss: 0.3121 - val_acc: 0.9355\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9723\n",
      "Epoch 00027: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0866 - acc: 0.9723 - val_loss: 0.3328 - val_acc: 0.9257\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9771\n",
      "Epoch 00028: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0711 - acc: 0.9771 - val_loss: 0.3482 - val_acc: 0.9283\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9750\n",
      "Epoch 00029: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0776 - acc: 0.9750 - val_loss: 0.3452 - val_acc: 0.9266\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9752\n",
      "Epoch 00030: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0776 - acc: 0.9752 - val_loss: 0.3682 - val_acc: 0.9180\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9798\n",
      "Epoch 00031: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0608 - acc: 0.9798 - val_loss: 0.4068 - val_acc: 0.9110\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9810\n",
      "Epoch 00032: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0589 - acc: 0.9809 - val_loss: 0.3358 - val_acc: 0.9301\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9789\n",
      "Epoch 00033: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0647 - acc: 0.9788 - val_loss: 0.4465 - val_acc: 0.9143\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9776\n",
      "Epoch 00034: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0689 - acc: 0.9776 - val_loss: 0.3099 - val_acc: 0.9331\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9854\n",
      "Epoch 00035: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0482 - acc: 0.9854 - val_loss: 0.4460 - val_acc: 0.9192\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9812\n",
      "Epoch 00036: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0586 - acc: 0.9813 - val_loss: 0.4817 - val_acc: 0.9150\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9836\n",
      "Epoch 00037: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0499 - acc: 0.9836 - val_loss: 0.3680 - val_acc: 0.9299\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9850\n",
      "Epoch 00038: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0470 - acc: 0.9850 - val_loss: 0.3773 - val_acc: 0.9241\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9855\n",
      "Epoch 00039: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0474 - acc: 0.9855 - val_loss: 0.3777 - val_acc: 0.9301\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9839\n",
      "Epoch 00040: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0484 - acc: 0.9839 - val_loss: 0.6960 - val_acc: 0.8847\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9814\n",
      "Epoch 00041: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0593 - acc: 0.9814 - val_loss: 0.3686 - val_acc: 0.9217\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9878\n",
      "Epoch 00042: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0385 - acc: 0.9877 - val_loss: 0.3511 - val_acc: 0.9271\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9827\n",
      "Epoch 00043: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0516 - acc: 0.9827 - val_loss: 0.3772 - val_acc: 0.9292\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9881\n",
      "Epoch 00044: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0373 - acc: 0.9880 - val_loss: 0.4832 - val_acc: 0.9101\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9812\n",
      "Epoch 00045: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0563 - acc: 0.9812 - val_loss: 0.4483 - val_acc: 0.9173\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9848\n",
      "Epoch 00046: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0492 - acc: 0.9848 - val_loss: 0.3911 - val_acc: 0.9266\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9893\n",
      "Epoch 00047: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0329 - acc: 0.9893 - val_loss: 0.4362 - val_acc: 0.9217\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9902\n",
      "Epoch 00048: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0325 - acc: 0.9902 - val_loss: 0.3415 - val_acc: 0.9366\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9851\n",
      "Epoch 00049: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0470 - acc: 0.9851 - val_loss: 0.5323 - val_acc: 0.9003\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9858\n",
      "Epoch 00050: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0443 - acc: 0.9858 - val_loss: 0.3542 - val_acc: 0.9292\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9862\n",
      "Epoch 00051: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0427 - acc: 0.9863 - val_loss: 0.5942 - val_acc: 0.9057\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9901\n",
      "Epoch 00052: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0300 - acc: 0.9901 - val_loss: 0.4037 - val_acc: 0.9236\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9913\n",
      "Epoch 00053: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0274 - acc: 0.9913 - val_loss: 0.4149 - val_acc: 0.9250\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9877\n",
      "Epoch 00054: val_loss did not improve from 0.30548\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0395 - acc: 0.9877 - val_loss: 0.4905 - val_acc: 0.9108\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9917\n",
      "Epoch 00055: val_loss improved from 0.30548 to 0.29041, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv_checkpoint/055-0.2904.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0263 - acc: 0.9917 - val_loss: 0.2904 - val_acc: 0.9453\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9891\n",
      "Epoch 00056: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0321 - acc: 0.9891 - val_loss: 0.3935 - val_acc: 0.9227\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9881\n",
      "Epoch 00057: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0351 - acc: 0.9881 - val_loss: 0.3360 - val_acc: 0.9345\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9908\n",
      "Epoch 00058: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0294 - acc: 0.9908 - val_loss: 0.4346 - val_acc: 0.9241\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9902\n",
      "Epoch 00059: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0289 - acc: 0.9902 - val_loss: 0.4227 - val_acc: 0.9203\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9905\n",
      "Epoch 00060: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0302 - acc: 0.9905 - val_loss: 0.3779 - val_acc: 0.9250\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9879\n",
      "Epoch 00061: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 205s 6ms/sample - loss: 0.0391 - acc: 0.9879 - val_loss: 0.4550 - val_acc: 0.9278\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9912\n",
      "Epoch 00062: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0282 - acc: 0.9912 - val_loss: 0.3304 - val_acc: 0.9446\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9905\n",
      "Epoch 00063: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0298 - acc: 0.9905 - val_loss: 0.4825 - val_acc: 0.9138\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9880\n",
      "Epoch 00064: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0391 - acc: 0.9879 - val_loss: 0.4499 - val_acc: 0.9224\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9897\n",
      "Epoch 00065: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0352 - acc: 0.9897 - val_loss: 0.3431 - val_acc: 0.9420\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9943\n",
      "Epoch 00066: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0183 - acc: 0.9943 - val_loss: 0.3899 - val_acc: 0.9322\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9915\n",
      "Epoch 00067: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0275 - acc: 0.9915 - val_loss: 0.3310 - val_acc: 0.9462\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9931\n",
      "Epoch 00068: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0216 - acc: 0.9931 - val_loss: 0.3494 - val_acc: 0.9380\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9908\n",
      "Epoch 00069: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0297 - acc: 0.9908 - val_loss: 0.4174 - val_acc: 0.9276\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9939\n",
      "Epoch 00070: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0199 - acc: 0.9939 - val_loss: 0.3315 - val_acc: 0.9408\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9914\n",
      "Epoch 00071: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0267 - acc: 0.9914 - val_loss: 0.4431 - val_acc: 0.9257\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9906\n",
      "Epoch 00072: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0306 - acc: 0.9905 - val_loss: 0.5081 - val_acc: 0.9257\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9888\n",
      "Epoch 00073: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0364 - acc: 0.9888 - val_loss: 0.3527 - val_acc: 0.9355\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9935\n",
      "Epoch 00074: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0206 - acc: 0.9935 - val_loss: 0.3804 - val_acc: 0.9378\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9924\n",
      "Epoch 00075: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0225 - acc: 0.9924 - val_loss: 0.3391 - val_acc: 0.9415\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9943\n",
      "Epoch 00076: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0178 - acc: 0.9943 - val_loss: 0.4037 - val_acc: 0.9350\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9907\n",
      "Epoch 00077: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0300 - acc: 0.9906 - val_loss: 0.4157 - val_acc: 0.9290\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9904\n",
      "Epoch 00078: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0321 - acc: 0.9904 - val_loss: 0.3870 - val_acc: 0.9324\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9953\n",
      "Epoch 00079: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0154 - acc: 0.9952 - val_loss: 0.3358 - val_acc: 0.9411\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9900\n",
      "Epoch 00080: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0327 - acc: 0.9899 - val_loss: 0.3036 - val_acc: 0.9455\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9915\n",
      "Epoch 00081: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0260 - acc: 0.9915 - val_loss: 0.3589 - val_acc: 0.9394\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9949\n",
      "Epoch 00082: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0157 - acc: 0.9948 - val_loss: 0.3975 - val_acc: 0.9348\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9910\n",
      "Epoch 00083: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0301 - acc: 0.9910 - val_loss: 0.3363 - val_acc: 0.9446\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9948\n",
      "Epoch 00084: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0178 - acc: 0.9948 - val_loss: 0.3818 - val_acc: 0.9380\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9937\n",
      "Epoch 00085: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0202 - acc: 0.9938 - val_loss: 0.3785 - val_acc: 0.9383\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9937\n",
      "Epoch 00086: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0203 - acc: 0.9936 - val_loss: 0.4638 - val_acc: 0.9213\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9892\n",
      "Epoch 00087: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0358 - acc: 0.9892 - val_loss: 0.4242 - val_acc: 0.9269\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9946\n",
      "Epoch 00088: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0181 - acc: 0.9946 - val_loss: 0.3475 - val_acc: 0.9385\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9933\n",
      "Epoch 00089: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0216 - acc: 0.9932 - val_loss: 0.3318 - val_acc: 0.9453\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9900\n",
      "Epoch 00090: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0307 - acc: 0.9900 - val_loss: 0.3356 - val_acc: 0.9425\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9957\n",
      "Epoch 00091: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.3445 - val_acc: 0.9446\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9957\n",
      "Epoch 00092: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0153 - acc: 0.9957 - val_loss: 0.4672 - val_acc: 0.9229\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9923\n",
      "Epoch 00093: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0227 - acc: 0.9923 - val_loss: 0.3252 - val_acc: 0.9469\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9945\n",
      "Epoch 00094: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0177 - acc: 0.9944 - val_loss: 0.4920 - val_acc: 0.9224\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9912\n",
      "Epoch 00095: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0289 - acc: 0.9912 - val_loss: 0.3301 - val_acc: 0.9457\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9948\n",
      "Epoch 00096: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0163 - acc: 0.9948 - val_loss: 0.4019 - val_acc: 0.9311\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9925\n",
      "Epoch 00097: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0227 - acc: 0.9925 - val_loss: 0.3900 - val_acc: 0.9387\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9950\n",
      "Epoch 00098: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0175 - acc: 0.9950 - val_loss: 0.3624 - val_acc: 0.9436\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9944\n",
      "Epoch 00099: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0169 - acc: 0.9944 - val_loss: 0.4426 - val_acc: 0.9317\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9945\n",
      "Epoch 00100: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0169 - acc: 0.9945 - val_loss: 0.4221 - val_acc: 0.9359\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9934\n",
      "Epoch 00101: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0212 - acc: 0.9934 - val_loss: 0.3539 - val_acc: 0.9397\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9951\n",
      "Epoch 00102: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0152 - acc: 0.9951 - val_loss: 0.4497 - val_acc: 0.9345\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9921\n",
      "Epoch 00103: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0265 - acc: 0.9921 - val_loss: 0.4170 - val_acc: 0.9313\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9942\n",
      "Epoch 00104: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0182 - acc: 0.9942 - val_loss: 0.3535 - val_acc: 0.9415\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9949\n",
      "Epoch 00105: val_loss did not improve from 0.29041\n",
      "36805/36805 [==============================] - 206s 6ms/sample - loss: 0.0163 - acc: 0.9949 - val_loss: 0.3892 - val_acc: 0.9439\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.3295 - acc: 0.9298\n",
      "Loss: 0.3295109964985505 Accuracy: 0.9298027\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2188 - acc: 0.3239\n",
      "Epoch 00001: val_loss improved from inf to 1.79542, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/001-1.7954.hdf5\n",
      "36805/36805 [==============================] - 285s 8ms/sample - loss: 2.2186 - acc: 0.3240 - val_loss: 1.7954 - val_acc: 0.4072\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2323 - acc: 0.6139\n",
      "Epoch 00002: val_loss improved from 1.79542 to 0.83331, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/002-0.8333.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 1.2322 - acc: 0.6140 - val_loss: 0.8333 - val_acc: 0.7664\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8234 - acc: 0.7527\n",
      "Epoch 00003: val_loss improved from 0.83331 to 0.54272, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/003-0.5427.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.8235 - acc: 0.7526 - val_loss: 0.5427 - val_acc: 0.8521\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5892 - acc: 0.8220\n",
      "Epoch 00004: val_loss improved from 0.54272 to 0.47853, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/004-0.4785.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.5893 - acc: 0.8220 - val_loss: 0.4785 - val_acc: 0.8609\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.8588\n",
      "Epoch 00005: val_loss improved from 0.47853 to 0.46434, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/005-0.4643.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.4644 - acc: 0.8588 - val_loss: 0.4643 - val_acc: 0.8661\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.8815\n",
      "Epoch 00006: val_loss did not improve from 0.46434\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.3850 - acc: 0.8815 - val_loss: 0.4823 - val_acc: 0.8595\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8969\n",
      "Epoch 00007: val_loss improved from 0.46434 to 0.43515, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/007-0.4352.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.3263 - acc: 0.8968 - val_loss: 0.4352 - val_acc: 0.8779\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9122\n",
      "Epoch 00008: val_loss improved from 0.43515 to 0.31840, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/008-0.3184.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.2790 - acc: 0.9122 - val_loss: 0.3184 - val_acc: 0.9124\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9210\n",
      "Epoch 00009: val_loss improved from 0.31840 to 0.24958, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/009-0.2496.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.2512 - acc: 0.9210 - val_loss: 0.2496 - val_acc: 0.9276\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9304\n",
      "Epoch 00010: val_loss did not improve from 0.24958\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.2214 - acc: 0.9303 - val_loss: 0.3172 - val_acc: 0.9157\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9347\n",
      "Epoch 00011: val_loss did not improve from 0.24958\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.2036 - acc: 0.9347 - val_loss: 0.2811 - val_acc: 0.9187\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9387\n",
      "Epoch 00012: val_loss improved from 0.24958 to 0.22702, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/012-0.2270.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1894 - acc: 0.9387 - val_loss: 0.2270 - val_acc: 0.9324\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9469\n",
      "Epoch 00013: val_loss did not improve from 0.22702\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1637 - acc: 0.9469 - val_loss: 0.2628 - val_acc: 0.9236\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9530\n",
      "Epoch 00014: val_loss did not improve from 0.22702\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1464 - acc: 0.9530 - val_loss: 0.2857 - val_acc: 0.9213\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9534\n",
      "Epoch 00015: val_loss did not improve from 0.22702\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1444 - acc: 0.9533 - val_loss: 0.2304 - val_acc: 0.9357\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9590\n",
      "Epoch 00016: val_loss did not improve from 0.22702\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1277 - acc: 0.9590 - val_loss: 0.2999 - val_acc: 0.9168\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9586\n",
      "Epoch 00017: val_loss did not improve from 0.22702\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1286 - acc: 0.9586 - val_loss: 0.2281 - val_acc: 0.9348\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9651\n",
      "Epoch 00018: val_loss improved from 0.22702 to 0.20780, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/018-0.2078.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.1049 - acc: 0.9651 - val_loss: 0.2078 - val_acc: 0.9415\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9673\n",
      "Epoch 00019: val_loss did not improve from 0.20780\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0982 - acc: 0.9673 - val_loss: 0.2492 - val_acc: 0.9317\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9691\n",
      "Epoch 00020: val_loss improved from 0.20780 to 0.19048, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv_checkpoint/020-0.1905.hdf5\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0940 - acc: 0.9691 - val_loss: 0.1905 - val_acc: 0.9399\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9718\n",
      "Epoch 00021: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0857 - acc: 0.9718 - val_loss: 0.2555 - val_acc: 0.9359\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9688\n",
      "Epoch 00022: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0953 - acc: 0.9688 - val_loss: 0.2658 - val_acc: 0.9338\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9709\n",
      "Epoch 00023: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0903 - acc: 0.9708 - val_loss: 0.2348 - val_acc: 0.9371\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9704\n",
      "Epoch 00024: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0898 - acc: 0.9704 - val_loss: 0.2346 - val_acc: 0.9446\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9793\n",
      "Epoch 00025: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0641 - acc: 0.9794 - val_loss: 0.2343 - val_acc: 0.9455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9813\n",
      "Epoch 00026: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0565 - acc: 0.9813 - val_loss: 0.2861 - val_acc: 0.9341\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9765\n",
      "Epoch 00027: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0711 - acc: 0.9765 - val_loss: 0.3084 - val_acc: 0.9245\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9822\n",
      "Epoch 00028: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0541 - acc: 0.9822 - val_loss: 0.2421 - val_acc: 0.9453\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9819\n",
      "Epoch 00029: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0558 - acc: 0.9819 - val_loss: 0.2551 - val_acc: 0.9299\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9817\n",
      "Epoch 00030: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0553 - acc: 0.9817 - val_loss: 0.2388 - val_acc: 0.9336\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9808\n",
      "Epoch 00031: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0560 - acc: 0.9807 - val_loss: 0.2363 - val_acc: 0.9390\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9803\n",
      "Epoch 00032: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 208s 6ms/sample - loss: 0.0572 - acc: 0.9803 - val_loss: 0.2001 - val_acc: 0.9488\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9818\n",
      "Epoch 00033: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0541 - acc: 0.9818 - val_loss: 0.2024 - val_acc: 0.9543\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9864\n",
      "Epoch 00034: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 208s 6ms/sample - loss: 0.0402 - acc: 0.9864 - val_loss: 0.3086 - val_acc: 0.9380\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9862\n",
      "Epoch 00035: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0414 - acc: 0.9862 - val_loss: 0.2048 - val_acc: 0.9502\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9851\n",
      "Epoch 00036: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0465 - acc: 0.9850 - val_loss: 0.2089 - val_acc: 0.9469\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9870\n",
      "Epoch 00037: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0403 - acc: 0.9870 - val_loss: 0.2366 - val_acc: 0.9481\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9898\n",
      "Epoch 00038: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 208s 6ms/sample - loss: 0.0321 - acc: 0.9898 - val_loss: 0.2014 - val_acc: 0.9534\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9868\n",
      "Epoch 00039: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0383 - acc: 0.9868 - val_loss: 0.2302 - val_acc: 0.9509\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9813\n",
      "Epoch 00040: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0548 - acc: 0.9813 - val_loss: 0.2175 - val_acc: 0.9515\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9913\n",
      "Epoch 00041: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0272 - acc: 0.9913 - val_loss: 0.2809 - val_acc: 0.9408\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9841\n",
      "Epoch 00042: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0493 - acc: 0.9841 - val_loss: 0.2086 - val_acc: 0.9534\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9922\n",
      "Epoch 00043: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0237 - acc: 0.9922 - val_loss: 0.2442 - val_acc: 0.9478\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9920\n",
      "Epoch 00044: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0261 - acc: 0.9920 - val_loss: 0.2632 - val_acc: 0.9446\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9902\n",
      "Epoch 00045: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0294 - acc: 0.9902 - val_loss: 0.2860 - val_acc: 0.9341\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9840\n",
      "Epoch 00046: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0508 - acc: 0.9840 - val_loss: 0.2259 - val_acc: 0.9457\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9863\n",
      "Epoch 00047: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0415 - acc: 0.9863 - val_loss: 0.2978 - val_acc: 0.9418\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9907\n",
      "Epoch 00048: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0310 - acc: 0.9907 - val_loss: 0.1953 - val_acc: 0.9597\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9944\n",
      "Epoch 00049: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0181 - acc: 0.9944 - val_loss: 0.3115 - val_acc: 0.9352\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9921\n",
      "Epoch 00050: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0259 - acc: 0.9921 - val_loss: 0.2758 - val_acc: 0.9448\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9908\n",
      "Epoch 00051: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0273 - acc: 0.9907 - val_loss: 0.2456 - val_acc: 0.9481\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9882\n",
      "Epoch 00052: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0385 - acc: 0.9882 - val_loss: 0.3057 - val_acc: 0.9399\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9883\n",
      "Epoch 00053: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0373 - acc: 0.9883 - val_loss: 0.2206 - val_acc: 0.9499\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9916\n",
      "Epoch 00054: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0264 - acc: 0.9916 - val_loss: 0.1952 - val_acc: 0.9585\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9946\n",
      "Epoch 00055: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0180 - acc: 0.9946 - val_loss: 0.2056 - val_acc: 0.9590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9935\n",
      "Epoch 00056: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 208s 6ms/sample - loss: 0.0207 - acc: 0.9935 - val_loss: 0.2054 - val_acc: 0.9585\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9932\n",
      "Epoch 00057: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0202 - acc: 0.9932 - val_loss: 0.2141 - val_acc: 0.9527\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9911\n",
      "Epoch 00058: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0274 - acc: 0.9911 - val_loss: 0.2190 - val_acc: 0.9499\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9923\n",
      "Epoch 00059: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0236 - acc: 0.9923 - val_loss: 0.2065 - val_acc: 0.9574\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9935\n",
      "Epoch 00060: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0204 - acc: 0.9935 - val_loss: 0.3407 - val_acc: 0.9380\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9934\n",
      "Epoch 00061: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0196 - acc: 0.9934 - val_loss: 0.2096 - val_acc: 0.9562\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9929\n",
      "Epoch 00062: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0222 - acc: 0.9929 - val_loss: 0.3223 - val_acc: 0.9345\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9904\n",
      "Epoch 00063: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0302 - acc: 0.9904 - val_loss: 0.2708 - val_acc: 0.9499\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9920\n",
      "Epoch 00064: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0250 - acc: 0.9920 - val_loss: 0.2317 - val_acc: 0.9518\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9953\n",
      "Epoch 00065: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 208s 6ms/sample - loss: 0.0144 - acc: 0.9953 - val_loss: 0.2539 - val_acc: 0.9525\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9930\n",
      "Epoch 00066: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0210 - acc: 0.9930 - val_loss: 0.2544 - val_acc: 0.9441\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9939\n",
      "Epoch 00067: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0178 - acc: 0.9939 - val_loss: 0.3193 - val_acc: 0.9422\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9918\n",
      "Epoch 00068: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0238 - acc: 0.9918 - val_loss: 0.2733 - val_acc: 0.9464\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9930\n",
      "Epoch 00069: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0221 - acc: 0.9930 - val_loss: 0.2219 - val_acc: 0.9541\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9963\n",
      "Epoch 00070: val_loss did not improve from 0.19048\n",
      "36805/36805 [==============================] - 209s 6ms/sample - loss: 0.0129 - acc: 0.9963 - val_loss: 0.2917 - val_acc: 0.9401\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.2393 - acc: 0.9321\n",
      "Loss: 0.23932929773073205 Accuracy: 0.93208724\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3701 - acc: 0.2876\n",
      "Epoch 00001: val_loss improved from inf to 1.57649, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/001-1.5765.hdf5\n",
      "36805/36805 [==============================] - 313s 9ms/sample - loss: 2.3699 - acc: 0.2876 - val_loss: 1.5765 - val_acc: 0.4805\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1926 - acc: 0.6228\n",
      "Epoch 00002: val_loss improved from 1.57649 to 0.74567, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/002-0.7457.hdf5\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 1.1925 - acc: 0.6228 - val_loss: 0.7457 - val_acc: 0.7803\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6958 - acc: 0.7839\n",
      "Epoch 00003: val_loss improved from 0.74567 to 0.60346, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/003-0.6035.hdf5\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.6961 - acc: 0.7839 - val_loss: 0.6035 - val_acc: 0.8230\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4874 - acc: 0.8479\n",
      "Epoch 00004: val_loss improved from 0.60346 to 0.34300, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/004-0.3430.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.4876 - acc: 0.8478 - val_loss: 0.3430 - val_acc: 0.8970\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8816\n",
      "Epoch 00005: val_loss improved from 0.34300 to 0.29941, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/005-0.2994.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.3815 - acc: 0.8816 - val_loss: 0.2994 - val_acc: 0.9136\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9032\n",
      "Epoch 00006: val_loss improved from 0.29941 to 0.27631, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/006-0.2763.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.3091 - acc: 0.9032 - val_loss: 0.2763 - val_acc: 0.9192\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9176\n",
      "Epoch 00007: val_loss improved from 0.27631 to 0.22796, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/007-0.2280.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.2673 - acc: 0.9176 - val_loss: 0.2280 - val_acc: 0.9341\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9296\n",
      "Epoch 00008: val_loss did not improve from 0.22796\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.2260 - acc: 0.9296 - val_loss: 0.2637 - val_acc: 0.9210\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9303\n",
      "Epoch 00009: val_loss improved from 0.22796 to 0.21855, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/009-0.2186.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.2176 - acc: 0.9303 - val_loss: 0.2186 - val_acc: 0.9352\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9416\n",
      "Epoch 00010: val_loss improved from 0.21855 to 0.21689, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/010-0.2169.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.1814 - acc: 0.9416 - val_loss: 0.2169 - val_acc: 0.9385\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9494\n",
      "Epoch 00011: val_loss did not improve from 0.21689\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.1589 - acc: 0.9494 - val_loss: 0.2247 - val_acc: 0.9366\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9530\n",
      "Epoch 00012: val_loss did not improve from 0.21689\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.1469 - acc: 0.9530 - val_loss: 0.2416 - val_acc: 0.9297\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9579\n",
      "Epoch 00013: val_loss did not improve from 0.21689\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.1340 - acc: 0.9578 - val_loss: 0.2194 - val_acc: 0.9373\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9577\n",
      "Epoch 00014: val_loss improved from 0.21689 to 0.14636, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/014-0.1464.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.1359 - acc: 0.9577 - val_loss: 0.1464 - val_acc: 0.9592\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9607\n",
      "Epoch 00015: val_loss did not improve from 0.14636\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.1212 - acc: 0.9607 - val_loss: 0.1664 - val_acc: 0.9532\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9675\n",
      "Epoch 00016: val_loss improved from 0.14636 to 0.13879, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/016-0.1388.hdf5\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0993 - acc: 0.9675 - val_loss: 0.1388 - val_acc: 0.9602\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9652\n",
      "Epoch 00017: val_loss did not improve from 0.13879\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.1050 - acc: 0.9652 - val_loss: 0.2147 - val_acc: 0.9364\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9717\n",
      "Epoch 00018: val_loss did not improve from 0.13879\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0857 - acc: 0.9717 - val_loss: 0.1735 - val_acc: 0.9557\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9741\n",
      "Epoch 00019: val_loss did not improve from 0.13879\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0798 - acc: 0.9741 - val_loss: 0.1971 - val_acc: 0.9467\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9732\n",
      "Epoch 00020: val_loss did not improve from 0.13879\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0785 - acc: 0.9732 - val_loss: 0.1624 - val_acc: 0.9553\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9776\n",
      "Epoch 00021: val_loss did not improve from 0.13879\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0684 - acc: 0.9776 - val_loss: 0.1741 - val_acc: 0.9513\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9735\n",
      "Epoch 00022: val_loss did not improve from 0.13879\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0809 - acc: 0.9735 - val_loss: 0.1675 - val_acc: 0.9560\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9768\n",
      "Epoch 00023: val_loss did not improve from 0.13879\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0708 - acc: 0.9768 - val_loss: 0.1876 - val_acc: 0.9511\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9758\n",
      "Epoch 00024: val_loss did not improve from 0.13879\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0732 - acc: 0.9758 - val_loss: 0.1803 - val_acc: 0.9536\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9780\n",
      "Epoch 00025: val_loss improved from 0.13879 to 0.12459, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/025-0.1246.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0665 - acc: 0.9780 - val_loss: 0.1246 - val_acc: 0.9644\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9866\n",
      "Epoch 00026: val_loss did not improve from 0.12459\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0442 - acc: 0.9866 - val_loss: 0.1569 - val_acc: 0.9576\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9837\n",
      "Epoch 00027: val_loss did not improve from 0.12459\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0512 - acc: 0.9838 - val_loss: 0.1880 - val_acc: 0.9518\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9847\n",
      "Epoch 00028: val_loss did not improve from 0.12459\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0480 - acc: 0.9847 - val_loss: 0.1719 - val_acc: 0.9555\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9852\n",
      "Epoch 00029: val_loss did not improve from 0.12459\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0463 - acc: 0.9852 - val_loss: 0.2751 - val_acc: 0.9304\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9858\n",
      "Epoch 00030: val_loss did not improve from 0.12459\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0450 - acc: 0.9857 - val_loss: 0.2009 - val_acc: 0.9481\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9816\n",
      "Epoch 00031: val_loss did not improve from 0.12459\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0546 - acc: 0.9816 - val_loss: 0.1712 - val_acc: 0.9546\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9845\n",
      "Epoch 00032: val_loss improved from 0.12459 to 0.12355, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv_checkpoint/032-0.1235.hdf5\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0490 - acc: 0.9845 - val_loss: 0.1235 - val_acc: 0.9665\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9895\n",
      "Epoch 00033: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0329 - acc: 0.9895 - val_loss: 0.1247 - val_acc: 0.9651\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9885\n",
      "Epoch 00034: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0352 - acc: 0.9885 - val_loss: 0.1841 - val_acc: 0.9518\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9890\n",
      "Epoch 00035: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0341 - acc: 0.9890 - val_loss: 0.3443 - val_acc: 0.9220\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9832\n",
      "Epoch 00036: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0535 - acc: 0.9832 - val_loss: 0.1451 - val_acc: 0.9662\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9870\n",
      "Epoch 00037: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0405 - acc: 0.9870 - val_loss: 0.1377 - val_acc: 0.9653\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9907\n",
      "Epoch 00038: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0278 - acc: 0.9907 - val_loss: 0.2115 - val_acc: 0.9481\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9887\n",
      "Epoch 00039: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0365 - acc: 0.9887 - val_loss: 0.1754 - val_acc: 0.9606\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9892\n",
      "Epoch 00040: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0322 - acc: 0.9892 - val_loss: 0.1781 - val_acc: 0.9609\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9885\n",
      "Epoch 00041: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0365 - acc: 0.9885 - val_loss: 0.1723 - val_acc: 0.9571\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9924\n",
      "Epoch 00042: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0239 - acc: 0.9924 - val_loss: 0.1691 - val_acc: 0.9562\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9908\n",
      "Epoch 00043: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0272 - acc: 0.9908 - val_loss: 0.1727 - val_acc: 0.9604\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9871\n",
      "Epoch 00044: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0398 - acc: 0.9871 - val_loss: 0.1769 - val_acc: 0.9576\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9924\n",
      "Epoch 00045: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0218 - acc: 0.9924 - val_loss: 0.1530 - val_acc: 0.9641\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9931\n",
      "Epoch 00046: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0207 - acc: 0.9931 - val_loss: 0.2398 - val_acc: 0.9453\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9921\n",
      "Epoch 00047: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0242 - acc: 0.9921 - val_loss: 0.1976 - val_acc: 0.9571\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9921\n",
      "Epoch 00048: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0241 - acc: 0.9921 - val_loss: 0.1589 - val_acc: 0.9613\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9930\n",
      "Epoch 00049: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0214 - acc: 0.9930 - val_loss: 0.1952 - val_acc: 0.9574\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9928\n",
      "Epoch 00050: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0236 - acc: 0.9927 - val_loss: 0.2447 - val_acc: 0.9492\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9862\n",
      "Epoch 00051: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0420 - acc: 0.9863 - val_loss: 0.1749 - val_acc: 0.9574\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9952\n",
      "Epoch 00052: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0157 - acc: 0.9952 - val_loss: 0.1864 - val_acc: 0.9630\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9948\n",
      "Epoch 00053: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0178 - acc: 0.9948 - val_loss: 0.2174 - val_acc: 0.9576\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9894\n",
      "Epoch 00054: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0327 - acc: 0.9893 - val_loss: 0.1647 - val_acc: 0.9646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9915\n",
      "Epoch 00055: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0267 - acc: 0.9914 - val_loss: 0.1778 - val_acc: 0.9648\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9920\n",
      "Epoch 00056: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0256 - acc: 0.9920 - val_loss: 0.1495 - val_acc: 0.9655\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9960\n",
      "Epoch 00057: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0145 - acc: 0.9960 - val_loss: 0.1529 - val_acc: 0.9667\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9958\n",
      "Epoch 00058: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0146 - acc: 0.9958 - val_loss: 0.1775 - val_acc: 0.9599\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9904\n",
      "Epoch 00059: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0310 - acc: 0.9903 - val_loss: 0.1523 - val_acc: 0.9651\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9903\n",
      "Epoch 00060: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0307 - acc: 0.9903 - val_loss: 0.1374 - val_acc: 0.9655\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9928\n",
      "Epoch 00061: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0209 - acc: 0.9928 - val_loss: 0.1339 - val_acc: 0.9702\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9929\n",
      "Epoch 00062: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0224 - acc: 0.9929 - val_loss: 0.1391 - val_acc: 0.9662\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9967\n",
      "Epoch 00063: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0112 - acc: 0.9967 - val_loss: 0.1791 - val_acc: 0.9625\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9964\n",
      "Epoch 00064: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0125 - acc: 0.9964 - val_loss: 0.1841 - val_acc: 0.9644\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9933\n",
      "Epoch 00065: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0209 - acc: 0.9933 - val_loss: 0.2003 - val_acc: 0.9546\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 00066: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0146 - acc: 0.9954 - val_loss: 0.1739 - val_acc: 0.9665\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9949\n",
      "Epoch 00067: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0158 - acc: 0.9949 - val_loss: 0.1736 - val_acc: 0.9655\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9955\n",
      "Epoch 00068: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0136 - acc: 0.9955 - val_loss: 0.1837 - val_acc: 0.9623\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9948\n",
      "Epoch 00069: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0162 - acc: 0.9948 - val_loss: 0.2045 - val_acc: 0.9602\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9956\n",
      "Epoch 00070: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0143 - acc: 0.9956 - val_loss: 0.1791 - val_acc: 0.9674\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9964\n",
      "Epoch 00071: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0128 - acc: 0.9964 - val_loss: 0.2101 - val_acc: 0.9574\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9922\n",
      "Epoch 00072: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0231 - acc: 0.9922 - val_loss: 0.1700 - val_acc: 0.9632\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9943\n",
      "Epoch 00073: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0162 - acc: 0.9943 - val_loss: 0.1510 - val_acc: 0.9665\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9971\n",
      "Epoch 00074: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0096 - acc: 0.9971 - val_loss: 0.1556 - val_acc: 0.9720\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9951\n",
      "Epoch 00075: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0146 - acc: 0.9951 - val_loss: 0.1813 - val_acc: 0.9634\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9930\n",
      "Epoch 00076: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0210 - acc: 0.9930 - val_loss: 0.1550 - val_acc: 0.9630\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9943\n",
      "Epoch 00077: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0173 - acc: 0.9943 - val_loss: 0.1679 - val_acc: 0.9646\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9942\n",
      "Epoch 00078: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0178 - acc: 0.9942 - val_loss: 0.1667 - val_acc: 0.9658\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9954\n",
      "Epoch 00079: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0151 - acc: 0.9954 - val_loss: 0.1877 - val_acc: 0.9616\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9971\n",
      "Epoch 00080: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0097 - acc: 0.9971 - val_loss: 0.1560 - val_acc: 0.9681\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9982\n",
      "Epoch 00081: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0060 - acc: 0.9982 - val_loss: 0.1312 - val_acc: 0.9723\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9934\n",
      "Epoch 00082: val_loss did not improve from 0.12355\n",
      "36805/36805 [==============================] - 211s 6ms/sample - loss: 0.0218 - acc: 0.9933 - val_loss: 0.2125 - val_acc: 0.9613\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1635 - acc: 0.9545\n",
      "Loss: 0.16346425907783063 Accuracy: 0.9545171\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3987 - acc: 0.2868\n",
      "Epoch 00001: val_loss improved from inf to 1.76349, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/001-1.7635.hdf5\n",
      "36805/36805 [==============================] - 336s 9ms/sample - loss: 2.3989 - acc: 0.2868 - val_loss: 1.7635 - val_acc: 0.4419\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0961 - acc: 0.6549\n",
      "Epoch 00002: val_loss improved from 1.76349 to 0.66680, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/002-0.6668.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 1.0963 - acc: 0.6548 - val_loss: 0.6668 - val_acc: 0.7932\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6377 - acc: 0.8039\n",
      "Epoch 00003: val_loss improved from 0.66680 to 0.39611, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/003-0.3961.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.6378 - acc: 0.8039 - val_loss: 0.3961 - val_acc: 0.8777\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4475 - acc: 0.8632\n",
      "Epoch 00004: val_loss did not improve from 0.39611\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.4477 - acc: 0.8631 - val_loss: 0.4134 - val_acc: 0.8770\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.8888\n",
      "Epoch 00005: val_loss improved from 0.39611 to 0.23838, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/005-0.2384.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.3555 - acc: 0.8888 - val_loss: 0.2384 - val_acc: 0.9266\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9145\n",
      "Epoch 00006: val_loss did not improve from 0.23838\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.2744 - acc: 0.9145 - val_loss: 0.2638 - val_acc: 0.9201\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9208\n",
      "Epoch 00007: val_loss improved from 0.23838 to 0.18268, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/007-0.1827.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.2493 - acc: 0.9208 - val_loss: 0.1827 - val_acc: 0.9453\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9335\n",
      "Epoch 00008: val_loss did not improve from 0.18268\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.2137 - acc: 0.9334 - val_loss: 0.2350 - val_acc: 0.9285\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9388\n",
      "Epoch 00009: val_loss did not improve from 0.18268\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1878 - acc: 0.9388 - val_loss: 0.2863 - val_acc: 0.9152\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9462\n",
      "Epoch 00010: val_loss improved from 0.18268 to 0.17970, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/010-0.1797.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.1664 - acc: 0.9461 - val_loss: 0.1797 - val_acc: 0.9474\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9496\n",
      "Epoch 00011: val_loss did not improve from 0.17970\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1594 - acc: 0.9497 - val_loss: 0.1928 - val_acc: 0.9460\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9585\n",
      "Epoch 00012: val_loss improved from 0.17970 to 0.16286, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/012-0.1629.hdf5\n",
      "36805/36805 [==============================] - 214s 6ms/sample - loss: 0.1288 - acc: 0.9585 - val_loss: 0.1629 - val_acc: 0.9527\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9593\n",
      "Epoch 00013: val_loss did not improve from 0.16286\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1235 - acc: 0.9593 - val_loss: 0.2085 - val_acc: 0.9392\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9627\n",
      "Epoch 00014: val_loss did not improve from 0.16286\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1147 - acc: 0.9627 - val_loss: 0.2250 - val_acc: 0.9397\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9655\n",
      "Epoch 00015: val_loss improved from 0.16286 to 0.15942, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/015-0.1594.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.1052 - acc: 0.9655 - val_loss: 0.1594 - val_acc: 0.9553\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9695\n",
      "Epoch 00016: val_loss did not improve from 0.15942\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0970 - acc: 0.9695 - val_loss: 0.2079 - val_acc: 0.9380\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9711\n",
      "Epoch 00017: val_loss did not improve from 0.15942\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0894 - acc: 0.9711 - val_loss: 0.1891 - val_acc: 0.9434\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9723\n",
      "Epoch 00018: val_loss improved from 0.15942 to 0.13992, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/018-0.1399.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0857 - acc: 0.9723 - val_loss: 0.1399 - val_acc: 0.9634\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9770\n",
      "Epoch 00019: val_loss did not improve from 0.13992\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0721 - acc: 0.9770 - val_loss: 0.2289 - val_acc: 0.9376\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9737\n",
      "Epoch 00020: val_loss did not improve from 0.13992\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0806 - acc: 0.9737 - val_loss: 0.1984 - val_acc: 0.9481\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9783\n",
      "Epoch 00021: val_loss did not improve from 0.13992\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0659 - acc: 0.9783 - val_loss: 0.1704 - val_acc: 0.9513\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9758\n",
      "Epoch 00022: val_loss improved from 0.13992 to 0.13949, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/022-0.1395.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0735 - acc: 0.9758 - val_loss: 0.1395 - val_acc: 0.9627\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9818\n",
      "Epoch 00023: val_loss did not improve from 0.13949\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0555 - acc: 0.9819 - val_loss: 0.1561 - val_acc: 0.9602\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9822\n",
      "Epoch 00024: val_loss did not improve from 0.13949\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0554 - acc: 0.9821 - val_loss: 0.1983 - val_acc: 0.9462\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9774\n",
      "Epoch 00025: val_loss improved from 0.13949 to 0.12038, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/025-0.1204.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0711 - acc: 0.9773 - val_loss: 0.1204 - val_acc: 0.9669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9829\n",
      "Epoch 00026: val_loss did not improve from 0.12038\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0531 - acc: 0.9829 - val_loss: 0.1597 - val_acc: 0.9560\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9870\n",
      "Epoch 00027: val_loss did not improve from 0.12038\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0418 - acc: 0.9870 - val_loss: 0.1442 - val_acc: 0.9590\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9860\n",
      "Epoch 00028: val_loss improved from 0.12038 to 0.11801, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv_checkpoint/028-0.1180.hdf5\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0436 - acc: 0.9860 - val_loss: 0.1180 - val_acc: 0.9674\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9860\n",
      "Epoch 00029: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0435 - acc: 0.9860 - val_loss: 0.1240 - val_acc: 0.9667\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9873\n",
      "Epoch 00030: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0386 - acc: 0.9873 - val_loss: 0.1374 - val_acc: 0.9648\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9868\n",
      "Epoch 00031: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0402 - acc: 0.9868 - val_loss: 0.1602 - val_acc: 0.9555\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9864\n",
      "Epoch 00032: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0433 - acc: 0.9864 - val_loss: 0.2077 - val_acc: 0.9483\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9833\n",
      "Epoch 00033: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0520 - acc: 0.9833 - val_loss: 0.1431 - val_acc: 0.9618\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9910\n",
      "Epoch 00034: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0296 - acc: 0.9910 - val_loss: 0.1477 - val_acc: 0.9632\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9904\n",
      "Epoch 00035: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0310 - acc: 0.9904 - val_loss: 0.1693 - val_acc: 0.9571\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9890\n",
      "Epoch 00036: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0334 - acc: 0.9891 - val_loss: 0.1456 - val_acc: 0.9613\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9905\n",
      "Epoch 00037: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0303 - acc: 0.9905 - val_loss: 0.1745 - val_acc: 0.9627\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9897\n",
      "Epoch 00038: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0318 - acc: 0.9897 - val_loss: 0.3038 - val_acc: 0.9331\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9907\n",
      "Epoch 00039: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0302 - acc: 0.9906 - val_loss: 0.1762 - val_acc: 0.9560\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9880\n",
      "Epoch 00040: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0379 - acc: 0.9880 - val_loss: 0.1238 - val_acc: 0.9688\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9924\n",
      "Epoch 00041: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0247 - acc: 0.9924 - val_loss: 0.1523 - val_acc: 0.9588\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9908\n",
      "Epoch 00042: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0299 - acc: 0.9908 - val_loss: 0.1582 - val_acc: 0.9613\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9929\n",
      "Epoch 00043: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 212s 6ms/sample - loss: 0.0222 - acc: 0.9929 - val_loss: 0.2161 - val_acc: 0.9469\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9928\n",
      "Epoch 00044: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0234 - acc: 0.9928 - val_loss: 0.1366 - val_acc: 0.9676\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9934\n",
      "Epoch 00045: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0212 - acc: 0.9934 - val_loss: 0.1708 - val_acc: 0.9623\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9914\n",
      "Epoch 00046: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0270 - acc: 0.9914 - val_loss: 0.1511 - val_acc: 0.9637\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9881\n",
      "Epoch 00047: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0360 - acc: 0.9881 - val_loss: 0.1443 - val_acc: 0.9688\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9953\n",
      "Epoch 00048: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0163 - acc: 0.9953 - val_loss: 0.1780 - val_acc: 0.9597\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9946\n",
      "Epoch 00049: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0186 - acc: 0.9945 - val_loss: 0.1720 - val_acc: 0.9606\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9887\n",
      "Epoch 00050: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0353 - acc: 0.9887 - val_loss: 0.2123 - val_acc: 0.9555\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9953\n",
      "Epoch 00051: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0155 - acc: 0.9953 - val_loss: 0.1595 - val_acc: 0.9627\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9946\n",
      "Epoch 00052: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0185 - acc: 0.9946 - val_loss: 0.2050 - val_acc: 0.9611\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9929\n",
      "Epoch 00053: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0218 - acc: 0.9929 - val_loss: 0.1908 - val_acc: 0.9611\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9957\n",
      "Epoch 00054: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0139 - acc: 0.9956 - val_loss: 0.2256 - val_acc: 0.9525\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9909\n",
      "Epoch 00055: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0297 - acc: 0.9909 - val_loss: 0.1636 - val_acc: 0.9639\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9946\n",
      "Epoch 00056: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0170 - acc: 0.9946 - val_loss: 0.1263 - val_acc: 0.9693\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9950\n",
      "Epoch 00057: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0154 - acc: 0.9950 - val_loss: 0.1632 - val_acc: 0.9604\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9949\n",
      "Epoch 00058: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0155 - acc: 0.9949 - val_loss: 0.1701 - val_acc: 0.9655\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9950\n",
      "Epoch 00059: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0168 - acc: 0.9950 - val_loss: 0.2217 - val_acc: 0.9541\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9904\n",
      "Epoch 00060: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0308 - acc: 0.9904 - val_loss: 0.1402 - val_acc: 0.9690\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9963\n",
      "Epoch 00061: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0118 - acc: 0.9963 - val_loss: 0.1445 - val_acc: 0.9686\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9961\n",
      "Epoch 00062: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0121 - acc: 0.9960 - val_loss: 0.1557 - val_acc: 0.9620\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9929\n",
      "Epoch 00063: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0237 - acc: 0.9929 - val_loss: 0.1891 - val_acc: 0.9606\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9960\n",
      "Epoch 00064: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0124 - acc: 0.9960 - val_loss: 0.1485 - val_acc: 0.9690\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9965\n",
      "Epoch 00065: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0112 - acc: 0.9965 - val_loss: 0.1615 - val_acc: 0.9674\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9957\n",
      "Epoch 00066: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0128 - acc: 0.9957 - val_loss: 0.1929 - val_acc: 0.9581\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9928\n",
      "Epoch 00067: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0256 - acc: 0.9928 - val_loss: 0.2016 - val_acc: 0.9536\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9965\n",
      "Epoch 00068: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0114 - acc: 0.9965 - val_loss: 0.1713 - val_acc: 0.9639\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9952\n",
      "Epoch 00069: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0162 - acc: 0.9952 - val_loss: 0.1286 - val_acc: 0.9716\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9977\n",
      "Epoch 00070: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0079 - acc: 0.9977 - val_loss: 0.1349 - val_acc: 0.9706\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9956\n",
      "Epoch 00071: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0139 - acc: 0.9956 - val_loss: 0.2026 - val_acc: 0.9557\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9961\n",
      "Epoch 00072: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0127 - acc: 0.9961 - val_loss: 0.1827 - val_acc: 0.9646\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9909\n",
      "Epoch 00073: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0282 - acc: 0.9909 - val_loss: 0.1438 - val_acc: 0.9679\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9979\n",
      "Epoch 00074: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0075 - acc: 0.9978 - val_loss: 0.1623 - val_acc: 0.9655\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9918\n",
      "Epoch 00075: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0265 - acc: 0.9918 - val_loss: 0.1380 - val_acc: 0.9700\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9979\n",
      "Epoch 00076: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0072 - acc: 0.9979 - val_loss: 0.1477 - val_acc: 0.9695\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9968\n",
      "Epoch 00077: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0100 - acc: 0.9968 - val_loss: 0.1577 - val_acc: 0.9688\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9972\n",
      "Epoch 00078: val_loss did not improve from 0.11801\n",
      "36805/36805 [==============================] - 213s 6ms/sample - loss: 0.0097 - acc: 0.9972 - val_loss: 0.1831 - val_acc: 0.9646\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 3ms/sample - loss: 0.1843 - acc: 0.9516\n",
      "Loss: 0.1843351250156559 Accuracy: 0.95160955\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0713 - acc: 0.3908\n",
      "Epoch 00001: val_loss improved from inf to 1.44285, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/001-1.4429.hdf5\n",
      "36805/36805 [==============================] - 365s 10ms/sample - loss: 2.0713 - acc: 0.3909 - val_loss: 1.4429 - val_acc: 0.5318\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8217 - acc: 0.7424\n",
      "Epoch 00002: val_loss improved from 1.44285 to 0.53528, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/002-0.5353.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.8218 - acc: 0.7424 - val_loss: 0.5353 - val_acc: 0.8281\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.8394\n",
      "Epoch 00003: val_loss improved from 0.53528 to 0.41795, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/003-0.4180.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.5136 - acc: 0.8393 - val_loss: 0.4180 - val_acc: 0.8679\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8835\n",
      "Epoch 00004: val_loss improved from 0.41795 to 0.30082, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/004-0.3008.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.3710 - acc: 0.8835 - val_loss: 0.3008 - val_acc: 0.9031\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9052\n",
      "Epoch 00005: val_loss improved from 0.30082 to 0.29162, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/005-0.2916.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.2988 - acc: 0.9053 - val_loss: 0.2916 - val_acc: 0.9110\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9222\n",
      "Epoch 00006: val_loss improved from 0.29162 to 0.24280, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/006-0.2428.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.2431 - acc: 0.9221 - val_loss: 0.2428 - val_acc: 0.9229\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9295\n",
      "Epoch 00007: val_loss improved from 0.24280 to 0.21862, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/007-0.2186.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.2158 - acc: 0.9295 - val_loss: 0.2186 - val_acc: 0.9341\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9423\n",
      "Epoch 00008: val_loss improved from 0.21862 to 0.19780, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/008-0.1978.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.1782 - acc: 0.9423 - val_loss: 0.1978 - val_acc: 0.9411\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9470\n",
      "Epoch 00009: val_loss did not improve from 0.19780\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.1649 - acc: 0.9469 - val_loss: 0.2234 - val_acc: 0.9306\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9527\n",
      "Epoch 00010: val_loss did not improve from 0.19780\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 0.1467 - acc: 0.9526 - val_loss: 0.2010 - val_acc: 0.9415\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9457\n",
      "Epoch 00011: val_loss improved from 0.19780 to 0.16755, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/011-0.1676.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.1700 - acc: 0.9457 - val_loss: 0.1676 - val_acc: 0.9525\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9616\n",
      "Epoch 00012: val_loss did not improve from 0.16755\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.1155 - acc: 0.9616 - val_loss: 0.1819 - val_acc: 0.9476\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9671\n",
      "Epoch 00013: val_loss did not improve from 0.16755\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.1010 - acc: 0.9671 - val_loss: 0.2044 - val_acc: 0.9408\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9661\n",
      "Epoch 00014: val_loss did not improve from 0.16755\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 0.1004 - acc: 0.9661 - val_loss: 0.2039 - val_acc: 0.9425\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9688\n",
      "Epoch 00015: val_loss did not improve from 0.16755\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 0.0948 - acc: 0.9688 - val_loss: 0.2415 - val_acc: 0.9334\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9689\n",
      "Epoch 00016: val_loss did not improve from 0.16755\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 0.0953 - acc: 0.9689 - val_loss: 0.2055 - val_acc: 0.9406\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9729\n",
      "Epoch 00017: val_loss did not improve from 0.16755\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 0.0853 - acc: 0.9729 - val_loss: 0.1715 - val_acc: 0.9534\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9789\n",
      "Epoch 00018: val_loss did not improve from 0.16755\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0650 - acc: 0.9789 - val_loss: 0.1985 - val_acc: 0.9432\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9745\n",
      "Epoch 00019: val_loss did not improve from 0.16755\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0793 - acc: 0.9744 - val_loss: 0.1869 - val_acc: 0.9492\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9754\n",
      "Epoch 00020: val_loss improved from 0.16755 to 0.14690, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/020-0.1469.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0749 - acc: 0.9754 - val_loss: 0.1469 - val_acc: 0.9585\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9810\n",
      "Epoch 00021: val_loss did not improve from 0.14690\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0588 - acc: 0.9810 - val_loss: 0.1916 - val_acc: 0.9518\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9832\n",
      "Epoch 00022: val_loss improved from 0.14690 to 0.13891, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/022-0.1389.hdf5\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0511 - acc: 0.9832 - val_loss: 0.1389 - val_acc: 0.9620\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9859\n",
      "Epoch 00023: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 215s 6ms/sample - loss: 0.0447 - acc: 0.9859 - val_loss: 0.1785 - val_acc: 0.9532\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9773\n",
      "Epoch 00024: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0720 - acc: 0.9772 - val_loss: 0.1746 - val_acc: 0.9546\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9802\n",
      "Epoch 00025: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0618 - acc: 0.9802 - val_loss: 0.1525 - val_acc: 0.9588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9886\n",
      "Epoch 00026: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0360 - acc: 0.9886 - val_loss: 0.1930 - val_acc: 0.9504\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9854\n",
      "Epoch 00027: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0442 - acc: 0.9854 - val_loss: 0.2087 - val_acc: 0.9532\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9867\n",
      "Epoch 00028: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0420 - acc: 0.9867 - val_loss: 0.1892 - val_acc: 0.9504\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9851\n",
      "Epoch 00029: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0445 - acc: 0.9851 - val_loss: 0.1669 - val_acc: 0.9578\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9900\n",
      "Epoch 00030: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0316 - acc: 0.9900 - val_loss: 0.1808 - val_acc: 0.9555\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9889\n",
      "Epoch 00031: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0336 - acc: 0.9889 - val_loss: 0.2565 - val_acc: 0.9390\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9892\n",
      "Epoch 00032: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0332 - acc: 0.9892 - val_loss: 0.1974 - val_acc: 0.9502\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9902\n",
      "Epoch 00033: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0307 - acc: 0.9902 - val_loss: 0.2212 - val_acc: 0.9509\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9917\n",
      "Epoch 00034: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0268 - acc: 0.9917 - val_loss: 0.1786 - val_acc: 0.9543\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9890\n",
      "Epoch 00035: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0332 - acc: 0.9890 - val_loss: 0.1664 - val_acc: 0.9604\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9844\n",
      "Epoch 00036: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0472 - acc: 0.9844 - val_loss: 0.1469 - val_acc: 0.9644\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9918\n",
      "Epoch 00037: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0275 - acc: 0.9918 - val_loss: 0.1436 - val_acc: 0.9672\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9942\n",
      "Epoch 00038: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0190 - acc: 0.9942 - val_loss: 0.1933 - val_acc: 0.9585\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9924\n",
      "Epoch 00039: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0234 - acc: 0.9923 - val_loss: 0.1602 - val_acc: 0.9618\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9887\n",
      "Epoch 00040: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0359 - acc: 0.9887 - val_loss: 0.1603 - val_acc: 0.9604\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9930\n",
      "Epoch 00041: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0213 - acc: 0.9930 - val_loss: 0.1513 - val_acc: 0.9662\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9913\n",
      "Epoch 00042: val_loss did not improve from 0.13891\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0260 - acc: 0.9913 - val_loss: 0.1929 - val_acc: 0.9562\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9922\n",
      "Epoch 00043: val_loss improved from 0.13891 to 0.13304, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv_checkpoint/043-0.1330.hdf5\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0248 - acc: 0.9922 - val_loss: 0.1330 - val_acc: 0.9683\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9942\n",
      "Epoch 00044: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0184 - acc: 0.9942 - val_loss: 0.2406 - val_acc: 0.9485\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9900\n",
      "Epoch 00045: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0322 - acc: 0.9900 - val_loss: 0.1771 - val_acc: 0.9578\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9942\n",
      "Epoch 00046: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0180 - acc: 0.9942 - val_loss: 0.1958 - val_acc: 0.9562\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9942\n",
      "Epoch 00047: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0188 - acc: 0.9942 - val_loss: 0.2000 - val_acc: 0.9595\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9948\n",
      "Epoch 00048: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0163 - acc: 0.9948 - val_loss: 0.1669 - val_acc: 0.9623\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9951\n",
      "Epoch 00049: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 219s 6ms/sample - loss: 0.0162 - acc: 0.9951 - val_loss: 0.2204 - val_acc: 0.9504\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9930\n",
      "Epoch 00050: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0224 - acc: 0.9929 - val_loss: 0.1872 - val_acc: 0.9555\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9886\n",
      "Epoch 00051: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0389 - acc: 0.9886 - val_loss: 0.1410 - val_acc: 0.9683\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9959\n",
      "Epoch 00052: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0126 - acc: 0.9959 - val_loss: 0.1983 - val_acc: 0.9599\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9947\n",
      "Epoch 00053: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0150 - acc: 0.9947 - val_loss: 0.1546 - val_acc: 0.9658\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9970\n",
      "Epoch 00054: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.2031 - val_acc: 0.9576\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9941\n",
      "Epoch 00055: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0181 - acc: 0.9941 - val_loss: 0.1758 - val_acc: 0.9632\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9940\n",
      "Epoch 00056: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0183 - acc: 0.9939 - val_loss: 0.2840 - val_acc: 0.9518\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9920\n",
      "Epoch 00057: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0257 - acc: 0.9920 - val_loss: 0.1592 - val_acc: 0.9646\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9965\n",
      "Epoch 00058: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0125 - acc: 0.9965 - val_loss: 0.1634 - val_acc: 0.9639\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9946\n",
      "Epoch 00059: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0176 - acc: 0.9945 - val_loss: 0.1885 - val_acc: 0.9639\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9928\n",
      "Epoch 00060: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0228 - acc: 0.9928 - val_loss: 0.1489 - val_acc: 0.9693\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9951\n",
      "Epoch 00061: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0159 - acc: 0.9951 - val_loss: 0.1667 - val_acc: 0.9648\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9963\n",
      "Epoch 00062: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0110 - acc: 0.9963 - val_loss: 0.1445 - val_acc: 0.9676\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9971\n",
      "Epoch 00063: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0089 - acc: 0.9971 - val_loss: 0.2006 - val_acc: 0.9634\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9919\n",
      "Epoch 00064: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0293 - acc: 0.9919 - val_loss: 0.1499 - val_acc: 0.9667\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9986\n",
      "Epoch 00065: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.1597 - val_acc: 0.9667\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9959\n",
      "Epoch 00066: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0128 - acc: 0.9959 - val_loss: 0.1688 - val_acc: 0.9674\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9952\n",
      "Epoch 00067: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0155 - acc: 0.9952 - val_loss: 0.1493 - val_acc: 0.9700\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9974\n",
      "Epoch 00068: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0089 - acc: 0.9974 - val_loss: 0.1793 - val_acc: 0.9639\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9949\n",
      "Epoch 00069: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0156 - acc: 0.9949 - val_loss: 0.1873 - val_acc: 0.9634\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9970\n",
      "Epoch 00070: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0096 - acc: 0.9970 - val_loss: 0.2126 - val_acc: 0.9574\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9962\n",
      "Epoch 00071: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0134 - acc: 0.9961 - val_loss: 0.2295 - val_acc: 0.9536\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9924\n",
      "Epoch 00072: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0251 - acc: 0.9924 - val_loss: 0.1651 - val_acc: 0.9639\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9980\n",
      "Epoch 00073: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0063 - acc: 0.9980 - val_loss: 0.1540 - val_acc: 0.9697\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 00074: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0068 - acc: 0.9977 - val_loss: 0.2044 - val_acc: 0.9541\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9968\n",
      "Epoch 00075: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0099 - acc: 0.9967 - val_loss: 0.2208 - val_acc: 0.9616\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9944\n",
      "Epoch 00076: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0198 - acc: 0.9943 - val_loss: 0.1591 - val_acc: 0.9683\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9953\n",
      "Epoch 00077: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 216s 6ms/sample - loss: 0.0148 - acc: 0.9953 - val_loss: 0.1720 - val_acc: 0.9679\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9975\n",
      "Epoch 00078: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0085 - acc: 0.9974 - val_loss: 0.2126 - val_acc: 0.9571\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9931\n",
      "Epoch 00079: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0210 - acc: 0.9931 - val_loss: 0.1672 - val_acc: 0.9665\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 00080: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0046 - acc: 0.9987 - val_loss: 0.1904 - val_acc: 0.9646\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9980\n",
      "Epoch 00081: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0064 - acc: 0.9980 - val_loss: 0.1665 - val_acc: 0.9674\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9964\n",
      "Epoch 00082: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0100 - acc: 0.9964 - val_loss: 0.2001 - val_acc: 0.9609\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9971\n",
      "Epoch 00083: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0091 - acc: 0.9971 - val_loss: 0.1767 - val_acc: 0.9634\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9970\n",
      "Epoch 00084: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0088 - acc: 0.9970 - val_loss: 0.1801 - val_acc: 0.9637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9970\n",
      "Epoch 00085: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0090 - acc: 0.9970 - val_loss: 0.1908 - val_acc: 0.9655\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9951\n",
      "Epoch 00086: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0146 - acc: 0.9951 - val_loss: 0.1849 - val_acc: 0.9648\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9977\n",
      "Epoch 00087: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0077 - acc: 0.9977 - val_loss: 0.1855 - val_acc: 0.9644\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9963\n",
      "Epoch 00088: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0121 - acc: 0.9963 - val_loss: 0.1712 - val_acc: 0.9660\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9957\n",
      "Epoch 00089: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0140 - acc: 0.9957 - val_loss: 0.1532 - val_acc: 0.9653\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9964\n",
      "Epoch 00090: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0129 - acc: 0.9964 - val_loss: 0.1896 - val_acc: 0.9623\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9979\n",
      "Epoch 00091: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0069 - acc: 0.9979 - val_loss: 0.1640 - val_acc: 0.9681\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9955\n",
      "Epoch 00092: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 218s 6ms/sample - loss: 0.0149 - acc: 0.9955 - val_loss: 0.1541 - val_acc: 0.9690\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9981\n",
      "Epoch 00093: val_loss did not improve from 0.13304\n",
      "36805/36805 [==============================] - 217s 6ms/sample - loss: 0.0060 - acc: 0.9981 - val_loss: 0.1586 - val_acc: 0.9669\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.1681 - acc: 0.9562\n",
      "Loss: 0.16810978780329414 Accuracy: 0.9561786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_pool_2_DO_BN'\n",
    "\n",
    "for i in range(1, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_3_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_188 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_189 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_190 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_191 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_192 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_193 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                4096016   \n",
      "=================================================================\n",
      "Total params: 4,159,568\n",
      "Trainable params: 4,158,800\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.8901 - acc: 0.4876\n",
      "Loss: 1.8901062693551323 Accuracy: 0.4876428\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_194 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_194 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_195 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_196 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_196 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_197 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_198 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_199 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_200 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_201 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,136,784\n",
      "Trainable params: 2,135,760\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.4117 - acc: 0.6154\n",
      "Loss: 1.4116695821470933 Accuracy: 0.61536866\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_202 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_202 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_203 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_204 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_205 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_206 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_206 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_207 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_208 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_209 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_210 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_210 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_211 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_211 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,211,792\n",
      "Trainable params: 2,210,256\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.2342 - acc: 0.6503\n",
      "Loss: 1.2341685036880081 Accuracy: 0.6502596\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_212 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_212 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_212 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_213 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_213 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_214 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_214 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_215 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_215 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_216 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_216 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_217 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_217 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_218 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_218 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_218 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_219 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_219 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_220 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_220 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_221 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_221 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_222 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_222 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_222 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_223 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_223 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,287,376\n",
      "Trainable params: 1,285,328\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.9189 - acc: 0.7697\n",
      "Loss: 0.9188817410825569 Accuracy: 0.7696781\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_224 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_224 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_224 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_225 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_225 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_225 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_226 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_226 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_226 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_227 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_227 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_227 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_228 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_228 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_228 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_229 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_229 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_229 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_230 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_230 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_230 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_231 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_231 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_231 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_232 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_232 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_232 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_233 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_233 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_233 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_234 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_234 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_234 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_235 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_235 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_236 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_236 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_236 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_237 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_237 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_237 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 874,960\n",
      "Trainable params: 872,400\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.6970 - acc: 0.7992\n",
      "Loss: 0.6969671004657805 Accuracy: 0.79916924\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_238 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_238 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_238 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_239 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_239 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_239 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_240 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_240 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_240 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_241 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_241 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_242 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_242 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_243 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_243 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_244 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_244 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_245 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_245 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_246 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_246 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_247 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_247 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_248 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_248 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_249 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_249 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_250 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_250 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_251 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_251 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_251 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_252 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_252 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_252 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_253 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_253 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_253 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 718,544\n",
      "Trainable params: 715,472\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.4885 - acc: 0.8773\n",
      "Loss: 0.48854290922854177 Accuracy: 0.87725854\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_254 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_254 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_254 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_255 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_255 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_255 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_256 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_256 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_256 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_257 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_257 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_258 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_258 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_259 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_259 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_260 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_260 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_261 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_261 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_262 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_262 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_263 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_263 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_264 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_264 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_265 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_265 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_265 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_266 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_266 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_266 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_267 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_267 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_267 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_268 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_268 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_268 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_269 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_269 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_269 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_270 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_270 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_270 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_271 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_271 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_271 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                253968    \n",
      "=================================================================\n",
      "Total params: 1,013,968\n",
      "Trainable params: 1,009,872\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.3295 - acc: 0.9298\n",
      "Loss: 0.3295109964985505 Accuracy: 0.9298027\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_272 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_272 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_272 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_273 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_273 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_273 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_274 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_274 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_274 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_275 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_275 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_275 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_276 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_276 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_276 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_277 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_277 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_277 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_278 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_278 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_278 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_279 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_279 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_279 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_280 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_280 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_280 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_281 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_281 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_281 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_282 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_282 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_282 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_283 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_283 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_283 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_284 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_284 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_284 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_285 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_285 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_285 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_286 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_286 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_286 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_287 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_287 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_287 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_288 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_288 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_288 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_289 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_289 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_289 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_290 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_290 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_290 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_291 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_291 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_291 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 1,282,768\n",
      "Trainable params: 1,277,648\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.2393 - acc: 0.9321\n",
      "Loss: 0.23932929773073205 Accuracy: 0.93208724\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_292 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_292 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_292 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_293 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_293 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_293 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_294 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_294 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_294 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_295 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_295 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_295 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_296 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_296 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_296 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_297 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_297 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_297 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_298 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_298 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_298 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_299 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_299 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_299 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_300 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_300 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_300 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_301 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_301 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_301 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_302 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_302 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_302 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_303 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_303 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_303 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_304 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_304 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_304 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_305 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_305 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_305 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_306 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_306 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_306 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_307 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_307 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_307 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_308 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_308 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_308 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_309 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_309 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_309 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_310 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_310 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_310 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_311 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_311 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_311 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_312 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_312 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_312 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_313 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_313 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_313 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                61456     \n",
      "=================================================================\n",
      "Total params: 1,613,008\n",
      "Trainable params: 1,606,864\n",
      "Non-trainable params: 6,144\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1635 - acc: 0.9545\n",
      "Loss: 0.16346425907783063 Accuracy: 0.9545171\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_314 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_314 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_314 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_315 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_315 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_315 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_316 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_316 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_316 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_317 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_317 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_317 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_318 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_318 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_318 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_319 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_319 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_319 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_320 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_320 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_320 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_321 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_321 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_321 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_322 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_322 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_322 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_323 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_323 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_323 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_324 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_324 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_324 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_325 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_325 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_325 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_326 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_326 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_326 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_327 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_327 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_327 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_328 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_328 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_329 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_329 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_330 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_331 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_332 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_332 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_333 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_333 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_334 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_334 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_334 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_335 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_335 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_335 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_336 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_336 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_336 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_337 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_337 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_337 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,976,016\n",
      "Trainable params: 1,968,848\n",
      "Non-trainable params: 7,168\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1843 - acc: 0.9516\n",
      "Loss: 0.1843351250156559 Accuracy: 0.95160955\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_338 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_338 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_338 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_339 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_339 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_340 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_340 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_341 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_341 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_342 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_342 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_343 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_343 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_344 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_344 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_345 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_345 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_346 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_346 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_347 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_347 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_348 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_348 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_349 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_349 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_350 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_350 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_351 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_351 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_352 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_352 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_353 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_353 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_354 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_354 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_355 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_355 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_356 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_357 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_357 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_358 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_358 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_358 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_359 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_359 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_359 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_360 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_360 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_360 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_361 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_361 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_361 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_362 (Conv1D)          (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_362 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_362 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_363 (Conv1D)          (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_363 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_363 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_155 (MaxPoolin (None, 3, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 16)                24592     \n",
      "=================================================================\n",
      "Total params: 3,156,688\n",
      "Trainable params: 3,147,472\n",
      "Non-trainable params: 9,216\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1681 - acc: 0.9562\n",
      "Loss: 0.16810978780329414 Accuracy: 0.9561786\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_pool_2_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 14):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_188 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_189 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_190 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_191 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_192 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_193 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                4096016   \n",
      "=================================================================\n",
      "Total params: 4,159,568\n",
      "Trainable params: 4,158,800\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 3.4462 - acc: 0.5375\n",
      "Loss: 3.4461869029860135 Accuracy: 0.53748703\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_194 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_194 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_195 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_196 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_196 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_197 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_198 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_199 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_200 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_201 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,136,784\n",
      "Trainable params: 2,135,760\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 2.4423 - acc: 0.6351\n",
      "Loss: 2.442262884241027 Accuracy: 0.63509864\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_202 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_202 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_203 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_204 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_205 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_206 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_206 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_207 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_208 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_209 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_210 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_210 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_211 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_211 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,211,792\n",
      "Trainable params: 2,210,256\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 2.0770 - acc: 0.6881\n",
      "Loss: 2.0769994277696617 Accuracy: 0.68805814\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_212 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_212 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_212 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_213 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_213 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_214 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_214 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_215 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_215 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_216 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_216 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_217 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_217 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_218 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_218 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_218 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_219 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_219 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_220 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_220 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_221 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_221 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_222 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_222 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_222 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_223 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_223 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,287,376\n",
      "Trainable params: 1,285,328\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 1.3861 - acc: 0.7695\n",
      "Loss: 1.3861420767329564 Accuracy: 0.7694704\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_224 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_224 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_224 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_225 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_225 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_225 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_226 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_226 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_226 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_227 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_227 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_227 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_228 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_228 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_228 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_229 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_229 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_229 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_230 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_230 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_230 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_231 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_231 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_231 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_232 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_232 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_232 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_233 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_233 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_233 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_234 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_234 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_234 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_235 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_235 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_236 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_236 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_236 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_237 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_237 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_237 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 874,960\n",
      "Trainable params: 872,400\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.8560 - acc: 0.8444\n",
      "Loss: 0.8560446065162944 Accuracy: 0.84444445\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_238 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_238 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_238 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_239 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_239 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_239 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_240 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_240 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_240 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_241 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_241 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_242 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_242 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_243 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_243 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_244 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_244 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_245 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_245 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_246 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_246 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_247 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_247 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_248 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_248 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_249 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_249 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_250 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_250 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_251 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_251 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_251 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_252 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_252 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_252 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_253 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_253 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_253 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 718,544\n",
      "Trainable params: 715,472\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 1.1732 - acc: 0.8181\n",
      "Loss: 1.1731528491864942 Accuracy: 0.81806856\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_254 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_254 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_254 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_255 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_255 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_255 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_256 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_256 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_256 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_257 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_257 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_258 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_258 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_259 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_259 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_260 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_260 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_261 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_261 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_262 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_262 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_263 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_263 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_264 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_264 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_265 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_265 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_265 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_266 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_266 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_266 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_267 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_267 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_267 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_268 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_268 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_268 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_269 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_269 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_269 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_270 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_270 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_270 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_271 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_271 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_271 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                253968    \n",
      "=================================================================\n",
      "Total params: 1,013,968\n",
      "Trainable params: 1,009,872\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.4205 - acc: 0.9225\n",
      "Loss: 0.4205197477591372 Accuracy: 0.92253375\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_272 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_272 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_272 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_273 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_273 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_273 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_274 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_274 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_274 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_275 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_275 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_275 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_276 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_276 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_276 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_277 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_277 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_277 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_278 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_278 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_278 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_279 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_279 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_279 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_280 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_280 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_280 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_281 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_281 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_281 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_282 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_282 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_282 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_283 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_283 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_283 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_284 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_284 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_284 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_285 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_285 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_285 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_286 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_286 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_286 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_287 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_287 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_287 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_288 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_288 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_288 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_289 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_289 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_289 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_290 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_290 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_290 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_291 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_291 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_291 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 1,282,768\n",
      "Trainable params: 1,277,648\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 3ms/sample - loss: 0.3732 - acc: 0.9298\n",
      "Loss: 0.373246261121769 Accuracy: 0.9298027\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_292 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_292 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_292 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_293 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_293 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_293 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_294 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_294 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_294 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_295 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_295 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_295 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_296 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_296 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_296 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_297 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_297 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_297 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_298 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_298 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_298 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_299 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_299 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_299 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_300 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_300 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_300 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_301 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_301 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_301 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_302 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_302 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_302 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_303 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_303 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_303 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_304 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_304 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_304 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_305 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_305 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_305 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_306 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_306 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_306 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_307 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_307 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_307 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_308 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_308 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_308 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_309 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_309 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_309 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_310 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_310 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_310 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_311 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_311 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_311 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_312 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_312 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_312 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_313 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_313 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_313 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                61456     \n",
      "=================================================================\n",
      "Total params: 1,613,008\n",
      "Trainable params: 1,606,864\n",
      "Non-trainable params: 6,144\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 3ms/sample - loss: 0.2265 - acc: 0.9529\n",
      "Loss: 0.22646390257309884 Accuracy: 0.95285565\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_314 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_314 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_314 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_315 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_315 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_315 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_316 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_316 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_316 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_317 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_317 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_317 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_318 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_318 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_318 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_319 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_319 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_319 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_320 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_320 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_320 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_321 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_321 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_321 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_322 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_322 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_322 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_323 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_323 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_323 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_324 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_324 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_324 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_325 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_325 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_325 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_326 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_326 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_326 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_327 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_327 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_327 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_328 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_328 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_329 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_329 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_330 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_331 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_332 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_332 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_333 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_333 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_334 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_334 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_334 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_335 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_335 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_335 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_336 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_336 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_336 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_337 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_337 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_337 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,976,016\n",
      "Trainable params: 1,968,848\n",
      "Non-trainable params: 7,168\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.2041 - acc: 0.9560\n",
      "Loss: 0.20412863804431688 Accuracy: 0.95597094\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_DO_BN_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_338 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_338 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_338 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_339 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_339 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_340 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_340 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_341 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_341 ( (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_342 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_342 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_343 (Conv1D)          (None, 8000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_343 ( (None, 8000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 8000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_344 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_344 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_345 (Conv1D)          (None, 4000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_345 ( (None, 4000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 4000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_346 (Conv1D)          (None, 2000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_346 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_347 (Conv1D)          (None, 2000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_347 ( (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_348 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_348 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_349 (Conv1D)          (None, 1000, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_349 ( (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_350 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_350 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_351 (Conv1D)          (None, 500, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_351 ( (None, 500, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_352 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_352 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_353 (Conv1D)          (None, 250, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_353 ( (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_354 (Conv1D)          (None, 125, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_354 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_355 (Conv1D)          (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_355 ( (None, 125, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_356 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_357 (Conv1D)          (None, 62, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_357 ( (None, 62, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_358 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_358 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_358 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_359 (Conv1D)          (None, 31, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_359 ( (None, 31, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_359 (Activation)  (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_360 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_360 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_360 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_361 (Conv1D)          (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_361 ( (None, 15, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_361 (Activation)  (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_362 (Conv1D)          (None, 7, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_362 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_362 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_363 (Conv1D)          (None, 7, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_363 ( (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_363 (Activation)  (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_155 (MaxPoolin (None, 3, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 16)                24592     \n",
      "=================================================================\n",
      "Total params: 3,156,688\n",
      "Trainable params: 3,147,472\n",
      "Non-trainable params: 9,216\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.1841 - acc: 0.9622\n",
      "Loss: 0.18411765205093006 Accuracy: 0.9622015\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
