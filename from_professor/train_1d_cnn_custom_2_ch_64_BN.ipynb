{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_BN(conv_num=1):\n",
    "    init_channel = 64\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 1024000)           4096000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 20,480,656\n",
      "Trainable params: 18,432,528\n",
      "Non-trainable params: 2,048,128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 341312)            1365248   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 6,847,696\n",
      "Trainable params: 6,164,816\n",
      "Non-trainable params: 682,880\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 113728)            454912    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 2,316,816\n",
      "Trainable params: 2,088,976\n",
      "Non-trainable params: 227,840\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 37888)             151552    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 820,816\n",
      "Trainable params: 744,528\n",
      "Non-trainable params: 76,288\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 6304)              25216     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                100880    \n",
      "=================================================================\n",
      "Total params: 199,536\n",
      "Trainable params: 186,352\n",
      "Non-trainable params: 13,184\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 2080)              8320      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 120,336\n",
      "Trainable params: 115,536\n",
      "Non-trainable params: 4,800\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 672)               2688      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 97,456\n",
      "Trainable params: 95,408\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 224)               896       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 93,776\n",
      "Trainable params: 92,560\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 16)             2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 7, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 92,576\n",
      "Trainable params: 91,712\n",
      "Non-trainable params: 864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0623 - acc: 0.3663\n",
      "Epoch 00001: val_loss improved from inf to 1.68792, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/001-1.6879.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 2.0624 - acc: 0.3663 - val_loss: 1.6879 - val_acc: 0.4517\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3169 - acc: 0.5971\n",
      "Epoch 00002: val_loss improved from 1.68792 to 1.15169, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/002-1.1517.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.3169 - acc: 0.5971 - val_loss: 1.1517 - val_acc: 0.6408\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0799 - acc: 0.6752\n",
      "Epoch 00003: val_loss improved from 1.15169 to 0.98016, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/003-0.9802.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.0800 - acc: 0.6752 - val_loss: 0.9802 - val_acc: 0.7035\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9569 - acc: 0.7131\n",
      "Epoch 00004: val_loss did not improve from 0.98016\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.9571 - acc: 0.7130 - val_loss: 1.1048 - val_acc: 0.6590\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8663 - acc: 0.7423\n",
      "Epoch 00005: val_loss improved from 0.98016 to 0.92412, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/005-0.9241.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.8663 - acc: 0.7423 - val_loss: 0.9241 - val_acc: 0.7242\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7931 - acc: 0.7638\n",
      "Epoch 00006: val_loss improved from 0.92412 to 0.78460, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/006-0.7846.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7932 - acc: 0.7637 - val_loss: 0.7846 - val_acc: 0.7680\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7514 - acc: 0.7786\n",
      "Epoch 00007: val_loss improved from 0.78460 to 0.71562, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/007-0.7156.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7514 - acc: 0.7786 - val_loss: 0.7156 - val_acc: 0.7929\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7032 - acc: 0.7961\n",
      "Epoch 00008: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7034 - acc: 0.7960 - val_loss: 0.9812 - val_acc: 0.7081\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6714 - acc: 0.8058\n",
      "Epoch 00009: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6715 - acc: 0.8057 - val_loss: 0.8336 - val_acc: 0.7536\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6349 - acc: 0.8152\n",
      "Epoch 00010: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6350 - acc: 0.8151 - val_loss: 0.7353 - val_acc: 0.7852\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5981 - acc: 0.8264\n",
      "Epoch 00011: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5982 - acc: 0.8263 - val_loss: 0.7175 - val_acc: 0.7892\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.8341\n",
      "Epoch 00012: val_loss improved from 0.71562 to 0.67074, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/012-0.6707.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5740 - acc: 0.8341 - val_loss: 0.6707 - val_acc: 0.8062\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.8396\n",
      "Epoch 00013: val_loss improved from 0.67074 to 0.64164, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/013-0.6416.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5539 - acc: 0.8396 - val_loss: 0.6416 - val_acc: 0.8176\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5300 - acc: 0.8456\n",
      "Epoch 00014: val_loss did not improve from 0.64164\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5301 - acc: 0.8456 - val_loss: 0.6887 - val_acc: 0.7966\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.8520\n",
      "Epoch 00015: val_loss improved from 0.64164 to 0.61067, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/015-0.6107.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5089 - acc: 0.8521 - val_loss: 0.6107 - val_acc: 0.8272\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4856 - acc: 0.8600\n",
      "Epoch 00016: val_loss did not improve from 0.61067\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4857 - acc: 0.8600 - val_loss: 0.6597 - val_acc: 0.8043\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4685 - acc: 0.8636\n",
      "Epoch 00017: val_loss did not improve from 0.61067\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4686 - acc: 0.8636 - val_loss: 0.7624 - val_acc: 0.7773\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4450 - acc: 0.8715\n",
      "Epoch 00018: val_loss did not improve from 0.61067\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4451 - acc: 0.8715 - val_loss: 0.6204 - val_acc: 0.8253\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.8773\n",
      "Epoch 00019: val_loss did not improve from 0.61067\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4288 - acc: 0.8773 - val_loss: 0.9192 - val_acc: 0.7331\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8816\n",
      "Epoch 00020: val_loss did not improve from 0.61067\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4150 - acc: 0.8816 - val_loss: 0.6839 - val_acc: 0.8102\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4054 - acc: 0.8820\n",
      "Epoch 00021: val_loss did not improve from 0.61067\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4055 - acc: 0.8819 - val_loss: 0.6377 - val_acc: 0.8197\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8891\n",
      "Epoch 00022: val_loss did not improve from 0.61067\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3862 - acc: 0.8891 - val_loss: 0.6400 - val_acc: 0.8148\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8925\n",
      "Epoch 00023: val_loss improved from 0.61067 to 0.57830, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/023-0.5783.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3730 - acc: 0.8924 - val_loss: 0.5783 - val_acc: 0.8374\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8967\n",
      "Epoch 00024: val_loss did not improve from 0.57830\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3590 - acc: 0.8966 - val_loss: 0.5903 - val_acc: 0.8365\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3460 - acc: 0.9007\n",
      "Epoch 00025: val_loss did not improve from 0.57830\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3462 - acc: 0.9006 - val_loss: 0.6258 - val_acc: 0.8265\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.9052\n",
      "Epoch 00026: val_loss did not improve from 0.57830\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3313 - acc: 0.9051 - val_loss: 0.7466 - val_acc: 0.7913\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.9085\n",
      "Epoch 00027: val_loss did not improve from 0.57830\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3181 - acc: 0.9084 - val_loss: 0.6237 - val_acc: 0.8323\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9131\n",
      "Epoch 00028: val_loss did not improve from 0.57830\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3102 - acc: 0.9131 - val_loss: 0.6623 - val_acc: 0.8174\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.9164\n",
      "Epoch 00029: val_loss did not improve from 0.57830\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2964 - acc: 0.9163 - val_loss: 0.6185 - val_acc: 0.8276\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.9183\n",
      "Epoch 00030: val_loss improved from 0.57830 to 0.54955, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_6_conv_checkpoint/030-0.5496.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2865 - acc: 0.9182 - val_loss: 0.5496 - val_acc: 0.8481\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9193\n",
      "Epoch 00031: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2803 - acc: 0.9193 - val_loss: 0.6163 - val_acc: 0.8295\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9218\n",
      "Epoch 00032: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2690 - acc: 0.9217 - val_loss: 0.6588 - val_acc: 0.8185\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9259\n",
      "Epoch 00033: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2605 - acc: 0.9259 - val_loss: 0.5711 - val_acc: 0.8467\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9320\n",
      "Epoch 00034: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2464 - acc: 0.9319 - val_loss: 0.5762 - val_acc: 0.8444\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9272\n",
      "Epoch 00035: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2529 - acc: 0.9272 - val_loss: 0.5657 - val_acc: 0.8463\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9345\n",
      "Epoch 00036: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2337 - acc: 0.9345 - val_loss: 0.6338 - val_acc: 0.8260\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9352\n",
      "Epoch 00037: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2303 - acc: 0.9352 - val_loss: 0.5506 - val_acc: 0.8577\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9416\n",
      "Epoch 00038: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2133 - acc: 0.9416 - val_loss: 0.5880 - val_acc: 0.8416\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9415\n",
      "Epoch 00039: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2096 - acc: 0.9415 - val_loss: 0.5570 - val_acc: 0.8523\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9439\n",
      "Epoch 00040: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2054 - acc: 0.9439 - val_loss: 0.5774 - val_acc: 0.8425\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9446\n",
      "Epoch 00041: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1964 - acc: 0.9446 - val_loss: 0.6092 - val_acc: 0.8446\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9430\n",
      "Epoch 00042: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2020 - acc: 0.9430 - val_loss: 0.6404 - val_acc: 0.8330\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9503\n",
      "Epoch 00043: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1835 - acc: 0.9503 - val_loss: 0.5678 - val_acc: 0.8488\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9530\n",
      "Epoch 00044: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1745 - acc: 0.9530 - val_loss: 0.6560 - val_acc: 0.8316\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9523\n",
      "Epoch 00045: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1744 - acc: 0.9523 - val_loss: 0.5681 - val_acc: 0.8546\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9566\n",
      "Epoch 00046: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1653 - acc: 0.9566 - val_loss: 0.6389 - val_acc: 0.8379\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9573\n",
      "Epoch 00047: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1596 - acc: 0.9573 - val_loss: 0.6157 - val_acc: 0.8404\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9548\n",
      "Epoch 00048: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1656 - acc: 0.9547 - val_loss: 0.6144 - val_acc: 0.8421\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9574\n",
      "Epoch 00049: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1570 - acc: 0.9574 - val_loss: 0.5774 - val_acc: 0.8544\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9597\n",
      "Epoch 00050: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1490 - acc: 0.9597 - val_loss: 0.6317 - val_acc: 0.8346\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9596\n",
      "Epoch 00051: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1476 - acc: 0.9595 - val_loss: 0.6127 - val_acc: 0.8421\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9628\n",
      "Epoch 00052: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1399 - acc: 0.9627 - val_loss: 0.6937 - val_acc: 0.8204\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9654\n",
      "Epoch 00053: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1339 - acc: 0.9653 - val_loss: 0.6472 - val_acc: 0.8367\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9658\n",
      "Epoch 00054: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1309 - acc: 0.9657 - val_loss: 0.6969 - val_acc: 0.8297\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9665\n",
      "Epoch 00055: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1299 - acc: 0.9664 - val_loss: 0.6170 - val_acc: 0.8451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9687\n",
      "Epoch 00056: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1225 - acc: 0.9687 - val_loss: 0.5920 - val_acc: 0.8488\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9694\n",
      "Epoch 00057: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1204 - acc: 0.9693 - val_loss: 0.6993 - val_acc: 0.8302\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9690\n",
      "Epoch 00058: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1203 - acc: 0.9690 - val_loss: 0.5985 - val_acc: 0.8521\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9721\n",
      "Epoch 00059: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1124 - acc: 0.9721 - val_loss: 0.7285 - val_acc: 0.8276\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9711\n",
      "Epoch 00060: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1121 - acc: 0.9710 - val_loss: 0.6035 - val_acc: 0.8495\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9726\n",
      "Epoch 00061: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1095 - acc: 0.9726 - val_loss: 0.6454 - val_acc: 0.8421\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9712\n",
      "Epoch 00062: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1183 - acc: 0.9712 - val_loss: 0.5991 - val_acc: 0.8488\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9760\n",
      "Epoch 00063: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0986 - acc: 0.9760 - val_loss: 0.7075 - val_acc: 0.8318\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9727\n",
      "Epoch 00064: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1052 - acc: 0.9726 - val_loss: 0.6515 - val_acc: 0.8407\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9732\n",
      "Epoch 00065: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1042 - acc: 0.9732 - val_loss: 0.6013 - val_acc: 0.8532\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9770\n",
      "Epoch 00066: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0946 - acc: 0.9769 - val_loss: 0.7056 - val_acc: 0.8337\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9746\n",
      "Epoch 00067: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1010 - acc: 0.9746 - val_loss: 0.6257 - val_acc: 0.8528\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9789\n",
      "Epoch 00068: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0893 - acc: 0.9789 - val_loss: 0.6238 - val_acc: 0.8507\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9774\n",
      "Epoch 00069: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0911 - acc: 0.9774 - val_loss: 0.6397 - val_acc: 0.8411\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9799\n",
      "Epoch 00070: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0841 - acc: 0.9799 - val_loss: 0.6939 - val_acc: 0.8372\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9758\n",
      "Epoch 00071: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0974 - acc: 0.9757 - val_loss: 0.6494 - val_acc: 0.8460\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9803\n",
      "Epoch 00072: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0827 - acc: 0.9803 - val_loss: 0.6243 - val_acc: 0.8500\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9794\n",
      "Epoch 00073: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0845 - acc: 0.9794 - val_loss: 0.6550 - val_acc: 0.8439\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9809\n",
      "Epoch 00074: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0790 - acc: 0.9808 - val_loss: 0.8099 - val_acc: 0.8150\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9792\n",
      "Epoch 00075: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0838 - acc: 0.9792 - val_loss: 0.6360 - val_acc: 0.8532\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9843\n",
      "Epoch 00076: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0712 - acc: 0.9842 - val_loss: 0.6717 - val_acc: 0.8369\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9832\n",
      "Epoch 00077: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0730 - acc: 0.9832 - val_loss: 0.7009 - val_acc: 0.8402\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9791\n",
      "Epoch 00078: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0832 - acc: 0.9791 - val_loss: 0.6354 - val_acc: 0.8526\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9851\n",
      "Epoch 00079: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0668 - acc: 0.9850 - val_loss: 0.7435 - val_acc: 0.8323\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9810\n",
      "Epoch 00080: val_loss did not improve from 0.54955\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0775 - acc: 0.9809 - val_loss: 0.7557 - val_acc: 0.8297\n",
      "\n",
      "1D_CNN_custom_2_ch_64_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 889us/sample - loss: 0.6642 - acc: 0.8085\n",
      "Loss: 0.6641759217837642 Accuracy: 0.8085151\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3944 - acc: 0.2611\n",
      "Epoch 00001: val_loss improved from inf to 1.98862, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/001-1.9886.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 2.3944 - acc: 0.2611 - val_loss: 1.9886 - val_acc: 0.3454\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5628 - acc: 0.5163\n",
      "Epoch 00002: val_loss improved from 1.98862 to 1.36716, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/002-1.3672.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.5629 - acc: 0.5163 - val_loss: 1.3672 - val_acc: 0.5772\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2386 - acc: 0.6241\n",
      "Epoch 00003: val_loss improved from 1.36716 to 1.07559, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/003-1.0756.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.2386 - acc: 0.6241 - val_loss: 1.0756 - val_acc: 0.6744\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0470 - acc: 0.6888\n",
      "Epoch 00004: val_loss improved from 1.07559 to 1.03170, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/004-1.0317.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0470 - acc: 0.6888 - val_loss: 1.0317 - val_acc: 0.6844\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9188 - acc: 0.7313\n",
      "Epoch 00005: val_loss improved from 1.03170 to 0.85345, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/005-0.8535.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9188 - acc: 0.7313 - val_loss: 0.8535 - val_acc: 0.7510\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8261 - acc: 0.7596\n",
      "Epoch 00006: val_loss improved from 0.85345 to 0.76180, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/006-0.7618.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8262 - acc: 0.7596 - val_loss: 0.7618 - val_acc: 0.7757\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7438 - acc: 0.7874\n",
      "Epoch 00007: val_loss improved from 0.76180 to 0.69726, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/007-0.6973.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7440 - acc: 0.7874 - val_loss: 0.6973 - val_acc: 0.8022\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6820 - acc: 0.8026\n",
      "Epoch 00008: val_loss did not improve from 0.69726\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6820 - acc: 0.8026 - val_loss: 0.7946 - val_acc: 0.7722\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6261 - acc: 0.8201\n",
      "Epoch 00009: val_loss improved from 0.69726 to 0.61324, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/009-0.6132.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6261 - acc: 0.8201 - val_loss: 0.6132 - val_acc: 0.8237\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5793 - acc: 0.8348\n",
      "Epoch 00010: val_loss improved from 0.61324 to 0.61085, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/010-0.6109.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5793 - acc: 0.8348 - val_loss: 0.6109 - val_acc: 0.8281\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.8460\n",
      "Epoch 00011: val_loss improved from 0.61085 to 0.58468, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/011-0.5847.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5377 - acc: 0.8459 - val_loss: 0.5847 - val_acc: 0.8339\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5088 - acc: 0.8552\n",
      "Epoch 00012: val_loss did not improve from 0.58468\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5089 - acc: 0.8551 - val_loss: 0.5972 - val_acc: 0.8290\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4790 - acc: 0.8642\n",
      "Epoch 00013: val_loss improved from 0.58468 to 0.47989, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/013-0.4799.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4790 - acc: 0.8642 - val_loss: 0.4799 - val_acc: 0.8586\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4475 - acc: 0.8721\n",
      "Epoch 00014: val_loss did not improve from 0.47989\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4476 - acc: 0.8720 - val_loss: 0.5018 - val_acc: 0.8509\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4306 - acc: 0.8766\n",
      "Epoch 00015: val_loss did not improve from 0.47989\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4306 - acc: 0.8766 - val_loss: 0.5318 - val_acc: 0.8472\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8854\n",
      "Epoch 00016: val_loss improved from 0.47989 to 0.46920, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/016-0.4692.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4036 - acc: 0.8853 - val_loss: 0.4692 - val_acc: 0.8654\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8885\n",
      "Epoch 00017: val_loss improved from 0.46920 to 0.44988, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/017-0.4499.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3903 - acc: 0.8884 - val_loss: 0.4499 - val_acc: 0.8698\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8942\n",
      "Epoch 00018: val_loss improved from 0.44988 to 0.41513, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/018-0.4151.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3714 - acc: 0.8941 - val_loss: 0.4151 - val_acc: 0.8807\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.9001\n",
      "Epoch 00019: val_loss improved from 0.41513 to 0.39437, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/019-0.3944.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3537 - acc: 0.9000 - val_loss: 0.3944 - val_acc: 0.8915\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.9027\n",
      "Epoch 00020: val_loss did not improve from 0.39437\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3415 - acc: 0.9026 - val_loss: 0.4127 - val_acc: 0.8805\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.9076\n",
      "Epoch 00021: val_loss did not improve from 0.39437\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3244 - acc: 0.9076 - val_loss: 0.4064 - val_acc: 0.8826\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.9125\n",
      "Epoch 00022: val_loss did not improve from 0.39437\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3099 - acc: 0.9125 - val_loss: 0.4042 - val_acc: 0.8894\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.9132\n",
      "Epoch 00023: val_loss did not improve from 0.39437\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3051 - acc: 0.9132 - val_loss: 0.4235 - val_acc: 0.8842\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9179\n",
      "Epoch 00024: val_loss improved from 0.39437 to 0.38070, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/024-0.3807.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2944 - acc: 0.9179 - val_loss: 0.3807 - val_acc: 0.8931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9228\n",
      "Epoch 00025: val_loss improved from 0.38070 to 0.36858, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/025-0.3686.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2734 - acc: 0.9228 - val_loss: 0.3686 - val_acc: 0.8973\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9235\n",
      "Epoch 00026: val_loss did not improve from 0.36858\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2656 - acc: 0.9235 - val_loss: 0.3732 - val_acc: 0.8908\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9260\n",
      "Epoch 00027: val_loss improved from 0.36858 to 0.36145, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/027-0.3615.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2581 - acc: 0.9260 - val_loss: 0.3615 - val_acc: 0.9043\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9295\n",
      "Epoch 00028: val_loss did not improve from 0.36145\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2480 - acc: 0.9295 - val_loss: 0.4035 - val_acc: 0.8912\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9305\n",
      "Epoch 00029: val_loss did not improve from 0.36145\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2406 - acc: 0.9305 - val_loss: 0.3995 - val_acc: 0.8819\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9347\n",
      "Epoch 00030: val_loss improved from 0.36145 to 0.35869, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/030-0.3587.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2305 - acc: 0.9346 - val_loss: 0.3587 - val_acc: 0.8970\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9362\n",
      "Epoch 00031: val_loss improved from 0.35869 to 0.34605, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/031-0.3461.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2253 - acc: 0.9362 - val_loss: 0.3461 - val_acc: 0.9043\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9384\n",
      "Epoch 00032: val_loss did not improve from 0.34605\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2188 - acc: 0.9383 - val_loss: 0.3513 - val_acc: 0.8984\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9403\n",
      "Epoch 00033: val_loss improved from 0.34605 to 0.33871, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/033-0.3387.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2100 - acc: 0.9403 - val_loss: 0.3387 - val_acc: 0.8994\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9399\n",
      "Epoch 00034: val_loss did not improve from 0.33871\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2028 - acc: 0.9399 - val_loss: 0.3405 - val_acc: 0.9045\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9436\n",
      "Epoch 00035: val_loss improved from 0.33871 to 0.33036, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/035-0.3304.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1971 - acc: 0.9436 - val_loss: 0.3304 - val_acc: 0.9038\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9468\n",
      "Epoch 00036: val_loss did not improve from 0.33036\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1856 - acc: 0.9468 - val_loss: 0.4063 - val_acc: 0.8877\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9471\n",
      "Epoch 00037: val_loss did not improve from 0.33036\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1836 - acc: 0.9472 - val_loss: 0.3460 - val_acc: 0.9036\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9489\n",
      "Epoch 00038: val_loss did not improve from 0.33036\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1770 - acc: 0.9489 - val_loss: 0.3445 - val_acc: 0.9015\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9504\n",
      "Epoch 00039: val_loss did not improve from 0.33036\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1729 - acc: 0.9504 - val_loss: 0.3558 - val_acc: 0.9005\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9507\n",
      "Epoch 00040: val_loss improved from 0.33036 to 0.32197, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/040-0.3220.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1716 - acc: 0.9507 - val_loss: 0.3220 - val_acc: 0.9068\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9564\n",
      "Epoch 00041: val_loss did not improve from 0.32197\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1576 - acc: 0.9564 - val_loss: 0.3459 - val_acc: 0.9029\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9547\n",
      "Epoch 00042: val_loss did not improve from 0.32197\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1585 - acc: 0.9546 - val_loss: 0.3326 - val_acc: 0.9073\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9561\n",
      "Epoch 00043: val_loss did not improve from 0.32197\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1545 - acc: 0.9561 - val_loss: 0.3394 - val_acc: 0.9096\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9588\n",
      "Epoch 00044: val_loss did not improve from 0.32197\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1449 - acc: 0.9587 - val_loss: 0.3680 - val_acc: 0.9026\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9583\n",
      "Epoch 00045: val_loss did not improve from 0.32197\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1467 - acc: 0.9583 - val_loss: 0.3370 - val_acc: 0.9066\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9631\n",
      "Epoch 00046: val_loss did not improve from 0.32197\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1353 - acc: 0.9630 - val_loss: 0.3425 - val_acc: 0.9057\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9623\n",
      "Epoch 00047: val_loss did not improve from 0.32197\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1367 - acc: 0.9623 - val_loss: 0.3605 - val_acc: 0.9029\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9651\n",
      "Epoch 00048: val_loss did not improve from 0.32197\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1262 - acc: 0.9651 - val_loss: 0.3537 - val_acc: 0.9059\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9661\n",
      "Epoch 00049: val_loss did not improve from 0.32197\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1252 - acc: 0.9660 - val_loss: 0.3602 - val_acc: 0.9015\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9632\n",
      "Epoch 00050: val_loss improved from 0.32197 to 0.31156, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_7_conv_checkpoint/050-0.3116.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1300 - acc: 0.9631 - val_loss: 0.3116 - val_acc: 0.9140\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9639\n",
      "Epoch 00051: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1237 - acc: 0.9638 - val_loss: 0.3437 - val_acc: 0.9075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9685\n",
      "Epoch 00052: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1156 - acc: 0.9685 - val_loss: 0.3293 - val_acc: 0.9101\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9712\n",
      "Epoch 00053: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1073 - acc: 0.9711 - val_loss: 0.3598 - val_acc: 0.9080\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9697\n",
      "Epoch 00054: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1112 - acc: 0.9697 - val_loss: 0.3520 - val_acc: 0.9026\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9717\n",
      "Epoch 00055: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1056 - acc: 0.9716 - val_loss: 0.3801 - val_acc: 0.9064\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9694\n",
      "Epoch 00056: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1109 - acc: 0.9694 - val_loss: 0.3568 - val_acc: 0.9050\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9718\n",
      "Epoch 00057: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1044 - acc: 0.9718 - val_loss: 0.3401 - val_acc: 0.9099\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9723\n",
      "Epoch 00058: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1015 - acc: 0.9723 - val_loss: 0.3613 - val_acc: 0.9040\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9756\n",
      "Epoch 00059: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0911 - acc: 0.9755 - val_loss: 0.3430 - val_acc: 0.9099\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9744\n",
      "Epoch 00060: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0934 - acc: 0.9744 - val_loss: 0.3679 - val_acc: 0.8991\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9775\n",
      "Epoch 00061: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0875 - acc: 0.9774 - val_loss: 0.3823 - val_acc: 0.9036\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9757\n",
      "Epoch 00062: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0916 - acc: 0.9757 - val_loss: 0.3580 - val_acc: 0.9038\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9787\n",
      "Epoch 00063: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0830 - acc: 0.9786 - val_loss: 0.4363 - val_acc: 0.8903\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9767\n",
      "Epoch 00064: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0857 - acc: 0.9767 - val_loss: 0.3361 - val_acc: 0.9131\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9802\n",
      "Epoch 00065: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0777 - acc: 0.9802 - val_loss: 0.3474 - val_acc: 0.9101\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9771\n",
      "Epoch 00066: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0845 - acc: 0.9770 - val_loss: 0.4519 - val_acc: 0.8842\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9758\n",
      "Epoch 00067: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0876 - acc: 0.9757 - val_loss: 0.3511 - val_acc: 0.9103\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9726\n",
      "Epoch 00068: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0963 - acc: 0.9726 - val_loss: 0.3343 - val_acc: 0.9124\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9830\n",
      "Epoch 00069: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0689 - acc: 0.9830 - val_loss: 0.3361 - val_acc: 0.9129\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9830\n",
      "Epoch 00070: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0695 - acc: 0.9830 - val_loss: 0.3813 - val_acc: 0.9015\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9844\n",
      "Epoch 00071: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0652 - acc: 0.9844 - val_loss: 0.3367 - val_acc: 0.9126\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9833\n",
      "Epoch 00072: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0671 - acc: 0.9833 - val_loss: 0.3816 - val_acc: 0.9045\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9834\n",
      "Epoch 00073: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0650 - acc: 0.9833 - val_loss: 0.3534 - val_acc: 0.9119\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9774\n",
      "Epoch 00074: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0805 - acc: 0.9774 - val_loss: 0.3711 - val_acc: 0.9106\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9845\n",
      "Epoch 00075: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0617 - acc: 0.9845 - val_loss: 0.3601 - val_acc: 0.9092\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9822\n",
      "Epoch 00076: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0661 - acc: 0.9821 - val_loss: 0.3450 - val_acc: 0.9113\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9827\n",
      "Epoch 00077: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0656 - acc: 0.9826 - val_loss: 0.4298 - val_acc: 0.9010\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9827\n",
      "Epoch 00078: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0668 - acc: 0.9827 - val_loss: 0.3579 - val_acc: 0.9133\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9820\n",
      "Epoch 00079: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0723 - acc: 0.9819 - val_loss: 0.3489 - val_acc: 0.9140\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9865\n",
      "Epoch 00080: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0553 - acc: 0.9865 - val_loss: 0.3667 - val_acc: 0.9024\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9879\n",
      "Epoch 00081: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0517 - acc: 0.9879 - val_loss: 0.4874 - val_acc: 0.8821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9815\n",
      "Epoch 00082: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0680 - acc: 0.9814 - val_loss: 0.3826 - val_acc: 0.9080\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9830\n",
      "Epoch 00083: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0640 - acc: 0.9830 - val_loss: 0.4254 - val_acc: 0.8966\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9848\n",
      "Epoch 00084: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0573 - acc: 0.9848 - val_loss: 0.3595 - val_acc: 0.9131\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9860\n",
      "Epoch 00085: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0552 - acc: 0.9860 - val_loss: 0.3615 - val_acc: 0.9122\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9867\n",
      "Epoch 00086: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0528 - acc: 0.9867 - val_loss: 0.3584 - val_acc: 0.9126\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9871\n",
      "Epoch 00087: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0513 - acc: 0.9871 - val_loss: 0.3874 - val_acc: 0.9073\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9893\n",
      "Epoch 00088: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0461 - acc: 0.9893 - val_loss: 0.4268 - val_acc: 0.8947\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9886\n",
      "Epoch 00089: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0486 - acc: 0.9885 - val_loss: 0.3587 - val_acc: 0.9161\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9817\n",
      "Epoch 00090: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0656 - acc: 0.9816 - val_loss: 0.4432 - val_acc: 0.8947\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9855\n",
      "Epoch 00091: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0564 - acc: 0.9855 - val_loss: 0.3764 - val_acc: 0.9133\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9912\n",
      "Epoch 00092: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0409 - acc: 0.9912 - val_loss: 0.3558 - val_acc: 0.9129\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9854\n",
      "Epoch 00093: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0544 - acc: 0.9853 - val_loss: 0.3686 - val_acc: 0.9145\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9904\n",
      "Epoch 00094: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0426 - acc: 0.9904 - val_loss: 0.4008 - val_acc: 0.9096\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9887\n",
      "Epoch 00095: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0460 - acc: 0.9886 - val_loss: 0.3947 - val_acc: 0.9101\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9864\n",
      "Epoch 00096: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0519 - acc: 0.9864 - val_loss: 0.3544 - val_acc: 0.9152\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9913\n",
      "Epoch 00097: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0393 - acc: 0.9912 - val_loss: 0.3666 - val_acc: 0.9133\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9887\n",
      "Epoch 00098: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0454 - acc: 0.9887 - val_loss: 0.3995 - val_acc: 0.9113\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9902\n",
      "Epoch 00099: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0412 - acc: 0.9902 - val_loss: 0.3724 - val_acc: 0.9131\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9886\n",
      "Epoch 00100: val_loss did not improve from 0.31156\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0461 - acc: 0.9886 - val_loss: 0.3816 - val_acc: 0.9110\n",
      "\n",
      "1D_CNN_custom_2_ch_64_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 910us/sample - loss: 0.3951 - acc: 0.8854\n",
      "Loss: 0.39509355115741956 Accuracy: 0.8853583\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3080 - acc: 0.2892\n",
      "Epoch 00001: val_loss improved from inf to 1.92401, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/001-1.9240.hdf5\n",
      "36805/36805 [==============================] - 99s 3ms/sample - loss: 2.3080 - acc: 0.2892 - val_loss: 1.9240 - val_acc: 0.4067\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4858 - acc: 0.5458\n",
      "Epoch 00002: val_loss improved from 1.92401 to 1.25807, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/002-1.2581.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.4859 - acc: 0.5458 - val_loss: 1.2581 - val_acc: 0.6164\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1454 - acc: 0.6620\n",
      "Epoch 00003: val_loss improved from 1.25807 to 1.00768, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/003-1.0077.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1455 - acc: 0.6620 - val_loss: 1.0077 - val_acc: 0.7023\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9373 - acc: 0.7264\n",
      "Epoch 00004: val_loss improved from 1.00768 to 0.84374, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/004-0.8437.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9373 - acc: 0.7264 - val_loss: 0.8437 - val_acc: 0.7589\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7937 - acc: 0.7740\n",
      "Epoch 00005: val_loss improved from 0.84374 to 0.83966, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/005-0.8397.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7936 - acc: 0.7740 - val_loss: 0.8397 - val_acc: 0.7452\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.8050\n",
      "Epoch 00006: val_loss improved from 0.83966 to 0.60748, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/006-0.6075.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6807 - acc: 0.8050 - val_loss: 0.6075 - val_acc: 0.8262\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5960 - acc: 0.8290\n",
      "Epoch 00007: val_loss improved from 0.60748 to 0.58138, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/007-0.5814.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5961 - acc: 0.8290 - val_loss: 0.5814 - val_acc: 0.8311\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5399 - acc: 0.8462\n",
      "Epoch 00008: val_loss improved from 0.58138 to 0.49913, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/008-0.4991.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5399 - acc: 0.8462 - val_loss: 0.4991 - val_acc: 0.8658\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4864 - acc: 0.8616\n",
      "Epoch 00009: val_loss improved from 0.49913 to 0.48169, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/009-0.4817.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4865 - acc: 0.8616 - val_loss: 0.4817 - val_acc: 0.8672\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4407 - acc: 0.8745\n",
      "Epoch 00010: val_loss did not improve from 0.48169\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4409 - acc: 0.8744 - val_loss: 0.5119 - val_acc: 0.8519\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8849\n",
      "Epoch 00011: val_loss improved from 0.48169 to 0.37674, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/011-0.3767.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4072 - acc: 0.8848 - val_loss: 0.3767 - val_acc: 0.8963\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8939\n",
      "Epoch 00012: val_loss did not improve from 0.37674\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3732 - acc: 0.8938 - val_loss: 0.4218 - val_acc: 0.8800\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.9005\n",
      "Epoch 00013: val_loss improved from 0.37674 to 0.35169, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/013-0.3517.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3503 - acc: 0.9006 - val_loss: 0.3517 - val_acc: 0.8968\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.9082\n",
      "Epoch 00014: val_loss did not improve from 0.35169\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3248 - acc: 0.9082 - val_loss: 0.3772 - val_acc: 0.8882\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.9124\n",
      "Epoch 00015: val_loss improved from 0.35169 to 0.33308, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/015-0.3331.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3035 - acc: 0.9124 - val_loss: 0.3331 - val_acc: 0.9029\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9181\n",
      "Epoch 00016: val_loss improved from 0.33308 to 0.31122, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/016-0.3112.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2862 - acc: 0.9181 - val_loss: 0.3112 - val_acc: 0.9092\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9230\n",
      "Epoch 00017: val_loss improved from 0.31122 to 0.29157, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/017-0.2916.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2731 - acc: 0.9229 - val_loss: 0.2916 - val_acc: 0.9157\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9256\n",
      "Epoch 00018: val_loss did not improve from 0.29157\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2590 - acc: 0.9256 - val_loss: 0.2975 - val_acc: 0.9126\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9308\n",
      "Epoch 00019: val_loss improved from 0.29157 to 0.27704, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/019-0.2770.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2418 - acc: 0.9307 - val_loss: 0.2770 - val_acc: 0.9224\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9334\n",
      "Epoch 00020: val_loss did not improve from 0.27704\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2305 - acc: 0.9334 - val_loss: 0.2865 - val_acc: 0.9129\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9363\n",
      "Epoch 00021: val_loss improved from 0.27704 to 0.27562, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/021-0.2756.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2230 - acc: 0.9363 - val_loss: 0.2756 - val_acc: 0.9208\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9402\n",
      "Epoch 00022: val_loss improved from 0.27562 to 0.24335, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/022-0.2434.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2095 - acc: 0.9402 - val_loss: 0.2434 - val_acc: 0.9278\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9419\n",
      "Epoch 00023: val_loss did not improve from 0.24335\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2018 - acc: 0.9418 - val_loss: 0.2458 - val_acc: 0.9264\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9430\n",
      "Epoch 00024: val_loss improved from 0.24335 to 0.24012, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/024-0.2401.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1945 - acc: 0.9430 - val_loss: 0.2401 - val_acc: 0.9283\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9465\n",
      "Epoch 00025: val_loss did not improve from 0.24012\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1847 - acc: 0.9465 - val_loss: 0.2802 - val_acc: 0.9129\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9469\n",
      "Epoch 00026: val_loss did not improve from 0.24012\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1808 - acc: 0.9469 - val_loss: 0.2908 - val_acc: 0.9117\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9508\n",
      "Epoch 00027: val_loss did not improve from 0.24012\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1729 - acc: 0.9507 - val_loss: 0.2605 - val_acc: 0.9227\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9523\n",
      "Epoch 00028: val_loss did not improve from 0.24012\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1646 - acc: 0.9523 - val_loss: 0.2825 - val_acc: 0.9168\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9522\n",
      "Epoch 00029: val_loss improved from 0.24012 to 0.23572, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/029-0.2357.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1626 - acc: 0.9521 - val_loss: 0.2357 - val_acc: 0.9294\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9553\n",
      "Epoch 00030: val_loss improved from 0.23572 to 0.22895, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/030-0.2289.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1528 - acc: 0.9553 - val_loss: 0.2289 - val_acc: 0.9322\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9577\n",
      "Epoch 00031: val_loss did not improve from 0.22895\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1444 - acc: 0.9578 - val_loss: 0.2298 - val_acc: 0.9334\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9592\n",
      "Epoch 00032: val_loss did not improve from 0.22895\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1385 - acc: 0.9592 - val_loss: 0.2456 - val_acc: 0.9322\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9620\n",
      "Epoch 00033: val_loss improved from 0.22895 to 0.22879, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/033-0.2288.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1324 - acc: 0.9620 - val_loss: 0.2288 - val_acc: 0.9341\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9621\n",
      "Epoch 00034: val_loss did not improve from 0.22879\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1294 - acc: 0.9622 - val_loss: 0.2392 - val_acc: 0.9331\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9663\n",
      "Epoch 00035: val_loss improved from 0.22879 to 0.22333, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/035-0.2233.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1198 - acc: 0.9662 - val_loss: 0.2233 - val_acc: 0.9348\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9620\n",
      "Epoch 00036: val_loss did not improve from 0.22333\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1279 - acc: 0.9620 - val_loss: 0.2396 - val_acc: 0.9299\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9654\n",
      "Epoch 00037: val_loss did not improve from 0.22333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1176 - acc: 0.9653 - val_loss: 0.2384 - val_acc: 0.9304\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9703\n",
      "Epoch 00038: val_loss did not improve from 0.22333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1073 - acc: 0.9703 - val_loss: 0.2436 - val_acc: 0.9301\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9679\n",
      "Epoch 00039: val_loss did not improve from 0.22333\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1094 - acc: 0.9679 - val_loss: 0.2260 - val_acc: 0.9366\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9731\n",
      "Epoch 00040: val_loss did not improve from 0.22333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0983 - acc: 0.9730 - val_loss: 0.2357 - val_acc: 0.9322\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9703\n",
      "Epoch 00041: val_loss improved from 0.22333 to 0.20856, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_8_conv_checkpoint/041-0.2086.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1009 - acc: 0.9703 - val_loss: 0.2086 - val_acc: 0.9401\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9741\n",
      "Epoch 00042: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0933 - acc: 0.9741 - val_loss: 0.2475 - val_acc: 0.9352\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9762\n",
      "Epoch 00043: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0877 - acc: 0.9762 - val_loss: 0.2221 - val_acc: 0.9373\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9732\n",
      "Epoch 00044: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0955 - acc: 0.9732 - val_loss: 0.2221 - val_acc: 0.9373\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9758\n",
      "Epoch 00045: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0879 - acc: 0.9758 - val_loss: 0.2445 - val_acc: 0.9306\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9780\n",
      "Epoch 00046: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0798 - acc: 0.9779 - val_loss: 0.2410 - val_acc: 0.9322\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9753\n",
      "Epoch 00047: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0869 - acc: 0.9752 - val_loss: 0.2390 - val_acc: 0.9341\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9766\n",
      "Epoch 00048: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0831 - acc: 0.9766 - val_loss: 0.2335 - val_acc: 0.9343\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9816\n",
      "Epoch 00049: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0703 - acc: 0.9816 - val_loss: 0.2332 - val_acc: 0.9334\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9798\n",
      "Epoch 00050: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0750 - acc: 0.9798 - val_loss: 0.2580 - val_acc: 0.9301\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9781\n",
      "Epoch 00051: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0792 - acc: 0.9780 - val_loss: 0.2295 - val_acc: 0.9348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9827\n",
      "Epoch 00052: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0654 - acc: 0.9826 - val_loss: 0.2363 - val_acc: 0.9327\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9835\n",
      "Epoch 00053: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0623 - acc: 0.9835 - val_loss: 0.2581 - val_acc: 0.9338\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9816\n",
      "Epoch 00054: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0683 - acc: 0.9816 - val_loss: 0.2483 - val_acc: 0.9322\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9850\n",
      "Epoch 00055: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0588 - acc: 0.9850 - val_loss: 0.2239 - val_acc: 0.9411\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9858\n",
      "Epoch 00056: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0562 - acc: 0.9858 - val_loss: 0.2653 - val_acc: 0.9327\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9808\n",
      "Epoch 00057: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0667 - acc: 0.9808 - val_loss: 0.2262 - val_acc: 0.9392\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9847\n",
      "Epoch 00058: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0588 - acc: 0.9847 - val_loss: 0.2452 - val_acc: 0.9364\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9843\n",
      "Epoch 00059: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0592 - acc: 0.9843 - val_loss: 0.2441 - val_acc: 0.9366\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9881\n",
      "Epoch 00060: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0482 - acc: 0.9880 - val_loss: 0.2682 - val_acc: 0.9308\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9776\n",
      "Epoch 00061: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0788 - acc: 0.9776 - val_loss: 0.2304 - val_acc: 0.9401\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9893\n",
      "Epoch 00062: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0443 - acc: 0.9892 - val_loss: 0.2369 - val_acc: 0.9355\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9870\n",
      "Epoch 00063: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0517 - acc: 0.9870 - val_loss: 0.2414 - val_acc: 0.9394\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9880\n",
      "Epoch 00064: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0458 - acc: 0.9880 - val_loss: 0.2388 - val_acc: 0.9364\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9899\n",
      "Epoch 00065: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0416 - acc: 0.9899 - val_loss: 0.2483 - val_acc: 0.9364\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9880\n",
      "Epoch 00066: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0458 - acc: 0.9880 - val_loss: 0.2389 - val_acc: 0.9385\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9906\n",
      "Epoch 00067: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0399 - acc: 0.9905 - val_loss: 0.2487 - val_acc: 0.9364\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9857\n",
      "Epoch 00068: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0516 - acc: 0.9857 - val_loss: 0.2276 - val_acc: 0.9406\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9910\n",
      "Epoch 00069: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0380 - acc: 0.9910 - val_loss: 0.2234 - val_acc: 0.9418\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9909\n",
      "Epoch 00070: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0373 - acc: 0.9909 - val_loss: 0.2651 - val_acc: 0.9366\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9907\n",
      "Epoch 00071: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0394 - acc: 0.9907 - val_loss: 0.2551 - val_acc: 0.9345\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9823\n",
      "Epoch 00072: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0630 - acc: 0.9823 - val_loss: 0.2239 - val_acc: 0.9429\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9927\n",
      "Epoch 00073: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0315 - acc: 0.9927 - val_loss: 0.2766 - val_acc: 0.9297\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9896\n",
      "Epoch 00074: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0401 - acc: 0.9896 - val_loss: 0.2596 - val_acc: 0.9366\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9940\n",
      "Epoch 00075: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0291 - acc: 0.9939 - val_loss: 0.2519 - val_acc: 0.9352\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9877\n",
      "Epoch 00076: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0432 - acc: 0.9877 - val_loss: 0.2407 - val_acc: 0.9392\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9928\n",
      "Epoch 00077: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0316 - acc: 0.9928 - val_loss: 0.2521 - val_acc: 0.9352\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9925\n",
      "Epoch 00078: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0313 - acc: 0.9925 - val_loss: 0.2490 - val_acc: 0.9427\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9863\n",
      "Epoch 00079: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0478 - acc: 0.9863 - val_loss: 0.2490 - val_acc: 0.9373\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9937\n",
      "Epoch 00080: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0288 - acc: 0.9937 - val_loss: 0.2615 - val_acc: 0.9390\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9938\n",
      "Epoch 00081: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0291 - acc: 0.9938 - val_loss: 0.2566 - val_acc: 0.9378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9862\n",
      "Epoch 00082: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0467 - acc: 0.9862 - val_loss: 0.2484 - val_acc: 0.9413\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9941\n",
      "Epoch 00083: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0269 - acc: 0.9941 - val_loss: 0.2410 - val_acc: 0.9436\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9943\n",
      "Epoch 00084: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0259 - acc: 0.9943 - val_loss: 0.2544 - val_acc: 0.9371\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9940\n",
      "Epoch 00085: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0279 - acc: 0.9939 - val_loss: 0.2351 - val_acc: 0.9434\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9873\n",
      "Epoch 00086: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0443 - acc: 0.9873 - val_loss: 0.2504 - val_acc: 0.9408\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9936\n",
      "Epoch 00087: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0290 - acc: 0.9936 - val_loss: 0.2457 - val_acc: 0.9415\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9884\n",
      "Epoch 00088: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0415 - acc: 0.9883 - val_loss: 0.2448 - val_acc: 0.9411\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9943\n",
      "Epoch 00089: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0253 - acc: 0.9942 - val_loss: 0.2510 - val_acc: 0.9397\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9935\n",
      "Epoch 00090: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0270 - acc: 0.9935 - val_loss: 0.2888 - val_acc: 0.9331\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9912\n",
      "Epoch 00091: val_loss did not improve from 0.20856\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0329 - acc: 0.9912 - val_loss: 0.2635 - val_acc: 0.9406\n",
      "\n",
      "1D_CNN_custom_2_ch_64_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 914us/sample - loss: 0.2823 - acc: 0.9205\n",
      "Loss: 0.28225236821026073 Accuracy: 0.9204569\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4010 - acc: 0.2596\n",
      "Epoch 00001: val_loss improved from inf to 1.97689, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/001-1.9769.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 2.4010 - acc: 0.2596 - val_loss: 1.9769 - val_acc: 0.3995\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5321 - acc: 0.5379\n",
      "Epoch 00002: val_loss improved from 1.97689 to 1.26088, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/002-1.2609.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.5322 - acc: 0.5378 - val_loss: 1.2609 - val_acc: 0.6278\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1771 - acc: 0.6605\n",
      "Epoch 00003: val_loss improved from 1.26088 to 1.03454, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/003-1.0345.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.1772 - acc: 0.6605 - val_loss: 1.0345 - val_acc: 0.7172\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9597 - acc: 0.7336\n",
      "Epoch 00004: val_loss improved from 1.03454 to 0.87668, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/004-0.8767.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.9597 - acc: 0.7336 - val_loss: 0.8767 - val_acc: 0.7657\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7954 - acc: 0.7858\n",
      "Epoch 00005: val_loss improved from 0.87668 to 0.68903, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/005-0.6890.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.7954 - acc: 0.7858 - val_loss: 0.6890 - val_acc: 0.8239\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.8184\n",
      "Epoch 00006: val_loss improved from 0.68903 to 0.59320, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/006-0.5932.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6807 - acc: 0.8184 - val_loss: 0.5932 - val_acc: 0.8439\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5903 - acc: 0.8433\n",
      "Epoch 00007: val_loss improved from 0.59320 to 0.52274, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/007-0.5227.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5903 - acc: 0.8433 - val_loss: 0.5227 - val_acc: 0.8672\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.8595\n",
      "Epoch 00008: val_loss improved from 0.52274 to 0.45891, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/008-0.4589.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5237 - acc: 0.8594 - val_loss: 0.4589 - val_acc: 0.8812\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4716 - acc: 0.8712\n",
      "Epoch 00009: val_loss did not improve from 0.45891\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4716 - acc: 0.8712 - val_loss: 0.5183 - val_acc: 0.8560\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8821\n",
      "Epoch 00010: val_loss improved from 0.45891 to 0.39880, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/010-0.3988.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4274 - acc: 0.8821 - val_loss: 0.3988 - val_acc: 0.8935\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8924\n",
      "Epoch 00011: val_loss improved from 0.39880 to 0.36551, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/011-0.3655.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3923 - acc: 0.8925 - val_loss: 0.3655 - val_acc: 0.9026\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8992\n",
      "Epoch 00012: val_loss improved from 0.36551 to 0.34541, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/012-0.3454.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3634 - acc: 0.8992 - val_loss: 0.3454 - val_acc: 0.9031\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.9063\n",
      "Epoch 00013: val_loss improved from 0.34541 to 0.33728, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/013-0.3373.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3381 - acc: 0.9062 - val_loss: 0.3373 - val_acc: 0.9033\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.9118\n",
      "Epoch 00014: val_loss improved from 0.33728 to 0.33055, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/014-0.3305.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3167 - acc: 0.9118 - val_loss: 0.3305 - val_acc: 0.9029\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9171\n",
      "Epoch 00015: val_loss did not improve from 0.33055\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2960 - acc: 0.9170 - val_loss: 0.3339 - val_acc: 0.9059\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9174\n",
      "Epoch 00016: val_loss improved from 0.33055 to 0.29735, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/016-0.2974.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2881 - acc: 0.9174 - val_loss: 0.2974 - val_acc: 0.9124\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2629 - acc: 0.9258\n",
      "Epoch 00017: val_loss did not improve from 0.29735\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2629 - acc: 0.9258 - val_loss: 0.3039 - val_acc: 0.9066\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.9271\n",
      "Epoch 00018: val_loss did not improve from 0.29735\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2532 - acc: 0.9271 - val_loss: 0.3305 - val_acc: 0.9033\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9292\n",
      "Epoch 00019: val_loss improved from 0.29735 to 0.28087, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/019-0.2809.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2443 - acc: 0.9292 - val_loss: 0.2809 - val_acc: 0.9189\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9342\n",
      "Epoch 00020: val_loss improved from 0.28087 to 0.26117, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/020-0.2612.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2299 - acc: 0.9341 - val_loss: 0.2612 - val_acc: 0.9229\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9363\n",
      "Epoch 00021: val_loss did not improve from 0.26117\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2199 - acc: 0.9363 - val_loss: 0.2617 - val_acc: 0.9227\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9412\n",
      "Epoch 00022: val_loss did not improve from 0.26117\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2047 - acc: 0.9412 - val_loss: 0.3360 - val_acc: 0.9010\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9402\n",
      "Epoch 00023: val_loss improved from 0.26117 to 0.24341, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/023-0.2434.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2060 - acc: 0.9402 - val_loss: 0.2434 - val_acc: 0.9269\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9471\n",
      "Epoch 00024: val_loss improved from 0.24341 to 0.23690, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/024-0.2369.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1875 - acc: 0.9471 - val_loss: 0.2369 - val_acc: 0.9294\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9509\n",
      "Epoch 00025: val_loss did not improve from 0.23690\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1779 - acc: 0.9509 - val_loss: 0.2713 - val_acc: 0.9203\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9499\n",
      "Epoch 00026: val_loss did not improve from 0.23690\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1733 - acc: 0.9499 - val_loss: 0.2417 - val_acc: 0.9266\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9523\n",
      "Epoch 00027: val_loss improved from 0.23690 to 0.22771, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/027-0.2277.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1668 - acc: 0.9523 - val_loss: 0.2277 - val_acc: 0.9331\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9540\n",
      "Epoch 00028: val_loss did not improve from 0.22771\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1580 - acc: 0.9540 - val_loss: 0.2469 - val_acc: 0.9241\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9576\n",
      "Epoch 00029: val_loss did not improve from 0.22771\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1508 - acc: 0.9575 - val_loss: 0.2326 - val_acc: 0.9348\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9575\n",
      "Epoch 00030: val_loss improved from 0.22771 to 0.22705, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/030-0.2270.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1482 - acc: 0.9575 - val_loss: 0.2270 - val_acc: 0.9301\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9580\n",
      "Epoch 00031: val_loss did not improve from 0.22705\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1439 - acc: 0.9580 - val_loss: 0.2408 - val_acc: 0.9297\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9627\n",
      "Epoch 00032: val_loss improved from 0.22705 to 0.22640, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_BN_9_conv_checkpoint/032-0.2264.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1316 - acc: 0.9627 - val_loss: 0.2264 - val_acc: 0.9317\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9635\n",
      "Epoch 00033: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1272 - acc: 0.9635 - val_loss: 0.2419 - val_acc: 0.9264\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9654\n",
      "Epoch 00034: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1225 - acc: 0.9654 - val_loss: 0.2676 - val_acc: 0.9206\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9663\n",
      "Epoch 00035: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1184 - acc: 0.9663 - val_loss: 0.2405 - val_acc: 0.9292\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9657\n",
      "Epoch 00036: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1239 - acc: 0.9656 - val_loss: 0.2382 - val_acc: 0.9269\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9676\n",
      "Epoch 00037: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1122 - acc: 0.9675 - val_loss: 0.2312 - val_acc: 0.9329\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9705\n",
      "Epoch 00038: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1070 - acc: 0.9704 - val_loss: 0.2370 - val_acc: 0.9308\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9705\n",
      "Epoch 00039: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1058 - acc: 0.9705 - val_loss: 0.2389 - val_acc: 0.9320\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9740\n",
      "Epoch 00040: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0962 - acc: 0.9740 - val_loss: 0.2281 - val_acc: 0.9331\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9705\n",
      "Epoch 00041: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1007 - acc: 0.9704 - val_loss: 0.2392 - val_acc: 0.9311\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9733\n",
      "Epoch 00042: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0963 - acc: 0.9733 - val_loss: 0.2409 - val_acc: 0.9301\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9772\n",
      "Epoch 00043: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0850 - acc: 0.9772 - val_loss: 0.2345 - val_acc: 0.9311\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9768\n",
      "Epoch 00044: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0833 - acc: 0.9767 - val_loss: 0.2389 - val_acc: 0.9301\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9741\n",
      "Epoch 00045: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0917 - acc: 0.9741 - val_loss: 0.2351 - val_acc: 0.9336\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9778\n",
      "Epoch 00046: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0808 - acc: 0.9778 - val_loss: 0.2330 - val_acc: 0.9362\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9821\n",
      "Epoch 00047: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0704 - acc: 0.9820 - val_loss: 0.2480 - val_acc: 0.9299\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9768\n",
      "Epoch 00048: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0808 - acc: 0.9767 - val_loss: 0.2378 - val_acc: 0.9343\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9782\n",
      "Epoch 00049: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0783 - acc: 0.9782 - val_loss: 0.2491 - val_acc: 0.9313\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9817\n",
      "Epoch 00050: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0673 - acc: 0.9816 - val_loss: 0.2475 - val_acc: 0.9329\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9781\n",
      "Epoch 00051: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0764 - acc: 0.9781 - val_loss: 0.2594 - val_acc: 0.9317\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9836\n",
      "Epoch 00052: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0634 - acc: 0.9836 - val_loss: 0.2459 - val_acc: 0.9322\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9822\n",
      "Epoch 00053: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0649 - acc: 0.9821 - val_loss: 0.2760 - val_acc: 0.9252\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9813\n",
      "Epoch 00054: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0693 - acc: 0.9813 - val_loss: 0.2517 - val_acc: 0.9350\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9855\n",
      "Epoch 00055: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0561 - acc: 0.9854 - val_loss: 0.2584 - val_acc: 0.9322\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9834\n",
      "Epoch 00056: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0610 - acc: 0.9833 - val_loss: 0.2482 - val_acc: 0.9352\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9827\n",
      "Epoch 00057: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0619 - acc: 0.9827 - val_loss: 0.2567 - val_acc: 0.9329\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9878\n",
      "Epoch 00058: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0498 - acc: 0.9877 - val_loss: 0.2850 - val_acc: 0.9292\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9856\n",
      "Epoch 00059: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0545 - acc: 0.9856 - val_loss: 0.2605 - val_acc: 0.9341\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9883\n",
      "Epoch 00060: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0471 - acc: 0.9882 - val_loss: 0.2737 - val_acc: 0.9269\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9759\n",
      "Epoch 00061: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0793 - acc: 0.9759 - val_loss: 0.2590 - val_acc: 0.9324\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9853\n",
      "Epoch 00062: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0550 - acc: 0.9853 - val_loss: 0.2608 - val_acc: 0.9341\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9847\n",
      "Epoch 00063: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0549 - acc: 0.9846 - val_loss: 0.2737 - val_acc: 0.9308\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9877\n",
      "Epoch 00064: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0458 - acc: 0.9877 - val_loss: 0.2637 - val_acc: 0.9345\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9884\n",
      "Epoch 00065: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0455 - acc: 0.9884 - val_loss: 0.2546 - val_acc: 0.9334\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9882\n",
      "Epoch 00066: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0450 - acc: 0.9881 - val_loss: 0.2675 - val_acc: 0.9297\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9797\n",
      "Epoch 00067: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0676 - acc: 0.9797 - val_loss: 0.2544 - val_acc: 0.9406\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9917\n",
      "Epoch 00068: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0363 - acc: 0.9917 - val_loss: 0.2600 - val_acc: 0.9320\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9904\n",
      "Epoch 00069: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0383 - acc: 0.9903 - val_loss: 0.2657 - val_acc: 0.9341\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9830\n",
      "Epoch 00070: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0572 - acc: 0.9830 - val_loss: 0.2486 - val_acc: 0.9411\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9916\n",
      "Epoch 00071: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0350 - acc: 0.9916 - val_loss: 0.2555 - val_acc: 0.9359\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9918\n",
      "Epoch 00072: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0336 - acc: 0.9918 - val_loss: 0.2705 - val_acc: 0.9345\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9890\n",
      "Epoch 00073: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0408 - acc: 0.9890 - val_loss: 0.2547 - val_acc: 0.9394\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9930\n",
      "Epoch 00074: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0315 - acc: 0.9930 - val_loss: 0.2660 - val_acc: 0.9348\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9908\n",
      "Epoch 00075: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0356 - acc: 0.9908 - val_loss: 0.3048 - val_acc: 0.9287\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9908\n",
      "Epoch 00076: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0365 - acc: 0.9908 - val_loss: 0.3326 - val_acc: 0.9173\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9801\n",
      "Epoch 00077: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0652 - acc: 0.9801 - val_loss: 0.2499 - val_acc: 0.9362\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9927\n",
      "Epoch 00078: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0306 - acc: 0.9927 - val_loss: 0.2723 - val_acc: 0.9394\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9899\n",
      "Epoch 00079: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0370 - acc: 0.9899 - val_loss: 0.2646 - val_acc: 0.9357\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9932\n",
      "Epoch 00080: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0281 - acc: 0.9932 - val_loss: 0.2870 - val_acc: 0.9301\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9841\n",
      "Epoch 00081: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0534 - acc: 0.9841 - val_loss: 0.2929 - val_acc: 0.9324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9902\n",
      "Epoch 00082: val_loss did not improve from 0.22640\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0344 - acc: 0.9901 - val_loss: 0.2750 - val_acc: 0.9338\n",
      "\n",
      "1D_CNN_custom_2_ch_64_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 965us/sample - loss: 0.3099 - acc: 0.9074\n",
      "Loss: 0.30990468353992434 Accuracy: 0.9073728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_ch_64_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_ch_64_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 2080)              8320      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 120,336\n",
      "Trainable params: 115,536\n",
      "Non-trainable params: 4,800\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 956us/sample - loss: 0.6642 - acc: 0.8085\n",
      "Loss: 0.6641759217837642 Accuracy: 0.8085151\n",
      "\n",
      "1D_CNN_custom_2_ch_64_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 672)               2688      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 97,456\n",
      "Trainable params: 95,408\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 976us/sample - loss: 0.3951 - acc: 0.8854\n",
      "Loss: 0.39509355115741956 Accuracy: 0.8853583\n",
      "\n",
      "1D_CNN_custom_2_ch_64_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 224)               896       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 93,776\n",
      "Trainable params: 92,560\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2823 - acc: 0.9205\n",
      "Loss: 0.28225236821026073 Accuracy: 0.9204569\n",
      "\n",
      "1D_CNN_custom_2_ch_64_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 16)             2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 7, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 92,576\n",
      "Trainable params: 91,712\n",
      "Non-trainable params: 864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3099 - acc: 0.9074\n",
      "Loss: 0.30990468353992434 Accuracy: 0.9073728\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_ch_64_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_ch_64_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 2080)              8320      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 120,336\n",
      "Trainable params: 115,536\n",
      "Non-trainable params: 4,800\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.9105 - acc: 0.7923\n",
      "Loss: 0.9104824557235061 Accuracy: 0.79231566\n",
      "\n",
      "1D_CNN_custom_2_ch_64_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 672)               2688      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 97,456\n",
      "Trainable params: 95,408\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4460 - acc: 0.8864\n",
      "Loss: 0.44596569009535286 Accuracy: 0.8863967\n",
      "\n",
      "1D_CNN_custom_2_ch_64_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 224)               896       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 93,776\n",
      "Trainable params: 92,560\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3534 - acc: 0.9134\n",
      "Loss: 0.35337255095767084 Accuracy: 0.91339564\n",
      "\n",
      "1D_CNN_custom_2_ch_64_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 16)             2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 7, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 92,576\n",
      "Trainable params: 91,712\n",
      "Non-trainable params: 864\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3694 - acc: 0.9092\n",
      "Loss: 0.36937547893508077 Accuracy: 0.909242\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
