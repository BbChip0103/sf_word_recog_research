{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 32)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 32)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1040        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,920\n",
      "Trainable params: 11,728\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 32)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 32)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1040        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,200\n",
      "Trainable params: 16,944\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 32)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 96)           0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 96)           0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           1552        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,272\n",
      "Trainable params: 27,888\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2064        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 49,584\n",
      "Trainable params: 49,072\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 64)           0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 64)           0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           2064        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 70,384\n",
      "Trainable params: 69,744\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 64)           0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 64)           0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2064        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 91,184\n",
      "Trainable params: 90,416\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 7.5153 - acc: 0.0786\n",
      "Epoch 00001: val_loss improved from inf to 2.63257, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/001-2.6326.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 7.5152 - acc: 0.0786 - val_loss: 2.6326 - val_acc: 0.1297\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 4.9158 - acc: 0.0833\n",
      "Epoch 00002: val_loss improved from 2.63257 to 2.55878, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/002-2.5588.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 4.9150 - acc: 0.0834 - val_loss: 2.5588 - val_acc: 0.2001\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.0639 - acc: 0.0943\n",
      "Epoch 00003: val_loss improved from 2.55878 to 2.47577, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/003-2.4758.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 4.0640 - acc: 0.0943 - val_loss: 2.4758 - val_acc: 0.2159\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4932 - acc: 0.1112\n",
      "Epoch 00004: val_loss improved from 2.47577 to 2.37998, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/004-2.3800.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 3.4932 - acc: 0.1112 - val_loss: 2.3800 - val_acc: 0.2614\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1433 - acc: 0.1248\n",
      "Epoch 00005: val_loss improved from 2.37998 to 2.29289, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/005-2.2929.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 3.1431 - acc: 0.1248 - val_loss: 2.2929 - val_acc: 0.2972\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.8733 - acc: 0.1485\n",
      "Epoch 00006: val_loss improved from 2.29289 to 2.19442, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/006-2.1944.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 2.8733 - acc: 0.1486 - val_loss: 2.1944 - val_acc: 0.3433\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.6731 - acc: 0.1645\n",
      "Epoch 00007: val_loss improved from 2.19442 to 2.11424, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/007-2.1142.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 2.6738 - acc: 0.1644 - val_loss: 2.1142 - val_acc: 0.3706\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5174 - acc: 0.1841\n",
      "Epoch 00008: val_loss improved from 2.11424 to 2.05916, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/008-2.0592.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 2.5174 - acc: 0.1841 - val_loss: 2.0592 - val_acc: 0.3825\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3994 - acc: 0.2085\n",
      "Epoch 00009: val_loss improved from 2.05916 to 1.99968, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/009-1.9997.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 2.3995 - acc: 0.2084 - val_loss: 1.9997 - val_acc: 0.4037\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3170 - acc: 0.2270\n",
      "Epoch 00010: val_loss improved from 1.99968 to 1.94687, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/010-1.9469.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 2.3170 - acc: 0.2270 - val_loss: 1.9469 - val_acc: 0.4172\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2488 - acc: 0.2439\n",
      "Epoch 00011: val_loss improved from 1.94687 to 1.90313, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/011-1.9031.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 2.2488 - acc: 0.2439 - val_loss: 1.9031 - val_acc: 0.4305\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1841 - acc: 0.2619\n",
      "Epoch 00012: val_loss improved from 1.90313 to 1.85766, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/012-1.8577.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 2.1841 - acc: 0.2619 - val_loss: 1.8577 - val_acc: 0.4486\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1212 - acc: 0.2855\n",
      "Epoch 00013: val_loss improved from 1.85766 to 1.81270, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/013-1.8127.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 2.1211 - acc: 0.2855 - val_loss: 1.8127 - val_acc: 0.4633\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0779 - acc: 0.3009\n",
      "Epoch 00014: val_loss improved from 1.81270 to 1.76562, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/014-1.7656.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 2.0778 - acc: 0.3009 - val_loss: 1.7656 - val_acc: 0.4812\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0325 - acc: 0.3158\n",
      "Epoch 00015: val_loss improved from 1.76562 to 1.72518, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/015-1.7252.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 2.0326 - acc: 0.3158 - val_loss: 1.7252 - val_acc: 0.4964\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9884 - acc: 0.3323\n",
      "Epoch 00016: val_loss improved from 1.72518 to 1.67860, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/016-1.6786.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.9884 - acc: 0.3323 - val_loss: 1.6786 - val_acc: 0.5136\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9463 - acc: 0.3463\n",
      "Epoch 00017: val_loss improved from 1.67860 to 1.65357, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/017-1.6536.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.9466 - acc: 0.3461 - val_loss: 1.6536 - val_acc: 0.5241\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9073 - acc: 0.3628\n",
      "Epoch 00018: val_loss improved from 1.65357 to 1.59568, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/018-1.5957.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.9072 - acc: 0.3628 - val_loss: 1.5957 - val_acc: 0.5353\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8763 - acc: 0.3758\n",
      "Epoch 00019: val_loss improved from 1.59568 to 1.57728, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/019-1.5773.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.8762 - acc: 0.3758 - val_loss: 1.5773 - val_acc: 0.5437\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8430 - acc: 0.3883\n",
      "Epoch 00020: val_loss improved from 1.57728 to 1.53249, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/020-1.5325.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.8430 - acc: 0.3883 - val_loss: 1.5325 - val_acc: 0.5604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8135 - acc: 0.3989\n",
      "Epoch 00021: val_loss improved from 1.53249 to 1.51471, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/021-1.5147.hdf5\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.8135 - acc: 0.3989 - val_loss: 1.5147 - val_acc: 0.5712\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7788 - acc: 0.4082\n",
      "Epoch 00022: val_loss improved from 1.51471 to 1.46595, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/022-1.4660.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.7788 - acc: 0.4082 - val_loss: 1.4660 - val_acc: 0.5821\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7486 - acc: 0.4214\n",
      "Epoch 00023: val_loss improved from 1.46595 to 1.45387, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/023-1.4539.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.7485 - acc: 0.4215 - val_loss: 1.4539 - val_acc: 0.5926\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7253 - acc: 0.4293\n",
      "Epoch 00024: val_loss improved from 1.45387 to 1.42171, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/024-1.4217.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.7253 - acc: 0.4293 - val_loss: 1.4217 - val_acc: 0.5998\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7036 - acc: 0.4348\n",
      "Epoch 00025: val_loss improved from 1.42171 to 1.38633, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/025-1.3863.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.7033 - acc: 0.4349 - val_loss: 1.3863 - val_acc: 0.6047\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6751 - acc: 0.4471\n",
      "Epoch 00026: val_loss improved from 1.38633 to 1.36402, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/026-1.3640.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.6751 - acc: 0.4471 - val_loss: 1.3640 - val_acc: 0.6140\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6603 - acc: 0.4515\n",
      "Epoch 00027: val_loss improved from 1.36402 to 1.34678, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/027-1.3468.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.6603 - acc: 0.4515 - val_loss: 1.3468 - val_acc: 0.6136\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6362 - acc: 0.4608\n",
      "Epoch 00028: val_loss improved from 1.34678 to 1.30921, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/028-1.3092.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.6362 - acc: 0.4608 - val_loss: 1.3092 - val_acc: 0.6287\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6191 - acc: 0.4665\n",
      "Epoch 00029: val_loss improved from 1.30921 to 1.28175, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/029-1.2818.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.6191 - acc: 0.4665 - val_loss: 1.2818 - val_acc: 0.6348\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5965 - acc: 0.4772\n",
      "Epoch 00030: val_loss did not improve from 1.28175\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.5969 - acc: 0.4770 - val_loss: 1.3224 - val_acc: 0.6266\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5828 - acc: 0.4850\n",
      "Epoch 00031: val_loss improved from 1.28175 to 1.27267, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/031-1.2727.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.5830 - acc: 0.4850 - val_loss: 1.2727 - val_acc: 0.6343\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5720 - acc: 0.4875\n",
      "Epoch 00032: val_loss improved from 1.27267 to 1.25460, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/032-1.2546.hdf5\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.5719 - acc: 0.4875 - val_loss: 1.2546 - val_acc: 0.6462\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5535 - acc: 0.4923\n",
      "Epoch 00033: val_loss did not improve from 1.25460\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.5535 - acc: 0.4923 - val_loss: 1.2563 - val_acc: 0.6357\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5402 - acc: 0.4987\n",
      "Epoch 00034: val_loss improved from 1.25460 to 1.22326, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/034-1.2233.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.5402 - acc: 0.4987 - val_loss: 1.2233 - val_acc: 0.6564\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5225 - acc: 0.5040\n",
      "Epoch 00035: val_loss improved from 1.22326 to 1.22317, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/035-1.2232.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.5224 - acc: 0.5041 - val_loss: 1.2232 - val_acc: 0.6669\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5217 - acc: 0.5054\n",
      "Epoch 00036: val_loss improved from 1.22317 to 1.22282, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/036-1.2228.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.5218 - acc: 0.5053 - val_loss: 1.2228 - val_acc: 0.6553\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5004 - acc: 0.5099\n",
      "Epoch 00037: val_loss improved from 1.22282 to 1.17453, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/037-1.1745.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.5004 - acc: 0.5099 - val_loss: 1.1745 - val_acc: 0.6713\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4894 - acc: 0.5178\n",
      "Epoch 00038: val_loss improved from 1.17453 to 1.17088, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/038-1.1709.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.4893 - acc: 0.5178 - val_loss: 1.1709 - val_acc: 0.6676\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4827 - acc: 0.5172\n",
      "Epoch 00039: val_loss did not improve from 1.17088\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.4827 - acc: 0.5172 - val_loss: 1.1776 - val_acc: 0.6618\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4733 - acc: 0.5225\n",
      "Epoch 00040: val_loss improved from 1.17088 to 1.13946, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/040-1.1395.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.4733 - acc: 0.5225 - val_loss: 1.1395 - val_acc: 0.6785\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4700 - acc: 0.5219\n",
      "Epoch 00041: val_loss did not improve from 1.13946\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.4702 - acc: 0.5219 - val_loss: 1.1524 - val_acc: 0.6860\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4573 - acc: 0.5300\n",
      "Epoch 00042: val_loss did not improve from 1.13946\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.4575 - acc: 0.5299 - val_loss: 1.1420 - val_acc: 0.6881\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4493 - acc: 0.5298\n",
      "Epoch 00043: val_loss did not improve from 1.13946\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.4493 - acc: 0.5298 - val_loss: 1.1676 - val_acc: 0.6851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4439 - acc: 0.5314\n",
      "Epoch 00044: val_loss improved from 1.13946 to 1.11686, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/044-1.1169.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.4441 - acc: 0.5314 - val_loss: 1.1169 - val_acc: 0.6914\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4386 - acc: 0.5349\n",
      "Epoch 00045: val_loss improved from 1.11686 to 1.10216, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/045-1.1022.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.4388 - acc: 0.5349 - val_loss: 1.1022 - val_acc: 0.6897\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4256 - acc: 0.5383\n",
      "Epoch 00046: val_loss improved from 1.10216 to 1.08820, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/046-1.0882.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.4256 - acc: 0.5382 - val_loss: 1.0882 - val_acc: 0.6834\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4251 - acc: 0.5408\n",
      "Epoch 00047: val_loss did not improve from 1.08820\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.4251 - acc: 0.5407 - val_loss: 1.0922 - val_acc: 0.6958\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4125 - acc: 0.5428\n",
      "Epoch 00048: val_loss did not improve from 1.08820\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.4126 - acc: 0.5427 - val_loss: 1.1340 - val_acc: 0.6986\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4073 - acc: 0.5453\n",
      "Epoch 00049: val_loss improved from 1.08820 to 1.06014, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/049-1.0601.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.4073 - acc: 0.5453 - val_loss: 1.0601 - val_acc: 0.7053\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4038 - acc: 0.5458\n",
      "Epoch 00050: val_loss did not improve from 1.06014\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.4045 - acc: 0.5456 - val_loss: 1.0625 - val_acc: 0.6995\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4064 - acc: 0.5422\n",
      "Epoch 00051: val_loss did not improve from 1.06014\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.4063 - acc: 0.5422 - val_loss: 1.0617 - val_acc: 0.6986\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3927 - acc: 0.5476\n",
      "Epoch 00052: val_loss improved from 1.06014 to 1.05305, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/052-1.0531.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.3926 - acc: 0.5476 - val_loss: 1.0531 - val_acc: 0.7058\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3886 - acc: 0.5523\n",
      "Epoch 00053: val_loss did not improve from 1.05305\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.3892 - acc: 0.5522 - val_loss: 1.0610 - val_acc: 0.7070\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3825 - acc: 0.5533\n",
      "Epoch 00054: val_loss improved from 1.05305 to 1.04071, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/054-1.0407.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.3824 - acc: 0.5533 - val_loss: 1.0407 - val_acc: 0.7112\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3802 - acc: 0.5537\n",
      "Epoch 00055: val_loss did not improve from 1.04071\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.3803 - acc: 0.5537 - val_loss: 1.0494 - val_acc: 0.6976\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3800 - acc: 0.5549\n",
      "Epoch 00056: val_loss did not improve from 1.04071\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 1.3801 - acc: 0.5548 - val_loss: 1.0658 - val_acc: 0.7014\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3735 - acc: 0.5597\n",
      "Epoch 00057: val_loss did not improve from 1.04071\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.3735 - acc: 0.5598 - val_loss: 1.0469 - val_acc: 0.7067\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3586 - acc: 0.5626\n",
      "Epoch 00058: val_loss improved from 1.04071 to 1.03088, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/058-1.0309.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.3587 - acc: 0.5626 - val_loss: 1.0309 - val_acc: 0.7060\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3625 - acc: 0.5603\n",
      "Epoch 00059: val_loss improved from 1.03088 to 1.01670, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/059-1.0167.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.3625 - acc: 0.5602 - val_loss: 1.0167 - val_acc: 0.7060\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3532 - acc: 0.5645\n",
      "Epoch 00060: val_loss did not improve from 1.01670\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.3533 - acc: 0.5645 - val_loss: 1.0239 - val_acc: 0.7156\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3552 - acc: 0.5639\n",
      "Epoch 00061: val_loss improved from 1.01670 to 1.01517, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/061-1.0152.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.3552 - acc: 0.5639 - val_loss: 1.0152 - val_acc: 0.7242\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3513 - acc: 0.5629\n",
      "Epoch 00062: val_loss improved from 1.01517 to 1.00874, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/062-1.0087.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.3513 - acc: 0.5629 - val_loss: 1.0087 - val_acc: 0.7151\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3458 - acc: 0.5652\n",
      "Epoch 00063: val_loss improved from 1.00874 to 1.00135, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/063-1.0013.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.3457 - acc: 0.5652 - val_loss: 1.0013 - val_acc: 0.7219\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3337 - acc: 0.5699\n",
      "Epoch 00064: val_loss improved from 1.00135 to 0.99382, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/064-0.9938.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.3338 - acc: 0.5699 - val_loss: 0.9938 - val_acc: 0.7270\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3434 - acc: 0.5685\n",
      "Epoch 00065: val_loss did not improve from 0.99382\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.3435 - acc: 0.5684 - val_loss: 1.0655 - val_acc: 0.6965\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3394 - acc: 0.5671\n",
      "Epoch 00066: val_loss improved from 0.99382 to 0.97644, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/066-0.9764.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.3394 - acc: 0.5671 - val_loss: 0.9764 - val_acc: 0.7247\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3266 - acc: 0.5744\n",
      "Epoch 00067: val_loss did not improve from 0.97644\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.3266 - acc: 0.5743 - val_loss: 0.9792 - val_acc: 0.7279\n",
      "Epoch 68/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3304 - acc: 0.5721\n",
      "Epoch 00068: val_loss did not improve from 0.97644\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.3305 - acc: 0.5721 - val_loss: 1.0035 - val_acc: 0.7219\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3318 - acc: 0.5686\n",
      "Epoch 00069: val_loss did not improve from 0.97644\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.3317 - acc: 0.5686 - val_loss: 1.0527 - val_acc: 0.7147\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3225 - acc: 0.5720\n",
      "Epoch 00070: val_loss did not improve from 0.97644\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.3226 - acc: 0.5721 - val_loss: 0.9928 - val_acc: 0.7284\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3200 - acc: 0.5740\n",
      "Epoch 00071: val_loss did not improve from 0.97644\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.3202 - acc: 0.5739 - val_loss: 0.9775 - val_acc: 0.7277\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3195 - acc: 0.5768\n",
      "Epoch 00072: val_loss did not improve from 0.97644\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.3194 - acc: 0.5768 - val_loss: 0.9797 - val_acc: 0.7116\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3206 - acc: 0.5750\n",
      "Epoch 00073: val_loss improved from 0.97644 to 0.97253, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/073-0.9725.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.3205 - acc: 0.5751 - val_loss: 0.9725 - val_acc: 0.7298\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3145 - acc: 0.5761\n",
      "Epoch 00074: val_loss improved from 0.97253 to 0.96576, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/074-0.9658.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.3145 - acc: 0.5761 - val_loss: 0.9658 - val_acc: 0.7303\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3118 - acc: 0.5760\n",
      "Epoch 00075: val_loss improved from 0.96576 to 0.96466, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/075-0.9647.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.3118 - acc: 0.5760 - val_loss: 0.9647 - val_acc: 0.7209\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3102 - acc: 0.5786\n",
      "Epoch 00076: val_loss improved from 0.96466 to 0.95443, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/076-0.9544.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.3101 - acc: 0.5786 - val_loss: 0.9544 - val_acc: 0.7282\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3104 - acc: 0.5780\n",
      "Epoch 00077: val_loss did not improve from 0.95443\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.3102 - acc: 0.5779 - val_loss: 0.9661 - val_acc: 0.7284\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3007 - acc: 0.5820\n",
      "Epoch 00078: val_loss did not improve from 0.95443\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.3008 - acc: 0.5820 - val_loss: 0.9673 - val_acc: 0.7305\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3032 - acc: 0.5811\n",
      "Epoch 00079: val_loss improved from 0.95443 to 0.93994, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/079-0.9399.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.3036 - acc: 0.5809 - val_loss: 0.9399 - val_acc: 0.7333\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2969 - acc: 0.5817\n",
      "Epoch 00080: val_loss did not improve from 0.93994\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.2970 - acc: 0.5817 - val_loss: 0.9726 - val_acc: 0.7333\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2970 - acc: 0.5820\n",
      "Epoch 00081: val_loss did not improve from 0.93994\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2971 - acc: 0.5820 - val_loss: 0.9784 - val_acc: 0.7258\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2941 - acc: 0.5843\n",
      "Epoch 00082: val_loss did not improve from 0.93994\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2942 - acc: 0.5843 - val_loss: 1.0468 - val_acc: 0.7088\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2944 - acc: 0.5839\n",
      "Epoch 00083: val_loss improved from 0.93994 to 0.92136, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/083-0.9214.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.2944 - acc: 0.5840 - val_loss: 0.9214 - val_acc: 0.7393\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2898 - acc: 0.5845\n",
      "Epoch 00084: val_loss did not improve from 0.92136\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2899 - acc: 0.5844 - val_loss: 0.9574 - val_acc: 0.7338\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2854 - acc: 0.5860\n",
      "Epoch 00085: val_loss improved from 0.92136 to 0.92046, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/085-0.9205.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2854 - acc: 0.5860 - val_loss: 0.9205 - val_acc: 0.7391\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2842 - acc: 0.5859\n",
      "Epoch 00086: val_loss did not improve from 0.92046\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2842 - acc: 0.5858 - val_loss: 0.9474 - val_acc: 0.7419\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2773 - acc: 0.5858\n",
      "Epoch 00087: val_loss did not improve from 0.92046\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2772 - acc: 0.5858 - val_loss: 0.9427 - val_acc: 0.7386\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2842 - acc: 0.5857\n",
      "Epoch 00088: val_loss did not improve from 0.92046\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2842 - acc: 0.5857 - val_loss: 0.9256 - val_acc: 0.7398\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2779 - acc: 0.5841\n",
      "Epoch 00089: val_loss did not improve from 0.92046\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2782 - acc: 0.5840 - val_loss: 0.9371 - val_acc: 0.7463\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2787 - acc: 0.5878\n",
      "Epoch 00090: val_loss did not improve from 0.92046\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2783 - acc: 0.5879 - val_loss: 0.9631 - val_acc: 0.7333\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2736 - acc: 0.5922\n",
      "Epoch 00091: val_loss improved from 0.92046 to 0.91266, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/091-0.9127.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2736 - acc: 0.5923 - val_loss: 0.9127 - val_acc: 0.7428\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2738 - acc: 0.5888\n",
      "Epoch 00092: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2738 - acc: 0.5888 - val_loss: 0.9217 - val_acc: 0.7447\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2704 - acc: 0.5923\n",
      "Epoch 00093: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2705 - acc: 0.5922 - val_loss: 0.9217 - val_acc: 0.7433\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2672 - acc: 0.5925\n",
      "Epoch 00094: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.2677 - acc: 0.5924 - val_loss: 0.9269 - val_acc: 0.7424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2677 - acc: 0.5923\n",
      "Epoch 00095: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2678 - acc: 0.5922 - val_loss: 0.9305 - val_acc: 0.7419\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2691 - acc: 0.5927\n",
      "Epoch 00096: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2688 - acc: 0.5928 - val_loss: 0.9495 - val_acc: 0.7398\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2691 - acc: 0.5893\n",
      "Epoch 00097: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2690 - acc: 0.5894 - val_loss: 0.9216 - val_acc: 0.7498\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2653 - acc: 0.5921\n",
      "Epoch 00098: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.2654 - acc: 0.5921 - val_loss: 0.9609 - val_acc: 0.7324\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2592 - acc: 0.5928\n",
      "Epoch 00099: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2593 - acc: 0.5928 - val_loss: 0.9766 - val_acc: 0.7244\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2652 - acc: 0.5937\n",
      "Epoch 00100: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.2651 - acc: 0.5938 - val_loss: 0.9459 - val_acc: 0.7400\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2604 - acc: 0.5960\n",
      "Epoch 00101: val_loss did not improve from 0.91266\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2604 - acc: 0.5960 - val_loss: 0.9263 - val_acc: 0.7449\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2547 - acc: 0.5948\n",
      "Epoch 00102: val_loss improved from 0.91266 to 0.89732, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/102-0.8973.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2547 - acc: 0.5948 - val_loss: 0.8973 - val_acc: 0.7536\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2638 - acc: 0.5929\n",
      "Epoch 00103: val_loss did not improve from 0.89732\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2637 - acc: 0.5930 - val_loss: 0.9159 - val_acc: 0.7424\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2555 - acc: 0.5980\n",
      "Epoch 00104: val_loss did not improve from 0.89732\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.2555 - acc: 0.5980 - val_loss: 0.8996 - val_acc: 0.7442\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2579 - acc: 0.5952\n",
      "Epoch 00105: val_loss did not improve from 0.89732\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2579 - acc: 0.5952 - val_loss: 0.9404 - val_acc: 0.7317\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2541 - acc: 0.5997\n",
      "Epoch 00106: val_loss did not improve from 0.89732\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.2542 - acc: 0.5996 - val_loss: 0.9338 - val_acc: 0.7396\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2555 - acc: 0.5957\n",
      "Epoch 00107: val_loss did not improve from 0.89732\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.2554 - acc: 0.5958 - val_loss: 0.9152 - val_acc: 0.7410\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2463 - acc: 0.5994\n",
      "Epoch 00108: val_loss did not improve from 0.89732\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.2464 - acc: 0.5994 - val_loss: 0.9044 - val_acc: 0.7503\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2471 - acc: 0.5992\n",
      "Epoch 00109: val_loss did not improve from 0.89732\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2473 - acc: 0.5992 - val_loss: 0.9981 - val_acc: 0.7230\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2445 - acc: 0.5995\n",
      "Epoch 00110: val_loss improved from 0.89732 to 0.89376, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/110-0.8938.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2445 - acc: 0.5995 - val_loss: 0.8938 - val_acc: 0.7461\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2433 - acc: 0.5984\n",
      "Epoch 00111: val_loss did not improve from 0.89376\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2433 - acc: 0.5984 - val_loss: 0.9436 - val_acc: 0.7433\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2406 - acc: 0.6005\n",
      "Epoch 00112: val_loss did not improve from 0.89376\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2406 - acc: 0.6005 - val_loss: 0.8975 - val_acc: 0.7519\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2414 - acc: 0.5978\n",
      "Epoch 00113: val_loss did not improve from 0.89376\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2414 - acc: 0.5978 - val_loss: 0.9313 - val_acc: 0.7466\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2452 - acc: 0.6019\n",
      "Epoch 00114: val_loss did not improve from 0.89376\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.2453 - acc: 0.6019 - val_loss: 0.9362 - val_acc: 0.7277\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2453 - acc: 0.5996\n",
      "Epoch 00115: val_loss did not improve from 0.89376\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2452 - acc: 0.5996 - val_loss: 0.9200 - val_acc: 0.7419\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2365 - acc: 0.6022\n",
      "Epoch 00116: val_loss did not improve from 0.89376\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2364 - acc: 0.6022 - val_loss: 0.9095 - val_acc: 0.7482\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2381 - acc: 0.6012\n",
      "Epoch 00117: val_loss improved from 0.89376 to 0.87731, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/117-0.8773.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2379 - acc: 0.6015 - val_loss: 0.8773 - val_acc: 0.7510\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2347 - acc: 0.6034\n",
      "Epoch 00118: val_loss did not improve from 0.87731\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2349 - acc: 0.6034 - val_loss: 0.9202 - val_acc: 0.7510\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2371 - acc: 0.6025\n",
      "Epoch 00119: val_loss did not improve from 0.87731\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2370 - acc: 0.6025 - val_loss: 0.9262 - val_acc: 0.7349\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2331 - acc: 0.6000\n",
      "Epoch 00120: val_loss did not improve from 0.87731\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.2330 - acc: 0.6000 - val_loss: 0.8789 - val_acc: 0.7531\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2320 - acc: 0.6029\n",
      "Epoch 00121: val_loss did not improve from 0.87731\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.2322 - acc: 0.6029 - val_loss: 0.9027 - val_acc: 0.7489\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2319 - acc: 0.6012\n",
      "Epoch 00122: val_loss did not improve from 0.87731\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2319 - acc: 0.6011 - val_loss: 0.9004 - val_acc: 0.7498\n",
      "Epoch 123/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2218 - acc: 0.6048\n",
      "Epoch 00123: val_loss did not improve from 0.87731\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2219 - acc: 0.6048 - val_loss: 0.9324 - val_acc: 0.7447\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2355 - acc: 0.6032\n",
      "Epoch 00124: val_loss did not improve from 0.87731\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.2356 - acc: 0.6032 - val_loss: 0.8853 - val_acc: 0.7440\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2241 - acc: 0.6084\n",
      "Epoch 00125: val_loss improved from 0.87731 to 0.86891, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/125-0.8689.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2241 - acc: 0.6084 - val_loss: 0.8689 - val_acc: 0.7512\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2217 - acc: 0.6079\n",
      "Epoch 00126: val_loss improved from 0.86891 to 0.86443, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/126-0.8644.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.2219 - acc: 0.6079 - val_loss: 0.8644 - val_acc: 0.7563\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2280 - acc: 0.6053\n",
      "Epoch 00127: val_loss did not improve from 0.86443\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2281 - acc: 0.6053 - val_loss: 0.9179 - val_acc: 0.7480\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2216 - acc: 0.6061\n",
      "Epoch 00128: val_loss did not improve from 0.86443\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2217 - acc: 0.6060 - val_loss: 0.9525 - val_acc: 0.7412\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2203 - acc: 0.6074\n",
      "Epoch 00129: val_loss did not improve from 0.86443\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2199 - acc: 0.6076 - val_loss: 0.9463 - val_acc: 0.7324\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2219 - acc: 0.6081\n",
      "Epoch 00130: val_loss improved from 0.86443 to 0.86239, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/130-0.8624.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2219 - acc: 0.6081 - val_loss: 0.8624 - val_acc: 0.7573\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2157 - acc: 0.6096\n",
      "Epoch 00131: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2156 - acc: 0.6096 - val_loss: 0.8675 - val_acc: 0.7510\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2193 - acc: 0.6094\n",
      "Epoch 00132: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2192 - acc: 0.6094 - val_loss: 0.9260 - val_acc: 0.7454\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2195 - acc: 0.6088\n",
      "Epoch 00133: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.2200 - acc: 0.6086 - val_loss: 0.8868 - val_acc: 0.7547\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2147 - acc: 0.6105\n",
      "Epoch 00134: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2144 - acc: 0.6105 - val_loss: 0.8648 - val_acc: 0.7473\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2132 - acc: 0.6069\n",
      "Epoch 00135: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2126 - acc: 0.6071 - val_loss: 0.8844 - val_acc: 0.7503\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2174 - acc: 0.6080\n",
      "Epoch 00136: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2174 - acc: 0.6080 - val_loss: 0.9069 - val_acc: 0.7510\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2068 - acc: 0.6150\n",
      "Epoch 00137: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2067 - acc: 0.6150 - val_loss: 0.8908 - val_acc: 0.7470\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2106 - acc: 0.6078\n",
      "Epoch 00138: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2106 - acc: 0.6078 - val_loss: 0.9267 - val_acc: 0.7498\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2116 - acc: 0.6093\n",
      "Epoch 00139: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2115 - acc: 0.6094 - val_loss: 0.8829 - val_acc: 0.7582\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2049 - acc: 0.6112\n",
      "Epoch 00140: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2050 - acc: 0.6112 - val_loss: 0.8881 - val_acc: 0.7498\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2064 - acc: 0.6099\n",
      "Epoch 00141: val_loss did not improve from 0.86239\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.2063 - acc: 0.6100 - val_loss: 0.8806 - val_acc: 0.7484\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2120 - acc: 0.6100\n",
      "Epoch 00142: val_loss improved from 0.86239 to 0.84446, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/142-0.8445.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2119 - acc: 0.6100 - val_loss: 0.8445 - val_acc: 0.7619\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2001 - acc: 0.6189\n",
      "Epoch 00143: val_loss did not improve from 0.84446\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1998 - acc: 0.6191 - val_loss: 0.8466 - val_acc: 0.7608\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2103 - acc: 0.6101\n",
      "Epoch 00144: val_loss improved from 0.84446 to 0.84331, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/144-0.8433.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2101 - acc: 0.6102 - val_loss: 0.8433 - val_acc: 0.7631\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2045 - acc: 0.6139\n",
      "Epoch 00145: val_loss did not improve from 0.84331\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2045 - acc: 0.6139 - val_loss: 0.9366 - val_acc: 0.7389\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2018 - acc: 0.6143\n",
      "Epoch 00146: val_loss did not improve from 0.84331\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.2019 - acc: 0.6143 - val_loss: 0.8670 - val_acc: 0.7470\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1964 - acc: 0.6143\n",
      "Epoch 00147: val_loss improved from 0.84331 to 0.83825, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/147-0.8383.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1964 - acc: 0.6143 - val_loss: 0.8383 - val_acc: 0.7666\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1984 - acc: 0.6154\n",
      "Epoch 00148: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1984 - acc: 0.6155 - val_loss: 0.8947 - val_acc: 0.7396\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2040 - acc: 0.6117\n",
      "Epoch 00149: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.2041 - acc: 0.6116 - val_loss: 0.8788 - val_acc: 0.7475\n",
      "Epoch 150/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2098 - acc: 0.6110\n",
      "Epoch 00150: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2098 - acc: 0.6109 - val_loss: 0.8852 - val_acc: 0.7503\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1925 - acc: 0.6168\n",
      "Epoch 00151: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1924 - acc: 0.6168 - val_loss: 0.9658 - val_acc: 0.7270\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1969 - acc: 0.6157\n",
      "Epoch 00152: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.1973 - acc: 0.6156 - val_loss: 0.8859 - val_acc: 0.7503\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1924 - acc: 0.6163\n",
      "Epoch 00153: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1918 - acc: 0.6162 - val_loss: 0.8616 - val_acc: 0.7549\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1960 - acc: 0.6150\n",
      "Epoch 00154: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1960 - acc: 0.6150 - val_loss: 0.9143 - val_acc: 0.7433\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1954 - acc: 0.6126\n",
      "Epoch 00155: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1955 - acc: 0.6126 - val_loss: 0.9161 - val_acc: 0.7463\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1929 - acc: 0.6152\n",
      "Epoch 00156: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1931 - acc: 0.6152 - val_loss: 0.8611 - val_acc: 0.7540\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1916 - acc: 0.6169\n",
      "Epoch 00157: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1915 - acc: 0.6170 - val_loss: 0.8625 - val_acc: 0.7505\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1867 - acc: 0.6181\n",
      "Epoch 00158: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1867 - acc: 0.6181 - val_loss: 0.8837 - val_acc: 0.7575\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1913 - acc: 0.6170\n",
      "Epoch 00159: val_loss did not improve from 0.83825\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1913 - acc: 0.6170 - val_loss: 0.8725 - val_acc: 0.7529\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1858 - acc: 0.6173\n",
      "Epoch 00160: val_loss improved from 0.83825 to 0.83710, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/160-0.8371.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1858 - acc: 0.6174 - val_loss: 0.8371 - val_acc: 0.7619\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1968 - acc: 0.6181\n",
      "Epoch 00161: val_loss did not improve from 0.83710\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1969 - acc: 0.6181 - val_loss: 0.8595 - val_acc: 0.7668\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1847 - acc: 0.6194\n",
      "Epoch 00162: val_loss did not improve from 0.83710\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1846 - acc: 0.6193 - val_loss: 0.8500 - val_acc: 0.7659\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1874 - acc: 0.6167\n",
      "Epoch 00163: val_loss did not improve from 0.83710\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1875 - acc: 0.6166 - val_loss: 0.8435 - val_acc: 0.7619\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1880 - acc: 0.6186\n",
      "Epoch 00164: val_loss improved from 0.83710 to 0.83292, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/164-0.8329.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1880 - acc: 0.6185 - val_loss: 0.8329 - val_acc: 0.7601\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1832 - acc: 0.6205\n",
      "Epoch 00165: val_loss did not improve from 0.83292\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1831 - acc: 0.6205 - val_loss: 0.8699 - val_acc: 0.7489\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1858 - acc: 0.6166\n",
      "Epoch 00166: val_loss did not improve from 0.83292\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1858 - acc: 0.6166 - val_loss: 0.8361 - val_acc: 0.7706\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1905 - acc: 0.6166\n",
      "Epoch 00167: val_loss improved from 0.83292 to 0.82558, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/167-0.8256.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1905 - acc: 0.6166 - val_loss: 0.8256 - val_acc: 0.7640\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1843 - acc: 0.6164\n",
      "Epoch 00168: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1844 - acc: 0.6164 - val_loss: 0.9552 - val_acc: 0.7289\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1885 - acc: 0.6158\n",
      "Epoch 00169: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1885 - acc: 0.6158 - val_loss: 0.8370 - val_acc: 0.7661\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1815 - acc: 0.6195\n",
      "Epoch 00170: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1814 - acc: 0.6195 - val_loss: 0.8576 - val_acc: 0.7640\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1815 - acc: 0.6193\n",
      "Epoch 00171: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1814 - acc: 0.6193 - val_loss: 0.8676 - val_acc: 0.7570\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1780 - acc: 0.6202\n",
      "Epoch 00172: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1780 - acc: 0.6202 - val_loss: 0.8947 - val_acc: 0.7519\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1781 - acc: 0.6198\n",
      "Epoch 00173: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1781 - acc: 0.6198 - val_loss: 0.8593 - val_acc: 0.7484\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1747 - acc: 0.6210\n",
      "Epoch 00174: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1749 - acc: 0.6209 - val_loss: 0.8362 - val_acc: 0.7601\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1731 - acc: 0.6234\n",
      "Epoch 00175: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1730 - acc: 0.6234 - val_loss: 0.8397 - val_acc: 0.7570\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1785 - acc: 0.6195\n",
      "Epoch 00176: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1785 - acc: 0.6195 - val_loss: 0.8673 - val_acc: 0.7561\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1766 - acc: 0.6198\n",
      "Epoch 00177: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1766 - acc: 0.6198 - val_loss: 0.8478 - val_acc: 0.7573\n",
      "Epoch 178/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1676 - acc: 0.6220\n",
      "Epoch 00178: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1676 - acc: 0.6220 - val_loss: 0.8319 - val_acc: 0.7701\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1725 - acc: 0.6218\n",
      "Epoch 00179: val_loss did not improve from 0.82558\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1724 - acc: 0.6218 - val_loss: 0.8461 - val_acc: 0.7584\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1798 - acc: 0.6229\n",
      "Epoch 00180: val_loss improved from 0.82558 to 0.82100, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/180-0.8210.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1798 - acc: 0.6229 - val_loss: 0.8210 - val_acc: 0.7687\n",
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1733 - acc: 0.6248\n",
      "Epoch 00181: val_loss improved from 0.82100 to 0.81901, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/181-0.8190.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1739 - acc: 0.6247 - val_loss: 0.8190 - val_acc: 0.7615\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1732 - acc: 0.6219\n",
      "Epoch 00182: val_loss did not improve from 0.81901\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1731 - acc: 0.6219 - val_loss: 0.8356 - val_acc: 0.7575\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1630 - acc: 0.6249\n",
      "Epoch 00183: val_loss did not improve from 0.81901\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1629 - acc: 0.6248 - val_loss: 0.8245 - val_acc: 0.7652\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1687 - acc: 0.6238\n",
      "Epoch 00184: val_loss improved from 0.81901 to 0.81648, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/184-0.8165.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1686 - acc: 0.6239 - val_loss: 0.8165 - val_acc: 0.7619\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1595 - acc: 0.6251\n",
      "Epoch 00185: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1596 - acc: 0.6251 - val_loss: 0.8556 - val_acc: 0.7561\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1651 - acc: 0.6263\n",
      "Epoch 00186: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1651 - acc: 0.6263 - val_loss: 0.8350 - val_acc: 0.7587\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1703 - acc: 0.6234\n",
      "Epoch 00187: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1703 - acc: 0.6234 - val_loss: 0.8876 - val_acc: 0.7580\n",
      "Epoch 188/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1685 - acc: 0.6239\n",
      "Epoch 00188: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1680 - acc: 0.6240 - val_loss: 0.8219 - val_acc: 0.7741\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1625 - acc: 0.6260\n",
      "Epoch 00189: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1625 - acc: 0.6260 - val_loss: 0.8610 - val_acc: 0.7522\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1669 - acc: 0.6220\n",
      "Epoch 00190: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1671 - acc: 0.6220 - val_loss: 0.8284 - val_acc: 0.7661\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1602 - acc: 0.6273\n",
      "Epoch 00191: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1601 - acc: 0.6273 - val_loss: 0.9291 - val_acc: 0.7340\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1673 - acc: 0.6246\n",
      "Epoch 00192: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1673 - acc: 0.6246 - val_loss: 0.8487 - val_acc: 0.7608\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1572 - acc: 0.6255\n",
      "Epoch 00193: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1572 - acc: 0.6255 - val_loss: 0.8239 - val_acc: 0.7598\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1591 - acc: 0.6287\n",
      "Epoch 00194: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1591 - acc: 0.6287 - val_loss: 0.8267 - val_acc: 0.7692\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1601 - acc: 0.6246\n",
      "Epoch 00195: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1601 - acc: 0.6246 - val_loss: 0.8253 - val_acc: 0.7689\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1579 - acc: 0.6279\n",
      "Epoch 00196: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1579 - acc: 0.6279 - val_loss: 0.8668 - val_acc: 0.7549\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1656 - acc: 0.6235\n",
      "Epoch 00197: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1654 - acc: 0.6236 - val_loss: 0.8541 - val_acc: 0.7568\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1549 - acc: 0.6284\n",
      "Epoch 00198: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1550 - acc: 0.6284 - val_loss: 0.8226 - val_acc: 0.7636\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1537 - acc: 0.6279\n",
      "Epoch 00199: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1537 - acc: 0.6279 - val_loss: 0.8380 - val_acc: 0.7584\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1671 - acc: 0.6259\n",
      "Epoch 00200: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1670 - acc: 0.6259 - val_loss: 0.8311 - val_acc: 0.7594\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1624 - acc: 0.6248\n",
      "Epoch 00201: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1625 - acc: 0.6247 - val_loss: 0.8663 - val_acc: 0.7608\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1526 - acc: 0.6271\n",
      "Epoch 00202: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1526 - acc: 0.6271 - val_loss: 0.8535 - val_acc: 0.7549\n",
      "Epoch 203/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1601 - acc: 0.6258\n",
      "Epoch 00203: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1599 - acc: 0.6258 - val_loss: 0.9017 - val_acc: 0.7475\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1621 - acc: 0.6251\n",
      "Epoch 00204: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1622 - acc: 0.6250 - val_loss: 0.8377 - val_acc: 0.7559\n",
      "Epoch 205/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1570 - acc: 0.6263\n",
      "Epoch 00205: val_loss did not improve from 0.81648\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1568 - acc: 0.6265 - val_loss: 0.8880 - val_acc: 0.7412\n",
      "Epoch 206/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1488 - acc: 0.6327\n",
      "Epoch 00206: val_loss improved from 0.81648 to 0.80905, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/206-0.8090.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1487 - acc: 0.6327 - val_loss: 0.8090 - val_acc: 0.7736\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1569 - acc: 0.6280\n",
      "Epoch 00207: val_loss did not improve from 0.80905\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1569 - acc: 0.6280 - val_loss: 0.8127 - val_acc: 0.7682\n",
      "Epoch 208/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1551 - acc: 0.6263\n",
      "Epoch 00208: val_loss did not improve from 0.80905\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1552 - acc: 0.6262 - val_loss: 0.8195 - val_acc: 0.7696\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1519 - acc: 0.6305\n",
      "Epoch 00209: val_loss did not improve from 0.80905\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1519 - acc: 0.6304 - val_loss: 0.8260 - val_acc: 0.7692\n",
      "Epoch 210/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1436 - acc: 0.6334\n",
      "Epoch 00210: val_loss improved from 0.80905 to 0.79756, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/210-0.7976.hdf5\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1434 - acc: 0.6335 - val_loss: 0.7976 - val_acc: 0.7745\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1487 - acc: 0.6276\n",
      "Epoch 00211: val_loss did not improve from 0.79756\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1486 - acc: 0.6276 - val_loss: 0.8733 - val_acc: 0.7498\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1487 - acc: 0.6308\n",
      "Epoch 00212: val_loss did not improve from 0.79756\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1484 - acc: 0.6310 - val_loss: 0.8324 - val_acc: 0.7608\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1527 - acc: 0.6275\n",
      "Epoch 00213: val_loss did not improve from 0.79756\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1528 - acc: 0.6274 - val_loss: 0.8815 - val_acc: 0.7463\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1603 - acc: 0.6258\n",
      "Epoch 00214: val_loss did not improve from 0.79756\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1603 - acc: 0.6257 - val_loss: 0.8229 - val_acc: 0.7680\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1458 - acc: 0.6323\n",
      "Epoch 00215: val_loss did not improve from 0.79756\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1458 - acc: 0.6324 - val_loss: 0.8489 - val_acc: 0.7575\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1460 - acc: 0.6343\n",
      "Epoch 00216: val_loss did not improve from 0.79756\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1460 - acc: 0.6343 - val_loss: 0.8333 - val_acc: 0.7615\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1443 - acc: 0.6334\n",
      "Epoch 00217: val_loss did not improve from 0.79756\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1444 - acc: 0.6334 - val_loss: 0.8498 - val_acc: 0.7659\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1409 - acc: 0.6328\n",
      "Epoch 00218: val_loss did not improve from 0.79756\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1408 - acc: 0.6328 - val_loss: 0.8084 - val_acc: 0.7729\n",
      "Epoch 219/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1483 - acc: 0.6301\n",
      "Epoch 00219: val_loss did not improve from 0.79756\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1479 - acc: 0.6303 - val_loss: 0.8325 - val_acc: 0.7617\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1426 - acc: 0.6340\n",
      "Epoch 00220: val_loss did not improve from 0.79756\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1427 - acc: 0.6340 - val_loss: 0.8183 - val_acc: 0.7706\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1458 - acc: 0.6309\n",
      "Epoch 00221: val_loss improved from 0.79756 to 0.77916, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/221-0.7792.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1458 - acc: 0.6309 - val_loss: 0.7792 - val_acc: 0.7775\n",
      "Epoch 222/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1372 - acc: 0.6344\n",
      "Epoch 00222: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1376 - acc: 0.6343 - val_loss: 0.8118 - val_acc: 0.7692\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1389 - acc: 0.6346\n",
      "Epoch 00223: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1391 - acc: 0.6346 - val_loss: 0.8272 - val_acc: 0.7708\n",
      "Epoch 224/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1386 - acc: 0.6323\n",
      "Epoch 00224: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1388 - acc: 0.6322 - val_loss: 0.8152 - val_acc: 0.7741\n",
      "Epoch 225/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1376 - acc: 0.6297\n",
      "Epoch 00225: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1376 - acc: 0.6298 - val_loss: 0.7976 - val_acc: 0.7743\n",
      "Epoch 226/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1524 - acc: 0.6298\n",
      "Epoch 00226: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1527 - acc: 0.6299 - val_loss: 0.8480 - val_acc: 0.7540\n",
      "Epoch 227/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1402 - acc: 0.6361\n",
      "Epoch 00227: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1401 - acc: 0.6361 - val_loss: 0.8184 - val_acc: 0.7680\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1347 - acc: 0.6364\n",
      "Epoch 00228: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1346 - acc: 0.6364 - val_loss: 0.9543 - val_acc: 0.7240\n",
      "Epoch 229/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1349 - acc: 0.6327\n",
      "Epoch 00229: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1352 - acc: 0.6326 - val_loss: 0.8482 - val_acc: 0.7608\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1322 - acc: 0.6363\n",
      "Epoch 00230: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1321 - acc: 0.6362 - val_loss: 0.8222 - val_acc: 0.7682\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1317 - acc: 0.6350\n",
      "Epoch 00231: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1318 - acc: 0.6349 - val_loss: 0.7968 - val_acc: 0.7738\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1373 - acc: 0.6323\n",
      "Epoch 00232: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1374 - acc: 0.6323 - val_loss: 0.8134 - val_acc: 0.7633\n",
      "Epoch 233/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1406 - acc: 0.6353\n",
      "Epoch 00233: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1405 - acc: 0.6353 - val_loss: 0.8050 - val_acc: 0.7738\n",
      "Epoch 234/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1378 - acc: 0.6330\n",
      "Epoch 00234: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1378 - acc: 0.6330 - val_loss: 0.7941 - val_acc: 0.7754\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1309 - acc: 0.6369\n",
      "Epoch 00235: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1308 - acc: 0.6369 - val_loss: 0.8453 - val_acc: 0.7629\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1303 - acc: 0.6376\n",
      "Epoch 00236: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1303 - acc: 0.6375 - val_loss: 0.8034 - val_acc: 0.7689\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1351 - acc: 0.6339\n",
      "Epoch 00237: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1351 - acc: 0.6338 - val_loss: 0.8203 - val_acc: 0.7568\n",
      "Epoch 238/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1337 - acc: 0.6363\n",
      "Epoch 00238: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1337 - acc: 0.6362 - val_loss: 0.7968 - val_acc: 0.7692\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1382 - acc: 0.6334\n",
      "Epoch 00239: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1382 - acc: 0.6334 - val_loss: 0.8597 - val_acc: 0.7491\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1269 - acc: 0.6387\n",
      "Epoch 00240: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1265 - acc: 0.6389 - val_loss: 0.7870 - val_acc: 0.7782\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1333 - acc: 0.6345\n",
      "Epoch 00241: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1333 - acc: 0.6345 - val_loss: 0.8059 - val_acc: 0.7694\n",
      "Epoch 242/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1296 - acc: 0.6371\n",
      "Epoch 00242: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1300 - acc: 0.6370 - val_loss: 0.7987 - val_acc: 0.7689\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1257 - acc: 0.6353\n",
      "Epoch 00243: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1257 - acc: 0.6353 - val_loss: 0.8324 - val_acc: 0.7636\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1241 - acc: 0.6371\n",
      "Epoch 00244: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 1.1243 - acc: 0.6370 - val_loss: 0.8194 - val_acc: 0.7741\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1338 - acc: 0.6336\n",
      "Epoch 00245: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1339 - acc: 0.6336 - val_loss: 0.7850 - val_acc: 0.7701\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1319 - acc: 0.6389\n",
      "Epoch 00246: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1313 - acc: 0.6391 - val_loss: 0.8218 - val_acc: 0.7692\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1227 - acc: 0.6399\n",
      "Epoch 00247: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1227 - acc: 0.6399 - val_loss: 0.7880 - val_acc: 0.7750\n",
      "Epoch 248/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1243 - acc: 0.6390\n",
      "Epoch 00248: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1245 - acc: 0.6390 - val_loss: 0.8190 - val_acc: 0.7654\n",
      "Epoch 249/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1260 - acc: 0.6363\n",
      "Epoch 00249: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.1264 - acc: 0.6361 - val_loss: 0.8662 - val_acc: 0.7508\n",
      "Epoch 250/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1364 - acc: 0.6328\n",
      "Epoch 00250: val_loss did not improve from 0.77916\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1364 - acc: 0.6328 - val_loss: 0.7802 - val_acc: 0.7794\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1229 - acc: 0.6356\n",
      "Epoch 00251: val_loss improved from 0.77916 to 0.77747, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/251-0.7775.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1228 - acc: 0.6356 - val_loss: 0.7775 - val_acc: 0.7801\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1312 - acc: 0.6355\n",
      "Epoch 00252: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1311 - acc: 0.6356 - val_loss: 0.8212 - val_acc: 0.7664\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1254 - acc: 0.6376\n",
      "Epoch 00253: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1255 - acc: 0.6375 - val_loss: 0.8089 - val_acc: 0.7692\n",
      "Epoch 254/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1231 - acc: 0.6380\n",
      "Epoch 00254: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1232 - acc: 0.6380 - val_loss: 0.8878 - val_acc: 0.7407\n",
      "Epoch 255/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1167 - acc: 0.6395\n",
      "Epoch 00255: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1166 - acc: 0.6394 - val_loss: 0.7997 - val_acc: 0.7734\n",
      "Epoch 256/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1219 - acc: 0.6382\n",
      "Epoch 00256: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1215 - acc: 0.6385 - val_loss: 0.7802 - val_acc: 0.7741\n",
      "Epoch 257/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1194 - acc: 0.6369\n",
      "Epoch 00257: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1197 - acc: 0.6367 - val_loss: 0.8015 - val_acc: 0.7731\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1243 - acc: 0.6391\n",
      "Epoch 00258: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1244 - acc: 0.6390 - val_loss: 0.8140 - val_acc: 0.7664\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1211 - acc: 0.6383\n",
      "Epoch 00259: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1215 - acc: 0.6383 - val_loss: 0.8252 - val_acc: 0.7710\n",
      "Epoch 260/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1247 - acc: 0.6373\n",
      "Epoch 00260: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1250 - acc: 0.6373 - val_loss: 0.7822 - val_acc: 0.7747\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1181 - acc: 0.6381\n",
      "Epoch 00261: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1181 - acc: 0.6381 - val_loss: 0.8202 - val_acc: 0.7692\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1233 - acc: 0.6400\n",
      "Epoch 00262: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1235 - acc: 0.6399 - val_loss: 0.7854 - val_acc: 0.7703\n",
      "Epoch 263/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1159 - acc: 0.6412\n",
      "Epoch 00263: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1160 - acc: 0.6412 - val_loss: 0.7954 - val_acc: 0.7775\n",
      "Epoch 264/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1145 - acc: 0.6402\n",
      "Epoch 00264: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1141 - acc: 0.6403 - val_loss: 0.8314 - val_acc: 0.7596\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1290 - acc: 0.6363\n",
      "Epoch 00265: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1294 - acc: 0.6362 - val_loss: 0.8436 - val_acc: 0.7519\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1115 - acc: 0.6414\n",
      "Epoch 00266: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1115 - acc: 0.6414 - val_loss: 0.7994 - val_acc: 0.7671\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1233 - acc: 0.6352\n",
      "Epoch 00267: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1234 - acc: 0.6351 - val_loss: 0.8096 - val_acc: 0.7734\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1228 - acc: 0.6381\n",
      "Epoch 00268: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1229 - acc: 0.6381 - val_loss: 0.8103 - val_acc: 0.7591\n",
      "Epoch 269/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1250 - acc: 0.6372\n",
      "Epoch 00269: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1245 - acc: 0.6374 - val_loss: 0.8441 - val_acc: 0.7568\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1180 - acc: 0.6410\n",
      "Epoch 00270: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1181 - acc: 0.6410 - val_loss: 0.8116 - val_acc: 0.7722\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1162 - acc: 0.6405\n",
      "Epoch 00271: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1163 - acc: 0.6405 - val_loss: 0.8286 - val_acc: 0.7626\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1181 - acc: 0.6388\n",
      "Epoch 00272: val_loss did not improve from 0.77747\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1177 - acc: 0.6389 - val_loss: 0.8013 - val_acc: 0.7745\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1119 - acc: 0.6388\n",
      "Epoch 00273: val_loss improved from 0.77747 to 0.76686, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/273-0.7669.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1119 - acc: 0.6388 - val_loss: 0.7669 - val_acc: 0.7724\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1144 - acc: 0.6413\n",
      "Epoch 00274: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1145 - acc: 0.6412 - val_loss: 0.8199 - val_acc: 0.7689\n",
      "Epoch 275/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1131 - acc: 0.6424\n",
      "Epoch 00275: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1130 - acc: 0.6424 - val_loss: 0.8446 - val_acc: 0.7661\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1140 - acc: 0.6412\n",
      "Epoch 00276: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1140 - acc: 0.6412 - val_loss: 0.8202 - val_acc: 0.7692\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1192 - acc: 0.6403\n",
      "Epoch 00277: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1191 - acc: 0.6403 - val_loss: 0.8350 - val_acc: 0.7549\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1150 - acc: 0.6432\n",
      "Epoch 00278: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1149 - acc: 0.6432 - val_loss: 0.9139 - val_acc: 0.7368\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1147 - acc: 0.6410\n",
      "Epoch 00279: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1146 - acc: 0.6410 - val_loss: 0.8302 - val_acc: 0.7536\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1121 - acc: 0.6413\n",
      "Epoch 00280: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1121 - acc: 0.6414 - val_loss: 0.7802 - val_acc: 0.7750\n",
      "Epoch 281/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1109 - acc: 0.6417\n",
      "Epoch 00281: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1111 - acc: 0.6417 - val_loss: 0.8567 - val_acc: 0.7587\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1154 - acc: 0.6395\n",
      "Epoch 00282: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1154 - acc: 0.6396 - val_loss: 0.7780 - val_acc: 0.7794\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1154 - acc: 0.6379\n",
      "Epoch 00283: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1154 - acc: 0.6379 - val_loss: 0.8285 - val_acc: 0.7647\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1122 - acc: 0.6418\n",
      "Epoch 00284: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1122 - acc: 0.6418 - val_loss: 0.9064 - val_acc: 0.7417\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1015 - acc: 0.6432\n",
      "Epoch 00285: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1017 - acc: 0.6431 - val_loss: 0.7816 - val_acc: 0.7785\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1130 - acc: 0.6417\n",
      "Epoch 00286: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1131 - acc: 0.6417 - val_loss: 0.7857 - val_acc: 0.7785\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1100 - acc: 0.6420\n",
      "Epoch 00287: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1099 - acc: 0.6420 - val_loss: 0.8048 - val_acc: 0.7745\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1042 - acc: 0.6448\n",
      "Epoch 00288: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1042 - acc: 0.6448 - val_loss: 0.7838 - val_acc: 0.7845\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1106 - acc: 0.6408\n",
      "Epoch 00289: val_loss did not improve from 0.76686\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1108 - acc: 0.6408 - val_loss: 0.8244 - val_acc: 0.7701\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1039 - acc: 0.6408\n",
      "Epoch 00290: val_loss improved from 0.76686 to 0.76100, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/290-0.7610.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1038 - acc: 0.6409 - val_loss: 0.7610 - val_acc: 0.7792\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1140 - acc: 0.6389\n",
      "Epoch 00291: val_loss did not improve from 0.76100\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1140 - acc: 0.6389 - val_loss: 0.7952 - val_acc: 0.7727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1076 - acc: 0.6424\n",
      "Epoch 00292: val_loss did not improve from 0.76100\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1075 - acc: 0.6424 - val_loss: 0.8388 - val_acc: 0.7647\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1051 - acc: 0.6425\n",
      "Epoch 00293: val_loss did not improve from 0.76100\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.1054 - acc: 0.6425 - val_loss: 0.7822 - val_acc: 0.7792\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1160 - acc: 0.6377\n",
      "Epoch 00294: val_loss did not improve from 0.76100\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1161 - acc: 0.6377 - val_loss: 0.8622 - val_acc: 0.7473\n",
      "Epoch 295/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1036 - acc: 0.6427\n",
      "Epoch 00295: val_loss did not improve from 0.76100\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1036 - acc: 0.6427 - val_loss: 0.7870 - val_acc: 0.7759\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1067 - acc: 0.6436\n",
      "Epoch 00296: val_loss did not improve from 0.76100\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1067 - acc: 0.6436 - val_loss: 0.8130 - val_acc: 0.7761\n",
      "Epoch 297/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1079 - acc: 0.6438\n",
      "Epoch 00297: val_loss did not improve from 0.76100\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.1080 - acc: 0.6437 - val_loss: 0.7723 - val_acc: 0.7778\n",
      "Epoch 298/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1026 - acc: 0.6436\n",
      "Epoch 00298: val_loss improved from 0.76100 to 0.76036, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/298-0.7604.hdf5\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1023 - acc: 0.6437 - val_loss: 0.7604 - val_acc: 0.7885\n",
      "Epoch 299/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1046 - acc: 0.6439\n",
      "Epoch 00299: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1046 - acc: 0.6439 - val_loss: 0.8816 - val_acc: 0.7435\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0986 - acc: 0.6448\n",
      "Epoch 00300: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0986 - acc: 0.6447 - val_loss: 0.7717 - val_acc: 0.7789\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1019 - acc: 0.6443\n",
      "Epoch 00301: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.1018 - acc: 0.6444 - val_loss: 0.8506 - val_acc: 0.7610\n",
      "Epoch 302/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0924 - acc: 0.6475\n",
      "Epoch 00302: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0923 - acc: 0.6475 - val_loss: 0.7634 - val_acc: 0.7827\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1058 - acc: 0.6448\n",
      "Epoch 00303: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1058 - acc: 0.6448 - val_loss: 0.7739 - val_acc: 0.7806\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1017 - acc: 0.6436\n",
      "Epoch 00304: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1017 - acc: 0.6437 - val_loss: 0.7980 - val_acc: 0.7766\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1033 - acc: 0.6468\n",
      "Epoch 00305: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1033 - acc: 0.6468 - val_loss: 0.7703 - val_acc: 0.7778\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0996 - acc: 0.6448\n",
      "Epoch 00306: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.0995 - acc: 0.6449 - val_loss: 0.7659 - val_acc: 0.7843\n",
      "Epoch 307/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0936 - acc: 0.6484\n",
      "Epoch 00307: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0936 - acc: 0.6485 - val_loss: 0.7745 - val_acc: 0.7710\n",
      "Epoch 308/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1078 - acc: 0.6435\n",
      "Epoch 00308: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1073 - acc: 0.6437 - val_loss: 0.7991 - val_acc: 0.7766\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1018 - acc: 0.6442\n",
      "Epoch 00309: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.1019 - acc: 0.6442 - val_loss: 0.7706 - val_acc: 0.7761\n",
      "Epoch 310/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1038 - acc: 0.6454\n",
      "Epoch 00310: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1040 - acc: 0.6454 - val_loss: 0.8188 - val_acc: 0.7659\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1012 - acc: 0.6458\n",
      "Epoch 00311: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1012 - acc: 0.6458 - val_loss: 0.7734 - val_acc: 0.7792\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1020 - acc: 0.6446\n",
      "Epoch 00312: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1020 - acc: 0.6446 - val_loss: 0.7775 - val_acc: 0.7787\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1034 - acc: 0.6451\n",
      "Epoch 00313: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1035 - acc: 0.6451 - val_loss: 0.7910 - val_acc: 0.7754\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0993 - acc: 0.6456\n",
      "Epoch 00314: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0994 - acc: 0.6456 - val_loss: 0.8268 - val_acc: 0.7694\n",
      "Epoch 315/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1020 - acc: 0.6457\n",
      "Epoch 00315: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1019 - acc: 0.6456 - val_loss: 0.7988 - val_acc: 0.7736\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1068 - acc: 0.6403\n",
      "Epoch 00316: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1067 - acc: 0.6403 - val_loss: 0.7704 - val_acc: 0.7780\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0982 - acc: 0.6477\n",
      "Epoch 00317: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0981 - acc: 0.6477 - val_loss: 0.7769 - val_acc: 0.7764\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0948 - acc: 0.6424\n",
      "Epoch 00318: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.0948 - acc: 0.6424 - val_loss: 0.7727 - val_acc: 0.7822\n",
      "Epoch 319/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1029 - acc: 0.6414\n",
      "Epoch 00319: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1032 - acc: 0.6413 - val_loss: 0.8622 - val_acc: 0.7540\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1001 - acc: 0.6476\n",
      "Epoch 00320: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1002 - acc: 0.6475 - val_loss: 0.7725 - val_acc: 0.7747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0948 - acc: 0.6454\n",
      "Epoch 00321: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0947 - acc: 0.6454 - val_loss: 0.7824 - val_acc: 0.7808\n",
      "Epoch 322/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0972 - acc: 0.6470\n",
      "Epoch 00322: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0970 - acc: 0.6470 - val_loss: 0.7796 - val_acc: 0.7745\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1037 - acc: 0.6448\n",
      "Epoch 00323: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1036 - acc: 0.6447 - val_loss: 0.7766 - val_acc: 0.7727\n",
      "Epoch 324/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0998 - acc: 0.6467\n",
      "Epoch 00324: val_loss did not improve from 0.76036\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0996 - acc: 0.6467 - val_loss: 0.7716 - val_acc: 0.7778\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0941 - acc: 0.6494\n",
      "Epoch 00325: val_loss improved from 0.76036 to 0.75863, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/325-0.7586.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.0942 - acc: 0.6494 - val_loss: 0.7586 - val_acc: 0.7894\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0993 - acc: 0.6488\n",
      "Epoch 00326: val_loss improved from 0.75863 to 0.74403, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/326-0.7440.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.0993 - acc: 0.6488 - val_loss: 0.7440 - val_acc: 0.7906\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0932 - acc: 0.6485\n",
      "Epoch 00327: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0932 - acc: 0.6485 - val_loss: 0.8349 - val_acc: 0.7526\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0978 - acc: 0.6457\n",
      "Epoch 00328: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0978 - acc: 0.6457 - val_loss: 0.8270 - val_acc: 0.7610\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0919 - acc: 0.6485\n",
      "Epoch 00329: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0918 - acc: 0.6486 - val_loss: 0.7999 - val_acc: 0.7631\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0934 - acc: 0.6472\n",
      "Epoch 00330: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0937 - acc: 0.6471 - val_loss: 0.8391 - val_acc: 0.7652\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0911 - acc: 0.6464\n",
      "Epoch 00331: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0911 - acc: 0.6465 - val_loss: 0.8247 - val_acc: 0.7640\n",
      "Epoch 332/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0984 - acc: 0.6446\n",
      "Epoch 00332: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0980 - acc: 0.6447 - val_loss: 0.8255 - val_acc: 0.7568\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0919 - acc: 0.6459\n",
      "Epoch 00333: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0919 - acc: 0.6459 - val_loss: 0.7551 - val_acc: 0.7831\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1007 - acc: 0.6454\n",
      "Epoch 00334: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.1006 - acc: 0.6454 - val_loss: 0.7688 - val_acc: 0.7829\n",
      "Epoch 335/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0982 - acc: 0.6473\n",
      "Epoch 00335: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.0981 - acc: 0.6474 - val_loss: 0.8663 - val_acc: 0.7515\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0944 - acc: 0.6496\n",
      "Epoch 00336: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0943 - acc: 0.6496 - val_loss: 0.8377 - val_acc: 0.7664\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0835 - acc: 0.6494\n",
      "Epoch 00337: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0835 - acc: 0.6494 - val_loss: 0.7746 - val_acc: 0.7759\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0828 - acc: 0.6512\n",
      "Epoch 00338: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0828 - acc: 0.6512 - val_loss: 0.7753 - val_acc: 0.7782\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0919 - acc: 0.6486\n",
      "Epoch 00339: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0919 - acc: 0.6486 - val_loss: 0.7760 - val_acc: 0.7796\n",
      "Epoch 340/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0924 - acc: 0.6486\n",
      "Epoch 00340: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.0923 - acc: 0.6487 - val_loss: 0.7814 - val_acc: 0.7778\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0959 - acc: 0.6457\n",
      "Epoch 00341: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0960 - acc: 0.6456 - val_loss: 0.8002 - val_acc: 0.7666\n",
      "Epoch 342/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0838 - acc: 0.6516\n",
      "Epoch 00342: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0837 - acc: 0.6515 - val_loss: 0.7853 - val_acc: 0.7722\n",
      "Epoch 343/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0919 - acc: 0.6498\n",
      "Epoch 00343: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.0918 - acc: 0.6499 - val_loss: 0.8652 - val_acc: 0.7575\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0869 - acc: 0.6496\n",
      "Epoch 00344: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0868 - acc: 0.6496 - val_loss: 0.7723 - val_acc: 0.7789\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0967 - acc: 0.6485\n",
      "Epoch 00345: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0966 - acc: 0.6485 - val_loss: 0.7926 - val_acc: 0.7708\n",
      "Epoch 346/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0895 - acc: 0.6479\n",
      "Epoch 00346: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0895 - acc: 0.6481 - val_loss: 0.7654 - val_acc: 0.7768\n",
      "Epoch 347/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0885 - acc: 0.6485\n",
      "Epoch 00347: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.0884 - acc: 0.6484 - val_loss: 0.7741 - val_acc: 0.7743\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0920 - acc: 0.6467\n",
      "Epoch 00348: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0922 - acc: 0.6467 - val_loss: 0.8557 - val_acc: 0.7391\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0918 - acc: 0.6484\n",
      "Epoch 00349: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0916 - acc: 0.6484 - val_loss: 0.7683 - val_acc: 0.7831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0814 - acc: 0.6515\n",
      "Epoch 00350: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0812 - acc: 0.6515 - val_loss: 0.7809 - val_acc: 0.7766\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0855 - acc: 0.6476\n",
      "Epoch 00351: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0854 - acc: 0.6477 - val_loss: 0.8349 - val_acc: 0.7612\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0866 - acc: 0.6485\n",
      "Epoch 00352: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.0866 - acc: 0.6484 - val_loss: 0.7843 - val_acc: 0.7745\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0872 - acc: 0.6508\n",
      "Epoch 00353: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.0871 - acc: 0.6508 - val_loss: 0.7523 - val_acc: 0.7859\n",
      "Epoch 354/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0909 - acc: 0.6469\n",
      "Epoch 00354: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.0909 - acc: 0.6471 - val_loss: 0.8107 - val_acc: 0.7713\n",
      "Epoch 355/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0852 - acc: 0.6516\n",
      "Epoch 00355: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0854 - acc: 0.6515 - val_loss: 0.7900 - val_acc: 0.7782\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0944 - acc: 0.6473\n",
      "Epoch 00356: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0944 - acc: 0.6474 - val_loss: 0.7636 - val_acc: 0.7801\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0823 - acc: 0.6529\n",
      "Epoch 00357: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0823 - acc: 0.6529 - val_loss: 0.8329 - val_acc: 0.7596\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0828 - acc: 0.6471\n",
      "Epoch 00358: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0829 - acc: 0.6470 - val_loss: 0.7815 - val_acc: 0.7787\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0866 - acc: 0.6498\n",
      "Epoch 00359: val_loss did not improve from 0.74403\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.0867 - acc: 0.6498 - val_loss: 0.7801 - val_acc: 0.7736\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0789 - acc: 0.6516\n",
      "Epoch 00360: val_loss improved from 0.74403 to 0.74383, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/360-0.7438.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0789 - acc: 0.6517 - val_loss: 0.7438 - val_acc: 0.7883\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0824 - acc: 0.6500\n",
      "Epoch 00361: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0824 - acc: 0.6500 - val_loss: 0.7813 - val_acc: 0.7778\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0871 - acc: 0.6484\n",
      "Epoch 00362: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0872 - acc: 0.6484 - val_loss: 0.7985 - val_acc: 0.7734\n",
      "Epoch 363/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0806 - acc: 0.6514\n",
      "Epoch 00363: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0806 - acc: 0.6514 - val_loss: 0.7837 - val_acc: 0.7801\n",
      "Epoch 364/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0866 - acc: 0.6487\n",
      "Epoch 00364: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0867 - acc: 0.6488 - val_loss: 0.7642 - val_acc: 0.7897\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0860 - acc: 0.6493\n",
      "Epoch 00365: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0860 - acc: 0.6494 - val_loss: 0.7604 - val_acc: 0.7813\n",
      "Epoch 366/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0899 - acc: 0.6475\n",
      "Epoch 00366: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0899 - acc: 0.6475 - val_loss: 0.7697 - val_acc: 0.7887\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0813 - acc: 0.6510\n",
      "Epoch 00367: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0813 - acc: 0.6510 - val_loss: 0.7792 - val_acc: 0.7808\n",
      "Epoch 368/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0876 - acc: 0.6503\n",
      "Epoch 00368: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0876 - acc: 0.6503 - val_loss: 0.7605 - val_acc: 0.7892\n",
      "Epoch 369/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0809 - acc: 0.6519\n",
      "Epoch 00369: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0810 - acc: 0.6518 - val_loss: 0.7581 - val_acc: 0.7918\n",
      "Epoch 370/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0803 - acc: 0.6514\n",
      "Epoch 00370: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0803 - acc: 0.6515 - val_loss: 0.8227 - val_acc: 0.7685\n",
      "Epoch 371/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0826 - acc: 0.6543\n",
      "Epoch 00371: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0826 - acc: 0.6543 - val_loss: 0.7672 - val_acc: 0.7843\n",
      "Epoch 372/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0751 - acc: 0.6561\n",
      "Epoch 00372: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0752 - acc: 0.6561 - val_loss: 0.7788 - val_acc: 0.7747\n",
      "Epoch 373/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0782 - acc: 0.6494\n",
      "Epoch 00373: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0788 - acc: 0.6494 - val_loss: 0.7881 - val_acc: 0.7778\n",
      "Epoch 374/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0789 - acc: 0.6536\n",
      "Epoch 00374: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0789 - acc: 0.6536 - val_loss: 0.7465 - val_acc: 0.7871\n",
      "Epoch 375/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0771 - acc: 0.6515\n",
      "Epoch 00375: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.0770 - acc: 0.6515 - val_loss: 0.7656 - val_acc: 0.7820\n",
      "Epoch 376/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0788 - acc: 0.6523\n",
      "Epoch 00376: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0785 - acc: 0.6522 - val_loss: 0.7918 - val_acc: 0.7757\n",
      "Epoch 377/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0816 - acc: 0.6495\n",
      "Epoch 00377: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0815 - acc: 0.6494 - val_loss: 0.7466 - val_acc: 0.7890\n",
      "Epoch 378/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0752 - acc: 0.6533\n",
      "Epoch 00378: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0752 - acc: 0.6533 - val_loss: 0.8257 - val_acc: 0.7678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0844 - acc: 0.6518\n",
      "Epoch 00379: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0841 - acc: 0.6519 - val_loss: 0.7693 - val_acc: 0.7815\n",
      "Epoch 380/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0880 - acc: 0.6489\n",
      "Epoch 00380: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0876 - acc: 0.6490 - val_loss: 0.8155 - val_acc: 0.7743\n",
      "Epoch 381/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0805 - acc: 0.6522\n",
      "Epoch 00381: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0804 - acc: 0.6522 - val_loss: 0.7633 - val_acc: 0.7813\n",
      "Epoch 382/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0742 - acc: 0.6536\n",
      "Epoch 00382: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0741 - acc: 0.6537 - val_loss: 0.7688 - val_acc: 0.7824\n",
      "Epoch 383/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0824 - acc: 0.6502\n",
      "Epoch 00383: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0825 - acc: 0.6502 - val_loss: 0.7532 - val_acc: 0.7859\n",
      "Epoch 384/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0785 - acc: 0.6510\n",
      "Epoch 00384: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.0785 - acc: 0.6511 - val_loss: 0.7735 - val_acc: 0.7771\n",
      "Epoch 385/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0818 - acc: 0.6510\n",
      "Epoch 00385: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0817 - acc: 0.6511 - val_loss: 0.7697 - val_acc: 0.7848\n",
      "Epoch 386/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0776 - acc: 0.6528\n",
      "Epoch 00386: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.0777 - acc: 0.6528 - val_loss: 0.7536 - val_acc: 0.7873\n",
      "Epoch 387/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0789 - acc: 0.6490\n",
      "Epoch 00387: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0791 - acc: 0.6491 - val_loss: 0.7551 - val_acc: 0.7883\n",
      "Epoch 388/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0795 - acc: 0.6520\n",
      "Epoch 00388: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0796 - acc: 0.6519 - val_loss: 0.7606 - val_acc: 0.7817\n",
      "Epoch 389/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0713 - acc: 0.6536\n",
      "Epoch 00389: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0713 - acc: 0.6536 - val_loss: 0.7646 - val_acc: 0.7789\n",
      "Epoch 390/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0751 - acc: 0.6526\n",
      "Epoch 00390: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0749 - acc: 0.6528 - val_loss: 0.7551 - val_acc: 0.7880\n",
      "Epoch 391/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0734 - acc: 0.6527\n",
      "Epoch 00391: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0734 - acc: 0.6527 - val_loss: 0.8335 - val_acc: 0.7640\n",
      "Epoch 392/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0703 - acc: 0.6561\n",
      "Epoch 00392: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0703 - acc: 0.6561 - val_loss: 0.7553 - val_acc: 0.7885\n",
      "Epoch 393/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0758 - acc: 0.6529\n",
      "Epoch 00393: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0758 - acc: 0.6529 - val_loss: 0.8142 - val_acc: 0.7657\n",
      "Epoch 394/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0815 - acc: 0.6543\n",
      "Epoch 00394: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.0822 - acc: 0.6543 - val_loss: 0.8191 - val_acc: 0.7675\n",
      "Epoch 395/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0765 - acc: 0.6489\n",
      "Epoch 00395: val_loss did not improve from 0.74383\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0767 - acc: 0.6489 - val_loss: 0.8095 - val_acc: 0.7720\n",
      "Epoch 396/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0707 - acc: 0.6550\n",
      "Epoch 00396: val_loss improved from 0.74383 to 0.73090, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv_checkpoint/396-0.7309.hdf5\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.0705 - acc: 0.6552 - val_loss: 0.7309 - val_acc: 0.7904\n",
      "Epoch 397/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0762 - acc: 0.6507\n",
      "Epoch 00397: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0761 - acc: 0.6506 - val_loss: 0.7963 - val_acc: 0.7789\n",
      "Epoch 398/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0694 - acc: 0.6538\n",
      "Epoch 00398: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0697 - acc: 0.6537 - val_loss: 0.7921 - val_acc: 0.7741\n",
      "Epoch 399/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0678 - acc: 0.6564\n",
      "Epoch 00399: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0678 - acc: 0.6565 - val_loss: 0.7336 - val_acc: 0.7941\n",
      "Epoch 400/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0699 - acc: 0.6538\n",
      "Epoch 00400: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.0699 - acc: 0.6538 - val_loss: 0.8577 - val_acc: 0.7512\n",
      "Epoch 401/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0692 - acc: 0.6541\n",
      "Epoch 00401: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0691 - acc: 0.6542 - val_loss: 0.7764 - val_acc: 0.7815\n",
      "Epoch 402/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0701 - acc: 0.6529\n",
      "Epoch 00402: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0700 - acc: 0.6529 - val_loss: 0.8017 - val_acc: 0.7694\n",
      "Epoch 403/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0816 - acc: 0.6500\n",
      "Epoch 00403: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0817 - acc: 0.6500 - val_loss: 0.7629 - val_acc: 0.7862\n",
      "Epoch 404/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0708 - acc: 0.6549\n",
      "Epoch 00404: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0709 - acc: 0.6549 - val_loss: 0.8417 - val_acc: 0.7563\n",
      "Epoch 405/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0716 - acc: 0.6542\n",
      "Epoch 00405: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0716 - acc: 0.6542 - val_loss: 0.7384 - val_acc: 0.7904\n",
      "Epoch 406/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0750 - acc: 0.6539\n",
      "Epoch 00406: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0749 - acc: 0.6539 - val_loss: 0.7578 - val_acc: 0.7885\n",
      "Epoch 407/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0736 - acc: 0.6530\n",
      "Epoch 00407: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.0737 - acc: 0.6530 - val_loss: 0.7639 - val_acc: 0.7829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 408/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0695 - acc: 0.6534\n",
      "Epoch 00408: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0696 - acc: 0.6534 - val_loss: 0.7529 - val_acc: 0.7848\n",
      "Epoch 409/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0729 - acc: 0.6502\n",
      "Epoch 00409: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0729 - acc: 0.6502 - val_loss: 0.7383 - val_acc: 0.7897\n",
      "Epoch 410/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0684 - acc: 0.6559\n",
      "Epoch 00410: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0683 - acc: 0.6559 - val_loss: 0.7601 - val_acc: 0.7822\n",
      "Epoch 411/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0719 - acc: 0.6530\n",
      "Epoch 00411: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0715 - acc: 0.6531 - val_loss: 0.7591 - val_acc: 0.7829\n",
      "Epoch 412/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0763 - acc: 0.6543\n",
      "Epoch 00412: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0768 - acc: 0.6542 - val_loss: 0.7665 - val_acc: 0.7803\n",
      "Epoch 413/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0706 - acc: 0.6538\n",
      "Epoch 00413: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0715 - acc: 0.6537 - val_loss: 0.7708 - val_acc: 0.7799\n",
      "Epoch 414/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0652 - acc: 0.6528\n",
      "Epoch 00414: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0651 - acc: 0.6528 - val_loss: 0.7668 - val_acc: 0.7887\n",
      "Epoch 415/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0688 - acc: 0.6537\n",
      "Epoch 00415: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0688 - acc: 0.6537 - val_loss: 0.7435 - val_acc: 0.7829\n",
      "Epoch 416/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0722 - acc: 0.6533\n",
      "Epoch 00416: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.0722 - acc: 0.6533 - val_loss: 0.7712 - val_acc: 0.7843\n",
      "Epoch 417/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0635 - acc: 0.6558\n",
      "Epoch 00417: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.0635 - acc: 0.6558 - val_loss: 0.7410 - val_acc: 0.7918\n",
      "Epoch 418/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0699 - acc: 0.6533\n",
      "Epoch 00418: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0700 - acc: 0.6533 - val_loss: 0.7586 - val_acc: 0.7808\n",
      "Epoch 419/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0732 - acc: 0.6545\n",
      "Epoch 00419: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0733 - acc: 0.6544 - val_loss: 0.7679 - val_acc: 0.7843\n",
      "Epoch 420/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.6527\n",
      "Epoch 00420: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0664 - acc: 0.6527 - val_loss: 0.7599 - val_acc: 0.7876\n",
      "Epoch 421/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0661 - acc: 0.6533\n",
      "Epoch 00421: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.0662 - acc: 0.6533 - val_loss: 0.7762 - val_acc: 0.7736\n",
      "Epoch 422/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0628 - acc: 0.6565\n",
      "Epoch 00422: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0627 - acc: 0.6565 - val_loss: 0.7629 - val_acc: 0.7801\n",
      "Epoch 423/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0735 - acc: 0.6536\n",
      "Epoch 00423: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0734 - acc: 0.6537 - val_loss: 0.7592 - val_acc: 0.7782\n",
      "Epoch 424/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0657 - acc: 0.6545\n",
      "Epoch 00424: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0656 - acc: 0.6545 - val_loss: 0.7617 - val_acc: 0.7822\n",
      "Epoch 425/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0621 - acc: 0.6557\n",
      "Epoch 00425: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0621 - acc: 0.6556 - val_loss: 0.8134 - val_acc: 0.7671\n",
      "Epoch 426/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0676 - acc: 0.6573\n",
      "Epoch 00426: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.0676 - acc: 0.6573 - val_loss: 0.7432 - val_acc: 0.7866\n",
      "Epoch 427/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0605 - acc: 0.6594\n",
      "Epoch 00427: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0606 - acc: 0.6594 - val_loss: 0.7537 - val_acc: 0.7852\n",
      "Epoch 428/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0697 - acc: 0.6560\n",
      "Epoch 00428: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0697 - acc: 0.6559 - val_loss: 0.7587 - val_acc: 0.7810\n",
      "Epoch 429/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0605 - acc: 0.6588\n",
      "Epoch 00429: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0612 - acc: 0.6588 - val_loss: 0.7740 - val_acc: 0.7766\n",
      "Epoch 430/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0719 - acc: 0.6553\n",
      "Epoch 00430: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0718 - acc: 0.6554 - val_loss: 0.7512 - val_acc: 0.7878\n",
      "Epoch 431/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0683 - acc: 0.6552\n",
      "Epoch 00431: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0680 - acc: 0.6552 - val_loss: 0.7726 - val_acc: 0.7757\n",
      "Epoch 432/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0642 - acc: 0.6574\n",
      "Epoch 00432: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0642 - acc: 0.6574 - val_loss: 0.7385 - val_acc: 0.7843\n",
      "Epoch 433/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0650 - acc: 0.6540\n",
      "Epoch 00433: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0650 - acc: 0.6539 - val_loss: 0.7736 - val_acc: 0.7787\n",
      "Epoch 434/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0684 - acc: 0.6569\n",
      "Epoch 00434: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0685 - acc: 0.6569 - val_loss: 0.7698 - val_acc: 0.7836\n",
      "Epoch 435/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0686 - acc: 0.6546\n",
      "Epoch 00435: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0686 - acc: 0.6547 - val_loss: 0.7822 - val_acc: 0.7801\n",
      "Epoch 436/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0630 - acc: 0.6568\n",
      "Epoch 00436: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0630 - acc: 0.6568 - val_loss: 0.7398 - val_acc: 0.7871\n",
      "Epoch 437/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0710 - acc: 0.6515\n",
      "Epoch 00437: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0710 - acc: 0.6515 - val_loss: 0.7441 - val_acc: 0.7894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0637 - acc: 0.6585\n",
      "Epoch 00438: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0637 - acc: 0.6584 - val_loss: 0.7505 - val_acc: 0.7836\n",
      "Epoch 439/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0728 - acc: 0.6517\n",
      "Epoch 00439: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0725 - acc: 0.6517 - val_loss: 0.8019 - val_acc: 0.7743\n",
      "Epoch 440/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0628 - acc: 0.6573\n",
      "Epoch 00440: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0629 - acc: 0.6573 - val_loss: 0.8742 - val_acc: 0.7433\n",
      "Epoch 441/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0612 - acc: 0.6550\n",
      "Epoch 00441: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0611 - acc: 0.6550 - val_loss: 0.8345 - val_acc: 0.7647\n",
      "Epoch 442/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0608 - acc: 0.6570\n",
      "Epoch 00442: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 1.0608 - acc: 0.6570 - val_loss: 0.7956 - val_acc: 0.7722\n",
      "Epoch 443/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0709 - acc: 0.6554\n",
      "Epoch 00443: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0710 - acc: 0.6553 - val_loss: 0.8191 - val_acc: 0.7724\n",
      "Epoch 444/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0655 - acc: 0.6515\n",
      "Epoch 00444: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.0655 - acc: 0.6515 - val_loss: 0.7448 - val_acc: 0.7887\n",
      "Epoch 445/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0680 - acc: 0.6551\n",
      "Epoch 00445: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.0679 - acc: 0.6551 - val_loss: 0.8093 - val_acc: 0.7633\n",
      "Epoch 446/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0592 - acc: 0.6587\n",
      "Epoch 00446: val_loss did not improve from 0.73090\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0593 - acc: 0.6587 - val_loss: 0.7838 - val_acc: 0.7692\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 383us/sample - loss: 0.8076 - acc: 0.7632\n",
      "Loss: 0.8075879554758438 Accuracy: 0.76323986\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.4897 - acc: 0.0891\n",
      "Epoch 00001: val_loss improved from inf to 2.63608, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/001-2.6361.hdf5\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 6.4893 - acc: 0.0891 - val_loss: 2.6361 - val_acc: 0.1705\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.1824 - acc: 0.1183\n",
      "Epoch 00002: val_loss improved from 2.63608 to 2.54053, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/002-2.5405.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 4.1826 - acc: 0.1183 - val_loss: 2.5405 - val_acc: 0.2590\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4557 - acc: 0.1304\n",
      "Epoch 00003: val_loss improved from 2.54053 to 2.36636, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/003-2.3664.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 3.4556 - acc: 0.1304 - val_loss: 2.3664 - val_acc: 0.2558\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9428 - acc: 0.1379\n",
      "Epoch 00004: val_loss improved from 2.36636 to 2.23894, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/004-2.2389.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 2.9428 - acc: 0.1379 - val_loss: 2.2389 - val_acc: 0.3024\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6911 - acc: 0.1542\n",
      "Epoch 00005: val_loss improved from 2.23894 to 2.15265, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/005-2.1526.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 2.6911 - acc: 0.1542 - val_loss: 2.1526 - val_acc: 0.3417\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5499 - acc: 0.1755\n",
      "Epoch 00006: val_loss improved from 2.15265 to 2.08561, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/006-2.0856.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 2.5499 - acc: 0.1755 - val_loss: 2.0856 - val_acc: 0.3941\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4457 - acc: 0.1948\n",
      "Epoch 00007: val_loss improved from 2.08561 to 1.99527, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/007-1.9953.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 2.4457 - acc: 0.1948 - val_loss: 1.9953 - val_acc: 0.4214\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3492 - acc: 0.2192\n",
      "Epoch 00008: val_loss improved from 1.99527 to 1.92073, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/008-1.9207.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 2.3493 - acc: 0.2192 - val_loss: 1.9207 - val_acc: 0.4503\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2496 - acc: 0.2483\n",
      "Epoch 00009: val_loss improved from 1.92073 to 1.84628, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/009-1.8463.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 2.2495 - acc: 0.2484 - val_loss: 1.8463 - val_acc: 0.4710\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1667 - acc: 0.2724\n",
      "Epoch 00010: val_loss improved from 1.84628 to 1.76274, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/010-1.7627.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 2.1668 - acc: 0.2724 - val_loss: 1.7627 - val_acc: 0.5029\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0791 - acc: 0.3025\n",
      "Epoch 00011: val_loss improved from 1.76274 to 1.68178, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/011-1.6818.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 2.0787 - acc: 0.3025 - val_loss: 1.6818 - val_acc: 0.5253\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9963 - acc: 0.3282\n",
      "Epoch 00012: val_loss improved from 1.68178 to 1.59273, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/012-1.5927.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 1.9963 - acc: 0.3282 - val_loss: 1.5927 - val_acc: 0.5490\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9145 - acc: 0.3552\n",
      "Epoch 00013: val_loss improved from 1.59273 to 1.52388, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/013-1.5239.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.9145 - acc: 0.3552 - val_loss: 1.5239 - val_acc: 0.5712\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8321 - acc: 0.3823\n",
      "Epoch 00014: val_loss improved from 1.52388 to 1.46038, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/014-1.4604.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.8320 - acc: 0.3823 - val_loss: 1.4604 - val_acc: 0.5931\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7764 - acc: 0.4077\n",
      "Epoch 00015: val_loss improved from 1.46038 to 1.40365, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/015-1.4036.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.7764 - acc: 0.4077 - val_loss: 1.4036 - val_acc: 0.6194\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7282 - acc: 0.4243\n",
      "Epoch 00016: val_loss improved from 1.40365 to 1.34420, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/016-1.3442.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.7278 - acc: 0.4245 - val_loss: 1.3442 - val_acc: 0.6385\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6713 - acc: 0.4445\n",
      "Epoch 00017: val_loss improved from 1.34420 to 1.31453, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/017-1.3145.hdf5\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 1.6713 - acc: 0.4445 - val_loss: 1.3145 - val_acc: 0.6385\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6257 - acc: 0.4648\n",
      "Epoch 00018: val_loss improved from 1.31453 to 1.26441, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/018-1.2644.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.6257 - acc: 0.4647 - val_loss: 1.2644 - val_acc: 0.6569\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5894 - acc: 0.4783\n",
      "Epoch 00019: val_loss improved from 1.26441 to 1.20979, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/019-1.2098.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.5890 - acc: 0.4784 - val_loss: 1.2098 - val_acc: 0.6769\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5460 - acc: 0.4938\n",
      "Epoch 00020: val_loss improved from 1.20979 to 1.19702, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/020-1.1970.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 1.5460 - acc: 0.4938 - val_loss: 1.1970 - val_acc: 0.6811\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5186 - acc: 0.5035\n",
      "Epoch 00021: val_loss improved from 1.19702 to 1.15711, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/021-1.1571.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.5188 - acc: 0.5035 - val_loss: 1.1571 - val_acc: 0.6981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4808 - acc: 0.5134\n",
      "Epoch 00022: val_loss improved from 1.15711 to 1.11233, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/022-1.1123.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 1.4807 - acc: 0.5134 - val_loss: 1.1123 - val_acc: 0.7074\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4524 - acc: 0.5245\n",
      "Epoch 00023: val_loss improved from 1.11233 to 1.08830, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/023-1.0883.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 1.4523 - acc: 0.5245 - val_loss: 1.0883 - val_acc: 0.7007\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4303 - acc: 0.5355\n",
      "Epoch 00024: val_loss improved from 1.08830 to 1.06968, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/024-1.0697.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.4305 - acc: 0.5355 - val_loss: 1.0697 - val_acc: 0.7084\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4014 - acc: 0.5433\n",
      "Epoch 00025: val_loss improved from 1.06968 to 1.03170, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/025-1.0317.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 1.4015 - acc: 0.5433 - val_loss: 1.0317 - val_acc: 0.7200\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3749 - acc: 0.5561\n",
      "Epoch 00026: val_loss did not improve from 1.03170\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.3750 - acc: 0.5560 - val_loss: 1.0376 - val_acc: 0.7195\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3580 - acc: 0.5602\n",
      "Epoch 00027: val_loss improved from 1.03170 to 0.99510, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/027-0.9951.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.3581 - acc: 0.5602 - val_loss: 0.9951 - val_acc: 0.7347\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3475 - acc: 0.5648\n",
      "Epoch 00028: val_loss improved from 0.99510 to 0.98574, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/028-0.9857.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.3474 - acc: 0.5649 - val_loss: 0.9857 - val_acc: 0.7363\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3272 - acc: 0.5710\n",
      "Epoch 00029: val_loss improved from 0.98574 to 0.97306, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/029-0.9731.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.3272 - acc: 0.5710 - val_loss: 0.9731 - val_acc: 0.7352\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3096 - acc: 0.5777\n",
      "Epoch 00030: val_loss did not improve from 0.97306\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.3094 - acc: 0.5780 - val_loss: 1.0065 - val_acc: 0.7249\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2924 - acc: 0.5817\n",
      "Epoch 00031: val_loss improved from 0.97306 to 0.94994, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/031-0.9499.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.2926 - acc: 0.5816 - val_loss: 0.9499 - val_acc: 0.7470\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2869 - acc: 0.5861\n",
      "Epoch 00032: val_loss improved from 0.94994 to 0.93342, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/032-0.9334.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.2869 - acc: 0.5861 - val_loss: 0.9334 - val_acc: 0.7522\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2642 - acc: 0.5945\n",
      "Epoch 00033: val_loss improved from 0.93342 to 0.91284, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/033-0.9128.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.2642 - acc: 0.5945 - val_loss: 0.9128 - val_acc: 0.7512\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2571 - acc: 0.5965\n",
      "Epoch 00034: val_loss did not improve from 0.91284\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 1.2571 - acc: 0.5965 - val_loss: 0.9130 - val_acc: 0.7440\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2387 - acc: 0.5999\n",
      "Epoch 00035: val_loss improved from 0.91284 to 0.87542, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/035-0.8754.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.2388 - acc: 0.5999 - val_loss: 0.8754 - val_acc: 0.7573\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2370 - acc: 0.6044\n",
      "Epoch 00036: val_loss did not improve from 0.87542\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.2370 - acc: 0.6044 - val_loss: 0.9015 - val_acc: 0.7421\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2356 - acc: 0.6087\n",
      "Epoch 00037: val_loss did not improve from 0.87542\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2355 - acc: 0.6087 - val_loss: 0.8935 - val_acc: 0.7547\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2189 - acc: 0.6130\n",
      "Epoch 00038: val_loss improved from 0.87542 to 0.86552, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/038-0.8655.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 1.2189 - acc: 0.6130 - val_loss: 0.8655 - val_acc: 0.7584\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2053 - acc: 0.6173\n",
      "Epoch 00039: val_loss did not improve from 0.86552\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.2054 - acc: 0.6173 - val_loss: 0.8865 - val_acc: 0.7575\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1995 - acc: 0.6179\n",
      "Epoch 00040: val_loss improved from 0.86552 to 0.84448, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/040-0.8445.hdf5\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 1.1995 - acc: 0.6179 - val_loss: 0.8445 - val_acc: 0.7633\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1890 - acc: 0.6188\n",
      "Epoch 00041: val_loss improved from 0.84448 to 0.82863, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/041-0.8286.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.1890 - acc: 0.6188 - val_loss: 0.8286 - val_acc: 0.7696\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1893 - acc: 0.6205\n",
      "Epoch 00042: val_loss did not improve from 0.82863\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.1894 - acc: 0.6204 - val_loss: 0.8625 - val_acc: 0.7678\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1738 - acc: 0.6266\n",
      "Epoch 00043: val_loss improved from 0.82863 to 0.81982, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/043-0.8198.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.1738 - acc: 0.6266 - val_loss: 0.8198 - val_acc: 0.7764\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1686 - acc: 0.6277\n",
      "Epoch 00044: val_loss improved from 0.81982 to 0.81855, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/044-0.8186.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.1686 - acc: 0.6277 - val_loss: 0.8186 - val_acc: 0.7794\n",
      "Epoch 45/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1616 - acc: 0.6318\n",
      "Epoch 00045: val_loss did not improve from 0.81855\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.1616 - acc: 0.6318 - val_loss: 0.8203 - val_acc: 0.7761\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1559 - acc: 0.6337\n",
      "Epoch 00046: val_loss did not improve from 0.81855\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 1.1558 - acc: 0.6336 - val_loss: 0.8203 - val_acc: 0.7720\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1539 - acc: 0.6361\n",
      "Epoch 00047: val_loss improved from 0.81855 to 0.79790, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/047-0.7979.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.1538 - acc: 0.6362 - val_loss: 0.7979 - val_acc: 0.7813\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1529 - acc: 0.6330\n",
      "Epoch 00048: val_loss did not improve from 0.79790\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 1.1528 - acc: 0.6330 - val_loss: 0.8140 - val_acc: 0.7706\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1347 - acc: 0.6389\n",
      "Epoch 00049: val_loss did not improve from 0.79790\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.1350 - acc: 0.6389 - val_loss: 0.8205 - val_acc: 0.7717\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1380 - acc: 0.6418\n",
      "Epoch 00050: val_loss did not improve from 0.79790\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.1381 - acc: 0.6418 - val_loss: 0.8032 - val_acc: 0.7789\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1260 - acc: 0.6405\n",
      "Epoch 00051: val_loss improved from 0.79790 to 0.77548, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/051-0.7755.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.1259 - acc: 0.6406 - val_loss: 0.7755 - val_acc: 0.7764\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1244 - acc: 0.6454\n",
      "Epoch 00052: val_loss did not improve from 0.77548\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 1.1245 - acc: 0.6453 - val_loss: 0.8348 - val_acc: 0.7582\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1077 - acc: 0.6494\n",
      "Epoch 00053: val_loss improved from 0.77548 to 0.77378, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/053-0.7738.hdf5\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 1.1077 - acc: 0.6494 - val_loss: 0.7738 - val_acc: 0.7850\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1106 - acc: 0.6504\n",
      "Epoch 00054: val_loss improved from 0.77378 to 0.77364, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/054-0.7736.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.1105 - acc: 0.6505 - val_loss: 0.7736 - val_acc: 0.7820\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1018 - acc: 0.6492\n",
      "Epoch 00055: val_loss improved from 0.77364 to 0.76892, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/055-0.7689.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.1017 - acc: 0.6492 - val_loss: 0.7689 - val_acc: 0.7869\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1019 - acc: 0.6499\n",
      "Epoch 00056: val_loss did not improve from 0.76892\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.1026 - acc: 0.6496 - val_loss: 0.8278 - val_acc: 0.7647\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0973 - acc: 0.6536\n",
      "Epoch 00057: val_loss improved from 0.76892 to 0.75981, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/057-0.7598.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.0973 - acc: 0.6537 - val_loss: 0.7598 - val_acc: 0.7859\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0935 - acc: 0.6567\n",
      "Epoch 00058: val_loss improved from 0.75981 to 0.73714, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/058-0.7371.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.0934 - acc: 0.6567 - val_loss: 0.7371 - val_acc: 0.7927\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0856 - acc: 0.6573\n",
      "Epoch 00059: val_loss did not improve from 0.73714\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.0857 - acc: 0.6572 - val_loss: 0.7693 - val_acc: 0.7876\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0853 - acc: 0.6566\n",
      "Epoch 00060: val_loss did not improve from 0.73714\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 1.0852 - acc: 0.6567 - val_loss: 0.7910 - val_acc: 0.7701\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0762 - acc: 0.6598\n",
      "Epoch 00061: val_loss did not improve from 0.73714\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.0760 - acc: 0.6599 - val_loss: 0.7407 - val_acc: 0.7964\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0677 - acc: 0.6638\n",
      "Epoch 00062: val_loss did not improve from 0.73714\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.0677 - acc: 0.6638 - val_loss: 0.7408 - val_acc: 0.7955\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0732 - acc: 0.6631\n",
      "Epoch 00063: val_loss improved from 0.73714 to 0.73299, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/063-0.7330.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.0732 - acc: 0.6631 - val_loss: 0.7330 - val_acc: 0.8022\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0625 - acc: 0.6662\n",
      "Epoch 00064: val_loss improved from 0.73299 to 0.71534, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/064-0.7153.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.0625 - acc: 0.6662 - val_loss: 0.7153 - val_acc: 0.7941\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0623 - acc: 0.6648\n",
      "Epoch 00065: val_loss did not improve from 0.71534\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 1.0623 - acc: 0.6648 - val_loss: 0.7399 - val_acc: 0.7894\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0459 - acc: 0.6682\n",
      "Epoch 00066: val_loss improved from 0.71534 to 0.70179, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/066-0.7018.hdf5\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 1.0458 - acc: 0.6681 - val_loss: 0.7018 - val_acc: 0.8001\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0460 - acc: 0.6695\n",
      "Epoch 00067: val_loss did not improve from 0.70179\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 1.0460 - acc: 0.6695 - val_loss: 0.7390 - val_acc: 0.7929\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0525 - acc: 0.6696\n",
      "Epoch 00068: val_loss did not improve from 0.70179\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.0527 - acc: 0.6696 - val_loss: 0.7133 - val_acc: 0.7966\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0387 - acc: 0.6714\n",
      "Epoch 00069: val_loss did not improve from 0.70179\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.0387 - acc: 0.6713 - val_loss: 0.7060 - val_acc: 0.8022\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0431 - acc: 0.6732\n",
      "Epoch 00070: val_loss improved from 0.70179 to 0.69789, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/070-0.6979.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.0432 - acc: 0.6732 - val_loss: 0.6979 - val_acc: 0.8074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0290 - acc: 0.6760\n",
      "Epoch 00071: val_loss did not improve from 0.69789\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 1.0290 - acc: 0.6759 - val_loss: 0.6992 - val_acc: 0.8046\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0308 - acc: 0.6748\n",
      "Epoch 00072: val_loss did not improve from 0.69789\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 1.0308 - acc: 0.6749 - val_loss: 0.7048 - val_acc: 0.8008\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0323 - acc: 0.6780\n",
      "Epoch 00073: val_loss did not improve from 0.69789\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 1.0324 - acc: 0.6781 - val_loss: 0.7122 - val_acc: 0.8013\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0275 - acc: 0.6772\n",
      "Epoch 00074: val_loss improved from 0.69789 to 0.67505, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/074-0.6751.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.0272 - acc: 0.6773 - val_loss: 0.6751 - val_acc: 0.8097\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0131 - acc: 0.6846\n",
      "Epoch 00075: val_loss did not improve from 0.67505\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 1.0130 - acc: 0.6847 - val_loss: 0.6853 - val_acc: 0.8067\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0173 - acc: 0.6792\n",
      "Epoch 00076: val_loss did not improve from 0.67505\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.0173 - acc: 0.6793 - val_loss: 0.6888 - val_acc: 0.8064\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0115 - acc: 0.6822\n",
      "Epoch 00077: val_loss did not improve from 0.67505\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 1.0118 - acc: 0.6822 - val_loss: 0.7116 - val_acc: 0.7950\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0155 - acc: 0.6802\n",
      "Epoch 00078: val_loss did not improve from 0.67505\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.0155 - acc: 0.6803 - val_loss: 0.6841 - val_acc: 0.8022\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.6858\n",
      "Epoch 00079: val_loss did not improve from 0.67505\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.0001 - acc: 0.6857 - val_loss: 0.6751 - val_acc: 0.8074\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0108 - acc: 0.6815\n",
      "Epoch 00080: val_loss did not improve from 0.67505\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 1.0108 - acc: 0.6815 - val_loss: 0.7105 - val_acc: 0.8011\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0018 - acc: 0.6854\n",
      "Epoch 00081: val_loss did not improve from 0.67505\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 1.0018 - acc: 0.6854 - val_loss: 0.6972 - val_acc: 0.7997\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9927 - acc: 0.6873\n",
      "Epoch 00082: val_loss improved from 0.67505 to 0.67005, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/082-0.6700.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.9927 - acc: 0.6872 - val_loss: 0.6700 - val_acc: 0.8125\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9990 - acc: 0.6880\n",
      "Epoch 00083: val_loss did not improve from 0.67005\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.9989 - acc: 0.6881 - val_loss: 0.7216 - val_acc: 0.8013\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9892 - acc: 0.6907\n",
      "Epoch 00084: val_loss improved from 0.67005 to 0.66427, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/084-0.6643.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.9892 - acc: 0.6907 - val_loss: 0.6643 - val_acc: 0.8111\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9896 - acc: 0.6870\n",
      "Epoch 00085: val_loss did not improve from 0.66427\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9895 - acc: 0.6870 - val_loss: 0.7613 - val_acc: 0.7706\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9925 - acc: 0.6885\n",
      "Epoch 00086: val_loss did not improve from 0.66427\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.9925 - acc: 0.6885 - val_loss: 0.6767 - val_acc: 0.8088\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9866 - acc: 0.6894\n",
      "Epoch 00087: val_loss improved from 0.66427 to 0.66253, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/087-0.6625.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9868 - acc: 0.6893 - val_loss: 0.6625 - val_acc: 0.8092\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9825 - acc: 0.6895\n",
      "Epoch 00088: val_loss did not improve from 0.66253\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.9824 - acc: 0.6895 - val_loss: 0.6987 - val_acc: 0.8025\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9862 - acc: 0.6910\n",
      "Epoch 00089: val_loss improved from 0.66253 to 0.65291, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/089-0.6529.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.9861 - acc: 0.6910 - val_loss: 0.6529 - val_acc: 0.8195\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9749 - acc: 0.6940\n",
      "Epoch 00090: val_loss did not improve from 0.65291\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.9749 - acc: 0.6941 - val_loss: 0.6794 - val_acc: 0.8069\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9743 - acc: 0.6928\n",
      "Epoch 00091: val_loss did not improve from 0.65291\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.9742 - acc: 0.6928 - val_loss: 0.6727 - val_acc: 0.8097\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9743 - acc: 0.6921\n",
      "Epoch 00092: val_loss did not improve from 0.65291\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.9742 - acc: 0.6921 - val_loss: 0.6551 - val_acc: 0.8123\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9681 - acc: 0.6963\n",
      "Epoch 00093: val_loss did not improve from 0.65291\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.9681 - acc: 0.6963 - val_loss: 0.6706 - val_acc: 0.8097\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9667 - acc: 0.6973\n",
      "Epoch 00094: val_loss improved from 0.65291 to 0.63542, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/094-0.6354.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.9668 - acc: 0.6972 - val_loss: 0.6354 - val_acc: 0.8190\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9627 - acc: 0.6968\n",
      "Epoch 00095: val_loss did not improve from 0.63542\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.9626 - acc: 0.6969 - val_loss: 0.6455 - val_acc: 0.8127\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9688 - acc: 0.6984\n",
      "Epoch 00096: val_loss did not improve from 0.63542\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.9692 - acc: 0.6984 - val_loss: 0.7034 - val_acc: 0.7945\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9514 - acc: 0.7029\n",
      "Epoch 00097: val_loss did not improve from 0.63542\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.9513 - acc: 0.7030 - val_loss: 0.6509 - val_acc: 0.8132\n",
      "Epoch 98/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9453 - acc: 0.7032\n",
      "Epoch 00098: val_loss improved from 0.63542 to 0.62261, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/098-0.6226.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.9457 - acc: 0.7031 - val_loss: 0.6226 - val_acc: 0.8220\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9581 - acc: 0.6995\n",
      "Epoch 00099: val_loss did not improve from 0.62261\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.9581 - acc: 0.6995 - val_loss: 0.6450 - val_acc: 0.8195\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9499 - acc: 0.7023\n",
      "Epoch 00100: val_loss did not improve from 0.62261\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.9498 - acc: 0.7022 - val_loss: 0.6282 - val_acc: 0.8246\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9446 - acc: 0.7009\n",
      "Epoch 00101: val_loss did not improve from 0.62261\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.9443 - acc: 0.7010 - val_loss: 0.6237 - val_acc: 0.8106\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9472 - acc: 0.7009\n",
      "Epoch 00102: val_loss did not improve from 0.62261\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.9471 - acc: 0.7010 - val_loss: 0.6672 - val_acc: 0.8076\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9390 - acc: 0.7063\n",
      "Epoch 00103: val_loss did not improve from 0.62261\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.9391 - acc: 0.7063 - val_loss: 0.6329 - val_acc: 0.8244\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9370 - acc: 0.7069\n",
      "Epoch 00104: val_loss did not improve from 0.62261\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.9372 - acc: 0.7069 - val_loss: 0.6369 - val_acc: 0.8202\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9441 - acc: 0.7032\n",
      "Epoch 00105: val_loss did not improve from 0.62261\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.9441 - acc: 0.7032 - val_loss: 0.6673 - val_acc: 0.8067\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9473 - acc: 0.7046\n",
      "Epoch 00106: val_loss improved from 0.62261 to 0.61532, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/106-0.6153.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.9475 - acc: 0.7045 - val_loss: 0.6153 - val_acc: 0.8274\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9390 - acc: 0.7054\n",
      "Epoch 00107: val_loss did not improve from 0.61532\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.9390 - acc: 0.7055 - val_loss: 0.6429 - val_acc: 0.8137\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9267 - acc: 0.7085\n",
      "Epoch 00108: val_loss improved from 0.61532 to 0.61518, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/108-0.6152.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.9268 - acc: 0.7085 - val_loss: 0.6152 - val_acc: 0.8246\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9353 - acc: 0.7075\n",
      "Epoch 00109: val_loss did not improve from 0.61518\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.9353 - acc: 0.7075 - val_loss: 0.6388 - val_acc: 0.8234\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9336 - acc: 0.7058\n",
      "Epoch 00110: val_loss did not improve from 0.61518\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.9336 - acc: 0.7058 - val_loss: 0.6322 - val_acc: 0.8190\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9269 - acc: 0.7118\n",
      "Epoch 00111: val_loss did not improve from 0.61518\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.9269 - acc: 0.7119 - val_loss: 0.6198 - val_acc: 0.8272\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9236 - acc: 0.7127\n",
      "Epoch 00112: val_loss did not improve from 0.61518\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.9239 - acc: 0.7126 - val_loss: 0.6164 - val_acc: 0.8255\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9252 - acc: 0.7128\n",
      "Epoch 00113: val_loss improved from 0.61518 to 0.61419, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/113-0.6142.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.9251 - acc: 0.7128 - val_loss: 0.6142 - val_acc: 0.8258\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9160 - acc: 0.7125\n",
      "Epoch 00114: val_loss did not improve from 0.61419\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.9156 - acc: 0.7127 - val_loss: 0.6474 - val_acc: 0.8190\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9211 - acc: 0.7112\n",
      "Epoch 00115: val_loss did not improve from 0.61419\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.9210 - acc: 0.7112 - val_loss: 0.6258 - val_acc: 0.8239\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9111 - acc: 0.7149\n",
      "Epoch 00116: val_loss did not improve from 0.61419\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.9114 - acc: 0.7148 - val_loss: 0.6215 - val_acc: 0.8185\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9178 - acc: 0.7147\n",
      "Epoch 00117: val_loss did not improve from 0.61419\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.9178 - acc: 0.7147 - val_loss: 0.6252 - val_acc: 0.8237\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9244 - acc: 0.7100\n",
      "Epoch 00118: val_loss did not improve from 0.61419\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.9240 - acc: 0.7100 - val_loss: 0.6413 - val_acc: 0.8127\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9143 - acc: 0.7140\n",
      "Epoch 00119: val_loss did not improve from 0.61419\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.9146 - acc: 0.7139 - val_loss: 0.6257 - val_acc: 0.8276\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9128 - acc: 0.7136\n",
      "Epoch 00120: val_loss improved from 0.61419 to 0.60909, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/120-0.6091.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.9125 - acc: 0.7136 - val_loss: 0.6091 - val_acc: 0.8232\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9091 - acc: 0.7163\n",
      "Epoch 00121: val_loss did not improve from 0.60909\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.9090 - acc: 0.7163 - val_loss: 0.6127 - val_acc: 0.8348\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9043 - acc: 0.7160\n",
      "Epoch 00122: val_loss did not improve from 0.60909\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.9047 - acc: 0.7158 - val_loss: 0.6183 - val_acc: 0.8297\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9069 - acc: 0.7153\n",
      "Epoch 00123: val_loss did not improve from 0.60909\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.9070 - acc: 0.7153 - val_loss: 0.6228 - val_acc: 0.8286\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9065 - acc: 0.7177\n",
      "Epoch 00124: val_loss improved from 0.60909 to 0.59521, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/124-0.5952.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.9066 - acc: 0.7177 - val_loss: 0.5952 - val_acc: 0.8381\n",
      "Epoch 125/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8981 - acc: 0.7183\n",
      "Epoch 00125: val_loss did not improve from 0.59521\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.8980 - acc: 0.7183 - val_loss: 0.5991 - val_acc: 0.8304\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9044 - acc: 0.7163\n",
      "Epoch 00126: val_loss did not improve from 0.59521\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.9044 - acc: 0.7163 - val_loss: 0.5974 - val_acc: 0.8316\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9067 - acc: 0.7169\n",
      "Epoch 00127: val_loss did not improve from 0.59521\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.9068 - acc: 0.7168 - val_loss: 0.5965 - val_acc: 0.8304\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8941 - acc: 0.7216\n",
      "Epoch 00128: val_loss improved from 0.59521 to 0.58705, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/128-0.5871.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8940 - acc: 0.7216 - val_loss: 0.5871 - val_acc: 0.8300\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9002 - acc: 0.7183\n",
      "Epoch 00129: val_loss did not improve from 0.58705\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.9002 - acc: 0.7182 - val_loss: 0.6209 - val_acc: 0.8234\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8919 - acc: 0.7201\n",
      "Epoch 00130: val_loss did not improve from 0.58705\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8921 - acc: 0.7200 - val_loss: 0.6031 - val_acc: 0.8260\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8911 - acc: 0.7210\n",
      "Epoch 00131: val_loss did not improve from 0.58705\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8911 - acc: 0.7209 - val_loss: 0.5873 - val_acc: 0.8330\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8912 - acc: 0.7232\n",
      "Epoch 00132: val_loss did not improve from 0.58705\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8912 - acc: 0.7232 - val_loss: 0.5919 - val_acc: 0.8314\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8933 - acc: 0.7217\n",
      "Epoch 00133: val_loss did not improve from 0.58705\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8932 - acc: 0.7217 - val_loss: 0.5942 - val_acc: 0.8307\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8840 - acc: 0.7253\n",
      "Epoch 00134: val_loss did not improve from 0.58705\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8840 - acc: 0.7255 - val_loss: 0.6503 - val_acc: 0.8162\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8863 - acc: 0.7235\n",
      "Epoch 00135: val_loss did not improve from 0.58705\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8870 - acc: 0.7235 - val_loss: 0.5960 - val_acc: 0.8330\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8899 - acc: 0.7202\n",
      "Epoch 00136: val_loss did not improve from 0.58705\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8899 - acc: 0.7202 - val_loss: 0.5932 - val_acc: 0.8314\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8834 - acc: 0.7207\n",
      "Epoch 00137: val_loss improved from 0.58705 to 0.58157, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/137-0.5816.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8836 - acc: 0.7207 - val_loss: 0.5816 - val_acc: 0.8376\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8801 - acc: 0.7250\n",
      "Epoch 00138: val_loss did not improve from 0.58157\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8800 - acc: 0.7250 - val_loss: 0.6248 - val_acc: 0.8213\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8840 - acc: 0.7236\n",
      "Epoch 00139: val_loss did not improve from 0.58157\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8840 - acc: 0.7236 - val_loss: 0.5867 - val_acc: 0.8325\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8790 - acc: 0.7236\n",
      "Epoch 00140: val_loss did not improve from 0.58157\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8790 - acc: 0.7236 - val_loss: 0.6096 - val_acc: 0.8232\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8765 - acc: 0.7236\n",
      "Epoch 00141: val_loss did not improve from 0.58157\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8765 - acc: 0.7236 - val_loss: 0.6005 - val_acc: 0.8288\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8859 - acc: 0.7233\n",
      "Epoch 00142: val_loss did not improve from 0.58157\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.8863 - acc: 0.7232 - val_loss: 0.6110 - val_acc: 0.8195\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8735 - acc: 0.7273\n",
      "Epoch 00143: val_loss did not improve from 0.58157\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8736 - acc: 0.7274 - val_loss: 0.6012 - val_acc: 0.8230\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8791 - acc: 0.7246\n",
      "Epoch 00144: val_loss improved from 0.58157 to 0.57369, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/144-0.5737.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8790 - acc: 0.7246 - val_loss: 0.5737 - val_acc: 0.8332\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8771 - acc: 0.7226\n",
      "Epoch 00145: val_loss did not improve from 0.57369\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8768 - acc: 0.7225 - val_loss: 0.5791 - val_acc: 0.8393\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8774 - acc: 0.7233\n",
      "Epoch 00146: val_loss did not improve from 0.57369\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8774 - acc: 0.7234 - val_loss: 0.5761 - val_acc: 0.8355\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8755 - acc: 0.7244\n",
      "Epoch 00147: val_loss did not improve from 0.57369\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8755 - acc: 0.7244 - val_loss: 0.5821 - val_acc: 0.8339\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8659 - acc: 0.7287\n",
      "Epoch 00148: val_loss improved from 0.57369 to 0.57335, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/148-0.5733.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8664 - acc: 0.7285 - val_loss: 0.5733 - val_acc: 0.8376\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8695 - acc: 0.7268\n",
      "Epoch 00149: val_loss did not improve from 0.57335\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8696 - acc: 0.7268 - val_loss: 0.6490 - val_acc: 0.8132\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8689 - acc: 0.7281\n",
      "Epoch 00150: val_loss did not improve from 0.57335\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8689 - acc: 0.7281 - val_loss: 0.5850 - val_acc: 0.8337\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8635 - acc: 0.7285\n",
      "Epoch 00151: val_loss did not improve from 0.57335\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8635 - acc: 0.7284 - val_loss: 0.5738 - val_acc: 0.8367\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8695 - acc: 0.7274\n",
      "Epoch 00152: val_loss improved from 0.57335 to 0.57073, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/152-0.5707.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8700 - acc: 0.7273 - val_loss: 0.5707 - val_acc: 0.8446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8595 - acc: 0.7303\n",
      "Epoch 00153: val_loss did not improve from 0.57073\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.8598 - acc: 0.7303 - val_loss: 0.5792 - val_acc: 0.8379\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8689 - acc: 0.7271\n",
      "Epoch 00154: val_loss did not improve from 0.57073\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.8689 - acc: 0.7272 - val_loss: 0.5708 - val_acc: 0.8397\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8573 - acc: 0.7323\n",
      "Epoch 00155: val_loss improved from 0.57073 to 0.56967, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/155-0.5697.hdf5\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8580 - acc: 0.7322 - val_loss: 0.5697 - val_acc: 0.8362\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8636 - acc: 0.7288\n",
      "Epoch 00156: val_loss did not improve from 0.56967\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.8636 - acc: 0.7288 - val_loss: 0.5725 - val_acc: 0.8334\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8690 - acc: 0.7272\n",
      "Epoch 00157: val_loss did not improve from 0.56967\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8690 - acc: 0.7272 - val_loss: 0.5908 - val_acc: 0.8323\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8606 - acc: 0.7337\n",
      "Epoch 00158: val_loss improved from 0.56967 to 0.56083, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/158-0.5608.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8608 - acc: 0.7336 - val_loss: 0.5608 - val_acc: 0.8423\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8567 - acc: 0.7310\n",
      "Epoch 00159: val_loss did not improve from 0.56083\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8570 - acc: 0.7310 - val_loss: 0.5812 - val_acc: 0.8297\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8582 - acc: 0.7308\n",
      "Epoch 00160: val_loss did not improve from 0.56083\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8579 - acc: 0.7309 - val_loss: 0.5708 - val_acc: 0.8458\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8660 - acc: 0.7301\n",
      "Epoch 00161: val_loss improved from 0.56083 to 0.55188, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/161-0.5519.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8660 - acc: 0.7302 - val_loss: 0.5519 - val_acc: 0.8407\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8560 - acc: 0.7314\n",
      "Epoch 00162: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8561 - acc: 0.7313 - val_loss: 0.6456 - val_acc: 0.8106\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8524 - acc: 0.7337\n",
      "Epoch 00163: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8527 - acc: 0.7337 - val_loss: 0.5647 - val_acc: 0.8381\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8545 - acc: 0.7340\n",
      "Epoch 00164: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8546 - acc: 0.7340 - val_loss: 0.5726 - val_acc: 0.8379\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8455 - acc: 0.7345\n",
      "Epoch 00165: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8454 - acc: 0.7345 - val_loss: 0.5827 - val_acc: 0.8328\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8574 - acc: 0.7307\n",
      "Epoch 00166: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8575 - acc: 0.7306 - val_loss: 0.5909 - val_acc: 0.8321\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8480 - acc: 0.7368\n",
      "Epoch 00167: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8481 - acc: 0.7367 - val_loss: 0.5634 - val_acc: 0.8411\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8450 - acc: 0.7350\n",
      "Epoch 00168: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8450 - acc: 0.7351 - val_loss: 0.5606 - val_acc: 0.8446\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8486 - acc: 0.7330\n",
      "Epoch 00169: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8486 - acc: 0.7330 - val_loss: 0.5911 - val_acc: 0.8302\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8472 - acc: 0.7336\n",
      "Epoch 00170: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.8472 - acc: 0.7336 - val_loss: 0.5542 - val_acc: 0.8388\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8411 - acc: 0.7370\n",
      "Epoch 00171: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.8410 - acc: 0.7370 - val_loss: 0.5594 - val_acc: 0.8379\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8413 - acc: 0.7376\n",
      "Epoch 00172: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8414 - acc: 0.7375 - val_loss: 0.5738 - val_acc: 0.8328\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8398 - acc: 0.7375\n",
      "Epoch 00173: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8401 - acc: 0.7375 - val_loss: 0.5769 - val_acc: 0.8302\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8430 - acc: 0.7357\n",
      "Epoch 00174: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.8433 - acc: 0.7356 - val_loss: 0.5573 - val_acc: 0.8421\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8413 - acc: 0.7369\n",
      "Epoch 00175: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8414 - acc: 0.7369 - val_loss: 0.5733 - val_acc: 0.8437\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8422 - acc: 0.7373\n",
      "Epoch 00176: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.8423 - acc: 0.7373 - val_loss: 0.5582 - val_acc: 0.8458\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8402 - acc: 0.7371\n",
      "Epoch 00177: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8402 - acc: 0.7371 - val_loss: 0.5619 - val_acc: 0.8407\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8376 - acc: 0.7397\n",
      "Epoch 00178: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8377 - acc: 0.7396 - val_loss: 0.6288 - val_acc: 0.8265\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8439 - acc: 0.7324\n",
      "Epoch 00179: val_loss did not improve from 0.55188\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8439 - acc: 0.7323 - val_loss: 0.5745 - val_acc: 0.8411\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8349 - acc: 0.7380\n",
      "Epoch 00180: val_loss improved from 0.55188 to 0.54117, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/180-0.5412.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8349 - acc: 0.7380 - val_loss: 0.5412 - val_acc: 0.8474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8277 - acc: 0.7377\n",
      "Epoch 00181: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8278 - acc: 0.7376 - val_loss: 0.5690 - val_acc: 0.8372\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8298 - acc: 0.7402\n",
      "Epoch 00182: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8298 - acc: 0.7402 - val_loss: 0.5578 - val_acc: 0.8418\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8284 - acc: 0.7405\n",
      "Epoch 00183: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8284 - acc: 0.7405 - val_loss: 0.5522 - val_acc: 0.8425\n",
      "Epoch 184/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8281 - acc: 0.7412\n",
      "Epoch 00184: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8286 - acc: 0.7410 - val_loss: 0.5649 - val_acc: 0.8428\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8259 - acc: 0.7405\n",
      "Epoch 00185: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8261 - acc: 0.7405 - val_loss: 0.5538 - val_acc: 0.8404\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8242 - acc: 0.7407\n",
      "Epoch 00186: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8241 - acc: 0.7407 - val_loss: 0.5612 - val_acc: 0.8369\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8322 - acc: 0.7384\n",
      "Epoch 00187: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8321 - acc: 0.7384 - val_loss: 0.5787 - val_acc: 0.8386\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8212 - acc: 0.7419\n",
      "Epoch 00188: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8212 - acc: 0.7419 - val_loss: 0.5755 - val_acc: 0.8386\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8267 - acc: 0.7417\n",
      "Epoch 00189: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8267 - acc: 0.7416 - val_loss: 0.5531 - val_acc: 0.8463\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8256 - acc: 0.7411\n",
      "Epoch 00190: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8256 - acc: 0.7411 - val_loss: 0.5560 - val_acc: 0.8449\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8228 - acc: 0.7407\n",
      "Epoch 00191: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.8229 - acc: 0.7407 - val_loss: 0.5901 - val_acc: 0.8295\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8194 - acc: 0.7422\n",
      "Epoch 00192: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8194 - acc: 0.7422 - val_loss: 0.5506 - val_acc: 0.8437\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8210 - acc: 0.7443\n",
      "Epoch 00193: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.8210 - acc: 0.7442 - val_loss: 0.5674 - val_acc: 0.8449\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8191 - acc: 0.7438\n",
      "Epoch 00194: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.8192 - acc: 0.7438 - val_loss: 0.5579 - val_acc: 0.8374\n",
      "Epoch 195/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8191 - acc: 0.7461\n",
      "Epoch 00195: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8187 - acc: 0.7462 - val_loss: 0.5472 - val_acc: 0.8458\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8229 - acc: 0.7432\n",
      "Epoch 00196: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.8228 - acc: 0.7432 - val_loss: 0.5558 - val_acc: 0.8428\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8205 - acc: 0.7449\n",
      "Epoch 00197: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8205 - acc: 0.7450 - val_loss: 0.5525 - val_acc: 0.8416\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8227 - acc: 0.7405\n",
      "Epoch 00198: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8232 - acc: 0.7404 - val_loss: 0.5548 - val_acc: 0.8446\n",
      "Epoch 199/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8141 - acc: 0.7470\n",
      "Epoch 00199: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8144 - acc: 0.7470 - val_loss: 0.5477 - val_acc: 0.8456\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8174 - acc: 0.7442\n",
      "Epoch 00200: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8174 - acc: 0.7442 - val_loss: 0.5553 - val_acc: 0.8383\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8135 - acc: 0.7429\n",
      "Epoch 00201: val_loss did not improve from 0.54117\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8136 - acc: 0.7428 - val_loss: 0.5677 - val_acc: 0.8386\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8158 - acc: 0.7451\n",
      "Epoch 00202: val_loss improved from 0.54117 to 0.52786, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/202-0.5279.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8158 - acc: 0.7451 - val_loss: 0.5279 - val_acc: 0.8519\n",
      "Epoch 203/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8113 - acc: 0.7442\n",
      "Epoch 00203: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8110 - acc: 0.7442 - val_loss: 0.5395 - val_acc: 0.8526\n",
      "Epoch 204/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8156 - acc: 0.7455\n",
      "Epoch 00204: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8155 - acc: 0.7456 - val_loss: 0.5673 - val_acc: 0.8446\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8135 - acc: 0.7464\n",
      "Epoch 00205: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8134 - acc: 0.7464 - val_loss: 0.5441 - val_acc: 0.8472\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8118 - acc: 0.7469\n",
      "Epoch 00206: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8118 - acc: 0.7468 - val_loss: 0.5363 - val_acc: 0.8505\n",
      "Epoch 207/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8085 - acc: 0.7468\n",
      "Epoch 00207: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8086 - acc: 0.7468 - val_loss: 0.5463 - val_acc: 0.8491\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8110 - acc: 0.7454\n",
      "Epoch 00208: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8111 - acc: 0.7454 - val_loss: 0.5427 - val_acc: 0.8402\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8040 - acc: 0.7462\n",
      "Epoch 00209: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8040 - acc: 0.7462 - val_loss: 0.5589 - val_acc: 0.8446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8110 - acc: 0.7462\n",
      "Epoch 00210: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8109 - acc: 0.7463 - val_loss: 0.5373 - val_acc: 0.8477\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8087 - acc: 0.7438\n",
      "Epoch 00211: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8094 - acc: 0.7438 - val_loss: 0.5846 - val_acc: 0.8325\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7999 - acc: 0.7478\n",
      "Epoch 00212: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7999 - acc: 0.7478 - val_loss: 0.5426 - val_acc: 0.8439\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8086 - acc: 0.7485\n",
      "Epoch 00213: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.8087 - acc: 0.7484 - val_loss: 0.5630 - val_acc: 0.8428\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8069 - acc: 0.7455\n",
      "Epoch 00214: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8069 - acc: 0.7456 - val_loss: 0.5537 - val_acc: 0.8425\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8065 - acc: 0.7469\n",
      "Epoch 00215: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8066 - acc: 0.7469 - val_loss: 0.5751 - val_acc: 0.8358\n",
      "Epoch 216/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7953 - acc: 0.7514\n",
      "Epoch 00216: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7955 - acc: 0.7514 - val_loss: 0.5594 - val_acc: 0.8456\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8019 - acc: 0.7486\n",
      "Epoch 00217: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8018 - acc: 0.7486 - val_loss: 0.5373 - val_acc: 0.8451\n",
      "Epoch 218/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7912 - acc: 0.7512\n",
      "Epoch 00218: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7915 - acc: 0.7513 - val_loss: 0.5466 - val_acc: 0.8488\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8025 - acc: 0.7483\n",
      "Epoch 00219: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8026 - acc: 0.7483 - val_loss: 0.5489 - val_acc: 0.8437\n",
      "Epoch 220/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8089 - acc: 0.7464\n",
      "Epoch 00220: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.8088 - acc: 0.7465 - val_loss: 0.5652 - val_acc: 0.8437\n",
      "Epoch 221/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7958 - acc: 0.7496\n",
      "Epoch 00221: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7956 - acc: 0.7497 - val_loss: 0.5312 - val_acc: 0.8509\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8013 - acc: 0.7486\n",
      "Epoch 00222: val_loss did not improve from 0.52786\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8014 - acc: 0.7486 - val_loss: 0.5429 - val_acc: 0.8411\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8086 - acc: 0.7442\n",
      "Epoch 00223: val_loss improved from 0.52786 to 0.52728, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/223-0.5273.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.8086 - acc: 0.7442 - val_loss: 0.5273 - val_acc: 0.8479\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7904 - acc: 0.7512\n",
      "Epoch 00224: val_loss did not improve from 0.52728\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7904 - acc: 0.7512 - val_loss: 0.5634 - val_acc: 0.8484\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7974 - acc: 0.7478\n",
      "Epoch 00225: val_loss improved from 0.52728 to 0.52660, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/225-0.5266.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7974 - acc: 0.7478 - val_loss: 0.5266 - val_acc: 0.8465\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7908 - acc: 0.7487\n",
      "Epoch 00226: val_loss did not improve from 0.52660\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.7908 - acc: 0.7487 - val_loss: 0.5398 - val_acc: 0.8514\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7928 - acc: 0.7507\n",
      "Epoch 00227: val_loss did not improve from 0.52660\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.7927 - acc: 0.7507 - val_loss: 0.5434 - val_acc: 0.8474\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7966 - acc: 0.7486\n",
      "Epoch 00228: val_loss did not improve from 0.52660\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.7966 - acc: 0.7486 - val_loss: 0.5291 - val_acc: 0.8514\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7968 - acc: 0.7501\n",
      "Epoch 00229: val_loss did not improve from 0.52660\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.7968 - acc: 0.7501 - val_loss: 0.5339 - val_acc: 0.8446\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7894 - acc: 0.7517\n",
      "Epoch 00230: val_loss did not improve from 0.52660\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7893 - acc: 0.7518 - val_loss: 0.5530 - val_acc: 0.8493\n",
      "Epoch 231/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7918 - acc: 0.7507\n",
      "Epoch 00231: val_loss did not improve from 0.52660\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7924 - acc: 0.7506 - val_loss: 0.5421 - val_acc: 0.8463\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7890 - acc: 0.7504\n",
      "Epoch 00232: val_loss did not improve from 0.52660\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7890 - acc: 0.7504 - val_loss: 0.5354 - val_acc: 0.8486\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7858 - acc: 0.7549\n",
      "Epoch 00233: val_loss improved from 0.52660 to 0.52420, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/233-0.5242.hdf5\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7858 - acc: 0.7548 - val_loss: 0.5242 - val_acc: 0.8516\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7943 - acc: 0.7498\n",
      "Epoch 00234: val_loss did not improve from 0.52420\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7949 - acc: 0.7498 - val_loss: 0.5581 - val_acc: 0.8376\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7941 - acc: 0.7491\n",
      "Epoch 00235: val_loss improved from 0.52420 to 0.51835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/235-0.5184.hdf5\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7941 - acc: 0.7490 - val_loss: 0.5184 - val_acc: 0.8546\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7820 - acc: 0.7538\n",
      "Epoch 00236: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7819 - acc: 0.7538 - val_loss: 0.5261 - val_acc: 0.8519\n",
      "Epoch 237/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7866 - acc: 0.7527\n",
      "Epoch 00237: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7865 - acc: 0.7527 - val_loss: 0.5334 - val_acc: 0.8477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7804 - acc: 0.7525\n",
      "Epoch 00238: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7806 - acc: 0.7525 - val_loss: 0.6365 - val_acc: 0.8190\n",
      "Epoch 239/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7842 - acc: 0.7541\n",
      "Epoch 00239: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7839 - acc: 0.7542 - val_loss: 0.5463 - val_acc: 0.8404\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7862 - acc: 0.7523\n",
      "Epoch 00240: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7866 - acc: 0.7522 - val_loss: 0.5420 - val_acc: 0.8479\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7895 - acc: 0.7516\n",
      "Epoch 00241: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7895 - acc: 0.7516 - val_loss: 0.5219 - val_acc: 0.8505\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7924 - acc: 0.7521\n",
      "Epoch 00242: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7925 - acc: 0.7521 - val_loss: 0.5449 - val_acc: 0.8491\n",
      "Epoch 243/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7860 - acc: 0.7528\n",
      "Epoch 00243: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7861 - acc: 0.7528 - val_loss: 0.5446 - val_acc: 0.8397\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7838 - acc: 0.7545\n",
      "Epoch 00244: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.7838 - acc: 0.7545 - val_loss: 0.5219 - val_acc: 0.8542\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7848 - acc: 0.7520\n",
      "Epoch 00245: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.7849 - acc: 0.7520 - val_loss: 0.5361 - val_acc: 0.8546\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7865 - acc: 0.7504\n",
      "Epoch 00246: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.7867 - acc: 0.7504 - val_loss: 0.5367 - val_acc: 0.8502\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7903 - acc: 0.7518\n",
      "Epoch 00247: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7902 - acc: 0.7518 - val_loss: 0.5570 - val_acc: 0.8479\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7846 - acc: 0.7543\n",
      "Epoch 00248: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.7848 - acc: 0.7543 - val_loss: 0.5323 - val_acc: 0.8486\n",
      "Epoch 249/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7780 - acc: 0.7547\n",
      "Epoch 00249: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7775 - acc: 0.7549 - val_loss: 0.5298 - val_acc: 0.8505\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7762 - acc: 0.7547\n",
      "Epoch 00250: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7762 - acc: 0.7547 - val_loss: 0.5521 - val_acc: 0.8395\n",
      "Epoch 251/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7913 - acc: 0.7533\n",
      "Epoch 00251: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7912 - acc: 0.7533 - val_loss: 0.5416 - val_acc: 0.8439\n",
      "Epoch 252/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7770 - acc: 0.7556\n",
      "Epoch 00252: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7768 - acc: 0.7556 - val_loss: 0.5842 - val_acc: 0.8337\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7758 - acc: 0.7565\n",
      "Epoch 00253: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7758 - acc: 0.7565 - val_loss: 0.5360 - val_acc: 0.8542\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7742 - acc: 0.7560\n",
      "Epoch 00254: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7742 - acc: 0.7559 - val_loss: 0.5241 - val_acc: 0.8505\n",
      "Epoch 255/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7769 - acc: 0.7556\n",
      "Epoch 00255: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7773 - acc: 0.7554 - val_loss: 0.5221 - val_acc: 0.8502\n",
      "Epoch 256/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7758 - acc: 0.7552\n",
      "Epoch 00256: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.7757 - acc: 0.7553 - val_loss: 0.5488 - val_acc: 0.8414\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7798 - acc: 0.7549\n",
      "Epoch 00257: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7797 - acc: 0.7549 - val_loss: 0.5366 - val_acc: 0.8442\n",
      "Epoch 258/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7748 - acc: 0.7533\n",
      "Epoch 00258: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7749 - acc: 0.7533 - val_loss: 0.5288 - val_acc: 0.8491\n",
      "Epoch 259/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7799 - acc: 0.7527\n",
      "Epoch 00259: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7799 - acc: 0.7528 - val_loss: 0.5570 - val_acc: 0.8390\n",
      "Epoch 260/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7783 - acc: 0.7556\n",
      "Epoch 00260: val_loss did not improve from 0.51835\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7786 - acc: 0.7554 - val_loss: 0.5289 - val_acc: 0.8500\n",
      "Epoch 261/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7759 - acc: 0.7567\n",
      "Epoch 00261: val_loss improved from 0.51835 to 0.51655, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/261-0.5166.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7759 - acc: 0.7567 - val_loss: 0.5166 - val_acc: 0.8532\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7760 - acc: 0.7556\n",
      "Epoch 00262: val_loss did not improve from 0.51655\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.7760 - acc: 0.7556 - val_loss: 0.5526 - val_acc: 0.8374\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7739 - acc: 0.7555\n",
      "Epoch 00263: val_loss did not improve from 0.51655\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.7739 - acc: 0.7555 - val_loss: 0.5317 - val_acc: 0.8472\n",
      "Epoch 264/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7744 - acc: 0.7566\n",
      "Epoch 00264: val_loss improved from 0.51655 to 0.51646, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/264-0.5165.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7743 - acc: 0.7567 - val_loss: 0.5165 - val_acc: 0.8551\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7732 - acc: 0.7561\n",
      "Epoch 00265: val_loss did not improve from 0.51646\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.7734 - acc: 0.7561 - val_loss: 0.5314 - val_acc: 0.8418\n",
      "Epoch 266/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7701 - acc: 0.7552\n",
      "Epoch 00266: val_loss did not improve from 0.51646\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7699 - acc: 0.7553 - val_loss: 0.5316 - val_acc: 0.8477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7710 - acc: 0.7569\n",
      "Epoch 00267: val_loss did not improve from 0.51646\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.7709 - acc: 0.7569 - val_loss: 0.5431 - val_acc: 0.8484\n",
      "Epoch 268/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7604 - acc: 0.7616\n",
      "Epoch 00268: val_loss did not improve from 0.51646\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7601 - acc: 0.7616 - val_loss: 0.5218 - val_acc: 0.8498\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7704 - acc: 0.7570\n",
      "Epoch 00269: val_loss did not improve from 0.51646\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7704 - acc: 0.7570 - val_loss: 0.5403 - val_acc: 0.8430\n",
      "Epoch 270/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7737 - acc: 0.7562\n",
      "Epoch 00270: val_loss did not improve from 0.51646\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7741 - acc: 0.7559 - val_loss: 0.5510 - val_acc: 0.8435\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7677 - acc: 0.7562\n",
      "Epoch 00271: val_loss did not improve from 0.51646\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7676 - acc: 0.7562 - val_loss: 0.5294 - val_acc: 0.8512\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7672 - acc: 0.7564\n",
      "Epoch 00272: val_loss did not improve from 0.51646\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7672 - acc: 0.7564 - val_loss: 0.5230 - val_acc: 0.8523\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7676 - acc: 0.7589\n",
      "Epoch 00273: val_loss did not improve from 0.51646\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7677 - acc: 0.7588 - val_loss: 0.5176 - val_acc: 0.8544\n",
      "Epoch 274/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7621 - acc: 0.7596\n",
      "Epoch 00274: val_loss improved from 0.51646 to 0.51398, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/274-0.5140.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7623 - acc: 0.7595 - val_loss: 0.5140 - val_acc: 0.8567\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7599 - acc: 0.7615\n",
      "Epoch 00275: val_loss did not improve from 0.51398\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.7600 - acc: 0.7614 - val_loss: 0.6065 - val_acc: 0.8185\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7647 - acc: 0.7585\n",
      "Epoch 00276: val_loss improved from 0.51398 to 0.51244, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/276-0.5124.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.7647 - acc: 0.7585 - val_loss: 0.5124 - val_acc: 0.8581\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7615 - acc: 0.7595\n",
      "Epoch 00277: val_loss improved from 0.51244 to 0.51120, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/277-0.5112.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.7614 - acc: 0.7596 - val_loss: 0.5112 - val_acc: 0.8535\n",
      "Epoch 278/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7603 - acc: 0.7608\n",
      "Epoch 00278: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.7603 - acc: 0.7607 - val_loss: 0.5159 - val_acc: 0.8519\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7660 - acc: 0.7597\n",
      "Epoch 00279: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.7660 - acc: 0.7597 - val_loss: 0.5512 - val_acc: 0.8418\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7673 - acc: 0.7567\n",
      "Epoch 00280: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.7672 - acc: 0.7567 - val_loss: 0.5178 - val_acc: 0.8526\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7573 - acc: 0.7615\n",
      "Epoch 00281: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.7573 - acc: 0.7615 - val_loss: 0.5657 - val_acc: 0.8374\n",
      "Epoch 282/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7580 - acc: 0.7619\n",
      "Epoch 00282: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.7582 - acc: 0.7617 - val_loss: 0.5612 - val_acc: 0.8397\n",
      "Epoch 283/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7652 - acc: 0.7586\n",
      "Epoch 00283: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.7657 - acc: 0.7584 - val_loss: 0.5320 - val_acc: 0.8539\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7663 - acc: 0.7577\n",
      "Epoch 00284: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.7663 - acc: 0.7577 - val_loss: 0.5606 - val_acc: 0.8428\n",
      "Epoch 285/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7647 - acc: 0.7599\n",
      "Epoch 00285: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7650 - acc: 0.7598 - val_loss: 0.5464 - val_acc: 0.8498\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7672 - acc: 0.7586\n",
      "Epoch 00286: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.7672 - acc: 0.7586 - val_loss: 0.5392 - val_acc: 0.8437\n",
      "Epoch 287/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7619 - acc: 0.7612\n",
      "Epoch 00287: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.7619 - acc: 0.7612 - val_loss: 0.5244 - val_acc: 0.8491\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7548 - acc: 0.7599\n",
      "Epoch 00288: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.7550 - acc: 0.7599 - val_loss: 0.5152 - val_acc: 0.8560\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7551 - acc: 0.7586\n",
      "Epoch 00289: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.7551 - acc: 0.7586 - val_loss: 0.5236 - val_acc: 0.8519\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7568 - acc: 0.7600\n",
      "Epoch 00290: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.7570 - acc: 0.7600 - val_loss: 0.5158 - val_acc: 0.8572\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7590 - acc: 0.7600\n",
      "Epoch 00291: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7590 - acc: 0.7600 - val_loss: 0.5614 - val_acc: 0.8430\n",
      "Epoch 292/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7563 - acc: 0.7613\n",
      "Epoch 00292: val_loss did not improve from 0.51120\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.7564 - acc: 0.7613 - val_loss: 0.5339 - val_acc: 0.8495\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7582 - acc: 0.7609\n",
      "Epoch 00293: val_loss improved from 0.51120 to 0.50975, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/293-0.5097.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.7582 - acc: 0.7608 - val_loss: 0.5097 - val_acc: 0.8581\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7621 - acc: 0.7625\n",
      "Epoch 00294: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.7622 - acc: 0.7625 - val_loss: 0.5309 - val_acc: 0.8472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7589 - acc: 0.7572\n",
      "Epoch 00295: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.7590 - acc: 0.7572 - val_loss: 0.5372 - val_acc: 0.8484\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7645 - acc: 0.7599\n",
      "Epoch 00296: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.7646 - acc: 0.7599 - val_loss: 0.5213 - val_acc: 0.8477\n",
      "Epoch 297/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7545 - acc: 0.7597\n",
      "Epoch 00297: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7545 - acc: 0.7597 - val_loss: 0.5185 - val_acc: 0.8488\n",
      "Epoch 298/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7614 - acc: 0.7604\n",
      "Epoch 00298: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.7616 - acc: 0.7604 - val_loss: 0.5317 - val_acc: 0.8486\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7528 - acc: 0.7609\n",
      "Epoch 00299: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7530 - acc: 0.7608 - val_loss: 0.5325 - val_acc: 0.8470\n",
      "Epoch 300/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7564 - acc: 0.7597\n",
      "Epoch 00300: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7563 - acc: 0.7598 - val_loss: 0.5194 - val_acc: 0.8493\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7559 - acc: 0.7594\n",
      "Epoch 00301: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.7558 - acc: 0.7594 - val_loss: 0.5364 - val_acc: 0.8542\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7510 - acc: 0.7621\n",
      "Epoch 00302: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7509 - acc: 0.7621 - val_loss: 0.5230 - val_acc: 0.8526\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7584 - acc: 0.7630\n",
      "Epoch 00303: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7583 - acc: 0.7630 - val_loss: 0.5118 - val_acc: 0.8598\n",
      "Epoch 304/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7502 - acc: 0.7617\n",
      "Epoch 00304: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7507 - acc: 0.7614 - val_loss: 0.5602 - val_acc: 0.8383\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7465 - acc: 0.7647\n",
      "Epoch 00305: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7464 - acc: 0.7647 - val_loss: 0.5182 - val_acc: 0.8551\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7549 - acc: 0.7614\n",
      "Epoch 00306: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7549 - acc: 0.7614 - val_loss: 0.5285 - val_acc: 0.8516\n",
      "Epoch 307/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7522 - acc: 0.7628\n",
      "Epoch 00307: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7523 - acc: 0.7627 - val_loss: 0.5248 - val_acc: 0.8474\n",
      "Epoch 308/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7544 - acc: 0.7626\n",
      "Epoch 00308: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7546 - acc: 0.7625 - val_loss: 0.5210 - val_acc: 0.8474\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7516 - acc: 0.7633\n",
      "Epoch 00309: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7515 - acc: 0.7633 - val_loss: 0.5893 - val_acc: 0.8293\n",
      "Epoch 310/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7484 - acc: 0.7611\n",
      "Epoch 00310: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7486 - acc: 0.7611 - val_loss: 0.5353 - val_acc: 0.8437\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7444 - acc: 0.7639\n",
      "Epoch 00311: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7443 - acc: 0.7639 - val_loss: 0.5240 - val_acc: 0.8498\n",
      "Epoch 312/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7488 - acc: 0.7645\n",
      "Epoch 00312: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7486 - acc: 0.7645 - val_loss: 0.5124 - val_acc: 0.8535\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7462 - acc: 0.7633\n",
      "Epoch 00313: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7461 - acc: 0.7633 - val_loss: 0.5197 - val_acc: 0.8519\n",
      "Epoch 314/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7460 - acc: 0.7647\n",
      "Epoch 00314: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7463 - acc: 0.7646 - val_loss: 0.5405 - val_acc: 0.8507\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7494 - acc: 0.7611\n",
      "Epoch 00315: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7493 - acc: 0.7611 - val_loss: 0.5291 - val_acc: 0.8488\n",
      "Epoch 316/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7496 - acc: 0.7652\n",
      "Epoch 00316: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7495 - acc: 0.7653 - val_loss: 0.5350 - val_acc: 0.8542\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7477 - acc: 0.7645\n",
      "Epoch 00317: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.7477 - acc: 0.7646 - val_loss: 0.5226 - val_acc: 0.8572\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7440 - acc: 0.7647\n",
      "Epoch 00318: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7439 - acc: 0.7647 - val_loss: 0.5135 - val_acc: 0.8539\n",
      "Epoch 319/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7399 - acc: 0.7667\n",
      "Epoch 00319: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7400 - acc: 0.7666 - val_loss: 0.5144 - val_acc: 0.8509\n",
      "Epoch 320/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7497 - acc: 0.7651\n",
      "Epoch 00320: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7495 - acc: 0.7651 - val_loss: 0.5444 - val_acc: 0.8388\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7507 - acc: 0.7626\n",
      "Epoch 00321: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7507 - acc: 0.7626 - val_loss: 0.5157 - val_acc: 0.8530\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7427 - acc: 0.7644\n",
      "Epoch 00322: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7426 - acc: 0.7644 - val_loss: 0.5285 - val_acc: 0.8446\n",
      "Epoch 323/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7408 - acc: 0.7672\n",
      "Epoch 00323: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7411 - acc: 0.7672 - val_loss: 0.5380 - val_acc: 0.8465\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7468 - acc: 0.7665\n",
      "Epoch 00324: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7468 - acc: 0.7665 - val_loss: 0.5173 - val_acc: 0.8605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7415 - acc: 0.7651\n",
      "Epoch 00325: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7415 - acc: 0.7651 - val_loss: 0.5234 - val_acc: 0.8502\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7389 - acc: 0.7635\n",
      "Epoch 00326: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7390 - acc: 0.7635 - val_loss: 0.5155 - val_acc: 0.8516\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7462 - acc: 0.7645\n",
      "Epoch 00327: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7462 - acc: 0.7645 - val_loss: 0.5265 - val_acc: 0.8458\n",
      "Epoch 328/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7486 - acc: 0.7650\n",
      "Epoch 00328: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7489 - acc: 0.7648 - val_loss: 0.5195 - val_acc: 0.8521\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7364 - acc: 0.7654\n",
      "Epoch 00329: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7365 - acc: 0.7654 - val_loss: 0.5230 - val_acc: 0.8505\n",
      "Epoch 330/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7390 - acc: 0.7654\n",
      "Epoch 00330: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7392 - acc: 0.7653 - val_loss: 0.5124 - val_acc: 0.8528\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7407 - acc: 0.7645\n",
      "Epoch 00331: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7407 - acc: 0.7646 - val_loss: 0.5323 - val_acc: 0.8537\n",
      "Epoch 332/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7436 - acc: 0.7644\n",
      "Epoch 00332: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7435 - acc: 0.7644 - val_loss: 0.5305 - val_acc: 0.8509\n",
      "Epoch 333/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7398 - acc: 0.7675\n",
      "Epoch 00333: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7401 - acc: 0.7674 - val_loss: 0.5314 - val_acc: 0.8460\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7373 - acc: 0.7696\n",
      "Epoch 00334: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7373 - acc: 0.7697 - val_loss: 0.5248 - val_acc: 0.8530\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7391 - acc: 0.7633\n",
      "Epoch 00335: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7391 - acc: 0.7633 - val_loss: 0.5430 - val_acc: 0.8491\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7346 - acc: 0.7680\n",
      "Epoch 00336: val_loss did not improve from 0.50975\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7346 - acc: 0.7680 - val_loss: 0.5399 - val_acc: 0.8465\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7442 - acc: 0.7639\n",
      "Epoch 00337: val_loss improved from 0.50975 to 0.50429, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv_checkpoint/337-0.5043.hdf5\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7441 - acc: 0.7639 - val_loss: 0.5043 - val_acc: 0.8539\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7360 - acc: 0.7692\n",
      "Epoch 00338: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7359 - acc: 0.7691 - val_loss: 0.5142 - val_acc: 0.8526\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7426 - acc: 0.7656\n",
      "Epoch 00339: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7426 - acc: 0.7656 - val_loss: 0.5229 - val_acc: 0.8526\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7382 - acc: 0.7673\n",
      "Epoch 00340: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7381 - acc: 0.7673 - val_loss: 0.5227 - val_acc: 0.8526\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7349 - acc: 0.7682\n",
      "Epoch 00341: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7351 - acc: 0.7682 - val_loss: 0.5272 - val_acc: 0.8579\n",
      "Epoch 342/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.7675\n",
      "Epoch 00342: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7357 - acc: 0.7675 - val_loss: 0.5146 - val_acc: 0.8519\n",
      "Epoch 343/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7404 - acc: 0.7661\n",
      "Epoch 00343: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7406 - acc: 0.7660 - val_loss: 0.5374 - val_acc: 0.8418\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7357 - acc: 0.7673\n",
      "Epoch 00344: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7356 - acc: 0.7673 - val_loss: 0.5101 - val_acc: 0.8537\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7405 - acc: 0.7665\n",
      "Epoch 00345: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7405 - acc: 0.7665 - val_loss: 0.5438 - val_acc: 0.8428\n",
      "Epoch 346/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7398 - acc: 0.7662\n",
      "Epoch 00346: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7393 - acc: 0.7662 - val_loss: 0.5538 - val_acc: 0.8348\n",
      "Epoch 347/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7367 - acc: 0.7678\n",
      "Epoch 00347: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.7369 - acc: 0.7678 - val_loss: 0.5107 - val_acc: 0.8563\n",
      "Epoch 348/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7423 - acc: 0.7647\n",
      "Epoch 00348: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7426 - acc: 0.7646 - val_loss: 0.5277 - val_acc: 0.8484\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7375 - acc: 0.7653\n",
      "Epoch 00349: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.7375 - acc: 0.7653 - val_loss: 0.5327 - val_acc: 0.8526\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7346 - acc: 0.7667\n",
      "Epoch 00350: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7347 - acc: 0.7667 - val_loss: 0.5320 - val_acc: 0.8495\n",
      "Epoch 351/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7388 - acc: 0.7654\n",
      "Epoch 00351: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7392 - acc: 0.7654 - val_loss: 0.5283 - val_acc: 0.8526\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7354 - acc: 0.7670\n",
      "Epoch 00352: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7354 - acc: 0.7670 - val_loss: 0.5374 - val_acc: 0.8526\n",
      "Epoch 353/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7304 - acc: 0.7658\n",
      "Epoch 00353: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7302 - acc: 0.7658 - val_loss: 0.5253 - val_acc: 0.8449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7354 - acc: 0.7677\n",
      "Epoch 00354: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7354 - acc: 0.7677 - val_loss: 0.5388 - val_acc: 0.8437\n",
      "Epoch 355/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7322 - acc: 0.7673\n",
      "Epoch 00355: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7322 - acc: 0.7673 - val_loss: 0.5129 - val_acc: 0.8563\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7318 - acc: 0.7696\n",
      "Epoch 00356: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.7318 - acc: 0.7696 - val_loss: 0.5166 - val_acc: 0.8591\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7341 - acc: 0.7670\n",
      "Epoch 00357: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7341 - acc: 0.7670 - val_loss: 0.5231 - val_acc: 0.8565\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7340 - acc: 0.7655\n",
      "Epoch 00358: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7340 - acc: 0.7655 - val_loss: 0.5184 - val_acc: 0.8530\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7319 - acc: 0.7679\n",
      "Epoch 00359: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7319 - acc: 0.7679 - val_loss: 0.5182 - val_acc: 0.8484\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7343 - acc: 0.7670\n",
      "Epoch 00360: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.7343 - acc: 0.7671 - val_loss: 0.5069 - val_acc: 0.8570\n",
      "Epoch 361/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7269 - acc: 0.7719\n",
      "Epoch 00361: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7272 - acc: 0.7719 - val_loss: 0.5303 - val_acc: 0.8526\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7314 - acc: 0.7692\n",
      "Epoch 00362: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7314 - acc: 0.7692 - val_loss: 0.5302 - val_acc: 0.8516\n",
      "Epoch 363/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7205 - acc: 0.7703\n",
      "Epoch 00363: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7203 - acc: 0.7705 - val_loss: 0.5208 - val_acc: 0.8530\n",
      "Epoch 364/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7362 - acc: 0.7657\n",
      "Epoch 00364: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7366 - acc: 0.7654 - val_loss: 0.5174 - val_acc: 0.8542\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7331 - acc: 0.7674\n",
      "Epoch 00365: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7332 - acc: 0.7674 - val_loss: 0.5124 - val_acc: 0.8549\n",
      "Epoch 366/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7208 - acc: 0.7713\n",
      "Epoch 00366: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7208 - acc: 0.7713 - val_loss: 0.5140 - val_acc: 0.8570\n",
      "Epoch 367/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7265 - acc: 0.7713\n",
      "Epoch 00367: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7263 - acc: 0.7712 - val_loss: 0.5088 - val_acc: 0.8539\n",
      "Epoch 368/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7253 - acc: 0.7704\n",
      "Epoch 00368: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7257 - acc: 0.7703 - val_loss: 0.5068 - val_acc: 0.8579\n",
      "Epoch 369/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7336 - acc: 0.7690\n",
      "Epoch 00369: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7335 - acc: 0.7691 - val_loss: 0.5122 - val_acc: 0.8556\n",
      "Epoch 370/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7281 - acc: 0.7699\n",
      "Epoch 00370: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7280 - acc: 0.7699 - val_loss: 0.5470 - val_acc: 0.8332\n",
      "Epoch 371/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7339 - acc: 0.7684\n",
      "Epoch 00371: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7339 - acc: 0.7684 - val_loss: 0.5198 - val_acc: 0.8470\n",
      "Epoch 372/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7281 - acc: 0.7677\n",
      "Epoch 00372: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7280 - acc: 0.7677 - val_loss: 0.5422 - val_acc: 0.8425\n",
      "Epoch 373/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7277 - acc: 0.7701\n",
      "Epoch 00373: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7278 - acc: 0.7701 - val_loss: 0.5352 - val_acc: 0.8493\n",
      "Epoch 374/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7320 - acc: 0.7696\n",
      "Epoch 00374: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7321 - acc: 0.7695 - val_loss: 0.5348 - val_acc: 0.8456\n",
      "Epoch 375/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7334 - acc: 0.7667\n",
      "Epoch 00375: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7333 - acc: 0.7667 - val_loss: 0.5748 - val_acc: 0.8286\n",
      "Epoch 376/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7255 - acc: 0.7698\n",
      "Epoch 00376: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7252 - acc: 0.7698 - val_loss: 0.5245 - val_acc: 0.8477\n",
      "Epoch 377/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7204 - acc: 0.7694\n",
      "Epoch 00377: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7205 - acc: 0.7694 - val_loss: 0.5300 - val_acc: 0.8493\n",
      "Epoch 378/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7289 - acc: 0.7690\n",
      "Epoch 00378: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7289 - acc: 0.7690 - val_loss: 0.5397 - val_acc: 0.8444\n",
      "Epoch 379/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7203 - acc: 0.7714\n",
      "Epoch 00379: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7202 - acc: 0.7714 - val_loss: 0.5263 - val_acc: 0.8500\n",
      "Epoch 380/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7164 - acc: 0.7732\n",
      "Epoch 00380: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7163 - acc: 0.7732 - val_loss: 0.5553 - val_acc: 0.8372\n",
      "Epoch 381/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7205 - acc: 0.7732\n",
      "Epoch 00381: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7210 - acc: 0.7732 - val_loss: 0.5294 - val_acc: 0.8488\n",
      "Epoch 382/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7254 - acc: 0.7698\n",
      "Epoch 00382: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7254 - acc: 0.7698 - val_loss: 0.5440 - val_acc: 0.8407\n",
      "Epoch 383/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7200 - acc: 0.7739\n",
      "Epoch 00383: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7200 - acc: 0.7738 - val_loss: 0.5045 - val_acc: 0.8574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7239 - acc: 0.7726\n",
      "Epoch 00384: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7239 - acc: 0.7726 - val_loss: 0.5290 - val_acc: 0.8388\n",
      "Epoch 385/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7216 - acc: 0.7721\n",
      "Epoch 00385: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7217 - acc: 0.7721 - val_loss: 0.5308 - val_acc: 0.8470\n",
      "Epoch 386/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7215 - acc: 0.7703\n",
      "Epoch 00386: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7220 - acc: 0.7702 - val_loss: 0.5110 - val_acc: 0.8514\n",
      "Epoch 387/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7220 - acc: 0.7735\n",
      "Epoch 00387: val_loss did not improve from 0.50429\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7221 - acc: 0.7734 - val_loss: 0.5366 - val_acc: 0.8470\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 378us/sample - loss: 0.5789 - acc: 0.8243\n",
      "Loss: 0.5789255082421585 Accuracy: 0.82429904\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 5.6703 - acc: 0.0917\n",
      "Epoch 00001: val_loss improved from inf to 2.44667, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/001-2.4467.hdf5\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 5.6679 - acc: 0.0917 - val_loss: 2.4467 - val_acc: 0.2222\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.6656 - acc: 0.1348\n",
      "Epoch 00002: val_loss improved from 2.44667 to 2.05958, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/002-2.0596.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 3.6656 - acc: 0.1348 - val_loss: 2.0596 - val_acc: 0.3864\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.9699 - acc: 0.1720\n",
      "Epoch 00003: val_loss improved from 2.05958 to 1.87651, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/003-1.8765.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 2.9694 - acc: 0.1722 - val_loss: 1.8765 - val_acc: 0.4512\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6343 - acc: 0.2136\n",
      "Epoch 00004: val_loss improved from 1.87651 to 1.68894, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/004-1.6889.hdf5\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 2.6342 - acc: 0.2136 - val_loss: 1.6889 - val_acc: 0.5036\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3808 - acc: 0.2555\n",
      "Epoch 00005: val_loss improved from 1.68894 to 1.53047, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/005-1.5305.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 2.3808 - acc: 0.2555 - val_loss: 1.5305 - val_acc: 0.5663\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1443 - acc: 0.3139\n",
      "Epoch 00006: val_loss improved from 1.53047 to 1.39812, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/006-1.3981.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 2.1442 - acc: 0.3139 - val_loss: 1.3981 - val_acc: 0.6275\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9395 - acc: 0.3654\n",
      "Epoch 00007: val_loss improved from 1.39812 to 1.26281, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/007-1.2628.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.9395 - acc: 0.3654 - val_loss: 1.2628 - val_acc: 0.6483\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7788 - acc: 0.4116\n",
      "Epoch 00008: val_loss improved from 1.26281 to 1.15811, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/008-1.1581.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.7787 - acc: 0.4117 - val_loss: 1.1581 - val_acc: 0.6674\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6380 - acc: 0.4591\n",
      "Epoch 00009: val_loss improved from 1.15811 to 1.09048, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/009-1.0905.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.6381 - acc: 0.4591 - val_loss: 1.0905 - val_acc: 0.6937\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5311 - acc: 0.4911\n",
      "Epoch 00010: val_loss improved from 1.09048 to 1.01896, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/010-1.0190.hdf5\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 1.5312 - acc: 0.4910 - val_loss: 1.0190 - val_acc: 0.7077\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4180 - acc: 0.5333\n",
      "Epoch 00011: val_loss improved from 1.01896 to 0.92213, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/011-0.9221.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.4179 - acc: 0.5332 - val_loss: 0.9221 - val_acc: 0.7463\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3333 - acc: 0.5622\n",
      "Epoch 00012: val_loss improved from 0.92213 to 0.86452, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/012-0.8645.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.3334 - acc: 0.5622 - val_loss: 0.8645 - val_acc: 0.7540\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2589 - acc: 0.5868\n",
      "Epoch 00013: val_loss did not improve from 0.86452\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.2590 - acc: 0.5868 - val_loss: 0.8803 - val_acc: 0.7659\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1794 - acc: 0.6153\n",
      "Epoch 00014: val_loss improved from 0.86452 to 0.74830, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/014-0.7483.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 1.1794 - acc: 0.6153 - val_loss: 0.7483 - val_acc: 0.8004\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1263 - acc: 0.6361\n",
      "Epoch 00015: val_loss improved from 0.74830 to 0.72195, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/015-0.7220.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 1.1262 - acc: 0.6361 - val_loss: 0.7220 - val_acc: 0.8076\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0815 - acc: 0.6510\n",
      "Epoch 00016: val_loss improved from 0.72195 to 0.69477, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/016-0.6948.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 1.0816 - acc: 0.6509 - val_loss: 0.6948 - val_acc: 0.8130\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0392 - acc: 0.6675\n",
      "Epoch 00017: val_loss improved from 0.69477 to 0.66262, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/017-0.6626.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 1.0390 - acc: 0.6676 - val_loss: 0.6626 - val_acc: 0.8209\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0039 - acc: 0.6753\n",
      "Epoch 00018: val_loss did not improve from 0.66262\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 1.0039 - acc: 0.6753 - val_loss: 0.6630 - val_acc: 0.8125\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9672 - acc: 0.6936\n",
      "Epoch 00019: val_loss did not improve from 0.66262\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.9672 - acc: 0.6936 - val_loss: 0.6801 - val_acc: 0.8137\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9420 - acc: 0.7033\n",
      "Epoch 00020: val_loss improved from 0.66262 to 0.59990, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/020-0.5999.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9422 - acc: 0.7033 - val_loss: 0.5999 - val_acc: 0.8411\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9066 - acc: 0.7146\n",
      "Epoch 00021: val_loss improved from 0.59990 to 0.58203, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/021-0.5820.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9067 - acc: 0.7144 - val_loss: 0.5820 - val_acc: 0.8416\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8848 - acc: 0.7201\n",
      "Epoch 00022: val_loss did not improve from 0.58203\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.8849 - acc: 0.7200 - val_loss: 0.5880 - val_acc: 0.8362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8674 - acc: 0.7247\n",
      "Epoch 00023: val_loss improved from 0.58203 to 0.54532, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/023-0.5453.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8675 - acc: 0.7247 - val_loss: 0.5453 - val_acc: 0.8519\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8462 - acc: 0.7355\n",
      "Epoch 00024: val_loss did not improve from 0.54532\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8463 - acc: 0.7355 - val_loss: 0.5488 - val_acc: 0.8444\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8271 - acc: 0.7402\n",
      "Epoch 00025: val_loss improved from 0.54532 to 0.54200, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/025-0.5420.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8271 - acc: 0.7401 - val_loss: 0.5420 - val_acc: 0.8442\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8101 - acc: 0.7458\n",
      "Epoch 00026: val_loss improved from 0.54200 to 0.51296, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/026-0.5130.hdf5\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8104 - acc: 0.7457 - val_loss: 0.5130 - val_acc: 0.8602\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8008 - acc: 0.7483\n",
      "Epoch 00027: val_loss did not improve from 0.51296\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8008 - acc: 0.7483 - val_loss: 0.5204 - val_acc: 0.8530\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7703 - acc: 0.7596\n",
      "Epoch 00028: val_loss improved from 0.51296 to 0.50788, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/028-0.5079.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.7702 - acc: 0.7596 - val_loss: 0.5079 - val_acc: 0.8574\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7630 - acc: 0.7633\n",
      "Epoch 00029: val_loss improved from 0.50788 to 0.49805, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/029-0.4981.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.7630 - acc: 0.7633 - val_loss: 0.4981 - val_acc: 0.8616\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7523 - acc: 0.7661\n",
      "Epoch 00030: val_loss improved from 0.49805 to 0.47199, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/030-0.4720.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.7523 - acc: 0.7661 - val_loss: 0.4720 - val_acc: 0.8665\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7362 - acc: 0.7734\n",
      "Epoch 00031: val_loss improved from 0.47199 to 0.46669, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/031-0.4667.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.7362 - acc: 0.7734 - val_loss: 0.4667 - val_acc: 0.8686\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7273 - acc: 0.7754\n",
      "Epoch 00032: val_loss improved from 0.46669 to 0.45876, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/032-0.4588.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.7272 - acc: 0.7754 - val_loss: 0.4588 - val_acc: 0.8763\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7184 - acc: 0.7767\n",
      "Epoch 00033: val_loss did not improve from 0.45876\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.7184 - acc: 0.7767 - val_loss: 0.4617 - val_acc: 0.8775\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7072 - acc: 0.7808\n",
      "Epoch 00034: val_loss improved from 0.45876 to 0.45295, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/034-0.4529.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.7071 - acc: 0.7808 - val_loss: 0.4529 - val_acc: 0.8730\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6961 - acc: 0.7833- ETA: 2s - \n",
      "Epoch 00035: val_loss improved from 0.45295 to 0.43197, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/035-0.4320.hdf5\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.6965 - acc: 0.7833 - val_loss: 0.4320 - val_acc: 0.8735\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6904 - acc: 0.7873\n",
      "Epoch 00036: val_loss improved from 0.43197 to 0.43128, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/036-0.4313.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.6905 - acc: 0.7873 - val_loss: 0.4313 - val_acc: 0.8798\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6838 - acc: 0.7878\n",
      "Epoch 00037: val_loss did not improve from 0.43128\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.6837 - acc: 0.7878 - val_loss: 0.4357 - val_acc: 0.8763\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6684 - acc: 0.7932\n",
      "Epoch 00038: val_loss did not improve from 0.43128\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.6684 - acc: 0.7932 - val_loss: 0.4643 - val_acc: 0.8721\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6638 - acc: 0.7954\n",
      "Epoch 00039: val_loss did not improve from 0.43128\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.6638 - acc: 0.7954 - val_loss: 0.4345 - val_acc: 0.8751\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6508 - acc: 0.7987\n",
      "Epoch 00040: val_loss improved from 0.43128 to 0.39593, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/040-0.3959.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.6508 - acc: 0.7987 - val_loss: 0.3959 - val_acc: 0.8896\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6474 - acc: 0.8012\n",
      "Epoch 00041: val_loss did not improve from 0.39593\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.6473 - acc: 0.8012 - val_loss: 0.4002 - val_acc: 0.8894\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6416 - acc: 0.8033\n",
      "Epoch 00042: val_loss did not improve from 0.39593\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.6416 - acc: 0.8033 - val_loss: 0.4223 - val_acc: 0.8770\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6359 - acc: 0.8066\n",
      "Epoch 00043: val_loss did not improve from 0.39593\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.6358 - acc: 0.8066 - val_loss: 0.4302 - val_acc: 0.8803\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6287 - acc: 0.8063\n",
      "Epoch 00044: val_loss did not improve from 0.39593\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.6288 - acc: 0.8063 - val_loss: 0.4486 - val_acc: 0.8703\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6214 - acc: 0.8085\n",
      "Epoch 00045: val_loss improved from 0.39593 to 0.39457, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/045-0.3946.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.6213 - acc: 0.8085 - val_loss: 0.3946 - val_acc: 0.8875\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6208 - acc: 0.8090\n",
      "Epoch 00046: val_loss improved from 0.39457 to 0.38389, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/046-0.3839.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.6208 - acc: 0.8090 - val_loss: 0.3839 - val_acc: 0.8887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.8123\n",
      "Epoch 00047: val_loss did not improve from 0.38389\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.6067 - acc: 0.8123 - val_loss: 0.4134 - val_acc: 0.8791\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6066 - acc: 0.8153\n",
      "Epoch 00048: val_loss did not improve from 0.38389\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.6065 - acc: 0.8153 - val_loss: 0.3841 - val_acc: 0.8905\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5999 - acc: 0.8136\n",
      "Epoch 00049: val_loss did not improve from 0.38389\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.5999 - acc: 0.8136 - val_loss: 0.4034 - val_acc: 0.8884\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6002 - acc: 0.8154\n",
      "Epoch 00050: val_loss did not improve from 0.38389\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.6002 - acc: 0.8154 - val_loss: 0.4073 - val_acc: 0.8875\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5874 - acc: 0.8180\n",
      "Epoch 00051: val_loss improved from 0.38389 to 0.36891, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/051-0.3689.hdf5\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.5874 - acc: 0.8179 - val_loss: 0.3689 - val_acc: 0.8991\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5853 - acc: 0.8209\n",
      "Epoch 00052: val_loss improved from 0.36891 to 0.35336, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/052-0.3534.hdf5\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.5852 - acc: 0.8209 - val_loss: 0.3534 - val_acc: 0.8989\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5767 - acc: 0.8225\n",
      "Epoch 00053: val_loss did not improve from 0.35336\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.5767 - acc: 0.8225 - val_loss: 0.3744 - val_acc: 0.8919\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5768 - acc: 0.8213\n",
      "Epoch 00054: val_loss did not improve from 0.35336\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.5766 - acc: 0.8214 - val_loss: 0.4339 - val_acc: 0.8791\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.8258\n",
      "Epoch 00055: val_loss did not improve from 0.35336\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5712 - acc: 0.8255 - val_loss: 0.3660 - val_acc: 0.8945\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5687 - acc: 0.8255\n",
      "Epoch 00056: val_loss improved from 0.35336 to 0.35275, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/056-0.3527.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5688 - acc: 0.8255 - val_loss: 0.3527 - val_acc: 0.8956\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5662 - acc: 0.8295\n",
      "Epoch 00057: val_loss did not improve from 0.35275\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.5662 - acc: 0.8295 - val_loss: 0.4010 - val_acc: 0.8861\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5657 - acc: 0.8252\n",
      "Epoch 00058: val_loss did not improve from 0.35275\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5658 - acc: 0.8252 - val_loss: 0.3593 - val_acc: 0.9015\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5593 - acc: 0.8282\n",
      "Epoch 00059: val_loss improved from 0.35275 to 0.34517, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/059-0.3452.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5592 - acc: 0.8283 - val_loss: 0.3452 - val_acc: 0.9001\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5525 - acc: 0.8294\n",
      "Epoch 00060: val_loss did not improve from 0.34517\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.5524 - acc: 0.8293 - val_loss: 0.3497 - val_acc: 0.9050\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.8315\n",
      "Epoch 00061: val_loss did not improve from 0.34517\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.5476 - acc: 0.8315 - val_loss: 0.3562 - val_acc: 0.9026\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.8307\n",
      "Epoch 00062: val_loss improved from 0.34517 to 0.34163, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/062-0.3416.hdf5\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.5505 - acc: 0.8307 - val_loss: 0.3416 - val_acc: 0.9026\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.8339\n",
      "Epoch 00063: val_loss did not improve from 0.34163\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.5420 - acc: 0.8339 - val_loss: 0.3664 - val_acc: 0.8908\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5384 - acc: 0.8342\n",
      "Epoch 00064: val_loss did not improve from 0.34163\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.5383 - acc: 0.8343 - val_loss: 0.4111 - val_acc: 0.8824\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.8362\n",
      "Epoch 00065: val_loss did not improve from 0.34163\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.5371 - acc: 0.8362 - val_loss: 0.3525 - val_acc: 0.9003\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.8334\n",
      "Epoch 00066: val_loss did not improve from 0.34163\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.5391 - acc: 0.8334 - val_loss: 0.3499 - val_acc: 0.9008\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5330 - acc: 0.8374\n",
      "Epoch 00067: val_loss did not improve from 0.34163\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.5330 - acc: 0.8374 - val_loss: 0.3655 - val_acc: 0.8982\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5345 - acc: 0.8349\n",
      "Epoch 00068: val_loss did not improve from 0.34163\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.5345 - acc: 0.8349 - val_loss: 0.3807 - val_acc: 0.8924\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.8373\n",
      "Epoch 00069: val_loss improved from 0.34163 to 0.32726, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/069-0.3273.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.5280 - acc: 0.8373 - val_loss: 0.3273 - val_acc: 0.9071\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.8392\n",
      "Epoch 00070: val_loss did not improve from 0.32726\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.5182 - acc: 0.8391 - val_loss: 0.3623 - val_acc: 0.8963\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5227 - acc: 0.8395\n",
      "Epoch 00071: val_loss did not improve from 0.32726\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.5227 - acc: 0.8395 - val_loss: 0.3988 - val_acc: 0.8845\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.8397\n",
      "Epoch 00072: val_loss did not improve from 0.32726\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5179 - acc: 0.8398 - val_loss: 0.3940 - val_acc: 0.8966\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5126 - acc: 0.8409\n",
      "Epoch 00073: val_loss improved from 0.32726 to 0.32222, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/073-0.3222.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5125 - acc: 0.8409 - val_loss: 0.3222 - val_acc: 0.9129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5137 - acc: 0.8427\n",
      "Epoch 00074: val_loss did not improve from 0.32222\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.5137 - acc: 0.8427 - val_loss: 0.3312 - val_acc: 0.9017\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.8468\n",
      "Epoch 00075: val_loss did not improve from 0.32222\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.5043 - acc: 0.8468 - val_loss: 0.3467 - val_acc: 0.9031\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.8433\n",
      "Epoch 00076: val_loss did not improve from 0.32222\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.5059 - acc: 0.8433 - val_loss: 0.3230 - val_acc: 0.9096\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.8438\n",
      "Epoch 00077: val_loss did not improve from 0.32222\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.5053 - acc: 0.8437 - val_loss: 0.3440 - val_acc: 0.9019\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.8437\n",
      "Epoch 00078: val_loss did not improve from 0.32222\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.5051 - acc: 0.8437 - val_loss: 0.3423 - val_acc: 0.8975\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8466\n",
      "Epoch 00079: val_loss did not improve from 0.32222\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.5021 - acc: 0.8465 - val_loss: 0.3258 - val_acc: 0.9106\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4942 - acc: 0.8479\n",
      "Epoch 00080: val_loss improved from 0.32222 to 0.31921, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/080-0.3192.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4942 - acc: 0.8479 - val_loss: 0.3192 - val_acc: 0.9082\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.8463\n",
      "Epoch 00081: val_loss improved from 0.31921 to 0.31198, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/081-0.3120.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4986 - acc: 0.8463 - val_loss: 0.3120 - val_acc: 0.9082\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4939 - acc: 0.8480\n",
      "Epoch 00082: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4939 - acc: 0.8480 - val_loss: 0.3354 - val_acc: 0.9029\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4873 - acc: 0.8496\n",
      "Epoch 00083: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4873 - acc: 0.8495 - val_loss: 0.3364 - val_acc: 0.9033\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4928 - acc: 0.8486\n",
      "Epoch 00084: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4929 - acc: 0.8486 - val_loss: 0.3146 - val_acc: 0.9082\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4815 - acc: 0.8519\n",
      "Epoch 00085: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4816 - acc: 0.8518 - val_loss: 0.3573 - val_acc: 0.8968\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4886 - acc: 0.8495\n",
      "Epoch 00086: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4885 - acc: 0.8494 - val_loss: 0.3413 - val_acc: 0.9059\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4858 - acc: 0.8514\n",
      "Epoch 00087: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.4857 - acc: 0.8514 - val_loss: 0.3140 - val_acc: 0.9094\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4812 - acc: 0.8540\n",
      "Epoch 00088: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4819 - acc: 0.8540 - val_loss: 0.3324 - val_acc: 0.9008\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.8510\n",
      "Epoch 00089: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4848 - acc: 0.8510 - val_loss: 0.3313 - val_acc: 0.9096\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.8547\n",
      "Epoch 00090: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.4732 - acc: 0.8547 - val_loss: 0.3153 - val_acc: 0.9108\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4700 - acc: 0.8546\n",
      "Epoch 00091: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4700 - acc: 0.8546 - val_loss: 0.3194 - val_acc: 0.9078\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.8549\n",
      "Epoch 00092: val_loss improved from 0.31198 to 0.30431, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/092-0.3043.hdf5\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4715 - acc: 0.8550 - val_loss: 0.3043 - val_acc: 0.9110\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4687 - acc: 0.8537\n",
      "Epoch 00093: val_loss did not improve from 0.30431\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.4688 - acc: 0.8536 - val_loss: 0.3511 - val_acc: 0.8938\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.8565\n",
      "Epoch 00094: val_loss did not improve from 0.30431\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4714 - acc: 0.8565 - val_loss: 0.3375 - val_acc: 0.9026\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4622 - acc: 0.8566\n",
      "Epoch 00095: val_loss improved from 0.30431 to 0.30425, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/095-0.3043.hdf5\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4622 - acc: 0.8566 - val_loss: 0.3043 - val_acc: 0.9115\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8572\n",
      "Epoch 00096: val_loss did not improve from 0.30425\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4654 - acc: 0.8572 - val_loss: 0.3259 - val_acc: 0.9064\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4646 - acc: 0.8583\n",
      "Epoch 00097: val_loss improved from 0.30425 to 0.29855, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/097-0.2986.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4645 - acc: 0.8584 - val_loss: 0.2986 - val_acc: 0.9140\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.8609\n",
      "Epoch 00098: val_loss did not improve from 0.29855\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.4565 - acc: 0.8609 - val_loss: 0.3165 - val_acc: 0.9094\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8599\n",
      "Epoch 00099: val_loss did not improve from 0.29855\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.4553 - acc: 0.8599 - val_loss: 0.3400 - val_acc: 0.8975\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8586\n",
      "Epoch 00100: val_loss did not improve from 0.29855\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.4581 - acc: 0.8586 - val_loss: 0.3082 - val_acc: 0.9108\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.8581\n",
      "Epoch 00101: val_loss did not improve from 0.29855\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4584 - acc: 0.8581 - val_loss: 0.3144 - val_acc: 0.9061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4510 - acc: 0.8604\n",
      "Epoch 00102: val_loss did not improve from 0.29855\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4510 - acc: 0.8603 - val_loss: 0.3040 - val_acc: 0.9103\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8607\n",
      "Epoch 00103: val_loss did not improve from 0.29855\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4543 - acc: 0.8606 - val_loss: 0.3195 - val_acc: 0.9026\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4477 - acc: 0.8609\n",
      "Epoch 00104: val_loss did not improve from 0.29855\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4477 - acc: 0.8609 - val_loss: 0.3531 - val_acc: 0.8977\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4482 - acc: 0.8627\n",
      "Epoch 00105: val_loss improved from 0.29855 to 0.29684, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/105-0.2968.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4481 - acc: 0.8627 - val_loss: 0.2968 - val_acc: 0.9140\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8613\n",
      "Epoch 00106: val_loss did not improve from 0.29684\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4479 - acc: 0.8613 - val_loss: 0.3496 - val_acc: 0.8994\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4456 - acc: 0.8640\n",
      "Epoch 00107: val_loss did not improve from 0.29684\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.4456 - acc: 0.8640 - val_loss: 0.3162 - val_acc: 0.9119\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.8630\n",
      "Epoch 00108: val_loss did not improve from 0.29684\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.4426 - acc: 0.8628 - val_loss: 0.3094 - val_acc: 0.9119\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4385 - acc: 0.8651\n",
      "Epoch 00109: val_loss did not improve from 0.29684\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4386 - acc: 0.8651 - val_loss: 0.3043 - val_acc: 0.9126\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4421 - acc: 0.8633\n",
      "Epoch 00110: val_loss did not improve from 0.29684\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4421 - acc: 0.8633 - val_loss: 0.3102 - val_acc: 0.9045\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4373 - acc: 0.8643\n",
      "Epoch 00111: val_loss did not improve from 0.29684\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4373 - acc: 0.8643 - val_loss: 0.3624 - val_acc: 0.8928\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.8645\n",
      "Epoch 00112: val_loss did not improve from 0.29684\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4366 - acc: 0.8646 - val_loss: 0.3080 - val_acc: 0.9126\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8661\n",
      "Epoch 00113: val_loss did not improve from 0.29684\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4362 - acc: 0.8661 - val_loss: 0.3055 - val_acc: 0.9071\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.8632\n",
      "Epoch 00114: val_loss improved from 0.29684 to 0.29037, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/114-0.2904.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4402 - acc: 0.8632 - val_loss: 0.2904 - val_acc: 0.9150\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4333 - acc: 0.8648\n",
      "Epoch 00115: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4334 - acc: 0.8648 - val_loss: 0.2968 - val_acc: 0.9131\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.8680\n",
      "Epoch 00116: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.4277 - acc: 0.8680 - val_loss: 0.3304 - val_acc: 0.8982\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8680\n",
      "Epoch 00117: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.4299 - acc: 0.8680 - val_loss: 0.3237 - val_acc: 0.9059\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.8682\n",
      "Epoch 00118: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4277 - acc: 0.8682 - val_loss: 0.3098 - val_acc: 0.9106\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.8672\n",
      "Epoch 00119: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4304 - acc: 0.8671 - val_loss: 0.3003 - val_acc: 0.9150\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.8670\n",
      "Epoch 00120: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4277 - acc: 0.8669 - val_loss: 0.3150 - val_acc: 0.9113\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8672\n",
      "Epoch 00121: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4225 - acc: 0.8672 - val_loss: 0.2929 - val_acc: 0.9168\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8692\n",
      "Epoch 00122: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4236 - acc: 0.8692 - val_loss: 0.3067 - val_acc: 0.9113\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4234 - acc: 0.8704\n",
      "Epoch 00123: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4235 - acc: 0.8704 - val_loss: 0.2966 - val_acc: 0.9138\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.8675\n",
      "Epoch 00124: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4263 - acc: 0.8675 - val_loss: 0.2923 - val_acc: 0.9168\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8712\n",
      "Epoch 00125: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.4148 - acc: 0.8712 - val_loss: 0.3274 - val_acc: 0.9080\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8714\n",
      "Epoch 00126: val_loss did not improve from 0.29037\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4147 - acc: 0.8714 - val_loss: 0.2965 - val_acc: 0.9096\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8696\n",
      "Epoch 00127: val_loss improved from 0.29037 to 0.28817, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/127-0.2882.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4180 - acc: 0.8696 - val_loss: 0.2882 - val_acc: 0.9152\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8715\n",
      "Epoch 00128: val_loss did not improve from 0.28817\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.4122 - acc: 0.8716 - val_loss: 0.2904 - val_acc: 0.9194\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.8707\n",
      "Epoch 00129: val_loss did not improve from 0.28817\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.4147 - acc: 0.8707 - val_loss: 0.2935 - val_acc: 0.9168\n",
      "Epoch 130/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8711\n",
      "Epoch 00130: val_loss did not improve from 0.28817\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.4129 - acc: 0.8711 - val_loss: 0.3007 - val_acc: 0.9117\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8702\n",
      "Epoch 00131: val_loss did not improve from 0.28817\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.4179 - acc: 0.8702 - val_loss: 0.3003 - val_acc: 0.9122\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4162 - acc: 0.8716\n",
      "Epoch 00132: val_loss did not improve from 0.28817\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.4162 - acc: 0.8716 - val_loss: 0.2955 - val_acc: 0.9131\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8736\n",
      "Epoch 00133: val_loss improved from 0.28817 to 0.27868, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/133-0.2787.hdf5\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.4092 - acc: 0.8737 - val_loss: 0.2787 - val_acc: 0.9206\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4104 - acc: 0.8734\n",
      "Epoch 00134: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.4104 - acc: 0.8734 - val_loss: 0.2951 - val_acc: 0.9161\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8696\n",
      "Epoch 00135: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.4135 - acc: 0.8696 - val_loss: 0.2914 - val_acc: 0.9178\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.8733\n",
      "Epoch 00136: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.4060 - acc: 0.8733 - val_loss: 0.2901 - val_acc: 0.9171\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.8734\n",
      "Epoch 00137: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.4065 - acc: 0.8732 - val_loss: 0.2928 - val_acc: 0.9168\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8731\n",
      "Epoch 00138: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.4120 - acc: 0.8730 - val_loss: 0.2970 - val_acc: 0.9152\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8752\n",
      "Epoch 00139: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.4067 - acc: 0.8752 - val_loss: 0.3440 - val_acc: 0.8956\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8756\n",
      "Epoch 00140: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.4074 - acc: 0.8755 - val_loss: 0.3277 - val_acc: 0.9064\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8757\n",
      "Epoch 00141: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3976 - acc: 0.8757 - val_loss: 0.3176 - val_acc: 0.9101\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8754\n",
      "Epoch 00142: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.4046 - acc: 0.8755 - val_loss: 0.2974 - val_acc: 0.9133\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8773\n",
      "Epoch 00143: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3964 - acc: 0.8773 - val_loss: 0.3231 - val_acc: 0.9075\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.8777\n",
      "Epoch 00144: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3937 - acc: 0.8777 - val_loss: 0.3068 - val_acc: 0.9096\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8762\n",
      "Epoch 00145: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3955 - acc: 0.8762 - val_loss: 0.2813 - val_acc: 0.9140\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8762\n",
      "Epoch 00146: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3950 - acc: 0.8762 - val_loss: 0.2860 - val_acc: 0.9175\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3943 - acc: 0.8750\n",
      "Epoch 00147: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3940 - acc: 0.8750 - val_loss: 0.2834 - val_acc: 0.9185\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8770\n",
      "Epoch 00148: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3916 - acc: 0.8769 - val_loss: 0.2916 - val_acc: 0.9129\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8765\n",
      "Epoch 00149: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3988 - acc: 0.8765 - val_loss: 0.2892 - val_acc: 0.9143\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8805\n",
      "Epoch 00150: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3898 - acc: 0.8805 - val_loss: 0.3067 - val_acc: 0.9117\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8796\n",
      "Epoch 00151: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3881 - acc: 0.8795 - val_loss: 0.3180 - val_acc: 0.9040\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8780\n",
      "Epoch 00152: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3942 - acc: 0.8779 - val_loss: 0.3026 - val_acc: 0.9108\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8791\n",
      "Epoch 00153: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3862 - acc: 0.8791 - val_loss: 0.2881 - val_acc: 0.9157\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8815\n",
      "Epoch 00154: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3775 - acc: 0.8816 - val_loss: 0.3175 - val_acc: 0.9066\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8786\n",
      "Epoch 00155: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3883 - acc: 0.8786 - val_loss: 0.2886 - val_acc: 0.9159\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8774\n",
      "Epoch 00156: val_loss did not improve from 0.27868\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3914 - acc: 0.8774 - val_loss: 0.2870 - val_acc: 0.9171\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3835 - acc: 0.8807\n",
      "Epoch 00157: val_loss improved from 0.27868 to 0.27749, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/157-0.2775.hdf5\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3837 - acc: 0.8807 - val_loss: 0.2775 - val_acc: 0.9173\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8798\n",
      "Epoch 00158: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3853 - acc: 0.8797 - val_loss: 0.2806 - val_acc: 0.9168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8799\n",
      "Epoch 00159: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3838 - acc: 0.8799 - val_loss: 0.2978 - val_acc: 0.9147\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3837 - acc: 0.8801\n",
      "Epoch 00160: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3837 - acc: 0.8802 - val_loss: 0.2969 - val_acc: 0.9147\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8838\n",
      "Epoch 00161: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3758 - acc: 0.8838 - val_loss: 0.2986 - val_acc: 0.9138\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3856 - acc: 0.8820\n",
      "Epoch 00162: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3856 - acc: 0.8820 - val_loss: 0.3200 - val_acc: 0.9057\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8805\n",
      "Epoch 00163: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3799 - acc: 0.8805 - val_loss: 0.2791 - val_acc: 0.9206\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8818\n",
      "Epoch 00164: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3807 - acc: 0.8818 - val_loss: 0.2799 - val_acc: 0.9210\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8850\n",
      "Epoch 00165: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.3730 - acc: 0.8850 - val_loss: 0.2930 - val_acc: 0.9117\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8830\n",
      "Epoch 00166: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3767 - acc: 0.8831 - val_loss: 0.2903 - val_acc: 0.9117\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8820\n",
      "Epoch 00167: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3790 - acc: 0.8821 - val_loss: 0.3032 - val_acc: 0.9117\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3698 - acc: 0.8817\n",
      "Epoch 00168: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3697 - acc: 0.8817 - val_loss: 0.3170 - val_acc: 0.9101\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8836\n",
      "Epoch 00169: val_loss did not improve from 0.27749\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3719 - acc: 0.8836 - val_loss: 0.3182 - val_acc: 0.9050\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3699 - acc: 0.8840\n",
      "Epoch 00170: val_loss improved from 0.27749 to 0.27710, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/170-0.2771.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3699 - acc: 0.8840 - val_loss: 0.2771 - val_acc: 0.9178\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8832\n",
      "Epoch 00171: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3733 - acc: 0.8832 - val_loss: 0.2901 - val_acc: 0.9138\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8814\n",
      "Epoch 00172: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3737 - acc: 0.8813 - val_loss: 0.2910 - val_acc: 0.9131\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8843\n",
      "Epoch 00173: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3704 - acc: 0.8843 - val_loss: 0.2819 - val_acc: 0.9208\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8835\n",
      "Epoch 00174: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.3709 - acc: 0.8835 - val_loss: 0.2808 - val_acc: 0.9150\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8860\n",
      "Epoch 00175: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3668 - acc: 0.8860 - val_loss: 0.2776 - val_acc: 0.9185\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8833\n",
      "Epoch 00176: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3719 - acc: 0.8833 - val_loss: 0.2824 - val_acc: 0.9185\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8842\n",
      "Epoch 00177: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3666 - acc: 0.8841 - val_loss: 0.2917 - val_acc: 0.9178\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8858\n",
      "Epoch 00178: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3651 - acc: 0.8858 - val_loss: 0.3185 - val_acc: 0.9087\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8870\n",
      "Epoch 00179: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3618 - acc: 0.8870 - val_loss: 0.2905 - val_acc: 0.9171\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3666 - acc: 0.8853\n",
      "Epoch 00180: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3668 - acc: 0.8853 - val_loss: 0.2819 - val_acc: 0.9189\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8831\n",
      "Epoch 00181: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3687 - acc: 0.8831 - val_loss: 0.2840 - val_acc: 0.9140\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8872\n",
      "Epoch 00182: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3653 - acc: 0.8872 - val_loss: 0.2941 - val_acc: 0.9159\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8868\n",
      "Epoch 00183: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3610 - acc: 0.8869 - val_loss: 0.2800 - val_acc: 0.9161\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8869\n",
      "Epoch 00184: val_loss did not improve from 0.27710\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3607 - acc: 0.8869 - val_loss: 0.2972 - val_acc: 0.9150\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8879\n",
      "Epoch 00185: val_loss improved from 0.27710 to 0.27107, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/185-0.2711.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3605 - acc: 0.8879 - val_loss: 0.2711 - val_acc: 0.9213\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8879\n",
      "Epoch 00186: val_loss did not improve from 0.27107\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3558 - acc: 0.8879 - val_loss: 0.3125 - val_acc: 0.9089\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8849\n",
      "Epoch 00187: val_loss did not improve from 0.27107\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3674 - acc: 0.8849 - val_loss: 0.2758 - val_acc: 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3604 - acc: 0.8858\n",
      "Epoch 00188: val_loss did not improve from 0.27107\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3604 - acc: 0.8857 - val_loss: 0.2944 - val_acc: 0.9124\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8879\n",
      "Epoch 00189: val_loss did not improve from 0.27107\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3569 - acc: 0.8879 - val_loss: 0.2788 - val_acc: 0.9194\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8897\n",
      "Epoch 00190: val_loss did not improve from 0.27107\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3532 - acc: 0.8897 - val_loss: 0.2827 - val_acc: 0.9178\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.8884\n",
      "Epoch 00191: val_loss did not improve from 0.27107\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3533 - acc: 0.8884 - val_loss: 0.2987 - val_acc: 0.9124\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8893\n",
      "Epoch 00192: val_loss improved from 0.27107 to 0.26644, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/192-0.2664.hdf5\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3502 - acc: 0.8892 - val_loss: 0.2664 - val_acc: 0.9231\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8891\n",
      "Epoch 00193: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3553 - acc: 0.8891 - val_loss: 0.2706 - val_acc: 0.9234\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3573 - acc: 0.8872\n",
      "Epoch 00194: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3573 - acc: 0.8872 - val_loss: 0.3022 - val_acc: 0.9168\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8877\n",
      "Epoch 00195: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3570 - acc: 0.8878 - val_loss: 0.2920 - val_acc: 0.9168\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8891\n",
      "Epoch 00196: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3521 - acc: 0.8891 - val_loss: 0.3022 - val_acc: 0.9119\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8899\n",
      "Epoch 00197: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3463 - acc: 0.8898 - val_loss: 0.2969 - val_acc: 0.9147\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8881\n",
      "Epoch 00198: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3515 - acc: 0.8881 - val_loss: 0.2805 - val_acc: 0.9192\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8874\n",
      "Epoch 00199: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3522 - acc: 0.8874 - val_loss: 0.2862 - val_acc: 0.9171\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8900\n",
      "Epoch 00200: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3502 - acc: 0.8900 - val_loss: 0.2799 - val_acc: 0.9210\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8914\n",
      "Epoch 00201: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3438 - acc: 0.8913 - val_loss: 0.2830 - val_acc: 0.9159\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.8902\n",
      "Epoch 00202: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3437 - acc: 0.8902 - val_loss: 0.2820 - val_acc: 0.9196\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8900\n",
      "Epoch 00203: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3479 - acc: 0.8900 - val_loss: 0.2837 - val_acc: 0.9133\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8918\n",
      "Epoch 00204: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3445 - acc: 0.8918 - val_loss: 0.2830 - val_acc: 0.9161\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8915\n",
      "Epoch 00205: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3472 - acc: 0.8916 - val_loss: 0.2949 - val_acc: 0.9124\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8904\n",
      "Epoch 00206: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.3482 - acc: 0.8903 - val_loss: 0.2806 - val_acc: 0.9157\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8904\n",
      "Epoch 00207: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3475 - acc: 0.8904 - val_loss: 0.2701 - val_acc: 0.9194\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8920\n",
      "Epoch 00208: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3466 - acc: 0.8920 - val_loss: 0.2851 - val_acc: 0.9229\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3421 - acc: 0.8918\n",
      "Epoch 00209: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3421 - acc: 0.8918 - val_loss: 0.2665 - val_acc: 0.9243\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8907\n",
      "Epoch 00210: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3471 - acc: 0.8907 - val_loss: 0.2765 - val_acc: 0.9187\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8929\n",
      "Epoch 00211: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3402 - acc: 0.8928 - val_loss: 0.2842 - val_acc: 0.9173\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8927\n",
      "Epoch 00212: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3433 - acc: 0.8927 - val_loss: 0.2925 - val_acc: 0.9147\n",
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.8918\n",
      "Epoch 00213: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.3411 - acc: 0.8917 - val_loss: 0.2688 - val_acc: 0.9250\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.8955\n",
      "Epoch 00214: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.3348 - acc: 0.8956 - val_loss: 0.2809 - val_acc: 0.9192\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8918\n",
      "Epoch 00215: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3402 - acc: 0.8918 - val_loss: 0.2901 - val_acc: 0.9145\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8898\n",
      "Epoch 00216: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3462 - acc: 0.8898 - val_loss: 0.2693 - val_acc: 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8919\n",
      "Epoch 00217: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3402 - acc: 0.8918 - val_loss: 0.2828 - val_acc: 0.9182\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8951\n",
      "Epoch 00218: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3394 - acc: 0.8951 - val_loss: 0.2800 - val_acc: 0.9194\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8949\n",
      "Epoch 00219: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3362 - acc: 0.8950 - val_loss: 0.2956 - val_acc: 0.9157\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.8944\n",
      "Epoch 00220: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3365 - acc: 0.8944 - val_loss: 0.3015 - val_acc: 0.9145\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8958\n",
      "Epoch 00221: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3346 - acc: 0.8958 - val_loss: 0.2842 - val_acc: 0.9150\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.8953\n",
      "Epoch 00222: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3330 - acc: 0.8953 - val_loss: 0.2847 - val_acc: 0.9166\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3359 - acc: 0.8941\n",
      "Epoch 00223: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3359 - acc: 0.8941 - val_loss: 0.2955 - val_acc: 0.9173\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8935\n",
      "Epoch 00224: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3382 - acc: 0.8935 - val_loss: 0.2992 - val_acc: 0.9140\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8982\n",
      "Epoch 00225: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3312 - acc: 0.8982 - val_loss: 0.2814 - val_acc: 0.9168\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.8960\n",
      "Epoch 00226: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3279 - acc: 0.8960 - val_loss: 0.2767 - val_acc: 0.9171\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8960\n",
      "Epoch 00227: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3311 - acc: 0.8959 - val_loss: 0.2759 - val_acc: 0.9213\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8945\n",
      "Epoch 00228: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3362 - acc: 0.8945 - val_loss: 0.3039 - val_acc: 0.9124\n",
      "Epoch 229/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3280 - acc: 0.8983\n",
      "Epoch 00229: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3284 - acc: 0.8982 - val_loss: 0.2796 - val_acc: 0.9203\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3328 - acc: 0.8937\n",
      "Epoch 00230: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3329 - acc: 0.8937 - val_loss: 0.2693 - val_acc: 0.9241\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.8953\n",
      "Epoch 00231: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3334 - acc: 0.8953 - val_loss: 0.2901 - val_acc: 0.9171\n",
      "Epoch 232/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.8965\n",
      "Epoch 00232: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3273 - acc: 0.8965 - val_loss: 0.2805 - val_acc: 0.9180\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8950\n",
      "Epoch 00233: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3276 - acc: 0.8950 - val_loss: 0.2862 - val_acc: 0.9168\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8964\n",
      "Epoch 00234: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3252 - acc: 0.8964 - val_loss: 0.2717 - val_acc: 0.9234\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8969\n",
      "Epoch 00235: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3219 - acc: 0.8969 - val_loss: 0.2902 - val_acc: 0.9189\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.8953\n",
      "Epoch 00236: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3309 - acc: 0.8953 - val_loss: 0.3018 - val_acc: 0.9133\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8972\n",
      "Epoch 00237: val_loss did not improve from 0.26644\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3258 - acc: 0.8972 - val_loss: 0.2860 - val_acc: 0.9187\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.8979\n",
      "Epoch 00238: val_loss improved from 0.26644 to 0.26567, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv_checkpoint/238-0.2657.hdf5\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3259 - acc: 0.8978 - val_loss: 0.2657 - val_acc: 0.9245\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8941\n",
      "Epoch 00239: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3314 - acc: 0.8941 - val_loss: 0.2728 - val_acc: 0.9206\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8982\n",
      "Epoch 00240: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3250 - acc: 0.8981 - val_loss: 0.2669 - val_acc: 0.9215\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8959\n",
      "Epoch 00241: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3252 - acc: 0.8959 - val_loss: 0.3487 - val_acc: 0.8917\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8990\n",
      "Epoch 00242: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3212 - acc: 0.8990 - val_loss: 0.2793 - val_acc: 0.9161\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8968\n",
      "Epoch 00243: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3257 - acc: 0.8968 - val_loss: 0.2748 - val_acc: 0.9187\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8980\n",
      "Epoch 00244: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3216 - acc: 0.8980 - val_loss: 0.2742 - val_acc: 0.9175\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.8972\n",
      "Epoch 00245: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3214 - acc: 0.8972 - val_loss: 0.2948 - val_acc: 0.9110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8980\n",
      "Epoch 00246: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3233 - acc: 0.8980 - val_loss: 0.2728 - val_acc: 0.9208\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.8989\n",
      "Epoch 00247: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3179 - acc: 0.8989 - val_loss: 0.2870 - val_acc: 0.9157\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.8994\n",
      "Epoch 00248: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3171 - acc: 0.8994 - val_loss: 0.2981 - val_acc: 0.9124\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8982\n",
      "Epoch 00249: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3222 - acc: 0.8982 - val_loss: 0.2663 - val_acc: 0.9241\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8983\n",
      "Epoch 00250: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3221 - acc: 0.8983 - val_loss: 0.2721 - val_acc: 0.9224\n",
      "Epoch 251/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3194 - acc: 0.8990\n",
      "Epoch 00251: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3192 - acc: 0.8991 - val_loss: 0.2737 - val_acc: 0.9220\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.9004\n",
      "Epoch 00252: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3224 - acc: 0.9004 - val_loss: 0.2793 - val_acc: 0.9206\n",
      "Epoch 253/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.8988\n",
      "Epoch 00253: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3232 - acc: 0.8988 - val_loss: 0.3468 - val_acc: 0.9008\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8968\n",
      "Epoch 00254: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3216 - acc: 0.8968 - val_loss: 0.2982 - val_acc: 0.9099\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.8965\n",
      "Epoch 00255: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3245 - acc: 0.8965 - val_loss: 0.3136 - val_acc: 0.9131\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8961\n",
      "Epoch 00256: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3257 - acc: 0.8961 - val_loss: 0.3238 - val_acc: 0.9050\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8975\n",
      "Epoch 00257: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3226 - acc: 0.8975 - val_loss: 0.2754 - val_acc: 0.9192\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.8995\n",
      "Epoch 00258: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3130 - acc: 0.8995 - val_loss: 0.2779 - val_acc: 0.9227\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.9000\n",
      "Epoch 00259: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3132 - acc: 0.9000 - val_loss: 0.2977 - val_acc: 0.9124\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9008\n",
      "Epoch 00260: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3137 - acc: 0.9007 - val_loss: 0.2913 - val_acc: 0.9175\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.8996\n",
      "Epoch 00261: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3135 - acc: 0.8996 - val_loss: 0.2792 - val_acc: 0.9180\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9010\n",
      "Epoch 00262: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3127 - acc: 0.9009 - val_loss: 0.2900 - val_acc: 0.9136\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.8987\n",
      "Epoch 00263: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.3142 - acc: 0.8987 - val_loss: 0.2756 - val_acc: 0.9238\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.9005\n",
      "Epoch 00264: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3140 - acc: 0.9005 - val_loss: 0.2703 - val_acc: 0.9250\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9016\n",
      "Epoch 00265: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3101 - acc: 0.9016 - val_loss: 0.2946 - val_acc: 0.9159\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9000\n",
      "Epoch 00266: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3176 - acc: 0.9000 - val_loss: 0.2860 - val_acc: 0.9180\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3123 - acc: 0.9016\n",
      "Epoch 00267: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3123 - acc: 0.9016 - val_loss: 0.2956 - val_acc: 0.9161\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9007\n",
      "Epoch 00268: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3150 - acc: 0.9006 - val_loss: 0.2720 - val_acc: 0.9243\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.9011\n",
      "Epoch 00269: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3132 - acc: 0.9010 - val_loss: 0.3086 - val_acc: 0.9101\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.9019\n",
      "Epoch 00270: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3082 - acc: 0.9019 - val_loss: 0.2796 - val_acc: 0.9171\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9014\n",
      "Epoch 00271: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3092 - acc: 0.9014 - val_loss: 0.2722 - val_acc: 0.9224\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.9014\n",
      "Epoch 00272: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3110 - acc: 0.9014 - val_loss: 0.3000 - val_acc: 0.9129\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.8998\n",
      "Epoch 00273: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3127 - acc: 0.8998 - val_loss: 0.3318 - val_acc: 0.9071\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9013\n",
      "Epoch 00274: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3089 - acc: 0.9012 - val_loss: 0.2780 - val_acc: 0.9173\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9019\n",
      "Epoch 00275: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3120 - acc: 0.9019 - val_loss: 0.3081 - val_acc: 0.9117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3067 - acc: 0.9035\n",
      "Epoch 00276: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3067 - acc: 0.9035 - val_loss: 0.2897 - val_acc: 0.9166\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9002\n",
      "Epoch 00277: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3042 - acc: 0.9002 - val_loss: 0.3017 - val_acc: 0.9131\n",
      "Epoch 278/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.9009\n",
      "Epoch 00278: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.3085 - acc: 0.9009 - val_loss: 0.2724 - val_acc: 0.9217\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9035\n",
      "Epoch 00279: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3061 - acc: 0.9035 - val_loss: 0.2985 - val_acc: 0.9166\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.9019\n",
      "Epoch 00280: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3114 - acc: 0.9019 - val_loss: 0.2740 - val_acc: 0.9208\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.9038\n",
      "Epoch 00281: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3038 - acc: 0.9037 - val_loss: 0.3369 - val_acc: 0.9033\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9012\n",
      "Epoch 00282: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3137 - acc: 0.9012 - val_loss: 0.3092 - val_acc: 0.9129\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9034\n",
      "Epoch 00283: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3025 - acc: 0.9034 - val_loss: 0.2867 - val_acc: 0.9180\n",
      "Epoch 284/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9028\n",
      "Epoch 00284: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3016 - acc: 0.9029 - val_loss: 0.2946 - val_acc: 0.9159\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9016\n",
      "Epoch 00285: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3086 - acc: 0.9016 - val_loss: 0.2683 - val_acc: 0.9222\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.9029\n",
      "Epoch 00286: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.3009 - acc: 0.9029 - val_loss: 0.2693 - val_acc: 0.9269\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9031\n",
      "Epoch 00287: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.2999 - acc: 0.9031 - val_loss: 0.3257 - val_acc: 0.9040\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3018 - acc: 0.9029\n",
      "Epoch 00288: val_loss did not improve from 0.26567\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.3018 - acc: 0.9029 - val_loss: 0.2698 - val_acc: 0.9210\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 432us/sample - loss: 0.3087 - acc: 0.9097\n",
      "Loss: 0.3087116699352443 Accuracy: 0.9096573\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.8162 - acc: 0.1184\n",
      "Epoch 00001: val_loss improved from inf to 2.21194, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/001-2.2119.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 4.8161 - acc: 0.1184 - val_loss: 2.2119 - val_acc: 0.2909\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0597 - acc: 0.2060\n",
      "Epoch 00002: val_loss improved from 2.21194 to 1.54305, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/002-1.5431.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 3.0599 - acc: 0.2060 - val_loss: 1.5431 - val_acc: 0.5695\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3943 - acc: 0.3039\n",
      "Epoch 00003: val_loss improved from 1.54305 to 1.20261, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/003-1.2026.hdf5\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 2.3946 - acc: 0.3039 - val_loss: 1.2026 - val_acc: 0.6795\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9607 - acc: 0.3999\n",
      "Epoch 00004: val_loss improved from 1.20261 to 0.99763, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/004-0.9976.hdf5\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 1.9609 - acc: 0.3998 - val_loss: 0.9976 - val_acc: 0.7140\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6438 - acc: 0.4819\n",
      "Epoch 00005: val_loss improved from 0.99763 to 0.78241, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/005-0.7824.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 1.6439 - acc: 0.4819 - val_loss: 0.7824 - val_acc: 0.7885\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3999 - acc: 0.5516\n",
      "Epoch 00006: val_loss improved from 0.78241 to 0.67387, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/006-0.6739.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 1.3999 - acc: 0.5516 - val_loss: 0.6739 - val_acc: 0.8183\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2036 - acc: 0.6126\n",
      "Epoch 00007: val_loss improved from 0.67387 to 0.57682, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/007-0.5768.hdf5\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 1.2036 - acc: 0.6126 - val_loss: 0.5768 - val_acc: 0.8472\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0902 - acc: 0.6500\n",
      "Epoch 00008: val_loss improved from 0.57682 to 0.53524, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/008-0.5352.hdf5\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 1.0903 - acc: 0.6500 - val_loss: 0.5352 - val_acc: 0.8635\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9823 - acc: 0.6863\n",
      "Epoch 00009: val_loss improved from 0.53524 to 0.48612, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/009-0.4861.hdf5\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.9822 - acc: 0.6863 - val_loss: 0.4861 - val_acc: 0.8661\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8943 - acc: 0.7138\n",
      "Epoch 00010: val_loss improved from 0.48612 to 0.44843, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/010-0.4484.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.8942 - acc: 0.7138 - val_loss: 0.4484 - val_acc: 0.8775\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8335 - acc: 0.7363\n",
      "Epoch 00011: val_loss improved from 0.44843 to 0.41720, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/011-0.4172.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.8335 - acc: 0.7363 - val_loss: 0.4172 - val_acc: 0.8833\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7738 - acc: 0.7536\n",
      "Epoch 00012: val_loss improved from 0.41720 to 0.39366, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/012-0.3937.hdf5\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.7738 - acc: 0.7536 - val_loss: 0.3937 - val_acc: 0.8915\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7289 - acc: 0.7707\n",
      "Epoch 00013: val_loss improved from 0.39366 to 0.38103, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/013-0.3810.hdf5\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.7289 - acc: 0.7706 - val_loss: 0.3810 - val_acc: 0.8919\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6977 - acc: 0.7790\n",
      "Epoch 00014: val_loss improved from 0.38103 to 0.37384, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/014-0.3738.hdf5\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.6976 - acc: 0.7790 - val_loss: 0.3738 - val_acc: 0.8921\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6555 - acc: 0.7958\n",
      "Epoch 00015: val_loss improved from 0.37384 to 0.36605, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/015-0.3661.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.6554 - acc: 0.7958 - val_loss: 0.3661 - val_acc: 0.8970\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6298 - acc: 0.8037\n",
      "Epoch 00016: val_loss improved from 0.36605 to 0.36172, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/016-0.3617.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.6297 - acc: 0.8037 - val_loss: 0.3617 - val_acc: 0.8963\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5997 - acc: 0.8110\n",
      "Epoch 00017: val_loss improved from 0.36172 to 0.32364, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/017-0.3236.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.6000 - acc: 0.8109 - val_loss: 0.3236 - val_acc: 0.9092\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5768 - acc: 0.8210\n",
      "Epoch 00018: val_loss improved from 0.32364 to 0.30828, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/018-0.3083.hdf5\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.5767 - acc: 0.8210 - val_loss: 0.3083 - val_acc: 0.9145\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.8234\n",
      "Epoch 00019: val_loss did not improve from 0.30828\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.5609 - acc: 0.8234 - val_loss: 0.3108 - val_acc: 0.9099\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.8332\n",
      "Epoch 00020: val_loss improved from 0.30828 to 0.29446, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/020-0.2945.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.5388 - acc: 0.8331 - val_loss: 0.2945 - val_acc: 0.9201\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.8382\n",
      "Epoch 00021: val_loss did not improve from 0.29446\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.5222 - acc: 0.8381 - val_loss: 0.2950 - val_acc: 0.9140\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.8420\n",
      "Epoch 00022: val_loss improved from 0.29446 to 0.29143, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/022-0.2914.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.5142 - acc: 0.8420 - val_loss: 0.2914 - val_acc: 0.9178\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4926 - acc: 0.8450\n",
      "Epoch 00023: val_loss improved from 0.29143 to 0.27247, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/023-0.2725.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4927 - acc: 0.8450 - val_loss: 0.2725 - val_acc: 0.9238\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4862 - acc: 0.8492\n",
      "Epoch 00024: val_loss did not improve from 0.27247\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4863 - acc: 0.8492 - val_loss: 0.2738 - val_acc: 0.9217\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.8560\n",
      "Epoch 00025: val_loss improved from 0.27247 to 0.25618, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/025-0.2562.hdf5\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4718 - acc: 0.8560 - val_loss: 0.2562 - val_acc: 0.9257\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8589\n",
      "Epoch 00026: val_loss improved from 0.25618 to 0.25293, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/026-0.2529.hdf5\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4595 - acc: 0.8590 - val_loss: 0.2529 - val_acc: 0.9248\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4470 - acc: 0.8628\n",
      "Epoch 00027: val_loss did not improve from 0.25293\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4469 - acc: 0.8628 - val_loss: 0.2593 - val_acc: 0.9229\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8661\n",
      "Epoch 00028: val_loss improved from 0.25293 to 0.24515, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/028-0.2451.hdf5\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4339 - acc: 0.8661 - val_loss: 0.2451 - val_acc: 0.9313\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8676\n",
      "Epoch 00029: val_loss improved from 0.24515 to 0.24368, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/029-0.2437.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4285 - acc: 0.8675 - val_loss: 0.2437 - val_acc: 0.9285\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.8712\n",
      "Epoch 00030: val_loss did not improve from 0.24368\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4219 - acc: 0.8712 - val_loss: 0.2444 - val_acc: 0.9301\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.8748\n",
      "Epoch 00031: val_loss improved from 0.24368 to 0.23860, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/031-0.2386.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.4150 - acc: 0.8748 - val_loss: 0.2386 - val_acc: 0.9299\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8770\n",
      "Epoch 00032: val_loss improved from 0.23860 to 0.22665, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/032-0.2267.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4035 - acc: 0.8770 - val_loss: 0.2267 - val_acc: 0.9345\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8790\n",
      "Epoch 00033: val_loss did not improve from 0.22665\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.3944 - acc: 0.8790 - val_loss: 0.2374 - val_acc: 0.9317\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8801\n",
      "Epoch 00034: val_loss did not improve from 0.22665\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.3923 - acc: 0.8801 - val_loss: 0.2312 - val_acc: 0.9317\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3857 - acc: 0.8820\n",
      "Epoch 00035: val_loss did not improve from 0.22665\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.3857 - acc: 0.8820 - val_loss: 0.2539 - val_acc: 0.9294\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8836\n",
      "Epoch 00036: val_loss did not improve from 0.22665\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.3757 - acc: 0.8836 - val_loss: 0.2336 - val_acc: 0.9311\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3753 - acc: 0.8845\n",
      "Epoch 00037: val_loss improved from 0.22665 to 0.21850, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/037-0.2185.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.3753 - acc: 0.8845 - val_loss: 0.2185 - val_acc: 0.9366\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8868\n",
      "Epoch 00038: val_loss improved from 0.21850 to 0.21263, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/038-0.2126.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.3652 - acc: 0.8867 - val_loss: 0.2126 - val_acc: 0.9406\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8887\n",
      "Epoch 00039: val_loss did not improve from 0.21263\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.3646 - acc: 0.8887 - val_loss: 0.2195 - val_acc: 0.9376\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8913\n",
      "Epoch 00040: val_loss did not improve from 0.21263\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.3508 - acc: 0.8913 - val_loss: 0.2206 - val_acc: 0.9371\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8940\n",
      "Epoch 00041: val_loss did not improve from 0.21263\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.3510 - acc: 0.8940 - val_loss: 0.2279 - val_acc: 0.9392\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3488 - acc: 0.8938\n",
      "Epoch 00042: val_loss improved from 0.21263 to 0.20660, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/042-0.2066.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.3488 - acc: 0.8938 - val_loss: 0.2066 - val_acc: 0.9397\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8973\n",
      "Epoch 00043: val_loss improved from 0.20660 to 0.20170, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/043-0.2017.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.3371 - acc: 0.8973 - val_loss: 0.2017 - val_acc: 0.9429\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8958\n",
      "Epoch 00044: val_loss did not improve from 0.20170\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.3408 - acc: 0.8958 - val_loss: 0.2127 - val_acc: 0.9418\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.8982\n",
      "Epoch 00045: val_loss did not improve from 0.20170\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.3316 - acc: 0.8981 - val_loss: 0.2228 - val_acc: 0.9348\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.8989\n",
      "Epoch 00046: val_loss did not improve from 0.20170\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.3295 - acc: 0.8988 - val_loss: 0.2400 - val_acc: 0.9248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8997\n",
      "Epoch 00047: val_loss did not improve from 0.20170\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.3266 - acc: 0.8997 - val_loss: 0.2827 - val_acc: 0.9185\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8998\n",
      "Epoch 00048: val_loss improved from 0.20170 to 0.20057, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/048-0.2006.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.3241 - acc: 0.8998 - val_loss: 0.2006 - val_acc: 0.9392\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.9010\n",
      "Epoch 00049: val_loss did not improve from 0.20057\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.3173 - acc: 0.9010 - val_loss: 0.2287 - val_acc: 0.9329\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.9039\n",
      "Epoch 00050: val_loss did not improve from 0.20057\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.3146 - acc: 0.9040 - val_loss: 0.2300 - val_acc: 0.9331\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9055\n",
      "Epoch 00051: val_loss improved from 0.20057 to 0.19590, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/051-0.1959.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.3088 - acc: 0.9055 - val_loss: 0.1959 - val_acc: 0.9462\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.9068\n",
      "Epoch 00052: val_loss improved from 0.19590 to 0.19014, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/052-0.1901.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.3055 - acc: 0.9068 - val_loss: 0.1901 - val_acc: 0.9497\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.9054\n",
      "Epoch 00053: val_loss improved from 0.19014 to 0.18885, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/053-0.1888.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.3055 - acc: 0.9054 - val_loss: 0.1888 - val_acc: 0.9469\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9066\n",
      "Epoch 00054: val_loss did not improve from 0.18885\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.3046 - acc: 0.9066 - val_loss: 0.1910 - val_acc: 0.9474\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9079\n",
      "Epoch 00055: val_loss improved from 0.18885 to 0.18044, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/055-0.1804.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.3008 - acc: 0.9078 - val_loss: 0.1804 - val_acc: 0.9492\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2981 - acc: 0.9093\n",
      "Epoch 00056: val_loss did not improve from 0.18044\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2981 - acc: 0.9094 - val_loss: 0.2182 - val_acc: 0.9357\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9094\n",
      "Epoch 00057: val_loss improved from 0.18044 to 0.18014, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/057-0.1801.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.2933 - acc: 0.9094 - val_loss: 0.1801 - val_acc: 0.9481\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9108\n",
      "Epoch 00058: val_loss did not improve from 0.18014\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.2889 - acc: 0.9108 - val_loss: 0.1887 - val_acc: 0.9457\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9125\n",
      "Epoch 00059: val_loss improved from 0.18014 to 0.17868, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/059-0.1787.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2839 - acc: 0.9125 - val_loss: 0.1787 - val_acc: 0.9502\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9140\n",
      "Epoch 00060: val_loss did not improve from 0.17868\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2804 - acc: 0.9140 - val_loss: 0.1831 - val_acc: 0.9483\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9127\n",
      "Epoch 00061: val_loss did not improve from 0.17868\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.2833 - acc: 0.9128 - val_loss: 0.2134 - val_acc: 0.9364\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.9141\n",
      "Epoch 00062: val_loss improved from 0.17868 to 0.17718, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/062-0.1772.hdf5\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.2759 - acc: 0.9141 - val_loss: 0.1772 - val_acc: 0.9474\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.9141\n",
      "Epoch 00063: val_loss improved from 0.17718 to 0.17345, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/063-0.1734.hdf5\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.2770 - acc: 0.9141 - val_loss: 0.1734 - val_acc: 0.9522\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9143\n",
      "Epoch 00064: val_loss did not improve from 0.17345\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.2768 - acc: 0.9143 - val_loss: 0.1799 - val_acc: 0.9467\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9157\n",
      "Epoch 00065: val_loss did not improve from 0.17345\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2728 - acc: 0.9157 - val_loss: 0.2006 - val_acc: 0.9418\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9168\n",
      "Epoch 00066: val_loss improved from 0.17345 to 0.16959, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/066-0.1696.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2686 - acc: 0.9169 - val_loss: 0.1696 - val_acc: 0.9520\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9187\n",
      "Epoch 00067: val_loss did not improve from 0.16959\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2635 - acc: 0.9187 - val_loss: 0.2110 - val_acc: 0.9390\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9165\n",
      "Epoch 00068: val_loss did not improve from 0.16959\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2691 - acc: 0.9165 - val_loss: 0.1728 - val_acc: 0.9495\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2594 - acc: 0.9198\n",
      "Epoch 00069: val_loss did not improve from 0.16959\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.2594 - acc: 0.9198 - val_loss: 0.1761 - val_acc: 0.9488\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.9210\n",
      "Epoch 00070: val_loss did not improve from 0.16959\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.2537 - acc: 0.9210 - val_loss: 0.1752 - val_acc: 0.9495\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9204\n",
      "Epoch 00071: val_loss did not improve from 0.16959\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2542 - acc: 0.9204 - val_loss: 0.1724 - val_acc: 0.9518\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9195\n",
      "Epoch 00072: val_loss improved from 0.16959 to 0.16822, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/072-0.1682.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.2581 - acc: 0.9195 - val_loss: 0.1682 - val_acc: 0.9499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9214\n",
      "Epoch 00073: val_loss did not improve from 0.16822\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.2524 - acc: 0.9214 - val_loss: 0.1743 - val_acc: 0.9481\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9229\n",
      "Epoch 00074: val_loss did not improve from 0.16822\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.2492 - acc: 0.9229 - val_loss: 0.1723 - val_acc: 0.9474\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9221\n",
      "Epoch 00075: val_loss did not improve from 0.16822\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.2531 - acc: 0.9221 - val_loss: 0.2536 - val_acc: 0.9245\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9248\n",
      "Epoch 00076: val_loss did not improve from 0.16822\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.2460 - acc: 0.9248 - val_loss: 0.1814 - val_acc: 0.9495\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9221\n",
      "Epoch 00077: val_loss improved from 0.16822 to 0.16669, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/077-0.1667.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.2477 - acc: 0.9221 - val_loss: 0.1667 - val_acc: 0.9511\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9245\n",
      "Epoch 00078: val_loss did not improve from 0.16669\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.2418 - acc: 0.9245 - val_loss: 0.1691 - val_acc: 0.9497\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9237\n",
      "Epoch 00079: val_loss did not improve from 0.16669\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2446 - acc: 0.9237 - val_loss: 0.1719 - val_acc: 0.9499\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9262\n",
      "Epoch 00080: val_loss did not improve from 0.16669\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.2400 - acc: 0.9262 - val_loss: 0.1832 - val_acc: 0.9462\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9279\n",
      "Epoch 00081: val_loss did not improve from 0.16669\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2330 - acc: 0.9279 - val_loss: 0.1775 - val_acc: 0.9478\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9255\n",
      "Epoch 00082: val_loss did not improve from 0.16669\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2361 - acc: 0.9255 - val_loss: 0.1730 - val_acc: 0.9492\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9285\n",
      "Epoch 00083: val_loss did not improve from 0.16669\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.2327 - acc: 0.9285 - val_loss: 0.1849 - val_acc: 0.9464\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9265\n",
      "Epoch 00084: val_loss did not improve from 0.16669\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2315 - acc: 0.9265 - val_loss: 0.1703 - val_acc: 0.9490\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9277\n",
      "Epoch 00085: val_loss improved from 0.16669 to 0.16166, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/085-0.1617.hdf5\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.2292 - acc: 0.9276 - val_loss: 0.1617 - val_acc: 0.9543\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9263\n",
      "Epoch 00086: val_loss did not improve from 0.16166\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2297 - acc: 0.9263 - val_loss: 0.1654 - val_acc: 0.9518\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9274\n",
      "Epoch 00087: val_loss did not improve from 0.16166\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.2318 - acc: 0.9274 - val_loss: 0.1636 - val_acc: 0.9522\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9290\n",
      "Epoch 00088: val_loss did not improve from 0.16166\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2241 - acc: 0.9290 - val_loss: 0.1728 - val_acc: 0.9504\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9306\n",
      "Epoch 00089: val_loss improved from 0.16166 to 0.15666, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/089-0.1567.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.2229 - acc: 0.9306 - val_loss: 0.1567 - val_acc: 0.9555\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9280\n",
      "Epoch 00090: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.2247 - acc: 0.9280 - val_loss: 0.1738 - val_acc: 0.9485\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9315\n",
      "Epoch 00091: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.2192 - acc: 0.9314 - val_loss: 0.1625 - val_acc: 0.9534\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9309\n",
      "Epoch 00092: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2195 - acc: 0.9309 - val_loss: 0.1768 - val_acc: 0.9492\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9318\n",
      "Epoch 00093: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.2177 - acc: 0.9318 - val_loss: 0.1971 - val_acc: 0.9401\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9298\n",
      "Epoch 00094: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.2218 - acc: 0.9298 - val_loss: 0.1685 - val_acc: 0.9504\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9322\n",
      "Epoch 00095: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.2164 - acc: 0.9322 - val_loss: 0.1830 - val_acc: 0.9474\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9337\n",
      "Epoch 00096: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2132 - acc: 0.9337 - val_loss: 0.1749 - val_acc: 0.9511\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9336\n",
      "Epoch 00097: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.2100 - acc: 0.9335 - val_loss: 0.1761 - val_acc: 0.9469\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9312\n",
      "Epoch 00098: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.2170 - acc: 0.9312 - val_loss: 0.1626 - val_acc: 0.9532\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9323\n",
      "Epoch 00099: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2144 - acc: 0.9323 - val_loss: 0.1613 - val_acc: 0.9536\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9328\n",
      "Epoch 00100: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.2147 - acc: 0.9328 - val_loss: 0.1627 - val_acc: 0.9504\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9336\n",
      "Epoch 00101: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.2113 - acc: 0.9336 - val_loss: 0.1643 - val_acc: 0.9532\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9358\n",
      "Epoch 00102: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2070 - acc: 0.9358 - val_loss: 0.1717 - val_acc: 0.9469\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9357\n",
      "Epoch 00103: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2062 - acc: 0.9357 - val_loss: 0.1683 - val_acc: 0.9495\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9368\n",
      "Epoch 00104: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.2042 - acc: 0.9368 - val_loss: 0.1616 - val_acc: 0.9518\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9339\n",
      "Epoch 00105: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.2043 - acc: 0.9338 - val_loss: 0.1660 - val_acc: 0.9485\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9348\n",
      "Epoch 00106: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2049 - acc: 0.9348 - val_loss: 0.1857 - val_acc: 0.9467\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9364\n",
      "Epoch 00107: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.2042 - acc: 0.9363 - val_loss: 0.1622 - val_acc: 0.9506\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9360\n",
      "Epoch 00108: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.2024 - acc: 0.9360 - val_loss: 0.1630 - val_acc: 0.9518\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9372\n",
      "Epoch 00109: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.2007 - acc: 0.9372 - val_loss: 0.1638 - val_acc: 0.9497\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9385\n",
      "Epoch 00110: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1971 - acc: 0.9385 - val_loss: 0.1654 - val_acc: 0.9518\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9368\n",
      "Epoch 00111: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.2005 - acc: 0.9368 - val_loss: 0.1642 - val_acc: 0.9513\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9381\n",
      "Epoch 00112: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1935 - acc: 0.9381 - val_loss: 0.2030 - val_acc: 0.9406\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9373\n",
      "Epoch 00113: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.1952 - acc: 0.9373 - val_loss: 0.1731 - val_acc: 0.9509\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9392\n",
      "Epoch 00114: val_loss did not improve from 0.15666\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1931 - acc: 0.9391 - val_loss: 0.1583 - val_acc: 0.9511\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9386\n",
      "Epoch 00115: val_loss improved from 0.15666 to 0.15390, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/115-0.1539.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1893 - acc: 0.9386 - val_loss: 0.1539 - val_acc: 0.9527\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9410\n",
      "Epoch 00116: val_loss did not improve from 0.15390\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1866 - acc: 0.9410 - val_loss: 0.1617 - val_acc: 0.9522\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9390\n",
      "Epoch 00117: val_loss did not improve from 0.15390\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1882 - acc: 0.9390 - val_loss: 0.1610 - val_acc: 0.9509\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9407\n",
      "Epoch 00118: val_loss improved from 0.15390 to 0.15318, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv_checkpoint/118-0.1532.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1861 - acc: 0.9406 - val_loss: 0.1532 - val_acc: 0.9546\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9392\n",
      "Epoch 00119: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1891 - acc: 0.9392 - val_loss: 0.1667 - val_acc: 0.9504\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9405\n",
      "Epoch 00120: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1890 - acc: 0.9405 - val_loss: 0.1664 - val_acc: 0.9506\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9415\n",
      "Epoch 00121: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1843 - acc: 0.9415 - val_loss: 0.1753 - val_acc: 0.9483\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9420\n",
      "Epoch 00122: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1825 - acc: 0.9420 - val_loss: 0.1552 - val_acc: 0.9539\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9409\n",
      "Epoch 00123: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1856 - acc: 0.9409 - val_loss: 0.1717 - val_acc: 0.9492\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9414\n",
      "Epoch 00124: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1838 - acc: 0.9414 - val_loss: 0.1592 - val_acc: 0.9529\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9438\n",
      "Epoch 00125: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1794 - acc: 0.9438 - val_loss: 0.1869 - val_acc: 0.9436\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9421\n",
      "Epoch 00126: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1792 - acc: 0.9420 - val_loss: 0.1620 - val_acc: 0.9522\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9419\n",
      "Epoch 00127: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1837 - acc: 0.9419 - val_loss: 0.1590 - val_acc: 0.9522\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9413\n",
      "Epoch 00128: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1800 - acc: 0.9413 - val_loss: 0.1578 - val_acc: 0.9525\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9432\n",
      "Epoch 00129: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1781 - acc: 0.9432 - val_loss: 0.1720 - val_acc: 0.9490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9440\n",
      "Epoch 00130: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1760 - acc: 0.9441 - val_loss: 0.1618 - val_acc: 0.9488\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9439\n",
      "Epoch 00131: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1766 - acc: 0.9439 - val_loss: 0.1686 - val_acc: 0.9474\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9434\n",
      "Epoch 00132: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1742 - acc: 0.9434 - val_loss: 0.1762 - val_acc: 0.9469\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9432\n",
      "Epoch 00133: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1755 - acc: 0.9432 - val_loss: 0.1788 - val_acc: 0.9436\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9438\n",
      "Epoch 00134: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1724 - acc: 0.9438 - val_loss: 0.1670 - val_acc: 0.9499\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9443\n",
      "Epoch 00135: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1698 - acc: 0.9443 - val_loss: 0.1574 - val_acc: 0.9518\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9451\n",
      "Epoch 00136: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1692 - acc: 0.9450 - val_loss: 0.1600 - val_acc: 0.9562\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1740 - acc: 0.9463\n",
      "Epoch 00137: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1740 - acc: 0.9463 - val_loss: 0.1579 - val_acc: 0.9534\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9452\n",
      "Epoch 00138: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1688 - acc: 0.9452 - val_loss: 0.1828 - val_acc: 0.9450\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9454\n",
      "Epoch 00139: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 829us/sample - loss: 0.1703 - acc: 0.9454 - val_loss: 0.1637 - val_acc: 0.9497\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9447\n",
      "Epoch 00140: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1705 - acc: 0.9447 - val_loss: 0.1556 - val_acc: 0.9557\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9472\n",
      "Epoch 00141: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1670 - acc: 0.9472 - val_loss: 0.1627 - val_acc: 0.9520\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9456\n",
      "Epoch 00142: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1698 - acc: 0.9456 - val_loss: 0.1678 - val_acc: 0.9478\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9462\n",
      "Epoch 00143: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1671 - acc: 0.9462 - val_loss: 0.1651 - val_acc: 0.9527\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9481\n",
      "Epoch 00144: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1632 - acc: 0.9481 - val_loss: 0.1640 - val_acc: 0.9527\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9462\n",
      "Epoch 00145: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1684 - acc: 0.9462 - val_loss: 0.1777 - val_acc: 0.9462\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9468\n",
      "Epoch 00146: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1663 - acc: 0.9468 - val_loss: 0.1778 - val_acc: 0.9481\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9464\n",
      "Epoch 00147: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1650 - acc: 0.9464 - val_loss: 0.1913 - val_acc: 0.9439\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9493\n",
      "Epoch 00148: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1594 - acc: 0.9492 - val_loss: 0.1581 - val_acc: 0.9520\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9479\n",
      "Epoch 00149: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1606 - acc: 0.9479 - val_loss: 0.1624 - val_acc: 0.9509\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9485\n",
      "Epoch 00150: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1607 - acc: 0.9485 - val_loss: 0.1638 - val_acc: 0.9495\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9472\n",
      "Epoch 00151: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1615 - acc: 0.9472 - val_loss: 0.1676 - val_acc: 0.9513\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9470\n",
      "Epoch 00152: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.1639 - acc: 0.9470 - val_loss: 0.1633 - val_acc: 0.9515\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9473\n",
      "Epoch 00153: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1602 - acc: 0.9473 - val_loss: 0.1785 - val_acc: 0.9450\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9474\n",
      "Epoch 00154: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1615 - acc: 0.9475 - val_loss: 0.1568 - val_acc: 0.9522\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9497\n",
      "Epoch 00155: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1556 - acc: 0.9497 - val_loss: 0.1677 - val_acc: 0.9520\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9505\n",
      "Epoch 00156: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1553 - acc: 0.9505 - val_loss: 0.1596 - val_acc: 0.9525\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9492\n",
      "Epoch 00157: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1564 - acc: 0.9492 - val_loss: 0.1704 - val_acc: 0.9511\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9504\n",
      "Epoch 00158: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1540 - acc: 0.9504 - val_loss: 0.1682 - val_acc: 0.9509\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9484\n",
      "Epoch 00159: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1572 - acc: 0.9484 - val_loss: 0.1676 - val_acc: 0.9492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9490\n",
      "Epoch 00160: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1550 - acc: 0.9490 - val_loss: 0.1750 - val_acc: 0.9464\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9508\n",
      "Epoch 00161: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1520 - acc: 0.9508 - val_loss: 0.1638 - val_acc: 0.9506\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9525\n",
      "Epoch 00162: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1489 - acc: 0.9525 - val_loss: 0.1928 - val_acc: 0.9446\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9508\n",
      "Epoch 00163: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1534 - acc: 0.9508 - val_loss: 0.1618 - val_acc: 0.9504\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9494\n",
      "Epoch 00164: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1546 - acc: 0.9494 - val_loss: 0.1681 - val_acc: 0.9539\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9524\n",
      "Epoch 00165: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1469 - acc: 0.9524 - val_loss: 0.1564 - val_acc: 0.9534\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9498\n",
      "Epoch 00166: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1542 - acc: 0.9498 - val_loss: 0.1713 - val_acc: 0.9522\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9521\n",
      "Epoch 00167: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1486 - acc: 0.9521 - val_loss: 0.1743 - val_acc: 0.9464\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9526\n",
      "Epoch 00168: val_loss did not improve from 0.15318\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1455 - acc: 0.9526 - val_loss: 0.1820 - val_acc: 0.9478\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEKCAYAAAD0Luk/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8XGW9+PHP98yafWvapumSFkrpnm5YfpWCosgiBUWoCIIL8PPi5crFH9eiqPVeuaKgIohiURSUVRYRQUCUUlC2trRQaKF0o2mbNkmbffbz/P54JmnSJmmSZrJMvu/X67xmeubMOd85nZzvPMt5HjHGoJRSanhyBjoApZRSA0eTgFJKDWOaBJRSahjTJKCUUsOYJgGllBrGNAkopdQwpklAKaWGMU0CSik1jHlTuXMR2Q40AAkgboyZn8rjKaWU6pmUJoGkjxhjqruz4YgRI0xZWVmKw1FKqfSxZs2aamNMcW/f3x9JoNvKyspYvXr1QIehlFJDhojsOJr3p7pNwADPisgaEbkixcdSSinVQ6kuCXzYGLNLREYCfxORTcaYVW03SCaHKwDGjx+f4nCUUkq1ldKSgDFmV/JxH/AYcEIH26wwxsw3xswvLu51tZZSSqleSFlJQESyAMcY05B8fhrw3z3dTywWo6KignA43OcxDgfBYJCxY8fi8/kGOhSl1CCUyuqgUcBjItJynPuMMU/3dCcVFRXk5ORQVlZGcl+qm4wx1NTUUFFRwcSJEwc6HKXUIJSyJGCM2QrMPtr9hMNhTQC9JCIUFRVRVVU10KEopQapIXHHsCaA3tNzp5TqypBIAkcSiewmHq8b6DCUUmrISYskEI1WEo/Xp2TftbW1/OIXv+jVe88880xqa2u7vf3y5cu5+eabe3UspZTqjbRIAiDY+9L6XldJIB6Pd/nep556ivz8/FSEpZRSfSItkoCIQ6qSwLJly9iyZQvl5eVce+21rFy5kpNOOoklS5Ywbdo0AM4991zmzZvH9OnTWbFiRet7y8rKqK6uZvv27UydOpXLL7+c6dOnc9pppxEKhbo87rp161i4cCGzZs3iU5/6FAcOHADg1ltvZdq0acyaNYvPfvazALzwwguUl5dTXl7OnDlzaGhoSMm5UEqln0E1dtCRbN58NY2N6w5bn0g0IeLBcYI93md2djmTJ9/S6es33ngjGzZsYN06e9yVK1eydu1aNmzY0Nrt8q677qKwsJBQKMSCBQs477zzKCoqOiT2zdx///3ceeedXHDBBTzyyCNcfPHFnR73kksu4bbbbuPkk0/mO9/5Dt/73ve45ZZbuPHGG9m2bRuBQKC1qunmm2/m9ttvZ9GiRTQ2NhIM9vw8KKWGp7QoCfS3E044oV2/+1tvvZXZs2ezcOFCdu7cyebNmw97z8SJEykvLwdg3rx5bN++vdP919XVUVtby8knnwzApZdeyqpVdrSNWbNmcdFFF/GHP/wBr9fm8EWLFnHNNddw6623Ultb27peKaWOZEhdLTr7xd7UtAHHySAj45h+iSMrK6v1+cqVK3nuued4+eWXyczM5JRTTunw7uZAIND63OPxHLE6qDNPPvkkq1at4oknnuCGG27grbfeYtmyZZx11lk89dRTLFq0iGeeeYbjjz++V/tXSg0vaVIScDDGTcmec3Jyuqxjr6uro6CggMzMTDZt2sQrr7xy1MfMy8ujoKCAF198EYDf//73nHzyybiuy86dO/nIRz7CD3/4Q+rq6mhsbGTLli3MnDmTb3zjGyxYsIBNmzYddQxKqeFhSJUEOpe63kFFRUUsWrSIGTNmcMYZZ3DWWWe1e/3000/njjvuYOrUqUyZMoWFCxf2yXHvvvtuvvKVr9Dc3MykSZP47W9/SyKR4OKLL6aurg5jDP/xH/9Bfn4+3/72t3n++edxHIfp06dzxhln9EkMSqn0J8ak5uLZG/PnzzeHTiqzceNGpk6d2uX7mps3AUJm5pQURjd0deccKqWGJhFZczRT96ZJdZAwmJKZUkoNFWmSBBwgNW0CSimVztIiCdhB0rQkoJRSPZUWSUCrg5RSqnfSJAmkbtgIpZRKZ2mSBARtE1BKqZ5LiyQw2NoEsrOze7ReKaUGSlokAXvH8OBJAkopNVSkSRJIXXXQsmXLuP3221v/3TLxS2NjI6eeeipz585l5syZPP74493epzGGa6+9lhkzZjBz5kwefPBBAPbs2cPixYspLy9nxowZvPjiiyQSCb7whS+0bvvTn/60zz+jUmr4GlrDRlx9Naw7fChpvxvBa6IYTw49nlG3vBxu6Xwo6aVLl3L11Vfz1a9+FYCHHnqIZ555hmAwyGOPPUZubi7V1dUsXLiQJUuWdGtO30cffZR169axfv16qqurWbBgAYsXL+a+++7jE5/4BN/61rdIJBI0Nzezbt06du3axYYNGwB6NFOZUkodydBKAp1K3WTqc+bMYd++fezevZuqqioKCgoYN24csViMb37zm6xatQrHcdi1axd79+5l9OjRR9znSy+9xIUXXojH42HUqFGcfPLJvP766yxYsIAvfelLxGIxzj33XMrLy5k0aRJbt27lqquu4qyzzuK0005L2WdVSg0/QysJdPKLPRapJBqtIDt7Doinzw97/vnn8/DDD1NZWcnSpUsBuPfee6mqqmLNmjX4fD7Kyso6HEK6JxYvXsyqVat48skn+cIXvsA111zDJZdcwvr163nmmWe44447eOihh7jrrrv64mMppVR6tAm0VMGkqnF46dKlPPDAAzz88MOcf/75gB1CeuTIkfh8Pp5//nl27NjR7f2ddNJJPPjggyQSCaqqqli1ahUnnHACO3bsYNSoUVx++eVcdtllrF27lurqalzX5bzzzuP73/8+a9euTclnVEoNT0OrJNCpluqg1CSB6dOn09DQQGlpKSUlJQBcdNFFnH322cycOZP58+f3aBKXT33qU7z88svMnj0bEeFHP/oRo0eP5u677+amm27C5/ORnZ3NPffcw65du/jiF7+I69qG7x/84Acp+YxKqeEpLYaSjkariER2kJU1C8fxpzLEIUmHklYqfelQ0oCI/Ripml1MKaXSVVokgVRXBymlVLrSJKCUUsNYWiSBluogHUROKaV6Ji2SQEtJYDA1ciul1FCQVklAq4OUUqpn0iIJHByvp++TQG1tLb/4xS969d4zzzxTx/pRSg1qKU8CIuIRkTdE5C+pO0rquoh2lQTi8XiX733qqafIz8/v85iUUqqv9EdJ4GvAxtQeInUlgWXLlrFlyxbKy8u59tprWblyJSeddBJLlixh2rRpAJx77rnMmzeP6dOns2LFitb3lpWVUV1dzfbt25k6dSqXX34506dP57TTTiMUCh12rCeeeIIPfehDzJkzh4997GPs3bsXgMbGRr74xS8yc+ZMZs2axSOPPALA008/zdy5c5k9ezannnpqn392pVT6S+mwESIyFjgLuAG45mj318lI0hgTwHWn4DhBujGScztHGEmaG2+8kQ0bNrAueeCVK1eydu1aNmzYwMSJEwG46667KCwsJBQKsWDBAs477zyKiora7Wfz5s3cf//93HnnnVxwwQU88sgjXHzxxe22+fCHP8wrr7yCiPDrX/+aH/3oR/z4xz/mf/7nf8jLy+Ott94C4MCBA1RVVXH55ZezatUqJk6cyP79+3v2wZVSitSPHXQL8F9ATioPcvDC3z8NwyeccEJrAgC49dZbeeyxxwDYuXMnmzdvPiwJTJw4kfLycgDmzZvH9u3bD9tvRUUFS5cuZc+ePUSj0dZjPPfcczzwwAOt2xUUFPDEE0+wePHi1m0KCwv79DMqpYaHlCUBEfkksM8Ys0ZETuliuyuAKwDGjx/f5T47+8Xuugmamt4lEBiP3z+ytyF3W1ZWVuvzlStX8txzz/Hyyy+TmZnJKaec0uGQ0oFAoPW5x+PpsDroqquu4pprrmHJkiWsXLmS5cuXpyR+pZRqkco2gUXAEhHZDjwAfFRE/nDoRsaYFcaY+caY+cXFxb08VOraBHJycmhoaOj09bq6OgoKCsjMzGTTpk288sorvT5WXV0dpaWlANx9992t6z/+8Y+3m+LywIEDLFy4kFWrVrFt2zYArQ5SSvVKypKAMeY6Y8xYY0wZ8FngH8aYi4/wtl5J5XwCRUVFLFq0iBkzZnDttdce9vrpp59OPB5n6tSpLFu2jIULF/b6WMuXL+f8889n3rx5jBgxonX99ddfz4EDB5gxYwazZ8/m+eefp7i4mBUrVvDpT3+a2bNnt052o5RSPdEvQ0knq4P+nzHmk11t19uhpI0xNDauwe8fQyAw5mjDTTs6lLRS6etoh5Lul0lljDErgZX9cKTUH0IppdJIGt0xLDp2kFJK9VBaJAHLQUsCSinVM2mTBGxpQIeSVkqpnkibJGC7iWpJQCmleiKNkoCjbQJKKdVDaZQEBk91UHZ29kCHoJRS3ZI2ScC2CWhJQCmleiJtkkCquoguW7as3ZANy5cv5+abb6axsZFTTz2VuXPnMnPmTB5//PEj7quzIac7GhK6s+GjlVKqL/XLzWJ95eqnr2ZdZQdjSQOJRDMiguNk9Gif5aPLueX0zseSXrp0KVdffTVf/epXAXjooYd45plnCAaDPPbYY+Tm5lJdXc3ChQtZsmRJm1nODtfRkNOu63Y4JHRHw0crpVRfG1JJoCv22tv3JYE5c+awb98+du/eTVVVFQUFBYwbN45YLMY3v/lNVq1aheM47Nq1i7179zJ69OhO99XRkNNVVVUdDgnd0fDRSinV14ZUEujqF3tz83sYkyArq+/HyDn//PN5+OGHqaysbB2o7d5776Wqqoo1a9bg8/koKyvrcAjpFt0dcloppfpTWrUJpKpheOnSpTzwwAM8/PDDnH/++YAd9nnkyJH4fD6ef/55duzY0eU+OhtyurMhoTsaPloppfpa2iQBkdQNGzF9+nQaGhooLS2lpKQEgIsuuojVq1czc+ZM7rnnHo4//vgu99HZkNOdDQnd0fDRSinV1/plKOnu6u1Q0gCh0FYSiSays2emKrwhS4eSVip9He1Q0mlTEtBhI5RSqufSJgmksjpIKaXS1ZBIAt2rshKMGRzDRgwmg6m6Tyk1+Az6JBAMBqmpqenGxUyrgw5ljKGmpoZgMDjQoSilBqlBf5/A2LFjqaiooKqqqsvt4vEDxOP1BIMb+ymyoSEYDDJ27NiBDkMpNUgN+iTg8/la76btyvbt32P79uWUl7tdDt2glFLqoEFfHdRdIn4AjIkOcCRKKTV0pE0ScBybBFxXk4BSSnVX2iQBkQCgJQGllOqJtEkCB0sCkQGORCmlho60SQItbQJaHaSUUt2XNkmgpSSg1UFKKdV9aZQEbJuAVgcppVT3pU0S0C6iSinVc2mTBLSLqFJK9VzaJIGWLqJaHaSUUt2XNklAG4aVUqrn0iYJaBdRpZTqubRJAi29g4zR6iCllOqulCUBEQmKyGsisl5E3haR76XqWKANw0op1RupHEo6AnzUGNMoIj7gJRH5qzHmlVQczHEyAUgkmlKxe6WUSkspSwLGTgXWmPynL7mkbOovrzcfgHi8NlWHUEqptJPSNgER8YjIOmAf8DdjzKupOpbHkw14iMcPpOoQSimVdlKaBIwxCWNMOTAWOEFEZhy6jYhcISKrRWT1kaaQ7IqI4PMVaBJQSqke6JfeQcaYWuB54PQOXlthjJlvjJlfXFx8VMfxevO1OkgppXoglb2DikUkP/k8A/g4sClVxwPwerUkoJRSPZHK3kElwN0i4sEmm4eMMX9J4fHweguIxTQJKKVUd6Wyd9CbwJxU7b8jXm8+4fCO/jykUkoNaWlzxzBodZBSSvVUWiWBlt5B9hYFpZRSR5JWScDrzceYGK4bGuhQlFJqSEizJFAAoFVCSinVTWmZBLSHkFJKdU+aJQEdP0gppXoizZKAVgcppVRPpFUS8Pk0CSilVE+kVRI4WB2kSUAppbojTZOAtgkopVR3dCsJiMjXRCRXrN+IyFoROS3VwfWUiAePJ1d7BymlVDd1tyTwJWNMPXAaUAB8HrgxZVEdBTuctCYBpZTqju4mAUk+ngn83hjzdpt1g4odP0irg5RSqju6mwTWiMiz2CTwjIjkAG7qwuo9nV1MKaW6r7tDSX8ZKAe2GmOaRaQQ+GLqwuo9rzefUOj9gQ5DKaWGhO6WBE4E3jXG1IrIxcD1QF3qwuo9rQ5SSqnu624S+CXQLCKzga8DW4B7UhbVUdDZxZRSqvu6mwTixg7Sfw7wc2PM7UBO6sLqPa83H9dtwnVjAx2KUkoNet1NAg0ich22a+iTIuIAvtSF1Xs6fpBSSnVfd5PAUiCCvV+gEhgL3JSyqI7CwfGDtF1AKaWOpFtJIHnhvxfIE5FPAmFjzCBtEygCIBarGuBIlFJq8OvusBEXAK8B5wMXAK+KyGdSGVhvBQJjAIhE9gxwJEopNfh19z6BbwELjDH7AESkGHgOeDhVgfWW318CQDS6e4AjUUqpwa+7bQJOSwJIqunBe/uVz1eEiI9oVEsCSil1JN0tCTwtIs8A9yf/vRR4KjUhHR0RB79/NJGIlgSUUupIupUEjDHXish5wKLkqhXGmMdSF9bR8fvHaElAKaW6obslAYwxjwCPpDCWPhMIlNDcvHmgw1BKqUGvy3p9EWkQkfoOlgYRqe+vILtkDJSWwne/27pKSwJKKdU9XZYEjDGDcmiIdkQgGoWqg/cF+P0lxOP7SSTCeDzBAQxOKaUGt0HZw6fHCgrgwMFhIlruFYhGKwcqIqWUGhLSIwnk50PtwWEi/P6WJKA9hJRSqivpkQQOKwm03DCm7QJKKdWV9EgC+fntkkBLSUDvFVBKqa6lLAmIyDgReV5E3hGRt0Xka6k6FgUF7aqD7F3DXi0JKKXUEXT7PoFeiANfN8asTU5Mv0ZE/maMeafPj9RSEjAGRJJ3DZdoSUAppY4gZSUBY8weY8za5PMGYCNQmpKDFRRALAahUOsqv79ESwJKKXUE/dImICJlwBzg1ZQcoMBOJHNoN1HtHaSUUl1LeRIQkWzscBNXG2MOu8tYRK4QkdUisrqqqpcTweTn28d2jcMlOqeAUkodQUqTgIj4sAngXmPMox1tY4xZYYyZb4yZX1xc3LsDtZQE2jQOBwKlxOM1JBLNvdunUkoNA6nsHSTAb4CNxpifpOo4QIclgWBwEgDh8LaUHloppYayVJYEFgGfBz4qIuuSy5kpOVIHJYGMjGMACIW2pOSQSimVDlLWRdQY8xIgqdp/Ox2UBDQJKKXUkaXPHcPQLgl4vYV4PHmaBJRSqgvpkQS8XsjJaVcdJCJkZBxDOKxJQCmlOpMeSQAOGz8IbJWQlgSUUqpz6ZMEDhk/CEiWBLZjTGKAglJKqcEtfZJAByWBYPAYjIkRDu8coKCUUmpwS58kcMicAnCwh5C2CyilVMfSJwkcMrsYaDdRpZQ6kvRJAh2UBAKBUkT8mgSUUqoT6ZME8vOhsRHi8dZVIh6CwYmaBJRSqhPpkwQ6GDoCWrqJvj8AASml1OCX9kkgK2sazc2bcN14B29SSqnhLX2SQAdDRwBkZc3CmAih0OYBCEoppQa39EkCHcwuBpCVNROApqa3+jsipZQa9NInCXRaEpgKeGhsfLP/Y1JKqUEufZLA6NH2cXf7eYUdJ0Bm5hQtCSilVAfSJwkUFUF2NmzffthLWVkzNQkopVQH0icJiEBZGWw7fDrJ7OxZhMPbiMcb+j8upZQaxNInCQBMnNhpSQCgqWlDPweklFKDW/olgW3bwJh2q7OzZwHQ1KSNw0op1VZ6JYGyMjt0xP797VYHAuPxeHK1h5BSSh0ivZLAxIn28ZB2AREhO3sODQ2rByAopZQavIZFEgDIzV1IY+MbJBLhfg5KKaUGr/RKAmVl9rGDxuG8vBMxJkZj49p+DUkppQaz9EoCeXl2+IgOSgI5OR8CoL7+5f6OSimlBq30SgJgSwMdlAQCgdEEg2XU17/S7yEppdRglX5JoKWbaAdyc0+krk5LAkop1SI9k8D27YfdKwC2cTga3UU4XNH/cSml1CCUfkmgrAzCYdi797CXcnNPBLRdQCmlWqRfEjj2WPu4adNhL2Vnz8bjyaWm5ol+DkoppQan9EsC8+bZx9WH3xjmOH5GjfocVVUPE4vVHva6UkoNN+mXBIqLYcIEeP31Dl8uKbkM1w2xb999/RyYUkoNPumXBAAWLOg0CWRnzyU7u5w9e37dz0EppdTgk75JYNs2qKk57CURoaTkchob36ChQe8eVkoNbylLAiJyl4jsE5H+H8R//nz72EG7AMDIkZ/DcYJaGlBKDXupLAn8Djg9hfvvXEvjcCdVQj5fPsXF57N3770kEs39GJhSSg0uKUsCxphVwP4jbpgKeXkwZUqnSQBsA3EiUU9V1cP9GJhSSg0u6dkmALZdoJPqIIC8vJPIyJisVUJKqWFtwJOAiFwhIqtFZHVVVVXf7Xj+fNi92y4dH5eSksuoq3tRZxxTR2SMwXQwFEncjdMUbSISjwxAVMNX3I3THNOq3L7gHegAjDErgBUA8+fPP/yvrLcWLLCPr78O55zT4SYlJZezY8cN7NjxfaZPf6jPDt0brnFxjYvX8bb+uynaRGO0kcZoIzE3Rl4gj2x/NiKCIDjiICK4xqWysZKK+goq6iuoDdeS4c0gy59Fpi+ThJugMdpIU8zurynaRDgexu/xU5BRwPEjjqd8dDmjs0e3xlMbruW5rc8xOns043LHISJsrNrIk5uf5L2a96gJ1eD3+MkN5JJwE0QSEaKJKLmBXM6afBYZ3gye3fosjdFGsv3Z7A/tp6qpioA3wIjMEZwy4RRGZo3khR0vsLF6I7vqd5Hlz2JC3gQm5E0gL5jHa7teY2/TXk6bdBrFWcW89MFLVDdXtztnjdFGQvEQAU+ADF8GGd4Mgt4gmb5MPjbpY1y54Eoe3/Q4t712GwfCB4gmongdLz7HZx89PnyOD5/H/juaiBJ34+QGcgl4AtRH6tnbtJcP6j4g4SbIDeTiGpdIIkIkHiFhEgA44jAxfyIBb4CK+gr8ToAxOWOYOWoWxxVNZl3lG2zev5kcXx7GdahqrsLBw4jgKA5EqtnZuI2oG0EQcv35BD0ZNMTq8Dl+xmVNxicB6qN1NMbqCCWayPJlkxfII8ubh+BQE95LQ6yOmBvFK36CThaucYm7MRImjk8ymJK5iAwnh3dD/6Q+Xo0xtFswIA44IriuPcciLWdb8JksPASodSsIUUM2JfjJpIkqAiaP4tgCwrKfGs9bxCUEAhiDg4+gKSJg8hDjxcGLg88+Gh8BtwDHBKhzttLk7CZOhAQREhLBlQgucXLix5AbnULYU0Wjdzsh7y6MJPAmsgnGRxOIjyTmqSfk3Y2ROI7xkRmZRCBaQsIJIa4Pf2wUEe8+GjM3YCSGuEEcN4i4XlwnhpEorhPBE8sj2DSZuK+WcNZ7GImB8SKuF4wHMV5wWx7t4sSzkVg2Cf9+XF89Uj8eQgWQuxuTvYt45i6MEwHXh7g+HDeIt2k80lRCwsQISDYNvxmYqmnp6NdNn+1cpAz4izFmRne2nz9/vlndRRVOjzQ3Q24uLFsG3/9+p5tt2/Ztduy4gQUL3iIra3r3dx9r5r2a99hZt5MMXwZFGUWs37uejVUbKc0tpTCjkF31u9hZv5OK+gribpwsfxZ14TpqQjV4HS9Bb5CgN0h9pJ71letpijWR7c/GNW7Kf+X4HB8xN9Zu3TEFxzB79GwKg4U89M5D1EfqD3tfhjeDGSNnUJRZRCwRoy5Sh9fxEvAE8DsBdtZXsKnmHQBKssYyMjiGxlgDOd4C8nzFxBIx9oQ+YFuT7TSW4ymiLDiHQu9YQolGqmM7qI7toMndT6lnDhlmBFvclSQIM9qZSb5MIB6HRBwSccHjZuNxM0hIhISESUiIGCEicoDajDdw3ACuEyGzYSZ50Wn4HD/ReJy4iYETwyVGzI3ZiyVxPCaAx/EQpQHXCeNN5OKNjMTTMAE37iXurcdNOLixAG4kiBsLYKJBCDTiK9mIKzES+8eCJwp5O2H0G5BTCfsnQdUM8DWCE4emkfYxuxJCRbD/GIhlgriQcQC8IQjng68Zijbb9ZFcCOfZ7fxNEKiDYJ19rXEURPIg4bfH9jeCcey/Ez7I2A8TXgRvGCo+BHXjO/5iSCfXA3FtLL5mqC+1MWdX2jibiyFrH1K6GgkXQmU5bjgHjM0g4o0iWdUQaAAnARK3n92JgyeCCR4AXzOe+ol4msYhiSCOG0DcAI7xg3GI571HLPc9POHR+Bon4Gsuw4ln4WbsJZGxl3hwL554Lr7QGBzjx3jCRLPfJxbciyeRhXGixAKVeGMFZDXMxjEZGCeM8UQwEsXBj+MGcPAR9+6nOWMz3kQe2eHj8ZggSAIjcfvoxFv/bSSBkRhxTwMxp56AW4jP5BDy7yDq1JIRKyUYHYsvXIrfCeIPxoglYoQSzYT9HxDxV+KTAHmeUbz7vcd79bcsImuMMfN79WZSmARE5H7gFGAEsBf4rjHmN129p0+TAMDs2VBSAk8/3ekmsVgNr7xSRlHRJ5k85R7e3/8+G6s30hBpwDUuCZMgmoiyu2E31c3VeMTD1tqtPL/teSKJw6sAvI6XuBtv/Xd+MJ9xuePweXw0RhvJDeQyInMECTdBOB4mHA8T9AYpH11OUUYRdZE6HHHI9me3W7yOl7pwHU2xJls1ga2ecI0hHod83yiKfGMp8IzFnyhgf0OY/Q1N7G9oJhLyEG/OJtacRbQxm0hjJtGwB3/AEPVWsyf+Drt5nUrfv6jzv0PIv5PC6rMo2vLvhN0mQp7dxGKQqBuDu/UU3EjGIefQLq0K37cXoqqp2J+DHcjZDcEDUD3VXqwOYw6+1xuy+4vktb6anW0Xj8f+Wm27eDwQCECs5CUOTLqTwvpTGLPvUhobHEIh+9sgELAx+3x2Pzk5kJFhxx5sbob8fPvvxsaD23W1JBJQVweOAyNGgN9v18VihrDbhCeRDUBWlj1eVpZ935GI2FgDAbt9IGA/X309NDSA12v/3faxs3U4CVzjQsKH40AweHDfwaBACRvwAAAb0UlEQVTdtrkZolHIzLTHDoXso99vj+/328XjOViCSCTsOueQ/0bXPXyd6nuDNgn0Rp8ngcsug8ceg+rqtuVajDFsrN7IvqZ97KzbyeNv3czqPW+yK+wh7iY63JVHPBRmFGIwFGUUcebkMzlx7IlMyJ9AKBZiX9M+phVPY2rxVKqbqzkQOkBpbinZ/uxOw0skoLYWDhzo/lJfb/8wW5ZwD6dM9nrtBS8QgEjExpCR0X4JBg9eIFqet13nbVOJaMzhF4iePG+5iHo87Revt308YC/IjmMvoHpxUco62iQw4G0CKbVgAfzmN/bu4UmTiCai/Py1n7NizQrerXm3dbPCjEJm5OSweGScU2b8kFklJ1KUUYQjDo44+Dw+RmSOaK2vP5Jcz0iaGkaybhNUVkJVlV327YPNm+H9921eqj+8tqWdQMDOltmyjBkDxx9/8OKYmXn4BbxlfU7OwV+4LY85OfbCK538OB/s8vKOvI1SqmfSPwkAvP46b2U1ccmfLmFd5TpOGn8SXz/x6xxTeAwjs0YyrXgaoeaNrFkznzzvX5hV8lVEuv6pGY/b0arXr7dTF1RX2x6pb7xhn3ckPx+OOQZOOAFGjmx/ge9oycjoeD9KKdVX0jsJzJhBPODjpvU/57vvvUpBRgF/Wvonzjn+8N5CWVnTOeaYn7B585VUVPyMceP+s93ru3fDk0/C2rX2Qr9+ffuqGMeBGTPg3HPtIKalpfaX+6hRdhkxont1wEop1Z/SOwn4/XzhklzuDbzEBVMv4PYzb2dE5ohONx8z5ivs3/80W7d+g9zck1m/fi6vvgovvGDblhMJ26g4Zw7827/ZxzlzYOxYW9Xi8fTjZ1NKqT6Q1knghe0vcG9pDde9BP/7pZuhiwQA9gay7Ozfctttv+bCC0ewZ49dX1YG114Ln/+8rZPXRkmlVLpI28tZwk3wtae/xvisMVz/AraBuAsbN8LSpXDssYXceed/MWbMNr73va/ywQfvs20b/OAHMG2aJgClVHpJy0uaa1yu+/t1rN+7npvPuIXMj37CJoF4/LBt6+ps1c6MGfDUU/D1r9sePCtX5vPRjz7Itm3l7Nnzu/7/EEop1Q/SLgk0x5o554FzuOlfN/HlOV/mM9M+A1dcARUVh900tnKl/XW/YgVcdRVs3Qo/+pGdqz47ezbz568jN3cB7777RbZuvb7DsWOUUmooS7skcMfqO/jLe3/htjNu486z70RE4OyzYfRo+MUvWrf77W/h4x+3Db2vvgq33GKnJ24rGBzL7NnPUVJyBR98cANbtnwd95ChFpRSaihLqyRgjOHOtXdy4tgT+fcT/t0mALB9M6+8Ev76V9y3N3LddfClL8Epp8DLLx+ciKwjIh6OO+4OSkuvoqLip6xeXc6BA8/3y+dRSqlUS6sk8M+d/2RT9SYun3v54S9+5SskApl8bkkjN94I//f/2jaA/Pwj71dEOPbYnzFjxp9x3RDr13+UzZuv0lnJlFJDXlolgTvX3kluIJcLpl9w+IvFxfx49j08uHUBP7i+kV/+smc3b4kII0aczYIFb1Na+jV27fo5r712PHv2/A5jOh5vSCmlBru0SQL1kXr++PYf+dyMz5Hlzzrs9TVr4Po3Ps1n+CPfePtSJBbt1XE8ngwmT76F8vKV+P2jePfdL/Lyy+PYsuVawuEPjvZjKKVUv0qbJPDm3jcJxUOcPeXsw16LROyNXiNHCr/6nyrksUfhvPPsC72Un38yc+e+xowZfyInZwEVFbfw6qvH8t57V9LU9PbRfBSllOo3aXPH8KbqTQBMHTH1sNduusneDPbUU1B4xpVQJLah+NJL4b77en0HmK0iOocRI84hHP6AHTtuYM+eX7N79y/JyppNTs48Cgo+RnHx+TjdHIFUKaX6U9pcmd6tfpegN8j4vPYzJr3/vp1Y7IIL4Iwzkiv/7d/sjBzf+IYd+Oemm456fOVgcDxTpvyKiRO/z969v6em5ilqav5MZeVdbN/+HUaOvJDMzCnk5S0mGBx3VMdSSqm+kjZJYFPNJo4rOg6P034Ut//8Tzsu/y23HPKGa6+FDz6AH//YVgvdckufjADn9xczbtw1jBt3DcYYamr+nJzH+AbATtyanT23tQSRlTXrYFdWpZTqZ+mTBKo3MWf0nHbrXnoJ/vIXuPFGO8tkOyJw66122qof/9hOPHPPPVBY2Gcxta0uct0Izc3vsn//X6mufpzt25ezfft3CQTGUVR0FkVFnyQvbzFeb06fHV8ppY4kLZJAJB5h64GtXDjjwtZ1xsB119mL/1VXdfJGx4Gbb4ZJk+Dqq2HWLPjd7+BjH+vzGB0nQHb2LLKzZzF+/DeIRCqpqfkL+/c/SWXl79m9+w4A/P7RZGRMbl0yM6eQm7uQQODQLKaUUkcvLZLA+/vfxzUux484vnXd00/bksAvf2mnW+zSlVfChz4EF15ox5I44ww7ktxJJ9n5GFMgEBjNmDGXMWbMZbhuhNraF2hoWEso9B6h0GZqap4kFtvbun0weAz5+SeRlTULrzcXv38MmZlTCAbLjjgLmlJKdSYtkkDLfMFtk8CNN9oZvr785W7uZN48ePNNuO02+N//taWB7GyYOROOO679MmWKbWjoI44ToLDwNAoLT2u3Ph6vp6npHerr/0lt7YtUVz9BZeXv2m3j9RaQk3MCXm8eHk8WOTnzyc6ejUiAQGAsgcDoPotTKZV+0iIJtHQPPa7oOMDO9btqFfzkJz2c0jEYtA3GV14Jf/87PPssvP02/O1vcPfdB7cbN842Nsya1Yef4nBeby55eQvJy1vIuHFfxxhDPF5LIlFPOLyT5uZNNDS8SkPDasLhbcTjtVRW/rbNHoTc3IVkZU1HxI/j+HGcLHJzF5KbewLxeD0iDsHgRG2cVmqYksE0PPL8+fPN6tWre/y+Sx67hJXbV/LBf9o7dj/3OTsf8M6ddpTQPtHYaCcaeOcd27W0ocGOSnreeTZ5DALGGMLh7TQ3v4sxMRob11NT8ziRyG6MieK6URKJJqD9MBdebyEZGcfi9eYlSxR5eL25eL2FZGZOTr5WgM9XhMeTqwlDqUFERNYYY7oYBvMI70+HJHDCnSeQF8zjb5//Gzt3wsSJtp335ptTECTY7PLJT9rqo9xc26MoFIJTT7U3JGRn2+6mxcU2mCM2SvSfRCJMff0/aWragNdbiOuGaWh4jXD4A+LxOhKJOuLxeuLxOly36bD3O04WgUApgUApjpOJiAcRBxE/fv9I/P7RrYvPNyr5fCSOk5q2FaWGu6NNAkO+OsgYw6bqTVw6+1LA9voE+I//SOFBx42DtWvhH/+Ahx+2CcAYW0V0333tt83JsQ0TCxdCNGobm8vKUhhc1zyeIAUFp1JQcGqbtW1GXd2zx3aruvZaEsdPSjZUbyORqCMa3UcksotodBeRyG7i8TrAxZgErhshFttHPF7b4XG93qLW5OD15hKP1yHiJStrGj7fyNb9gMHnK8bvH0MgMAa/fwx+/ygAIpEKvN58fL6CVJ0epYadIV8SiLtx7nvrPiYXTmZG/omMHQtnngn335+iILsSDsPrr9uEEItBVZVNDA8+eHBqSxHbA+n4423j8pYtdvslS+DEE23JIicHsrK6vou5vh7+8AeYPNk2YvdFFc2uXfDRj8J778FHPmLbRXq430QiTCy2l2i0kmi05bH9Eo/X4/Xm4bphmps34rqhI+xVkou92S4z83gyM6cRDE5AxIPrholG92FMtE3yKMGNhwk8/A8SHz+JwNi5raO9BoNl+HwFxOO1OE4mPt8IreJSQ5ZWB7Xxs5/ZaqDXXoMFC/owsKNVVQXV1ZBIwB//aJPCnj02aUycCM3NtoqpLRFbrVRYCB/+sO2V9PLLdj9lZbYUsn+/3XbuXLjsMvjMZw5OjxaJ2G0jEdixw46fcfLJdj9tGWMT0X33wR132LaO88+3U689+6ztYvXwwzbG6dPhs589cmJo+U5148JqjIvrRpNVSvaO7Wi0imh0N5HI7tZHSBAMlhGN7qW+/lVCoc3JUVvd1qooET/R6B7icXteiv8B0/8Hqk+EDf/beQweTw7B4CSCwfG4bphEogHHycLrzcHjycXrzcXjyQEE1w3juhEgQUbGsWRkTMHjyUq+1oTjZBEMluHxZOC6MYyJYkwM140h4iRLQgWadFSf0SSQFI/bH8XjxtmeQUOKMXas602b7EW4sdE+NjTA7t3w/PM2kUyfDqWl9qI9ZQp8+9u299JPfmIbrAFGjrTJY/t2cN32x3Ece4E/4QR7/8O//mVP1q5dB0soN9xwsFus69rjRiL2dWPscKy3325LKx3Ztw8+/WlbRfb443ZspkOFw7B8uZ3U+Ze/hKKiw19/7z0bh4htgN+5E/77v7vV3SuRCBNt+oDA3E8guyqRcJiGe5cTP+MkjHEJh7eRSNTj9eaTSDQSCm0lHN5KOPwBHk8mHk8OiUQziUQ98Xg9iUQ9iUQDACIBHMd2BIjHa44YCwY8YUhkHFxlu++W4PONxOvNJxrdTSi0FcfxJ5ONndHO683H6y1o9+g4GTiODxE/Il5sCcnenS4SSCas3NZHjyeDWOwAxsTI2u3De8DFXTgHEDyeTL3HJA1oEki69164+GL405/gnHP6OLCB5ro2MXTW1ckY20j917/aBNHQYC/ipaW2yqm01F6Mf/c7+NWvoK7Ovq+kxJYOFi+2N8i1bav4/e/hkkts6eLWW2HUKDsS3/Ll9nglJXZaNr/flhKMscnllVdsKcfrtfGecw5s2GBLPPPm2WqsBx+Et96y20yYYItwpaU24bz2Gvz851BZCWedBVOnHmzhP/10+MEPbPwej60yy8y0CWnkSJswPvjAxvPXv9ruvo8+Ctdfb5PSo4/C7Nl2u0TC7mfdOpsIfT4YP94uGRnwwgv2c5x9tj0/HYwrFQ3tJbRvPW6uH3DxNIMbrScUrMENN5Pxj41k//JZ/Gu2EPrsKTRdfS7OqpdhdwWNx3iIZIdxI3Ukjh+Pv3QGxsSTyUYwJkY8Xkc8foB4Q41tk/HUtpZCADxNUPIU+Gph13kQ7WLEk/y1MOM74G2CXUtg61dsYnKcDDyeLBwnE48nK/ncPhqTIB7fjzExQAhE88mqziExZTzi8RGP7seNhfEEc/FXQ9HP1xCfOp7QJR8hRi2xWA0eT7btWSa5eMjCycjBcQI4EsDZU493y25bonXjsPBEPJOm4/UW4PHYrBmP1xON7sH7QR2+J15AzjgDM306xsTblR479P778MYb9u9nwwb7nZg/3/5o6qqzRjQKL75oRxKYOPHw142xS9vRhxsa7NAzxx5rf8RcfbWtGl6xwrYDHqqhwX7fd++2fwt1dbBsWecxdUGTAPbvefp0e717441ejww9PBgDBw5AU5NNDF1VS1RUHP5L/p//tFVR27bZL3I0av+gYjH7mtdrq4+CQZsA9u+HadPsr/6qKruPsjJbmigogHPPtaWHtk49FRYtgh/+0JZCLrvM1u9deaX9z+6IiL1Qt7S9gN3Hiy/a5bTT7L5GjLCPDQ0Ht3Ocw0tNYBNcNGqTzJw59nNWVdnjOI79421qsom0tNQmmUjElmB27LB/2BMn2vaVu+/uPHbHsR0HSkpsMgqHbdIKhWwpbetW+/+WlQUTJ2ImlsGB/fDmBqS+HuM4EAxiTvkwxo1h4hFMIoqJRyERQ1wH75p3iU8qJnTiBHLufhWT6aP5Y1MITy8init4t9fg/eAAhgTxLEPzJB/eugT5LzXiOxADY/DtCSEGmic47F/gUPyiwXfApX6Wj+yNUTwhEBeax0H9VHAzfcSDMXy1MOJf4GmGxslgHMjabhPSocKjoHa2ffQ2CsZj8ERg9NPgxOw2tbOgfhrEcyBzT4DgrgTBXXHECG6mFzfThxOGjG0Hp381DjRPySBzU4josQWE5pXgqWrEqQ/j1EfwNMQAl1iRn8COEN76GMYj1J8+AW/IQ2BzLYmykbj52fhf34ynshbj82ACPozXwVNrj2X8XkxGEGkKY0YWIZVVxOZPxrOrGjOqGE5ajLy+Gs+/1rb/4GPG2L+3XlQTahIAHnjAjvjwxz/aH65qgBzaFtByYXUc+1plpb3wt72vorbWXkyrqmz7x5QpB0f7e/ttWzL4whfsPt94w/66Kyy0+25qskt9vd13NArHHGNLIM3NNpmMSw7bXVVlL9KvvWYv6vn5kJdnS0yLF9vkVVFhSxK1tbaRPi/PTkKxcqXtDZZI2CTiuvZYM2bYz3P//bB3L1x0EYwebZNOaamtevvEJ+y+166F556zVW6TJ9tfpg0N9ty8+KJtf6mrs8k0GLRLRoYt4cyYYRPS3r22pLd9u61CO/ZY+MpXbJzLl9vz5fF0vEyYYH8BFxTAq6/Cr39tz0dLu5LHY0tBjmPbklpKix/6kI0X7LkqLralyfXrbWKdMsV+rgkTMD/7Ke5bbyA/uhmprEYaGzENDZCRgXvmR0mMLsB59Q0QiB8/nviUUhLHjsaMKELiLs7La/D+cz3+V9/D2d+Emx1ADEgkTvOSuez/8kyy/rGFrL++i/+9KiSWIF6UQWx8HvEJRRiPSX4nmjEmTvOiCURPPI6EJ06sEKI5CYIvbGT8f7+PEzNEizwkcrx2yfYh4uCrSRArdKha7JD3RphRf2oiXAyNxxoy9oCvDuqmQ/N4cKIHl0gxhEdD9vsQrIQPLobQGJj0K7suXGLX52607606GUIlEMuHpjJgXCkn/p+KXv3ZDfsk4LoHq47ffFNLAWoA9KAhfFAxxia86mqbLFuSszG2DSYQsNWAHb0vFOr+/S/G9OzcHFrd0tH7o1Fb6uqsbaqvGIPBEItVJRv640ACYxIYE0/2LivEdaPE47W2+i75aEwMv38MPl9RstPCLppq38KfVUIweAyJRF2y88MejIkzYcI3exXisL9PoKnJ/mg7/XRNAGqADLWLfwsRWzIoKDh8/fjxHb+n5fWe3ADZ0/Mj0v49Hb3f70/Z4I6HxiJI670qXfH7R3T5elbW8YfcnzM4pDQJiMjpwM8AD/BrY8yNfX2MnBxbslVKKdVzKfvtLLbZ/nbgDGAacKGITEvV8ZRSSvVcKitQTgDeN8ZsNcZEgQeAdOu8qZRSQ1oqk0Ap0PY22IrkOqWUUoPEgDelisgVIrJaRFZXtfQjV0op1S9SmQR2AePa/Htscl07xpgVxpj5xpj5xS3j3iillOoXqUwCrwOTRWSiiPiBzwJ/TuHxlFJK9VDKuogaY+Ii8u/AM9guoncZY95O1fGUUkr1XErvEzDGPAU8lcpjKKWU6r1BNWyEiFQBO3r59hFAdR+G0x+GYswwNOMeijGDxt2fhmLMAFOMMb0eP2NQDRthjOl1y7CIrD6a8TMGwlCMGYZm3EMxZtC4+9NQjBls3Efz/gHvIqqUUmrgaBJQSqlhLJ2SwIqBDqAXhmLMMDTjHooxg8bdn4ZizHCUcQ+qhmGllFL9K51KAkoppXpoyCcBETldRN4VkfdFpHczNfcDERknIs+LyDsi8raIfC25frmI7BKRdcnlzIGOtS0R2S4ibyVjW51cVygifxORzcnHgiPtpz+JyJQ253OdiNSLyNWD8VyLyF0isk9ENrRZ1+H5FevW5Hf9TRGZO4hivklENiXjekxE8pPry0Qk1Oac3zEQMXcRd6ffCRG5Lnmu3xWRTwyimB9sE+92EVmXXN+7c22MGbIL9k7kLcAkwA+sB6YNdFydxFoCzE0+zwHew86zsBz4fwMdXxdxbwdGHLLuR8Cy5PNlwA8HOs4jfEcqgQmD8VwDi4G5wIYjnV/gTOCvgAALgVcHUcynAd7k8x+2ibms7XaD8Fx3+J1I/m2uBwLAxOR1xjMYYj7k9R8D3zmacz3USwJDZs4CY8weY8za5PMGYCNDd2jtc4C7k8/vBs4dwFiO5FRgizGmtzchppQxZhWw/5DVnZ3fc4B7jPUKkC8iJf0T6UEdxWyMedbYCXgBXsEOGDmodHKuO3MO8IAxJmKM2Qa8j73e9KuuYhYRAS4A7j+aYwz1JDAk5ywQkTJgDvBqctW/J4vRdw22qhXAAM+KyBoRuSK5bpQxZk/yeSVw5AlYB85naf9HMpjPdYvOzu9Q+b5/CVtiaTFRRN4QkRdE5KSBCqoLHX0nhsK5PgnYa4zZ3GZdj8/1UE8CQ46IZAOPAFcbY+qBXwLHAOXAHmzxbjD5sDFmLnaa0K+KyOK2LxpbDh2UXcySo9cuAf6YXDXYz/VhBvP57YiIfAuIA/cmV+0Bxhtj5gDXAPeJSO5AxdeBIfedaONC2v/A6dW5HupJoFtzFgwWIuLDJoB7jTGPAhhj9hpjEsYYF7iTAShydsUYsyv5uA94DBvf3pZqiOTjvoGLsEtnAGuNMXth8J/rNjo7v4P6+y4iXwA+CVyUTF4kq1Nqks/XYOvWjxuwIA/RxXdisJ9rL/Bp4MGWdb0910M9CQyZOQuS9Xe/ATYaY37SZn3bOt1PARsOfe9AEZEsEclpeY5t/NuAPceXJje7FHh8YCI8ona/lAbzuT5EZ+f3z8AlyV5CC4G6NtVGA0pETgf+C1hijGlus75YRDzJ55OAycDWgYnycF18J/4MfFZEAiIyERv3a/0dXxc+BmwyxlS0rOj1ue7v1u4UtJ6fie1pswX41kDH00WcH8YW698E1iWXM4HfA28l1/8ZKBnoWNvEPAnbQ2I98HbL+QWKgL8Dm4HngMKBjrWD2LOAGiCvzbpBd66xSWoPEMPWO3+5s/OL7RV0e/K7/hYwfxDF/D62Dr3lu31Hctvzkt+ddcBa4OxBdq47/U4A30qe63eBMwZLzMn1vwO+csi2vTrXesewUkoNY0O9OkgppdRR0CSglFLDmCYBpZQaxjQJKKXUMKZJQCmlhjFNAkr1ARE5RUT+MtBxKNVTmgSUUmoY0ySghhURuVhEXkuOt/4rEfGISKOI/FTsPA9/F5Hi5LblIvJKmzHyW8b1P1ZEnhOR9SKyVkSOSe4+W0QeTo6rf2/yLnGlBjVNAmrYEJGpwFJgkTGmHEgAF2HvLl5tjJkOvAB8N/mWe4BvGGNmYe8qbVl/L3C7MWY28H+wd3SCHRn2auxY9JOARSn/UEodJe9AB6BUPzoVmAe8nvyRnoEdnM3l4EBcfwAeFZE8IN8Y80Jy/d3AH5NjKZUaYx4DMMaEAZL7e80kx3JJzvZUBryU+o+lVO9pElDDiQB3G2Oua7dS5NuHbNfbsVQibZ4n0L8vNQRodZAaTv4OfEZERkLrXL4TsH8Hn0lu8zngJWNMHXCgzcQcnwdeMHZWuAoROTe5j4CIZPbrp1CqD+kvFTVsGGPeEZHrsTOlOdiRGb8KNAEnJF/bh203ADuM8x3Ji/xW4IvJ9Z8HfiUi/53cx/n9+DGU6lM6iqga9kSk0RiTPdBxKDUQtDpIKaWGMS0JKKXUMKYlAaWUGsY0CSil1DCmSUAppYYxTQJKKTWMaRJQSqlhTJOAUkoNY/8f/f3xMRFou6kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 457us/sample - loss: 0.1886 - acc: 0.9423\n",
      "Loss: 0.1885629646503294 Accuracy: 0.9422638\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.8668 - acc: 0.1655\n",
      "Epoch 00001: val_loss improved from inf to 1.85897, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/001-1.8590.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 3.8667 - acc: 0.1655 - val_loss: 1.8590 - val_acc: 0.4034\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3257 - acc: 0.3394\n",
      "Epoch 00002: val_loss improved from 1.85897 to 1.04933, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/002-1.0493.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 2.3259 - acc: 0.3393 - val_loss: 1.0493 - val_acc: 0.6883\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6934 - acc: 0.4861\n",
      "Epoch 00003: val_loss improved from 1.04933 to 0.73356, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/003-0.7336.hdf5\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 1.6933 - acc: 0.4862 - val_loss: 0.7336 - val_acc: 0.7941\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3062 - acc: 0.5946\n",
      "Epoch 00004: val_loss improved from 0.73356 to 0.59095, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/004-0.5910.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 1.3062 - acc: 0.5946 - val_loss: 0.5910 - val_acc: 0.8318\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0592 - acc: 0.6682\n",
      "Epoch 00005: val_loss improved from 0.59095 to 0.48556, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/005-0.4856.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 1.0593 - acc: 0.6681 - val_loss: 0.4856 - val_acc: 0.8658\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9020 - acc: 0.7187\n",
      "Epoch 00006: val_loss improved from 0.48556 to 0.37855, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/006-0.3785.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.9020 - acc: 0.7187 - val_loss: 0.3785 - val_acc: 0.8942\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7764 - acc: 0.7587\n",
      "Epoch 00007: val_loss improved from 0.37855 to 0.33696, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/007-0.3370.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.7765 - acc: 0.7587 - val_loss: 0.3370 - val_acc: 0.9061\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6968 - acc: 0.7848\n",
      "Epoch 00008: val_loss improved from 0.33696 to 0.33573, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/008-0.3357.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.6968 - acc: 0.7848 - val_loss: 0.3357 - val_acc: 0.9085\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6313 - acc: 0.8045\n",
      "Epoch 00009: val_loss improved from 0.33573 to 0.33564, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/009-0.3356.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.6312 - acc: 0.8046 - val_loss: 0.3356 - val_acc: 0.9024\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.8217\n",
      "Epoch 00010: val_loss improved from 0.33564 to 0.25567, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/010-0.2557.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.5777 - acc: 0.8217 - val_loss: 0.2557 - val_acc: 0.9285\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5381 - acc: 0.8347\n",
      "Epoch 00011: val_loss improved from 0.25567 to 0.23506, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/011-0.2351.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.5382 - acc: 0.8347 - val_loss: 0.2351 - val_acc: 0.9380\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.8486\n",
      "Epoch 00012: val_loss improved from 0.23506 to 0.22642, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/012-0.2264.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.5001 - acc: 0.8486 - val_loss: 0.2264 - val_acc: 0.9392\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4698 - acc: 0.8554\n",
      "Epoch 00013: val_loss improved from 0.22642 to 0.22182, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/013-0.2218.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.4698 - acc: 0.8554 - val_loss: 0.2218 - val_acc: 0.9364\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4428 - acc: 0.8630\n",
      "Epoch 00014: val_loss improved from 0.22182 to 0.20494, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/014-0.2049.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.4428 - acc: 0.8630 - val_loss: 0.2049 - val_acc: 0.9429\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.8695\n",
      "Epoch 00015: val_loss did not improve from 0.20494\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.4185 - acc: 0.8695 - val_loss: 0.2872 - val_acc: 0.9108\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8792\n",
      "Epoch 00016: val_loss did not improve from 0.20494\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.3980 - acc: 0.8792 - val_loss: 0.2088 - val_acc: 0.9401\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8839\n",
      "Epoch 00017: val_loss improved from 0.20494 to 0.20314, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/017-0.2031.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.3820 - acc: 0.8838 - val_loss: 0.2031 - val_acc: 0.9427\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8867\n",
      "Epoch 00018: val_loss improved from 0.20314 to 0.18705, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/018-0.1871.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.3631 - acc: 0.8867 - val_loss: 0.1871 - val_acc: 0.9469\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8930\n",
      "Epoch 00019: val_loss did not improve from 0.18705\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.3491 - acc: 0.8930 - val_loss: 0.1871 - val_acc: 0.9464\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8988\n",
      "Epoch 00020: val_loss improved from 0.18705 to 0.17427, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/020-0.1743.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.3364 - acc: 0.8988 - val_loss: 0.1743 - val_acc: 0.9495\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.8982\n",
      "Epoch 00021: val_loss did not improve from 0.17427\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.3261 - acc: 0.8981 - val_loss: 0.1862 - val_acc: 0.9476\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.9021\n",
      "Epoch 00022: val_loss improved from 0.17427 to 0.17196, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/022-0.1720.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.3192 - acc: 0.9021 - val_loss: 0.1720 - val_acc: 0.9515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9063\n",
      "Epoch 00023: val_loss improved from 0.17196 to 0.16852, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/023-0.1685.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.3059 - acc: 0.9063 - val_loss: 0.1685 - val_acc: 0.9525\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9104\n",
      "Epoch 00024: val_loss did not improve from 0.16852\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2923 - acc: 0.9104 - val_loss: 0.1719 - val_acc: 0.9497\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9122\n",
      "Epoch 00025: val_loss improved from 0.16852 to 0.16243, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/025-0.1624.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.2823 - acc: 0.9122 - val_loss: 0.1624 - val_acc: 0.9518\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9160\n",
      "Epoch 00026: val_loss did not improve from 0.16243\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2730 - acc: 0.9160 - val_loss: 0.1652 - val_acc: 0.9515\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2732 - acc: 0.9164\n",
      "Epoch 00027: val_loss did not improve from 0.16243\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2731 - acc: 0.9165 - val_loss: 0.1650 - val_acc: 0.9525\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9181\n",
      "Epoch 00028: val_loss did not improve from 0.16243\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.2642 - acc: 0.9180 - val_loss: 0.1792 - val_acc: 0.9497\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9218\n",
      "Epoch 00029: val_loss did not improve from 0.16243\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2552 - acc: 0.9218 - val_loss: 0.1646 - val_acc: 0.9520\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9252\n",
      "Epoch 00030: val_loss improved from 0.16243 to 0.15114, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/030-0.1511.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2456 - acc: 0.9253 - val_loss: 0.1511 - val_acc: 0.9562\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9267\n",
      "Epoch 00031: val_loss did not improve from 0.15114\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.2393 - acc: 0.9267 - val_loss: 0.1528 - val_acc: 0.9560\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9264\n",
      "Epoch 00032: val_loss did not improve from 0.15114\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2393 - acc: 0.9264 - val_loss: 0.1541 - val_acc: 0.9529\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9290\n",
      "Epoch 00033: val_loss did not improve from 0.15114\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.2294 - acc: 0.9290 - val_loss: 0.1662 - val_acc: 0.9522\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9285\n",
      "Epoch 00034: val_loss did not improve from 0.15114\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2284 - acc: 0.9285 - val_loss: 0.1590 - val_acc: 0.9539\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9321\n",
      "Epoch 00035: val_loss improved from 0.15114 to 0.14513, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/035-0.1451.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.2195 - acc: 0.9321 - val_loss: 0.1451 - val_acc: 0.9564\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9322\n",
      "Epoch 00036: val_loss did not improve from 0.14513\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.2184 - acc: 0.9322 - val_loss: 0.1474 - val_acc: 0.9597\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9357\n",
      "Epoch 00037: val_loss did not improve from 0.14513\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2113 - acc: 0.9357 - val_loss: 0.1501 - val_acc: 0.9548\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9367\n",
      "Epoch 00038: val_loss did not improve from 0.14513\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.2066 - acc: 0.9367 - val_loss: 0.1499 - val_acc: 0.9576\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9381\n",
      "Epoch 00039: val_loss did not improve from 0.14513\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.2006 - acc: 0.9381 - val_loss: 0.1474 - val_acc: 0.9599\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9393\n",
      "Epoch 00040: val_loss improved from 0.14513 to 0.14239, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/040-0.1424.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1993 - acc: 0.9393 - val_loss: 0.1424 - val_acc: 0.9604\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9415\n",
      "Epoch 00041: val_loss did not improve from 0.14239\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1914 - acc: 0.9415 - val_loss: 0.1461 - val_acc: 0.9564\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9394\n",
      "Epoch 00042: val_loss did not improve from 0.14239\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1918 - acc: 0.9394 - val_loss: 0.1616 - val_acc: 0.9548\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9400\n",
      "Epoch 00043: val_loss improved from 0.14239 to 0.14189, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/043-0.1419.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1915 - acc: 0.9400 - val_loss: 0.1419 - val_acc: 0.9569\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9413\n",
      "Epoch 00044: val_loss did not improve from 0.14189\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1900 - acc: 0.9413 - val_loss: 0.1445 - val_acc: 0.9562\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9433\n",
      "Epoch 00045: val_loss improved from 0.14189 to 0.13677, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/045-0.1368.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1795 - acc: 0.9432 - val_loss: 0.1368 - val_acc: 0.9585\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9452\n",
      "Epoch 00046: val_loss did not improve from 0.13677\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1776 - acc: 0.9452 - val_loss: 0.1596 - val_acc: 0.9532\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9456\n",
      "Epoch 00047: val_loss did not improve from 0.13677\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1752 - acc: 0.9456 - val_loss: 0.1370 - val_acc: 0.9581\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9471\n",
      "Epoch 00048: val_loss improved from 0.13677 to 0.13341, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/048-0.1334.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.1723 - acc: 0.9470 - val_loss: 0.1334 - val_acc: 0.9606\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9468\n",
      "Epoch 00049: val_loss did not improve from 0.13341\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1664 - acc: 0.9468 - val_loss: 0.1408 - val_acc: 0.9562\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9493\n",
      "Epoch 00050: val_loss did not improve from 0.13341\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.1639 - acc: 0.9492 - val_loss: 0.1392 - val_acc: 0.9592\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9485\n",
      "Epoch 00051: val_loss did not improve from 0.13341\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1642 - acc: 0.9485 - val_loss: 0.1574 - val_acc: 0.9532\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9501\n",
      "Epoch 00052: val_loss did not improve from 0.13341\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1594 - acc: 0.9501 - val_loss: 0.1652 - val_acc: 0.9457\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9497\n",
      "Epoch 00053: val_loss improved from 0.13341 to 0.13109, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/053-0.1311.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1600 - acc: 0.9497 - val_loss: 0.1311 - val_acc: 0.9599\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9514\n",
      "Epoch 00054: val_loss did not improve from 0.13109\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1581 - acc: 0.9514 - val_loss: 0.1381 - val_acc: 0.9574\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9524\n",
      "Epoch 00055: val_loss improved from 0.13109 to 0.12781, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/055-0.1278.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.1501 - acc: 0.9523 - val_loss: 0.1278 - val_acc: 0.9609\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9519\n",
      "Epoch 00056: val_loss did not improve from 0.12781\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1535 - acc: 0.9519 - val_loss: 0.1399 - val_acc: 0.9576\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9529\n",
      "Epoch 00057: val_loss did not improve from 0.12781\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1483 - acc: 0.9528 - val_loss: 0.1473 - val_acc: 0.9557\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9518\n",
      "Epoch 00058: val_loss did not improve from 0.12781\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1529 - acc: 0.9518 - val_loss: 0.1511 - val_acc: 0.9564\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9561\n",
      "Epoch 00059: val_loss improved from 0.12781 to 0.12287, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/059-0.1229.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1437 - acc: 0.9561 - val_loss: 0.1229 - val_acc: 0.9625\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9561\n",
      "Epoch 00060: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1411 - acc: 0.9561 - val_loss: 0.1451 - val_acc: 0.9550\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9574\n",
      "Epoch 00061: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.1386 - acc: 0.9573 - val_loss: 0.1279 - val_acc: 0.9618\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9555\n",
      "Epoch 00062: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1381 - acc: 0.9555 - val_loss: 0.1412 - val_acc: 0.9590\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9569\n",
      "Epoch 00063: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1367 - acc: 0.9569 - val_loss: 0.1325 - val_acc: 0.9604\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9573\n",
      "Epoch 00064: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1337 - acc: 0.9573 - val_loss: 0.1368 - val_acc: 0.9578\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9583\n",
      "Epoch 00065: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1300 - acc: 0.9583 - val_loss: 0.1318 - val_acc: 0.9630\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9592\n",
      "Epoch 00066: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1284 - acc: 0.9592 - val_loss: 0.1370 - val_acc: 0.9613\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9589\n",
      "Epoch 00067: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1296 - acc: 0.9589 - val_loss: 0.1256 - val_acc: 0.9627\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9599\n",
      "Epoch 00068: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1287 - acc: 0.9599 - val_loss: 0.1396 - val_acc: 0.9592\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9609\n",
      "Epoch 00069: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1243 - acc: 0.9608 - val_loss: 0.1359 - val_acc: 0.9599\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9589\n",
      "Epoch 00070: val_loss did not improve from 0.12287\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1319 - acc: 0.9588 - val_loss: 0.1240 - val_acc: 0.9648\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9610\n",
      "Epoch 00071: val_loss improved from 0.12287 to 0.12076, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/071-0.1208.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1244 - acc: 0.9610 - val_loss: 0.1208 - val_acc: 0.9646\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9617\n",
      "Epoch 00072: val_loss did not improve from 0.12076\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1229 - acc: 0.9616 - val_loss: 0.1237 - val_acc: 0.9620\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9605\n",
      "Epoch 00073: val_loss improved from 0.12076 to 0.12040, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/073-0.1204.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1242 - acc: 0.9605 - val_loss: 0.1204 - val_acc: 0.9641\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9637\n",
      "Epoch 00074: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1159 - acc: 0.9637 - val_loss: 0.1284 - val_acc: 0.9616\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9650\n",
      "Epoch 00075: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1128 - acc: 0.9650 - val_loss: 0.1322 - val_acc: 0.9632\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9651\n",
      "Epoch 00076: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1131 - acc: 0.9651 - val_loss: 0.1253 - val_acc: 0.9632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9643\n",
      "Epoch 00077: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1136 - acc: 0.9643 - val_loss: 0.1435 - val_acc: 0.9597\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9648\n",
      "Epoch 00078: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.1126 - acc: 0.9648 - val_loss: 0.1281 - val_acc: 0.9618\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9656\n",
      "Epoch 00079: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1096 - acc: 0.9655 - val_loss: 0.1329 - val_acc: 0.9609\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9654\n",
      "Epoch 00080: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1110 - acc: 0.9654 - val_loss: 0.1252 - val_acc: 0.9660\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9650\n",
      "Epoch 00081: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1087 - acc: 0.9650 - val_loss: 0.1317 - val_acc: 0.9606\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9661\n",
      "Epoch 00082: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1074 - acc: 0.9661 - val_loss: 0.1262 - val_acc: 0.9639\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9672\n",
      "Epoch 00083: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1044 - acc: 0.9672 - val_loss: 0.1287 - val_acc: 0.9632\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9688\n",
      "Epoch 00084: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1007 - acc: 0.9688 - val_loss: 0.1330 - val_acc: 0.9627\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9679\n",
      "Epoch 00085: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1033 - acc: 0.9679 - val_loss: 0.1319 - val_acc: 0.9627\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9656\n",
      "Epoch 00086: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1069 - acc: 0.9656 - val_loss: 0.1364 - val_acc: 0.9606\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9690\n",
      "Epoch 00087: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0980 - acc: 0.9690 - val_loss: 0.1312 - val_acc: 0.9616\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9696\n",
      "Epoch 00088: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0957 - acc: 0.9697 - val_loss: 0.1247 - val_acc: 0.9632\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9701\n",
      "Epoch 00089: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0943 - acc: 0.9700 - val_loss: 0.1338 - val_acc: 0.9585\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9697\n",
      "Epoch 00090: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0948 - acc: 0.9697 - val_loss: 0.1322 - val_acc: 0.9637\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9697\n",
      "Epoch 00091: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0947 - acc: 0.9697 - val_loss: 0.1266 - val_acc: 0.9639\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9705\n",
      "Epoch 00092: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0931 - acc: 0.9705 - val_loss: 0.1330 - val_acc: 0.9623\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9704\n",
      "Epoch 00093: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0920 - acc: 0.9703 - val_loss: 0.1367 - val_acc: 0.9618\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9685\n",
      "Epoch 00094: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0955 - acc: 0.9685 - val_loss: 0.1297 - val_acc: 0.9637\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9715\n",
      "Epoch 00095: val_loss did not improve from 0.12040\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0895 - acc: 0.9715 - val_loss: 0.1301 - val_acc: 0.9648\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9709\n",
      "Epoch 00096: val_loss improved from 0.12040 to 0.11634, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv_checkpoint/096-0.1163.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0924 - acc: 0.9709 - val_loss: 0.1163 - val_acc: 0.9632\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9714\n",
      "Epoch 00097: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0876 - acc: 0.9714 - val_loss: 0.1273 - val_acc: 0.9660\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9726\n",
      "Epoch 00098: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0857 - acc: 0.9726 - val_loss: 0.1450 - val_acc: 0.9597\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9723\n",
      "Epoch 00099: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0887 - acc: 0.9723 - val_loss: 0.1279 - val_acc: 0.9646\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9728\n",
      "Epoch 00100: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0855 - acc: 0.9728 - val_loss: 0.1286 - val_acc: 0.9646\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9709\n",
      "Epoch 00101: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0900 - acc: 0.9709 - val_loss: 0.1273 - val_acc: 0.9646\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9738\n",
      "Epoch 00102: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0824 - acc: 0.9738 - val_loss: 0.1377 - val_acc: 0.9646\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9740\n",
      "Epoch 00103: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0826 - acc: 0.9739 - val_loss: 0.1313 - val_acc: 0.9623\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9714\n",
      "Epoch 00104: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0897 - acc: 0.9714 - val_loss: 0.1236 - val_acc: 0.9660\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9753\n",
      "Epoch 00105: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0797 - acc: 0.9753 - val_loss: 0.1362 - val_acc: 0.9625\n",
      "Epoch 106/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9731\n",
      "Epoch 00106: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0832 - acc: 0.9730 - val_loss: 0.1400 - val_acc: 0.9588\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9728\n",
      "Epoch 00107: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0883 - acc: 0.9728 - val_loss: 0.1322 - val_acc: 0.9611\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9755\n",
      "Epoch 00108: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0793 - acc: 0.9755 - val_loss: 0.1282 - val_acc: 0.9646\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9752\n",
      "Epoch 00109: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0784 - acc: 0.9751 - val_loss: 0.1249 - val_acc: 0.9655\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9761\n",
      "Epoch 00110: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0757 - acc: 0.9761 - val_loss: 0.1268 - val_acc: 0.9639\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9744\n",
      "Epoch 00111: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0804 - acc: 0.9744 - val_loss: 0.1210 - val_acc: 0.9644\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9753\n",
      "Epoch 00112: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0759 - acc: 0.9753 - val_loss: 0.1397 - val_acc: 0.9613\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9760\n",
      "Epoch 00113: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0742 - acc: 0.9760 - val_loss: 0.1350 - val_acc: 0.9639\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9753\n",
      "Epoch 00114: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0785 - acc: 0.9753 - val_loss: 0.1292 - val_acc: 0.9641\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9747\n",
      "Epoch 00115: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0772 - acc: 0.9747 - val_loss: 0.1244 - val_acc: 0.9648\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9765\n",
      "Epoch 00116: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0754 - acc: 0.9765 - val_loss: 0.1300 - val_acc: 0.9651\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9772\n",
      "Epoch 00117: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0715 - acc: 0.9772 - val_loss: 0.1279 - val_acc: 0.9665\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9758\n",
      "Epoch 00118: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0734 - acc: 0.9758 - val_loss: 0.1344 - val_acc: 0.9648\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9766\n",
      "Epoch 00119: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0724 - acc: 0.9765 - val_loss: 0.1299 - val_acc: 0.9646\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9742\n",
      "Epoch 00120: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0804 - acc: 0.9742 - val_loss: 0.1358 - val_acc: 0.9660\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9777\n",
      "Epoch 00121: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0687 - acc: 0.9777 - val_loss: 0.1390 - val_acc: 0.9602\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9756\n",
      "Epoch 00122: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0754 - acc: 0.9756 - val_loss: 0.1434 - val_acc: 0.9606\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9793\n",
      "Epoch 00123: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0667 - acc: 0.9793 - val_loss: 0.1347 - val_acc: 0.9655\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9773\n",
      "Epoch 00124: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0728 - acc: 0.9773 - val_loss: 0.1544 - val_acc: 0.9562\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9787\n",
      "Epoch 00125: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0681 - acc: 0.9787 - val_loss: 0.1476 - val_acc: 0.9616\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9789\n",
      "Epoch 00126: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0672 - acc: 0.9789 - val_loss: 0.1353 - val_acc: 0.9620\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9793\n",
      "Epoch 00127: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0657 - acc: 0.9793 - val_loss: 0.1334 - val_acc: 0.9634\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9783\n",
      "Epoch 00128: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0683 - acc: 0.9783 - val_loss: 0.1361 - val_acc: 0.9634\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9780\n",
      "Epoch 00129: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0686 - acc: 0.9780 - val_loss: 0.1358 - val_acc: 0.9641\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9790\n",
      "Epoch 00130: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0662 - acc: 0.9790 - val_loss: 0.1303 - val_acc: 0.9651\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9791\n",
      "Epoch 00131: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0655 - acc: 0.9791 - val_loss: 0.1652 - val_acc: 0.9576\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9779\n",
      "Epoch 00132: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0687 - acc: 0.9779 - val_loss: 0.1465 - val_acc: 0.9623\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9799\n",
      "Epoch 00133: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0632 - acc: 0.9799 - val_loss: 0.1510 - val_acc: 0.9581\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9801\n",
      "Epoch 00134: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0608 - acc: 0.9801 - val_loss: 0.1398 - val_acc: 0.9641\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9795\n",
      "Epoch 00135: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0640 - acc: 0.9795 - val_loss: 0.1340 - val_acc: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9799\n",
      "Epoch 00136: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0636 - acc: 0.9799 - val_loss: 0.1351 - val_acc: 0.9634\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9793\n",
      "Epoch 00137: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0634 - acc: 0.9793 - val_loss: 0.1276 - val_acc: 0.9667\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9820\n",
      "Epoch 00138: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0577 - acc: 0.9820 - val_loss: 0.1433 - val_acc: 0.9613\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9814\n",
      "Epoch 00139: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0592 - acc: 0.9814 - val_loss: 0.1272 - val_acc: 0.9667\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9802\n",
      "Epoch 00140: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0617 - acc: 0.9802 - val_loss: 0.1323 - val_acc: 0.9625\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9808\n",
      "Epoch 00141: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0593 - acc: 0.9808 - val_loss: 0.1646 - val_acc: 0.9585\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9809\n",
      "Epoch 00142: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0615 - acc: 0.9809 - val_loss: 0.1422 - val_acc: 0.9658\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9816\n",
      "Epoch 00143: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0571 - acc: 0.9816 - val_loss: 0.1478 - val_acc: 0.9609\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9811\n",
      "Epoch 00144: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0580 - acc: 0.9811 - val_loss: 0.1441 - val_acc: 0.9623\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9814\n",
      "Epoch 00145: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0594 - acc: 0.9814 - val_loss: 0.1413 - val_acc: 0.9627\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9810\n",
      "Epoch 00146: val_loss did not improve from 0.11634\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0577 - acc: 0.9810 - val_loss: 0.1404 - val_acc: 0.9644\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 492us/sample - loss: 0.1573 - acc: 0.9504\n",
      "Loss: 0.1573159827327555 Accuracy: 0.95036346\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2943 - acc: 0.2097\n",
      "Epoch 00001: val_loss improved from inf to 1.67870, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/001-1.6787.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 3.2940 - acc: 0.2098 - val_loss: 1.6787 - val_acc: 0.4754\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8379 - acc: 0.4443\n",
      "Epoch 00002: val_loss improved from 1.67870 to 0.82929, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/002-0.8293.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 1.8381 - acc: 0.4442 - val_loss: 0.8293 - val_acc: 0.7622\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2851 - acc: 0.5979\n",
      "Epoch 00003: val_loss improved from 0.82929 to 0.66413, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/003-0.6641.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 1.2853 - acc: 0.5978 - val_loss: 0.6641 - val_acc: 0.8104\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9985 - acc: 0.6857\n",
      "Epoch 00004: val_loss improved from 0.66413 to 0.49847, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/004-0.4985.hdf5\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.9986 - acc: 0.6857 - val_loss: 0.4985 - val_acc: 0.8565\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8131 - acc: 0.7436\n",
      "Epoch 00005: val_loss improved from 0.49847 to 0.40483, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/005-0.4048.hdf5\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.8131 - acc: 0.7436 - val_loss: 0.4048 - val_acc: 0.8812\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6796 - acc: 0.7887\n",
      "Epoch 00006: val_loss improved from 0.40483 to 0.33668, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/006-0.3367.hdf5\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.6795 - acc: 0.7888 - val_loss: 0.3367 - val_acc: 0.9052\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5972 - acc: 0.8155\n",
      "Epoch 00007: val_loss improved from 0.33668 to 0.30012, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/007-0.3001.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.5972 - acc: 0.8155 - val_loss: 0.3001 - val_acc: 0.9182\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.8337\n",
      "Epoch 00008: val_loss improved from 0.30012 to 0.28101, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/008-0.2810.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.5366 - acc: 0.8337 - val_loss: 0.2810 - val_acc: 0.9215\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4765 - acc: 0.8530\n",
      "Epoch 00009: val_loss improved from 0.28101 to 0.26453, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/009-0.2645.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.4766 - acc: 0.8530 - val_loss: 0.2645 - val_acc: 0.9276\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8668\n",
      "Epoch 00010: val_loss improved from 0.26453 to 0.24591, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/010-0.2459.hdf5\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.4377 - acc: 0.8668 - val_loss: 0.2459 - val_acc: 0.9285\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8770\n",
      "Epoch 00011: val_loss improved from 0.24591 to 0.24499, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/011-0.2450.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.3973 - acc: 0.8770 - val_loss: 0.2450 - val_acc: 0.9290\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8850\n",
      "Epoch 00012: val_loss improved from 0.24499 to 0.22410, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/012-0.2241.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.3738 - acc: 0.8850 - val_loss: 0.2241 - val_acc: 0.9364\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8949\n",
      "Epoch 00013: val_loss improved from 0.22410 to 0.22255, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/013-0.2226.hdf5\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.3507 - acc: 0.8949 - val_loss: 0.2226 - val_acc: 0.9355\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3281 - acc: 0.8996\n",
      "Epoch 00014: val_loss improved from 0.22255 to 0.20691, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/014-0.2069.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.3282 - acc: 0.8996 - val_loss: 0.2069 - val_acc: 0.9399\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9049\n",
      "Epoch 00015: val_loss improved from 0.20691 to 0.18394, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/015-0.1839.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.3115 - acc: 0.9048 - val_loss: 0.1839 - val_acc: 0.9485\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9115\n",
      "Epoch 00016: val_loss improved from 0.18394 to 0.18026, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/016-0.1803.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.2935 - acc: 0.9115 - val_loss: 0.1803 - val_acc: 0.9506\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9165\n",
      "Epoch 00017: val_loss improved from 0.18026 to 0.17010, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/017-0.1701.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.2741 - acc: 0.9165 - val_loss: 0.1701 - val_acc: 0.9502\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.9185\n",
      "Epoch 00018: val_loss did not improve from 0.17010\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.2667 - acc: 0.9185 - val_loss: 0.2050 - val_acc: 0.9373\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.9239\n",
      "Epoch 00019: val_loss did not improve from 0.17010\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.2499 - acc: 0.9239 - val_loss: 0.1861 - val_acc: 0.9457\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9249\n",
      "Epoch 00020: val_loss did not improve from 0.17010\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.2450 - acc: 0.9249 - val_loss: 0.1805 - val_acc: 0.9469\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9267\n",
      "Epoch 00021: val_loss improved from 0.17010 to 0.16455, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/021-0.1646.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.2339 - acc: 0.9267 - val_loss: 0.1646 - val_acc: 0.9504\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9306\n",
      "Epoch 00022: val_loss did not improve from 0.16455\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.2286 - acc: 0.9306 - val_loss: 0.1819 - val_acc: 0.9420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9345\n",
      "Epoch 00023: val_loss did not improve from 0.16455\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.2163 - acc: 0.9345 - val_loss: 0.1774 - val_acc: 0.9485\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9347\n",
      "Epoch 00024: val_loss did not improve from 0.16455\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.2126 - acc: 0.9346 - val_loss: 0.1801 - val_acc: 0.9469\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9378\n",
      "Epoch 00025: val_loss improved from 0.16455 to 0.16081, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/025-0.1608.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.2033 - acc: 0.9378 - val_loss: 0.1608 - val_acc: 0.9511\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9387\n",
      "Epoch 00026: val_loss improved from 0.16081 to 0.16012, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/026-0.1601.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.2005 - acc: 0.9387 - val_loss: 0.1601 - val_acc: 0.9534\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9415\n",
      "Epoch 00027: val_loss improved from 0.16012 to 0.14460, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/027-0.1446.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1881 - acc: 0.9414 - val_loss: 0.1446 - val_acc: 0.9571\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9437\n",
      "Epoch 00028: val_loss did not improve from 0.14460\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1808 - acc: 0.9437 - val_loss: 0.1584 - val_acc: 0.9506\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9454\n",
      "Epoch 00029: val_loss did not improve from 0.14460\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.1759 - acc: 0.9454 - val_loss: 0.1595 - val_acc: 0.9502\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9476\n",
      "Epoch 00030: val_loss did not improve from 0.14460\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1690 - acc: 0.9475 - val_loss: 0.1470 - val_acc: 0.9564\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9485\n",
      "Epoch 00031: val_loss did not improve from 0.14460\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1659 - acc: 0.9485 - val_loss: 0.1677 - val_acc: 0.9488\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9506\n",
      "Epoch 00032: val_loss improved from 0.14460 to 0.13757, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/032-0.1376.hdf5\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1571 - acc: 0.9506 - val_loss: 0.1376 - val_acc: 0.9571\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9540\n",
      "Epoch 00033: val_loss did not improve from 0.13757\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.1516 - acc: 0.9540 - val_loss: 0.1591 - val_acc: 0.9546\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9533\n",
      "Epoch 00034: val_loss did not improve from 0.13757\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1513 - acc: 0.9533 - val_loss: 0.1699 - val_acc: 0.9478\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9537\n",
      "Epoch 00035: val_loss improved from 0.13757 to 0.13553, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/035-0.1355.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1486 - acc: 0.9537 - val_loss: 0.1355 - val_acc: 0.9602\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9551\n",
      "Epoch 00036: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1438 - acc: 0.9551 - val_loss: 0.1383 - val_acc: 0.9592\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.9568\n",
      "Epoch 00037: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1381 - acc: 0.9568 - val_loss: 0.1592 - val_acc: 0.9525\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9583\n",
      "Epoch 00038: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1325 - acc: 0.9583 - val_loss: 0.1511 - val_acc: 0.9567\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9580\n",
      "Epoch 00039: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1332 - acc: 0.9579 - val_loss: 0.1399 - val_acc: 0.9574\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9590\n",
      "Epoch 00040: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1287 - acc: 0.9589 - val_loss: 0.1449 - val_acc: 0.9564\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9596\n",
      "Epoch 00041: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1315 - acc: 0.9596 - val_loss: 0.1418 - val_acc: 0.9588\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9630\n",
      "Epoch 00042: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1167 - acc: 0.9630 - val_loss: 0.1381 - val_acc: 0.9567\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9632\n",
      "Epoch 00043: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1169 - acc: 0.9632 - val_loss: 0.1411 - val_acc: 0.9590\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9646\n",
      "Epoch 00044: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1151 - acc: 0.9646 - val_loss: 0.1368 - val_acc: 0.9588\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9645\n",
      "Epoch 00045: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1123 - acc: 0.9645 - val_loss: 0.1684 - val_acc: 0.9499\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9649\n",
      "Epoch 00046: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1123 - acc: 0.9649 - val_loss: 0.1462 - val_acc: 0.9567\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9646\n",
      "Epoch 00047: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1127 - acc: 0.9646 - val_loss: 0.1356 - val_acc: 0.9592\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9667\n",
      "Epoch 00048: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1068 - acc: 0.9666 - val_loss: 0.1435 - val_acc: 0.9578\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9692\n",
      "Epoch 00049: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1015 - acc: 0.9692 - val_loss: 0.1430 - val_acc: 0.9564\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9691\n",
      "Epoch 00050: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0970 - acc: 0.9691 - val_loss: 0.1478 - val_acc: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9686\n",
      "Epoch 00051: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0983 - acc: 0.9686 - val_loss: 0.1398 - val_acc: 0.9569\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9699\n",
      "Epoch 00052: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0948 - acc: 0.9699 - val_loss: 0.1553 - val_acc: 0.9555\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9718\n",
      "Epoch 00053: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0913 - acc: 0.9718 - val_loss: 0.1757 - val_acc: 0.9483\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9704\n",
      "Epoch 00054: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0948 - acc: 0.9704 - val_loss: 0.1462 - val_acc: 0.9567\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9708\n",
      "Epoch 00055: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0904 - acc: 0.9708 - val_loss: 0.1519 - val_acc: 0.9560\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9727\n",
      "Epoch 00056: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0869 - acc: 0.9727 - val_loss: 0.1428 - val_acc: 0.9576\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9720\n",
      "Epoch 00057: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0874 - acc: 0.9720 - val_loss: 0.1667 - val_acc: 0.9513\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9743\n",
      "Epoch 00058: val_loss did not improve from 0.13553\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0838 - acc: 0.9743 - val_loss: 0.1875 - val_acc: 0.9460\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9726\n",
      "Epoch 00059: val_loss improved from 0.13553 to 0.13130, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv_checkpoint/059-0.1313.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0855 - acc: 0.9725 - val_loss: 0.1313 - val_acc: 0.9620\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9739\n",
      "Epoch 00060: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0840 - acc: 0.9739 - val_loss: 0.1554 - val_acc: 0.9576\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9738\n",
      "Epoch 00061: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0819 - acc: 0.9738 - val_loss: 0.1490 - val_acc: 0.9562\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9764\n",
      "Epoch 00062: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0759 - acc: 0.9764 - val_loss: 0.1714 - val_acc: 0.9497\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9754\n",
      "Epoch 00063: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0778 - acc: 0.9754 - val_loss: 0.1594 - val_acc: 0.9548\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9760\n",
      "Epoch 00064: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0785 - acc: 0.9760 - val_loss: 0.1532 - val_acc: 0.9574\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9779\n",
      "Epoch 00065: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0696 - acc: 0.9779 - val_loss: 0.1647 - val_acc: 0.9525\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9771\n",
      "Epoch 00066: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0725 - acc: 0.9771 - val_loss: 0.1683 - val_acc: 0.9553\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9774\n",
      "Epoch 00067: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0714 - acc: 0.9774 - val_loss: 0.1491 - val_acc: 0.9574\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9792\n",
      "Epoch 00068: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0673 - acc: 0.9792 - val_loss: 0.1908 - val_acc: 0.9446\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9800\n",
      "Epoch 00069: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0661 - acc: 0.9800 - val_loss: 0.1904 - val_acc: 0.9525\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9791\n",
      "Epoch 00070: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0675 - acc: 0.9791 - val_loss: 0.1524 - val_acc: 0.9569\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9794\n",
      "Epoch 00071: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0656 - acc: 0.9794 - val_loss: 0.1492 - val_acc: 0.9578\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9790\n",
      "Epoch 00072: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0647 - acc: 0.9791 - val_loss: 0.1460 - val_acc: 0.9592\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9822\n",
      "Epoch 00073: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0584 - acc: 0.9822 - val_loss: 0.1496 - val_acc: 0.9590\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9811\n",
      "Epoch 00074: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0609 - acc: 0.9811 - val_loss: 0.1992 - val_acc: 0.9434\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9778\n",
      "Epoch 00075: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0688 - acc: 0.9778 - val_loss: 0.1558 - val_acc: 0.9585\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9825\n",
      "Epoch 00076: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0562 - acc: 0.9825 - val_loss: 0.1678 - val_acc: 0.9553\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9831\n",
      "Epoch 00077: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0550 - acc: 0.9831 - val_loss: 0.1469 - val_acc: 0.9599\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9830\n",
      "Epoch 00078: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0559 - acc: 0.9830 - val_loss: 0.1431 - val_acc: 0.9590\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9827\n",
      "Epoch 00079: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0551 - acc: 0.9827 - val_loss: 0.1640 - val_acc: 0.9550\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9821\n",
      "Epoch 00080: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0576 - acc: 0.9821 - val_loss: 0.1421 - val_acc: 0.9623\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9833\n",
      "Epoch 00081: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0527 - acc: 0.9833 - val_loss: 0.1521 - val_acc: 0.9597\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9817\n",
      "Epoch 00082: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0594 - acc: 0.9817 - val_loss: 0.1657 - val_acc: 0.9578\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9843\n",
      "Epoch 00083: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0501 - acc: 0.9843 - val_loss: 0.1745 - val_acc: 0.9536\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9844\n",
      "Epoch 00084: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0496 - acc: 0.9844 - val_loss: 0.1555 - val_acc: 0.9604\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9841\n",
      "Epoch 00085: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0506 - acc: 0.9841 - val_loss: 0.1511 - val_acc: 0.9611\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9852\n",
      "Epoch 00086: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0474 - acc: 0.9852 - val_loss: 0.1545 - val_acc: 0.9583\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9848\n",
      "Epoch 00087: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0486 - acc: 0.9848 - val_loss: 0.1505 - val_acc: 0.9597\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9856\n",
      "Epoch 00088: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0466 - acc: 0.9856 - val_loss: 0.1642 - val_acc: 0.9557\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9852\n",
      "Epoch 00089: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0474 - acc: 0.9852 - val_loss: 0.1641 - val_acc: 0.9597\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9856\n",
      "Epoch 00090: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0442 - acc: 0.9856 - val_loss: 0.1788 - val_acc: 0.9525\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9839\n",
      "Epoch 00091: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0499 - acc: 0.9839 - val_loss: 0.1699 - val_acc: 0.9555\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9852\n",
      "Epoch 00092: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0471 - acc: 0.9852 - val_loss: 0.1655 - val_acc: 0.9555\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9870\n",
      "Epoch 00093: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0435 - acc: 0.9870 - val_loss: 0.1745 - val_acc: 0.9578\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9837\n",
      "Epoch 00094: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0515 - acc: 0.9837 - val_loss: 0.1709 - val_acc: 0.9581\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9867\n",
      "Epoch 00095: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0441 - acc: 0.9867 - val_loss: 0.1541 - val_acc: 0.9604\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9876\n",
      "Epoch 00096: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0391 - acc: 0.9876 - val_loss: 0.1535 - val_acc: 0.9602\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9876\n",
      "Epoch 00097: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0409 - acc: 0.9876 - val_loss: 0.1728 - val_acc: 0.9590\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9847\n",
      "Epoch 00098: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0488 - acc: 0.9847 - val_loss: 0.1426 - val_acc: 0.9616\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9883\n",
      "Epoch 00099: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0395 - acc: 0.9883 - val_loss: 0.1670 - val_acc: 0.9599\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9889\n",
      "Epoch 00100: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0363 - acc: 0.9889 - val_loss: 0.2066 - val_acc: 0.9534\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9873\n",
      "Epoch 00101: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0404 - acc: 0.9873 - val_loss: 0.1796 - val_acc: 0.9532\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9885\n",
      "Epoch 00102: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0373 - acc: 0.9885 - val_loss: 0.1640 - val_acc: 0.9602\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9874\n",
      "Epoch 00103: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0415 - acc: 0.9874 - val_loss: 0.1763 - val_acc: 0.9602\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9887\n",
      "Epoch 00104: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0383 - acc: 0.9887 - val_loss: 0.1581 - val_acc: 0.9592\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9888\n",
      "Epoch 00105: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0365 - acc: 0.9888 - val_loss: 0.1700 - val_acc: 0.9618\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9867\n",
      "Epoch 00106: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0423 - acc: 0.9867 - val_loss: 0.1831 - val_acc: 0.9583\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9892\n",
      "Epoch 00107: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0350 - acc: 0.9892 - val_loss: 0.1844 - val_acc: 0.9569\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9899\n",
      "Epoch 00108: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0330 - acc: 0.9899 - val_loss: 0.1754 - val_acc: 0.9606\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9896\n",
      "Epoch 00109: val_loss did not improve from 0.13130\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0346 - acc: 0.9896 - val_loss: 0.1703 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 570us/sample - loss: 0.1795 - acc: 0.9497\n",
      "Loss: 0.17949633942269028 Accuracy: 0.9497404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GMP_ch_32_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1040        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,920\n",
      "Trainable params: 11,728\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 504us/sample - loss: 0.8076 - acc: 0.7632\n",
      "Loss: 0.8075879554758438 Accuracy: 0.76323986\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64)           0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64)           0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,200\n",
      "Trainable params: 16,944\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 554us/sample - loss: 0.5789 - acc: 0.8243\n",
      "Loss: 0.5789255082421585 Accuracy: 0.82429904\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 96)           0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           1552        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,272\n",
      "Trainable params: 27,888\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 548us/sample - loss: 0.3087 - acc: 0.9097\n",
      "Loss: 0.3087116699352443 Accuracy: 0.9096573\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2064        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 49,584\n",
      "Trainable params: 49,072\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 574us/sample - loss: 0.1886 - acc: 0.9423\n",
      "Loss: 0.1885629646503294 Accuracy: 0.9422638\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2064        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 70,384\n",
      "Trainable params: 69,744\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 576us/sample - loss: 0.1573 - acc: 0.9504\n",
      "Loss: 0.1573159827327555 Accuracy: 0.95036346\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           2064        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 91,184\n",
      "Trainable params: 90,416\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 638us/sample - loss: 0.1795 - acc: 0.9497\n",
      "Loss: 0.17949633942269028 Accuracy: 0.9497404\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GMP_ch_32_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1040        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,920\n",
      "Trainable params: 11,728\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 594us/sample - loss: 0.8497 - acc: 0.7475\n",
      "Loss: 0.8497202883999667 Accuracy: 0.7474559\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64)           0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64)           0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,200\n",
      "Trainable params: 16,944\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 628us/sample - loss: 0.6082 - acc: 0.8160\n",
      "Loss: 0.6082297337265649 Accuracy: 0.8159917\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 96)           0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           1552        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,272\n",
      "Trainable params: 27,888\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 677us/sample - loss: 0.3052 - acc: 0.9090\n",
      "Loss: 0.3051681723178733 Accuracy: 0.90903425\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2064        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 49,584\n",
      "Trainable params: 49,072\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 694us/sample - loss: 0.2124 - acc: 0.9387\n",
      "Loss: 0.2124126176627378 Accuracy: 0.9387331\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2064        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 70,384\n",
      "Trainable params: 69,744\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 712us/sample - loss: 0.1746 - acc: 0.9520\n",
      "Loss: 0.17461257426692936 Accuracy: 0.95202494\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           2064        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 91,184\n",
      "Trainable params: 90,416\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 707us/sample - loss: 0.2183 - acc: 0.9493\n",
      "Loss: 0.21832625797987448 Accuracy: 0.949325\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
