{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    init_channel = 128\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same', activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                32768016  \n",
      "=================================================================\n",
      "Total params: 32,768,784\n",
      "Trainable params: 32,768,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                10922000  \n",
      "=================================================================\n",
      "Total params: 11,004,816\n",
      "Trainable params: 11,004,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 3,804,176\n",
      "Trainable params: 3,804,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1212432   \n",
      "=================================================================\n",
      "Total params: 1,459,344\n",
      "Trainable params: 1,459,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 489,680\n",
      "Trainable params: 489,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 375,056\n",
      "Trainable params: 375,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 350,544\n",
      "Trainable params: 350,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 356,752\n",
      "Trainable params: 356,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 32)             10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 360,880\n",
      "Trainable params: 360,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2785 - acc: 0.2432\n",
      "Epoch 00001: val_loss improved from inf to 1.59926, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/001-1.5993.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 2.2784 - acc: 0.2432 - val_loss: 1.5993 - val_acc: 0.4959\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5779 - acc: 0.4813\n",
      "Epoch 00002: val_loss improved from 1.59926 to 1.35839, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/002-1.3584.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 1.5780 - acc: 0.4813 - val_loss: 1.3584 - val_acc: 0.5604\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3288 - acc: 0.5713\n",
      "Epoch 00003: val_loss improved from 1.35839 to 1.06009, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/003-1.0601.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 1.3286 - acc: 0.5713 - val_loss: 1.0601 - val_acc: 0.6846\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1357 - acc: 0.6388\n",
      "Epoch 00004: val_loss improved from 1.06009 to 0.99357, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/004-0.9936.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 1.1358 - acc: 0.6387 - val_loss: 0.9936 - val_acc: 0.7237\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9978 - acc: 0.6932\n",
      "Epoch 00005: val_loss improved from 0.99357 to 0.81093, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/005-0.8109.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.9977 - acc: 0.6932 - val_loss: 0.8109 - val_acc: 0.7622\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8995 - acc: 0.7236\n",
      "Epoch 00006: val_loss improved from 0.81093 to 0.74108, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/006-0.7411.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.8995 - acc: 0.7237 - val_loss: 0.7411 - val_acc: 0.7843\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8227 - acc: 0.7508\n",
      "Epoch 00007: val_loss improved from 0.74108 to 0.70641, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/007-0.7064.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.8226 - acc: 0.7508 - val_loss: 0.7064 - val_acc: 0.7904\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7587 - acc: 0.7724\n",
      "Epoch 00008: val_loss improved from 0.70641 to 0.63114, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/008-0.6311.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.7587 - acc: 0.7724 - val_loss: 0.6311 - val_acc: 0.8213\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7007 - acc: 0.7899\n",
      "Epoch 00009: val_loss improved from 0.63114 to 0.59779, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/009-0.5978.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.7006 - acc: 0.7900 - val_loss: 0.5978 - val_acc: 0.8311\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6550 - acc: 0.8040\n",
      "Epoch 00010: val_loss improved from 0.59779 to 0.56018, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/010-0.5602.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.6549 - acc: 0.8040 - val_loss: 0.5602 - val_acc: 0.8486\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6179 - acc: 0.8161\n",
      "Epoch 00011: val_loss did not improve from 0.56018\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.6181 - acc: 0.8161 - val_loss: 0.5614 - val_acc: 0.8428\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5789 - acc: 0.8274\n",
      "Epoch 00012: val_loss improved from 0.56018 to 0.51811, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/012-0.5181.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.5789 - acc: 0.8274 - val_loss: 0.5181 - val_acc: 0.8542\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.8371\n",
      "Epoch 00013: val_loss improved from 0.51811 to 0.47583, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/013-0.4758.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.5474 - acc: 0.8371 - val_loss: 0.4758 - val_acc: 0.8703\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8405\n",
      "Epoch 00014: val_loss improved from 0.47583 to 0.46634, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/014-0.4663.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.5276 - acc: 0.8405 - val_loss: 0.4663 - val_acc: 0.8733\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4946 - acc: 0.8529\n",
      "Epoch 00015: val_loss did not improve from 0.46634\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.4946 - acc: 0.8529 - val_loss: 0.4825 - val_acc: 0.8635\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4775 - acc: 0.8566\n",
      "Epoch 00016: val_loss improved from 0.46634 to 0.42869, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/016-0.4287.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.4774 - acc: 0.8566 - val_loss: 0.4287 - val_acc: 0.8868\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.8657\n",
      "Epoch 00017: val_loss did not improve from 0.42869\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.4522 - acc: 0.8657 - val_loss: 0.5045 - val_acc: 0.8591\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4389 - acc: 0.8680\n",
      "Epoch 00018: val_loss did not improve from 0.42869\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.4388 - acc: 0.8680 - val_loss: 0.4525 - val_acc: 0.8744\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8728\n",
      "Epoch 00019: val_loss improved from 0.42869 to 0.40593, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/019-0.4059.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.4235 - acc: 0.8728 - val_loss: 0.4059 - val_acc: 0.8903\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8792\n",
      "Epoch 00020: val_loss did not improve from 0.40593\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.4065 - acc: 0.8792 - val_loss: 0.4213 - val_acc: 0.8824\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8806\n",
      "Epoch 00021: val_loss did not improve from 0.40593\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3929 - acc: 0.8806 - val_loss: 0.4064 - val_acc: 0.8887\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8860\n",
      "Epoch 00022: val_loss did not improve from 0.40593\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3747 - acc: 0.8860 - val_loss: 0.4126 - val_acc: 0.8891\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8888\n",
      "Epoch 00023: val_loss improved from 0.40593 to 0.37986, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/023-0.3799.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3639 - acc: 0.8887 - val_loss: 0.3799 - val_acc: 0.8968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8924\n",
      "Epoch 00024: val_loss did not improve from 0.37986\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3523 - acc: 0.8924 - val_loss: 0.3858 - val_acc: 0.8970\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8964\n",
      "Epoch 00025: val_loss did not improve from 0.37986\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3366 - acc: 0.8963 - val_loss: 0.4099 - val_acc: 0.8919\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8999\n",
      "Epoch 00026: val_loss did not improve from 0.37986\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3306 - acc: 0.8999 - val_loss: 0.3945 - val_acc: 0.8940\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.9015\n",
      "Epoch 00027: val_loss did not improve from 0.37986\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3212 - acc: 0.9015 - val_loss: 0.4050 - val_acc: 0.8875\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9042\n",
      "Epoch 00028: val_loss improved from 0.37986 to 0.37958, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/028-0.3796.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3110 - acc: 0.9042 - val_loss: 0.3796 - val_acc: 0.9005\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2979 - acc: 0.9074\n",
      "Epoch 00029: val_loss did not improve from 0.37958\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2979 - acc: 0.9074 - val_loss: 0.3812 - val_acc: 0.8994\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9102\n",
      "Epoch 00030: val_loss improved from 0.37958 to 0.36742, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/030-0.3674.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2879 - acc: 0.9102 - val_loss: 0.3674 - val_acc: 0.9008\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9128\n",
      "Epoch 00031: val_loss did not improve from 0.36742\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2821 - acc: 0.9128 - val_loss: 0.4003 - val_acc: 0.8898\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9141\n",
      "Epoch 00032: val_loss improved from 0.36742 to 0.34581, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/032-0.3458.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2760 - acc: 0.9141 - val_loss: 0.3458 - val_acc: 0.9045\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9183\n",
      "Epoch 00033: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2639 - acc: 0.9182 - val_loss: 0.3511 - val_acc: 0.9080\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.9198\n",
      "Epoch 00034: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2598 - acc: 0.9198 - val_loss: 0.3675 - val_acc: 0.9012\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9204\n",
      "Epoch 00035: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2540 - acc: 0.9204 - val_loss: 0.3659 - val_acc: 0.9045\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9226\n",
      "Epoch 00036: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2500 - acc: 0.9226 - val_loss: 0.3589 - val_acc: 0.9071\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9225\n",
      "Epoch 00037: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2457 - acc: 0.9225 - val_loss: 0.3680 - val_acc: 0.9029\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9251\n",
      "Epoch 00038: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2335 - acc: 0.9251 - val_loss: 0.3599 - val_acc: 0.9078\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9270\n",
      "Epoch 00039: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2301 - acc: 0.9270 - val_loss: 0.3463 - val_acc: 0.9092\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9282\n",
      "Epoch 00040: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2262 - acc: 0.9282 - val_loss: 0.3725 - val_acc: 0.9101\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9279\n",
      "Epoch 00041: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2241 - acc: 0.9279 - val_loss: 0.3634 - val_acc: 0.9075\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9329\n",
      "Epoch 00042: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2131 - acc: 0.9329 - val_loss: 0.3488 - val_acc: 0.9159\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9330\n",
      "Epoch 00043: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2079 - acc: 0.9330 - val_loss: 0.3482 - val_acc: 0.9103\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9318\n",
      "Epoch 00044: val_loss did not improve from 0.34581\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2084 - acc: 0.9318 - val_loss: 0.3606 - val_acc: 0.9080\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9355\n",
      "Epoch 00045: val_loss improved from 0.34581 to 0.34507, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/045-0.3451.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2005 - acc: 0.9354 - val_loss: 0.3451 - val_acc: 0.9168\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9382\n",
      "Epoch 00046: val_loss did not improve from 0.34507\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1926 - acc: 0.9382 - val_loss: 0.3531 - val_acc: 0.9073\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9375\n",
      "Epoch 00047: val_loss did not improve from 0.34507\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1937 - acc: 0.9375 - val_loss: 0.3458 - val_acc: 0.9117\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9416\n",
      "Epoch 00048: val_loss did not improve from 0.34507\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1837 - acc: 0.9416 - val_loss: 0.3497 - val_acc: 0.9145\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9405\n",
      "Epoch 00049: val_loss improved from 0.34507 to 0.34439, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/049-0.3444.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1828 - acc: 0.9405 - val_loss: 0.3444 - val_acc: 0.9103\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9407\n",
      "Epoch 00050: val_loss did not improve from 0.34439\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1813 - acc: 0.9407 - val_loss: 0.3680 - val_acc: 0.9103\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9410\n",
      "Epoch 00051: val_loss did not improve from 0.34439\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1796 - acc: 0.9410 - val_loss: 0.3448 - val_acc: 0.9180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1740 - acc: 0.9434\n",
      "Epoch 00052: val_loss did not improve from 0.34439\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1740 - acc: 0.9434 - val_loss: 0.3760 - val_acc: 0.9124\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9447\n",
      "Epoch 00053: val_loss did not improve from 0.34439\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1694 - acc: 0.9447 - val_loss: 0.3847 - val_acc: 0.9138\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9441\n",
      "Epoch 00054: val_loss improved from 0.34439 to 0.33623, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_6_conv_checkpoint/054-0.3362.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1689 - acc: 0.9441 - val_loss: 0.3362 - val_acc: 0.9199\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9453\n",
      "Epoch 00055: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1642 - acc: 0.9453 - val_loss: 0.3434 - val_acc: 0.9220\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9477\n",
      "Epoch 00056: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1595 - acc: 0.9478 - val_loss: 0.3836 - val_acc: 0.9138\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9450\n",
      "Epoch 00057: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1660 - acc: 0.9450 - val_loss: 0.3409 - val_acc: 0.9171\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9474\n",
      "Epoch 00058: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1600 - acc: 0.9474 - val_loss: 0.3533 - val_acc: 0.9178\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9492\n",
      "Epoch 00059: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1552 - acc: 0.9492 - val_loss: 0.3891 - val_acc: 0.9196\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9490\n",
      "Epoch 00060: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1524 - acc: 0.9489 - val_loss: 0.3390 - val_acc: 0.9203\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9531\n",
      "Epoch 00061: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1462 - acc: 0.9531 - val_loss: 0.4152 - val_acc: 0.9019\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9498\n",
      "Epoch 00062: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1498 - acc: 0.9498 - val_loss: 0.3680 - val_acc: 0.9213\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9526\n",
      "Epoch 00063: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1395 - acc: 0.9525 - val_loss: 0.3523 - val_acc: 0.9210\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9523\n",
      "Epoch 00064: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1426 - acc: 0.9523 - val_loss: 0.3632 - val_acc: 0.9213\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9526\n",
      "Epoch 00065: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1397 - acc: 0.9526 - val_loss: 0.3586 - val_acc: 0.9252\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9540\n",
      "Epoch 00066: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1358 - acc: 0.9540 - val_loss: 0.3608 - val_acc: 0.9213\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9543\n",
      "Epoch 00067: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1377 - acc: 0.9543 - val_loss: 0.3647 - val_acc: 0.9215\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9585\n",
      "Epoch 00068: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1269 - acc: 0.9585 - val_loss: 0.3477 - val_acc: 0.9220\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.9535\n",
      "Epoch 00069: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1381 - acc: 0.9535 - val_loss: 0.3578 - val_acc: 0.9189\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9586\n",
      "Epoch 00070: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1271 - acc: 0.9586 - val_loss: 0.3429 - val_acc: 0.9203\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9576\n",
      "Epoch 00071: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1252 - acc: 0.9576 - val_loss: 0.3594 - val_acc: 0.9238\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9590\n",
      "Epoch 00072: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1245 - acc: 0.9590 - val_loss: 0.3568 - val_acc: 0.9234\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9597\n",
      "Epoch 00073: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1229 - acc: 0.9597 - val_loss: 0.3428 - val_acc: 0.9201\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9592\n",
      "Epoch 00074: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1236 - acc: 0.9592 - val_loss: 0.3776 - val_acc: 0.9206\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9575\n",
      "Epoch 00075: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1269 - acc: 0.9575 - val_loss: 0.3619 - val_acc: 0.9241\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9609\n",
      "Epoch 00076: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1158 - acc: 0.9609 - val_loss: 0.3854 - val_acc: 0.9206\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9601\n",
      "Epoch 00077: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1207 - acc: 0.9601 - val_loss: 0.3538 - val_acc: 0.9217\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9613\n",
      "Epoch 00078: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1152 - acc: 0.9612 - val_loss: 0.3614 - val_acc: 0.9213\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9603\n",
      "Epoch 00079: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1180 - acc: 0.9603 - val_loss: 0.3693 - val_acc: 0.9210\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9626\n",
      "Epoch 00080: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1114 - acc: 0.9626 - val_loss: 0.3442 - val_acc: 0.9206\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9610\n",
      "Epoch 00081: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1161 - acc: 0.9610 - val_loss: 0.3771 - val_acc: 0.9194\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9614\n",
      "Epoch 00082: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1140 - acc: 0.9614 - val_loss: 0.3635 - val_acc: 0.9231\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9644\n",
      "Epoch 00083: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1058 - acc: 0.9644 - val_loss: 0.3624 - val_acc: 0.9189\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9655\n",
      "Epoch 00084: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1060 - acc: 0.9655 - val_loss: 0.3684 - val_acc: 0.9264\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9634\n",
      "Epoch 00085: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1078 - acc: 0.9634 - val_loss: 0.3469 - val_acc: 0.9238\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9638\n",
      "Epoch 00086: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1090 - acc: 0.9638 - val_loss: 0.3555 - val_acc: 0.9224\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9648\n",
      "Epoch 00087: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1055 - acc: 0.9648 - val_loss: 0.3634 - val_acc: 0.9208\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9658\n",
      "Epoch 00088: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1012 - acc: 0.9658 - val_loss: 0.3715 - val_acc: 0.9238\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9650\n",
      "Epoch 00089: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1035 - acc: 0.9650 - val_loss: 0.3670 - val_acc: 0.9276\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9678\n",
      "Epoch 00090: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0955 - acc: 0.9678 - val_loss: 0.3715 - val_acc: 0.9206\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9663\n",
      "Epoch 00091: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0980 - acc: 0.9663 - val_loss: 0.3729 - val_acc: 0.9250\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9655\n",
      "Epoch 00092: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1039 - acc: 0.9655 - val_loss: 0.3597 - val_acc: 0.9217\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9683\n",
      "Epoch 00093: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0950 - acc: 0.9683 - val_loss: 0.3811 - val_acc: 0.9278\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9670\n",
      "Epoch 00094: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0985 - acc: 0.9670 - val_loss: 0.3821 - val_acc: 0.9227\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9686\n",
      "Epoch 00095: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0960 - acc: 0.9686 - val_loss: 0.3847 - val_acc: 0.9255\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9681\n",
      "Epoch 00096: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0983 - acc: 0.9681 - val_loss: 0.3777 - val_acc: 0.9229\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9690\n",
      "Epoch 00097: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0939 - acc: 0.9690 - val_loss: 0.3829 - val_acc: 0.9257\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9695\n",
      "Epoch 00098: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0899 - acc: 0.9695 - val_loss: 0.4115 - val_acc: 0.9147\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9687\n",
      "Epoch 00099: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0942 - acc: 0.9687 - val_loss: 0.4110 - val_acc: 0.9222\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9702\n",
      "Epoch 00100: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0897 - acc: 0.9702 - val_loss: 0.3805 - val_acc: 0.9238\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9707\n",
      "Epoch 00101: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0894 - acc: 0.9707 - val_loss: 0.3565 - val_acc: 0.9264\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9687\n",
      "Epoch 00102: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0931 - acc: 0.9687 - val_loss: 0.3739 - val_acc: 0.9290\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9704\n",
      "Epoch 00103: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0869 - acc: 0.9704 - val_loss: 0.3741 - val_acc: 0.9234\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9717\n",
      "Epoch 00104: val_loss did not improve from 0.33623\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0840 - acc: 0.9717 - val_loss: 0.3864 - val_acc: 0.9304\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3915 - acc: 0.8995\n",
      "Loss: 0.3914603922164081 Accuracy: 0.89948076\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3805 - acc: 0.2178\n",
      "Epoch 00001: val_loss improved from inf to 1.65214, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/001-1.6521.hdf5\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 2.3805 - acc: 0.2178 - val_loss: 1.6521 - val_acc: 0.4540\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6757 - acc: 0.4422\n",
      "Epoch 00002: val_loss improved from 1.65214 to 1.36536, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/002-1.3654.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.6757 - acc: 0.4422 - val_loss: 1.3654 - val_acc: 0.5642\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4603 - acc: 0.5148\n",
      "Epoch 00003: val_loss improved from 1.36536 to 1.17696, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/003-1.1770.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.4603 - acc: 0.5148 - val_loss: 1.1770 - val_acc: 0.6457\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2984 - acc: 0.5760\n",
      "Epoch 00004: val_loss improved from 1.17696 to 1.07160, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/004-1.0716.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.2983 - acc: 0.5760 - val_loss: 1.0716 - val_acc: 0.6723\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1369 - acc: 0.6359\n",
      "Epoch 00005: val_loss improved from 1.07160 to 0.85663, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/005-0.8566.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.1369 - acc: 0.6359 - val_loss: 0.8566 - val_acc: 0.7452\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9892 - acc: 0.6900\n",
      "Epoch 00006: val_loss improved from 0.85663 to 0.71693, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/006-0.7169.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.9891 - acc: 0.6900 - val_loss: 0.7169 - val_acc: 0.7901\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8597 - acc: 0.7327\n",
      "Epoch 00007: val_loss improved from 0.71693 to 0.62436, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/007-0.6244.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.8596 - acc: 0.7327 - val_loss: 0.6244 - val_acc: 0.8223\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7684 - acc: 0.7608\n",
      "Epoch 00008: val_loss improved from 0.62436 to 0.58388, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/008-0.5839.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.7683 - acc: 0.7608 - val_loss: 0.5839 - val_acc: 0.8286\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.7868\n",
      "Epoch 00009: val_loss improved from 0.58388 to 0.49887, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/009-0.4989.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.6929 - acc: 0.7867 - val_loss: 0.4989 - val_acc: 0.8544\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6400 - acc: 0.8028\n",
      "Epoch 00010: val_loss improved from 0.49887 to 0.46951, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/010-0.4695.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.6400 - acc: 0.8028 - val_loss: 0.4695 - val_acc: 0.8728\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5862 - acc: 0.8222\n",
      "Epoch 00011: val_loss improved from 0.46951 to 0.42055, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/011-0.4206.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.5863 - acc: 0.8221 - val_loss: 0.4206 - val_acc: 0.8824\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.8334\n",
      "Epoch 00012: val_loss improved from 0.42055 to 0.39370, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/012-0.3937.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.5476 - acc: 0.8334 - val_loss: 0.3937 - val_acc: 0.8945\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5123 - acc: 0.8445\n",
      "Epoch 00013: val_loss improved from 0.39370 to 0.38806, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/013-0.3881.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.5123 - acc: 0.8445 - val_loss: 0.3881 - val_acc: 0.8994\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4787 - acc: 0.8530\n",
      "Epoch 00014: val_loss improved from 0.38806 to 0.35534, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/014-0.3553.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4787 - acc: 0.8530 - val_loss: 0.3553 - val_acc: 0.9094\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4560 - acc: 0.8602\n",
      "Epoch 00015: val_loss improved from 0.35534 to 0.34868, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/015-0.3487.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4562 - acc: 0.8602 - val_loss: 0.3487 - val_acc: 0.9038\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.8676\n",
      "Epoch 00016: val_loss improved from 0.34868 to 0.30883, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/016-0.3088.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4311 - acc: 0.8676 - val_loss: 0.3088 - val_acc: 0.9180\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8726\n",
      "Epoch 00017: val_loss did not improve from 0.30883\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4120 - acc: 0.8726 - val_loss: 0.3245 - val_acc: 0.9087\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8801\n",
      "Epoch 00018: val_loss improved from 0.30883 to 0.29012, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/018-0.2901.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3910 - acc: 0.8801 - val_loss: 0.2901 - val_acc: 0.9224\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8828\n",
      "Epoch 00019: val_loss improved from 0.29012 to 0.27719, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/019-0.2772.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3784 - acc: 0.8828 - val_loss: 0.2772 - val_acc: 0.9262\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8866\n",
      "Epoch 00020: val_loss improved from 0.27719 to 0.27683, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/020-0.2768.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3672 - acc: 0.8866 - val_loss: 0.2768 - val_acc: 0.9273\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8929\n",
      "Epoch 00021: val_loss improved from 0.27683 to 0.27053, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/021-0.2705.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3491 - acc: 0.8929 - val_loss: 0.2705 - val_acc: 0.9287\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8960\n",
      "Epoch 00022: val_loss did not improve from 0.27053\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3390 - acc: 0.8960 - val_loss: 0.2736 - val_acc: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3239 - acc: 0.8990\n",
      "Epoch 00023: val_loss improved from 0.27053 to 0.25465, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/023-0.2546.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3239 - acc: 0.8990 - val_loss: 0.2546 - val_acc: 0.9338\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9033\n",
      "Epoch 00024: val_loss did not improve from 0.25465\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3134 - acc: 0.9034 - val_loss: 0.2779 - val_acc: 0.9175\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.9065\n",
      "Epoch 00025: val_loss improved from 0.25465 to 0.24859, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/025-0.2486.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3045 - acc: 0.9066 - val_loss: 0.2486 - val_acc: 0.9355\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9086\n",
      "Epoch 00026: val_loss improved from 0.24859 to 0.24288, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/026-0.2429.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2920 - acc: 0.9087 - val_loss: 0.2429 - val_acc: 0.9371\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9107\n",
      "Epoch 00027: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2887 - acc: 0.9107 - val_loss: 0.2775 - val_acc: 0.9175\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2835 - acc: 0.9110\n",
      "Epoch 00028: val_loss did not improve from 0.24288\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2835 - acc: 0.9110 - val_loss: 0.2520 - val_acc: 0.9276\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9147\n",
      "Epoch 00029: val_loss improved from 0.24288 to 0.22813, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/029-0.2281.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2721 - acc: 0.9147 - val_loss: 0.2281 - val_acc: 0.9378\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9177\n",
      "Epoch 00030: val_loss did not improve from 0.22813\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2613 - acc: 0.9177 - val_loss: 0.2360 - val_acc: 0.9415\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9196\n",
      "Epoch 00031: val_loss improved from 0.22813 to 0.22365, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/031-0.2236.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2563 - acc: 0.9196 - val_loss: 0.2236 - val_acc: 0.9399\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9194\n",
      "Epoch 00032: val_loss did not improve from 0.22365\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2555 - acc: 0.9194 - val_loss: 0.2243 - val_acc: 0.9355\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9240\n",
      "Epoch 00033: val_loss did not improve from 0.22365\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2404 - acc: 0.9240 - val_loss: 0.2296 - val_acc: 0.9348\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9230\n",
      "Epoch 00034: val_loss did not improve from 0.22365\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2426 - acc: 0.9230 - val_loss: 0.2335 - val_acc: 0.9401\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9270\n",
      "Epoch 00035: val_loss improved from 0.22365 to 0.21008, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/035-0.2101.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2351 - acc: 0.9270 - val_loss: 0.2101 - val_acc: 0.9464\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9273\n",
      "Epoch 00036: val_loss improved from 0.21008 to 0.19970, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/036-0.1997.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2258 - acc: 0.9273 - val_loss: 0.1997 - val_acc: 0.9450\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9291\n",
      "Epoch 00037: val_loss did not improve from 0.19970\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2232 - acc: 0.9291 - val_loss: 0.2095 - val_acc: 0.9467\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9295\n",
      "Epoch 00038: val_loss did not improve from 0.19970\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2215 - acc: 0.9295 - val_loss: 0.2140 - val_acc: 0.9418\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9322\n",
      "Epoch 00039: val_loss did not improve from 0.19970\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2149 - acc: 0.9322 - val_loss: 0.2115 - val_acc: 0.9436\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9330\n",
      "Epoch 00040: val_loss improved from 0.19970 to 0.19716, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/040-0.1972.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2109 - acc: 0.9330 - val_loss: 0.1972 - val_acc: 0.9460\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9353\n",
      "Epoch 00041: val_loss did not improve from 0.19716\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2050 - acc: 0.9353 - val_loss: 0.1999 - val_acc: 0.9446\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9344\n",
      "Epoch 00042: val_loss improved from 0.19716 to 0.19013, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/042-0.1901.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2020 - acc: 0.9344 - val_loss: 0.1901 - val_acc: 0.9502\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9352\n",
      "Epoch 00043: val_loss improved from 0.19013 to 0.18462, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/043-0.1846.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1977 - acc: 0.9353 - val_loss: 0.1846 - val_acc: 0.9497\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9387\n",
      "Epoch 00044: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1901 - acc: 0.9387 - val_loss: 0.2161 - val_acc: 0.9404\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9380\n",
      "Epoch 00045: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1898 - acc: 0.9379 - val_loss: 0.2091 - val_acc: 0.9457\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9398\n",
      "Epoch 00046: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1841 - acc: 0.9398 - val_loss: 0.1969 - val_acc: 0.9499\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9402\n",
      "Epoch 00047: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1822 - acc: 0.9402 - val_loss: 0.1916 - val_acc: 0.9478\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9413\n",
      "Epoch 00048: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1763 - acc: 0.9413 - val_loss: 0.1890 - val_acc: 0.9474\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9452\n",
      "Epoch 00049: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1716 - acc: 0.9452 - val_loss: 0.2115 - val_acc: 0.9467\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9420\n",
      "Epoch 00050: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1779 - acc: 0.9420 - val_loss: 0.1962 - val_acc: 0.9518\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9443\n",
      "Epoch 00051: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1715 - acc: 0.9443 - val_loss: 0.1949 - val_acc: 0.9478\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9446\n",
      "Epoch 00052: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1710 - acc: 0.9446 - val_loss: 0.1942 - val_acc: 0.9511\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9445\n",
      "Epoch 00053: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1664 - acc: 0.9445 - val_loss: 0.1979 - val_acc: 0.9502\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9478\n",
      "Epoch 00054: val_loss did not improve from 0.18462\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1594 - acc: 0.9478 - val_loss: 0.1887 - val_acc: 0.9509\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9483\n",
      "Epoch 00055: val_loss improved from 0.18462 to 0.18341, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/055-0.1834.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1571 - acc: 0.9483 - val_loss: 0.1834 - val_acc: 0.9509\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9481\n",
      "Epoch 00056: val_loss improved from 0.18341 to 0.18324, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/056-0.1832.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1575 - acc: 0.9481 - val_loss: 0.1832 - val_acc: 0.9509\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9489\n",
      "Epoch 00057: val_loss did not improve from 0.18324\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1556 - acc: 0.9489 - val_loss: 0.1988 - val_acc: 0.9504\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9513\n",
      "Epoch 00058: val_loss improved from 0.18324 to 0.17756, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/058-0.1776.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1510 - acc: 0.9513 - val_loss: 0.1776 - val_acc: 0.9536\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9505\n",
      "Epoch 00059: val_loss did not improve from 0.17756\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1496 - acc: 0.9506 - val_loss: 0.1885 - val_acc: 0.9504\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9505\n",
      "Epoch 00060: val_loss improved from 0.17756 to 0.17326, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/060-0.1733.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1512 - acc: 0.9505 - val_loss: 0.1733 - val_acc: 0.9557\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9531\n",
      "Epoch 00061: val_loss did not improve from 0.17326\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1440 - acc: 0.9531 - val_loss: 0.1810 - val_acc: 0.9539\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9521\n",
      "Epoch 00062: val_loss improved from 0.17326 to 0.17233, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_7_conv_checkpoint/062-0.1723.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1443 - acc: 0.9521 - val_loss: 0.1723 - val_acc: 0.9541\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9543\n",
      "Epoch 00063: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1389 - acc: 0.9543 - val_loss: 0.1837 - val_acc: 0.9543\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9525\n",
      "Epoch 00064: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1404 - acc: 0.9525 - val_loss: 0.1884 - val_acc: 0.9532\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9546\n",
      "Epoch 00065: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1347 - acc: 0.9547 - val_loss: 0.1808 - val_acc: 0.9525\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9548\n",
      "Epoch 00066: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1379 - acc: 0.9548 - val_loss: 0.1759 - val_acc: 0.9527\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9561\n",
      "Epoch 00067: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1308 - acc: 0.9561 - val_loss: 0.2002 - val_acc: 0.9513\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9554\n",
      "Epoch 00068: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1341 - acc: 0.9553 - val_loss: 0.1920 - val_acc: 0.9499\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9567\n",
      "Epoch 00069: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1313 - acc: 0.9567 - val_loss: 0.1743 - val_acc: 0.9567\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9579\n",
      "Epoch 00070: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1277 - acc: 0.9579 - val_loss: 0.1815 - val_acc: 0.9525\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9569- ETA: 0s - loss: 0.1290 - acc: 0.9\n",
      "Epoch 00071: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1289 - acc: 0.9569 - val_loss: 0.2091 - val_acc: 0.9539\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9570\n",
      "Epoch 00072: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1272 - acc: 0.9570 - val_loss: 0.1859 - val_acc: 0.9564\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9582\n",
      "Epoch 00073: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1215 - acc: 0.9582 - val_loss: 0.1818 - val_acc: 0.9550\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9588\n",
      "Epoch 00074: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1232 - acc: 0.9588 - val_loss: 0.2026 - val_acc: 0.9529\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9613\n",
      "Epoch 00075: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1167 - acc: 0.9613 - val_loss: 0.1884 - val_acc: 0.9550\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9590\n",
      "Epoch 00076: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1193 - acc: 0.9590 - val_loss: 0.1839 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9601\n",
      "Epoch 00077: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1160 - acc: 0.9601 - val_loss: 0.1891 - val_acc: 0.9527\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9621\n",
      "Epoch 00078: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1146 - acc: 0.9620 - val_loss: 0.1959 - val_acc: 0.9513\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9610\n",
      "Epoch 00079: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1141 - acc: 0.9610 - val_loss: 0.1777 - val_acc: 0.9529\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9631\n",
      "Epoch 00080: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1111 - acc: 0.9631 - val_loss: 0.2075 - val_acc: 0.9506\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9617\n",
      "Epoch 00081: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1116 - acc: 0.9617 - val_loss: 0.1918 - val_acc: 0.9529\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9635\n",
      "Epoch 00082: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1101 - acc: 0.9635 - val_loss: 0.1992 - val_acc: 0.9539\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9633\n",
      "Epoch 00083: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1099 - acc: 0.9633 - val_loss: 0.2006 - val_acc: 0.9550\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9626\n",
      "Epoch 00084: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1096 - acc: 0.9626 - val_loss: 0.1977 - val_acc: 0.9560\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9644\n",
      "Epoch 00085: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1066 - acc: 0.9644 - val_loss: 0.1919 - val_acc: 0.9562\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9646\n",
      "Epoch 00086: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1061 - acc: 0.9646 - val_loss: 0.1872 - val_acc: 0.9536\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9642\n",
      "Epoch 00087: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1005 - acc: 0.9642 - val_loss: 0.1898 - val_acc: 0.9525\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9645\n",
      "Epoch 00088: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1037 - acc: 0.9645 - val_loss: 0.1894 - val_acc: 0.9567\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9662\n",
      "Epoch 00089: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1007 - acc: 0.9662 - val_loss: 0.1740 - val_acc: 0.9555\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9657\n",
      "Epoch 00090: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1002 - acc: 0.9657 - val_loss: 0.1975 - val_acc: 0.9555\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9671\n",
      "Epoch 00091: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0983 - acc: 0.9671 - val_loss: 0.1865 - val_acc: 0.9546\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9690\n",
      "Epoch 00092: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0922 - acc: 0.9690 - val_loss: 0.1866 - val_acc: 0.9562\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9680\n",
      "Epoch 00093: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0927 - acc: 0.9680 - val_loss: 0.2425 - val_acc: 0.9476\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9668\n",
      "Epoch 00094: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0951 - acc: 0.9668 - val_loss: 0.2278 - val_acc: 0.9495\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9679\n",
      "Epoch 00095: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0931 - acc: 0.9679 - val_loss: 0.1909 - val_acc: 0.9576\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9690\n",
      "Epoch 00096: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0917 - acc: 0.9690 - val_loss: 0.1889 - val_acc: 0.9588\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9676\n",
      "Epoch 00097: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0933 - acc: 0.9676 - val_loss: 0.2010 - val_acc: 0.9553\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9697\n",
      "Epoch 00098: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0901 - acc: 0.9697 - val_loss: 0.2027 - val_acc: 0.9562\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9696\n",
      "Epoch 00099: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0879 - acc: 0.9696 - val_loss: 0.1922 - val_acc: 0.9522\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9689\n",
      "Epoch 00100: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0916 - acc: 0.9689 - val_loss: 0.1817 - val_acc: 0.9550\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9706\n",
      "Epoch 00101: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0867 - acc: 0.9706 - val_loss: 0.1919 - val_acc: 0.9567\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9723\n",
      "Epoch 00102: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0837 - acc: 0.9723 - val_loss: 0.1857 - val_acc: 0.9576\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9709\n",
      "Epoch 00103: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0853 - acc: 0.9709 - val_loss: 0.1859 - val_acc: 0.9562\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9711\n",
      "Epoch 00104: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0852 - acc: 0.9711 - val_loss: 0.1840 - val_acc: 0.9585\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9713\n",
      "Epoch 00105: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0836 - acc: 0.9713 - val_loss: 0.1973 - val_acc: 0.9574\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9729\n",
      "Epoch 00106: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0803 - acc: 0.9729 - val_loss: 0.1987 - val_acc: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9707\n",
      "Epoch 00107: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0854 - acc: 0.9707 - val_loss: 0.2055 - val_acc: 0.9562\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9712\n",
      "Epoch 00108: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0837 - acc: 0.9712 - val_loss: 0.1977 - val_acc: 0.9518\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9721\n",
      "Epoch 00109: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0817 - acc: 0.9721 - val_loss: 0.1909 - val_acc: 0.9553\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9741\n",
      "Epoch 00110: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0750 - acc: 0.9741 - val_loss: 0.1998 - val_acc: 0.9588\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9743\n",
      "Epoch 00111: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0779 - acc: 0.9744 - val_loss: 0.1919 - val_acc: 0.9564\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9717\n",
      "Epoch 00112: val_loss did not improve from 0.17233\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0810 - acc: 0.9717 - val_loss: 0.2092 - val_acc: 0.9569\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2034 - acc: 0.9421\n",
      "Loss: 0.20344936875539404 Accuracy: 0.94205606\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4123 - acc: 0.2112\n",
      "Epoch 00001: val_loss improved from inf to 1.77737, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/001-1.7774.hdf5\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 2.4122 - acc: 0.2112 - val_loss: 1.7774 - val_acc: 0.4458\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6858 - acc: 0.4477\n",
      "Epoch 00002: val_loss improved from 1.77737 to 1.18714, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/002-1.1871.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.6856 - acc: 0.4478 - val_loss: 1.1871 - val_acc: 0.6385\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3205 - acc: 0.5673\n",
      "Epoch 00003: val_loss improved from 1.18714 to 0.91731, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/003-0.9173.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.3205 - acc: 0.5672 - val_loss: 0.9173 - val_acc: 0.7165\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1175 - acc: 0.6374\n",
      "Epoch 00004: val_loss improved from 0.91731 to 0.79464, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/004-0.7946.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.1175 - acc: 0.6374 - val_loss: 0.7946 - val_acc: 0.7675\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9565 - acc: 0.6903\n",
      "Epoch 00005: val_loss improved from 0.79464 to 0.62499, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/005-0.6250.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.9565 - acc: 0.6903 - val_loss: 0.6250 - val_acc: 0.8092\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8408 - acc: 0.7287\n",
      "Epoch 00006: val_loss improved from 0.62499 to 0.55009, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/006-0.5501.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.8407 - acc: 0.7288 - val_loss: 0.5501 - val_acc: 0.8421\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7328 - acc: 0.7648\n",
      "Epoch 00007: val_loss improved from 0.55009 to 0.46732, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/007-0.4673.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.7328 - acc: 0.7648 - val_loss: 0.4673 - val_acc: 0.8565\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6521 - acc: 0.7912\n",
      "Epoch 00008: val_loss improved from 0.46732 to 0.40150, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/008-0.4015.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.6521 - acc: 0.7912 - val_loss: 0.4015 - val_acc: 0.8756\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5898 - acc: 0.8135\n",
      "Epoch 00009: val_loss improved from 0.40150 to 0.37100, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/009-0.3710.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.5897 - acc: 0.8136 - val_loss: 0.3710 - val_acc: 0.8891\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.8286\n",
      "Epoch 00010: val_loss improved from 0.37100 to 0.33079, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/010-0.3308.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.5439 - acc: 0.8286 - val_loss: 0.3308 - val_acc: 0.9057\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4878 - acc: 0.8461\n",
      "Epoch 00011: val_loss improved from 0.33079 to 0.30885, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/011-0.3088.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4879 - acc: 0.8461 - val_loss: 0.3088 - val_acc: 0.9064\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.8571\n",
      "Epoch 00012: val_loss improved from 0.30885 to 0.28167, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/012-0.2817.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4568 - acc: 0.8571 - val_loss: 0.2817 - val_acc: 0.9159\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.8639\n",
      "Epoch 00013: val_loss improved from 0.28167 to 0.25630, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/013-0.2563.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4268 - acc: 0.8639 - val_loss: 0.2563 - val_acc: 0.9278\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8740\n",
      "Epoch 00014: val_loss improved from 0.25630 to 0.25555, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/014-0.2556.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4009 - acc: 0.8741 - val_loss: 0.2556 - val_acc: 0.9257\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3796 - acc: 0.8793\n",
      "Epoch 00015: val_loss improved from 0.25555 to 0.23042, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/015-0.2304.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3796 - acc: 0.8793 - val_loss: 0.2304 - val_acc: 0.9299\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8875\n",
      "Epoch 00016: val_loss improved from 0.23042 to 0.22176, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/016-0.2218.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3583 - acc: 0.8875 - val_loss: 0.2218 - val_acc: 0.9341\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.8926\n",
      "Epoch 00017: val_loss did not improve from 0.22176\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3432 - acc: 0.8925 - val_loss: 0.2301 - val_acc: 0.9348\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8989\n",
      "Epoch 00018: val_loss improved from 0.22176 to 0.22046, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/018-0.2205.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3226 - acc: 0.8989 - val_loss: 0.2205 - val_acc: 0.9359\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.9010\n",
      "Epoch 00019: val_loss did not improve from 0.22046\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3140 - acc: 0.9010 - val_loss: 0.2344 - val_acc: 0.9294\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3030 - acc: 0.9046\n",
      "Epoch 00020: val_loss improved from 0.22046 to 0.20500, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/020-0.2050.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3030 - acc: 0.9046 - val_loss: 0.2050 - val_acc: 0.9404\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9076\n",
      "Epoch 00021: val_loss improved from 0.20500 to 0.18241, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/021-0.1824.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2905 - acc: 0.9076 - val_loss: 0.1824 - val_acc: 0.9483\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9124\n",
      "Epoch 00022: val_loss improved from 0.18241 to 0.17794, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/022-0.1779.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2779 - acc: 0.9124 - val_loss: 0.1779 - val_acc: 0.9481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9152\n",
      "Epoch 00023: val_loss did not improve from 0.17794\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2680 - acc: 0.9152 - val_loss: 0.1952 - val_acc: 0.9425\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9168\n",
      "Epoch 00024: val_loss did not improve from 0.17794\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2622 - acc: 0.9168 - val_loss: 0.1813 - val_acc: 0.9457\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9176\n",
      "Epoch 00025: val_loss did not improve from 0.17794\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2570 - acc: 0.9176 - val_loss: 0.1782 - val_acc: 0.9485\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9210\n",
      "Epoch 00026: val_loss did not improve from 0.17794\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2493 - acc: 0.9210 - val_loss: 0.1940 - val_acc: 0.9429\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9234\n",
      "Epoch 00027: val_loss improved from 0.17794 to 0.17084, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/027-0.1708.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2383 - acc: 0.9234 - val_loss: 0.1708 - val_acc: 0.9511\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9267\n",
      "Epoch 00028: val_loss did not improve from 0.17084\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2327 - acc: 0.9267 - val_loss: 0.1780 - val_acc: 0.9506\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9276\n",
      "Epoch 00029: val_loss improved from 0.17084 to 0.15970, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/029-0.1597.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2251 - acc: 0.9276 - val_loss: 0.1597 - val_acc: 0.9529\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9305\n",
      "Epoch 00030: val_loss improved from 0.15970 to 0.15796, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/030-0.1580.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2151 - acc: 0.9305 - val_loss: 0.1580 - val_acc: 0.9546\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9327\n",
      "Epoch 00031: val_loss did not improve from 0.15796\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2136 - acc: 0.9328 - val_loss: 0.1603 - val_acc: 0.9506\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9363\n",
      "Epoch 00032: val_loss improved from 0.15796 to 0.15742, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/032-0.1574.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1991 - acc: 0.9363 - val_loss: 0.1574 - val_acc: 0.9541\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9356\n",
      "Epoch 00033: val_loss improved from 0.15742 to 0.14040, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/033-0.1404.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1979 - acc: 0.9356 - val_loss: 0.1404 - val_acc: 0.9599\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9381\n",
      "Epoch 00034: val_loss did not improve from 0.14040\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1893 - acc: 0.9381 - val_loss: 0.1524 - val_acc: 0.9532\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9389\n",
      "Epoch 00035: val_loss did not improve from 0.14040\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1892 - acc: 0.9389 - val_loss: 0.1435 - val_acc: 0.9553\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9419\n",
      "Epoch 00036: val_loss did not improve from 0.14040\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1844 - acc: 0.9419 - val_loss: 0.1486 - val_acc: 0.9595\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9404\n",
      "Epoch 00037: val_loss did not improve from 0.14040\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1801 - acc: 0.9404 - val_loss: 0.1465 - val_acc: 0.9536\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9418\n",
      "Epoch 00038: val_loss did not improve from 0.14040\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1768 - acc: 0.9418 - val_loss: 0.1450 - val_acc: 0.9588\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9431\n",
      "Epoch 00039: val_loss did not improve from 0.14040\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1745 - acc: 0.9431 - val_loss: 0.1468 - val_acc: 0.9564\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9451\n",
      "Epoch 00040: val_loss did not improve from 0.14040\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1657 - acc: 0.9451 - val_loss: 0.1679 - val_acc: 0.9504\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9455\n",
      "Epoch 00041: val_loss did not improve from 0.14040\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1674 - acc: 0.9455 - val_loss: 0.1413 - val_acc: 0.9595\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9483\n",
      "Epoch 00042: val_loss improved from 0.14040 to 0.13775, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/042-0.1378.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1607 - acc: 0.9483 - val_loss: 0.1378 - val_acc: 0.9576\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9489\n",
      "Epoch 00043: val_loss did not improve from 0.13775\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1570 - acc: 0.9489 - val_loss: 0.1418 - val_acc: 0.9583\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9491\n",
      "Epoch 00044: val_loss did not improve from 0.13775\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1529 - acc: 0.9491 - val_loss: 0.1522 - val_acc: 0.9585\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9517\n",
      "Epoch 00045: val_loss did not improve from 0.13775\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1472 - acc: 0.9517 - val_loss: 0.1416 - val_acc: 0.9588\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9520\n",
      "Epoch 00046: val_loss improved from 0.13775 to 0.12881, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/046-0.1288.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1467 - acc: 0.9520 - val_loss: 0.1288 - val_acc: 0.9630\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9523\n",
      "Epoch 00047: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1446 - acc: 0.9523 - val_loss: 0.1369 - val_acc: 0.9625\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9532\n",
      "Epoch 00048: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1410 - acc: 0.9532 - val_loss: 0.1392 - val_acc: 0.9611\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9545\n",
      "Epoch 00049: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1379 - acc: 0.9544 - val_loss: 0.1540 - val_acc: 0.9578\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9546\n",
      "Epoch 00050: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1383 - acc: 0.9547 - val_loss: 0.1420 - val_acc: 0.9560\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9553\n",
      "Epoch 00051: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1356 - acc: 0.9553 - val_loss: 0.1289 - val_acc: 0.9637\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9566\n",
      "Epoch 00052: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1311 - acc: 0.9566 - val_loss: 0.1361 - val_acc: 0.9639\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9585\n",
      "Epoch 00053: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1251 - acc: 0.9585 - val_loss: 0.1388 - val_acc: 0.9630\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9573\n",
      "Epoch 00054: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1275 - acc: 0.9573 - val_loss: 0.1375 - val_acc: 0.9634\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9577\n",
      "Epoch 00055: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1248 - acc: 0.9578 - val_loss: 0.1352 - val_acc: 0.9632\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9597\n",
      "Epoch 00056: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1207 - acc: 0.9597 - val_loss: 0.1322 - val_acc: 0.9632\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9614\n",
      "Epoch 00057: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1174 - acc: 0.9614 - val_loss: 0.1384 - val_acc: 0.9613\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9613\n",
      "Epoch 00058: val_loss did not improve from 0.12881\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1154 - acc: 0.9613 - val_loss: 0.1368 - val_acc: 0.9616\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9616\n",
      "Epoch 00059: val_loss improved from 0.12881 to 0.12418, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/059-0.1242.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1143 - acc: 0.9616 - val_loss: 0.1242 - val_acc: 0.9637\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9618\n",
      "Epoch 00060: val_loss did not improve from 0.12418\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1132 - acc: 0.9619 - val_loss: 0.1303 - val_acc: 0.9639\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9636\n",
      "Epoch 00061: val_loss did not improve from 0.12418\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1072 - acc: 0.9636 - val_loss: 0.1358 - val_acc: 0.9616\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9622\n",
      "Epoch 00062: val_loss did not improve from 0.12418\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1085 - acc: 0.9622 - val_loss: 0.1260 - val_acc: 0.9655\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9641\n",
      "Epoch 00063: val_loss improved from 0.12418 to 0.12394, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/063-0.1239.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1052 - acc: 0.9641 - val_loss: 0.1239 - val_acc: 0.9660\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9638\n",
      "Epoch 00064: val_loss did not improve from 0.12394\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1069 - acc: 0.9638 - val_loss: 0.1324 - val_acc: 0.9616\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9652\n",
      "Epoch 00065: val_loss improved from 0.12394 to 0.12248, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/065-0.1225.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1033 - acc: 0.9652 - val_loss: 0.1225 - val_acc: 0.9648\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9652\n",
      "Epoch 00066: val_loss did not improve from 0.12248\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1038 - acc: 0.9652 - val_loss: 0.1449 - val_acc: 0.9609\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9661\n",
      "Epoch 00067: val_loss did not improve from 0.12248\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0997 - acc: 0.9661 - val_loss: 0.1243 - val_acc: 0.9632\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9665\n",
      "Epoch 00068: val_loss improved from 0.12248 to 0.12135, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_8_conv_checkpoint/068-0.1213.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0996 - acc: 0.9665 - val_loss: 0.1213 - val_acc: 0.9653\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9680\n",
      "Epoch 00069: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0946 - acc: 0.9680 - val_loss: 0.1230 - val_acc: 0.9653\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9667\n",
      "Epoch 00070: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0973 - acc: 0.9667 - val_loss: 0.1275 - val_acc: 0.9644\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9680\n",
      "Epoch 00071: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0932 - acc: 0.9680 - val_loss: 0.1233 - val_acc: 0.9653\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9686\n",
      "Epoch 00072: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0917 - acc: 0.9686 - val_loss: 0.1333 - val_acc: 0.9646\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9696\n",
      "Epoch 00073: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0901 - acc: 0.9697 - val_loss: 0.1373 - val_acc: 0.9660\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9689\n",
      "Epoch 00074: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0910 - acc: 0.9689 - val_loss: 0.1231 - val_acc: 0.9648\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9714\n",
      "Epoch 00075: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0853 - acc: 0.9714 - val_loss: 0.1466 - val_acc: 0.9620\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9703\n",
      "Epoch 00076: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0884 - acc: 0.9703 - val_loss: 0.1424 - val_acc: 0.9634\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9705\n",
      "Epoch 00077: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0871 - acc: 0.9705 - val_loss: 0.1498 - val_acc: 0.9604\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9714\n",
      "Epoch 00078: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0828 - acc: 0.9714 - val_loss: 0.1426 - val_acc: 0.9604\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9708\n",
      "Epoch 00079: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0856 - acc: 0.9708 - val_loss: 0.1341 - val_acc: 0.9681\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9730\n",
      "Epoch 00080: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0777 - acc: 0.9730 - val_loss: 0.1271 - val_acc: 0.9660\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9710\n",
      "Epoch 00081: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0821 - acc: 0.9710 - val_loss: 0.1546 - val_acc: 0.9606\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9732\n",
      "Epoch 00082: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0764 - acc: 0.9732 - val_loss: 0.1349 - val_acc: 0.9655\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9737\n",
      "Epoch 00083: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0769 - acc: 0.9737 - val_loss: 0.1328 - val_acc: 0.9672\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9727\n",
      "Epoch 00084: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0787 - acc: 0.9727 - val_loss: 0.1420 - val_acc: 0.9611\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9742\n",
      "Epoch 00085: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0780 - acc: 0.9742 - val_loss: 0.1215 - val_acc: 0.9686\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9732\n",
      "Epoch 00086: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0767 - acc: 0.9732 - val_loss: 0.1259 - val_acc: 0.9665\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9747\n",
      "Epoch 00087: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0772 - acc: 0.9747 - val_loss: 0.1219 - val_acc: 0.9690\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9758\n",
      "Epoch 00088: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0718 - acc: 0.9758 - val_loss: 0.1367 - val_acc: 0.9653\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9740\n",
      "Epoch 00089: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0748 - acc: 0.9740 - val_loss: 0.1447 - val_acc: 0.9662\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9749\n",
      "Epoch 00090: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0747 - acc: 0.9749 - val_loss: 0.1310 - val_acc: 0.9655\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9749\n",
      "Epoch 00091: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0744 - acc: 0.9749 - val_loss: 0.1481 - val_acc: 0.9648\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9769\n",
      "Epoch 00092: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0663 - acc: 0.9769 - val_loss: 0.1353 - val_acc: 0.9679\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9753\n",
      "Epoch 00093: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0711 - acc: 0.9753 - val_loss: 0.1269 - val_acc: 0.9658\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9775\n",
      "Epoch 00094: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0648 - acc: 0.9775 - val_loss: 0.1344 - val_acc: 0.9695\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9773\n",
      "Epoch 00095: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0634 - acc: 0.9773 - val_loss: 0.1336 - val_acc: 0.9674\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9767\n",
      "Epoch 00096: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0686 - acc: 0.9767 - val_loss: 0.1254 - val_acc: 0.9674\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9777\n",
      "Epoch 00097: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0650 - acc: 0.9777 - val_loss: 0.1253 - val_acc: 0.9669\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9771\n",
      "Epoch 00098: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0656 - acc: 0.9771 - val_loss: 0.1265 - val_acc: 0.9672\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9789\n",
      "Epoch 00099: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0610 - acc: 0.9789 - val_loss: 0.1326 - val_acc: 0.9660\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9777\n",
      "Epoch 00100: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0635 - acc: 0.9777 - val_loss: 0.1423 - val_acc: 0.9669\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9792\n",
      "Epoch 00101: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0627 - acc: 0.9792 - val_loss: 0.1391 - val_acc: 0.9674\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9778\n",
      "Epoch 00102: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0617 - acc: 0.9778 - val_loss: 0.1554 - val_acc: 0.9632\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9792\n",
      "Epoch 00103: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0647 - acc: 0.9792 - val_loss: 0.1335 - val_acc: 0.9672\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9790\n",
      "Epoch 00104: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0626 - acc: 0.9790 - val_loss: 0.1398 - val_acc: 0.9637\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9802\n",
      "Epoch 00105: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0555 - acc: 0.9802 - val_loss: 0.1380 - val_acc: 0.9688\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9801\n",
      "Epoch 00106: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0588 - acc: 0.9801 - val_loss: 0.1545 - val_acc: 0.9653\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9786\n",
      "Epoch 00107: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0608 - acc: 0.9786 - val_loss: 0.1275 - val_acc: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9799\n",
      "Epoch 00108: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0567 - acc: 0.9799 - val_loss: 0.1538 - val_acc: 0.9674\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9820\n",
      "Epoch 00109: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0539 - acc: 0.9820 - val_loss: 0.1347 - val_acc: 0.9669\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9800\n",
      "Epoch 00110: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0560 - acc: 0.9800 - val_loss: 0.1460 - val_acc: 0.9641\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9815\n",
      "Epoch 00111: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0532 - acc: 0.9816 - val_loss: 0.1366 - val_acc: 0.9679\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9817\n",
      "Epoch 00112: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0539 - acc: 0.9817 - val_loss: 0.1736 - val_acc: 0.9602\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9809\n",
      "Epoch 00113: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0563 - acc: 0.9809 - val_loss: 0.1467 - val_acc: 0.9683\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9824\n",
      "Epoch 00114: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0526 - acc: 0.9824 - val_loss: 0.1418 - val_acc: 0.9651\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9819\n",
      "Epoch 00115: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0510 - acc: 0.9819 - val_loss: 0.1424 - val_acc: 0.9686\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9805\n",
      "Epoch 00116: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0556 - acc: 0.9805 - val_loss: 0.1534 - val_acc: 0.9627\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9818\n",
      "Epoch 00117: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0531 - acc: 0.9818 - val_loss: 0.1340 - val_acc: 0.9669\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9819\n",
      "Epoch 00118: val_loss did not improve from 0.12135\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0517 - acc: 0.9819 - val_loss: 0.1495 - val_acc: 0.9672\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1828 - acc: 0.9516\n",
      "Loss: 0.18280254292958498 Accuracy: 0.95160955\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3524 - acc: 0.2341\n",
      "Epoch 00001: val_loss improved from inf to 1.55710, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/001-1.5571.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 2.3522 - acc: 0.2342 - val_loss: 1.5571 - val_acc: 0.5593\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6548 - acc: 0.4717\n",
      "Epoch 00002: val_loss improved from 1.55710 to 1.07656, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/002-1.0766.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.6546 - acc: 0.4718 - val_loss: 1.0766 - val_acc: 0.6851\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3454 - acc: 0.5662- ETA\n",
      "Epoch 00003: val_loss improved from 1.07656 to 0.90000, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/003-0.9000.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.3453 - acc: 0.5662 - val_loss: 0.9000 - val_acc: 0.7228\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1637 - acc: 0.6239\n",
      "Epoch 00004: val_loss improved from 0.90000 to 0.70257, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/004-0.7026.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.1636 - acc: 0.6239 - val_loss: 0.7026 - val_acc: 0.7932\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0304 - acc: 0.6721\n",
      "Epoch 00005: val_loss improved from 0.70257 to 0.61111, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/005-0.6111.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.0305 - acc: 0.6720 - val_loss: 0.6111 - val_acc: 0.8258\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9268 - acc: 0.7063\n",
      "Epoch 00006: val_loss improved from 0.61111 to 0.52377, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/006-0.5238.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.9267 - acc: 0.7063 - val_loss: 0.5238 - val_acc: 0.8514\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8404 - acc: 0.7352\n",
      "Epoch 00007: val_loss improved from 0.52377 to 0.47040, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/007-0.4704.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.8403 - acc: 0.7352 - val_loss: 0.4704 - val_acc: 0.8609\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7622 - acc: 0.7614\n",
      "Epoch 00008: val_loss improved from 0.47040 to 0.41523, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/008-0.4152.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.7622 - acc: 0.7614 - val_loss: 0.4152 - val_acc: 0.8789\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7032 - acc: 0.7798\n",
      "Epoch 00009: val_loss improved from 0.41523 to 0.36012, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/009-0.3601.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.7033 - acc: 0.7798 - val_loss: 0.3601 - val_acc: 0.8989\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6518 - acc: 0.7952\n",
      "Epoch 00010: val_loss improved from 0.36012 to 0.34572, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/010-0.3457.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.6518 - acc: 0.7952 - val_loss: 0.3457 - val_acc: 0.9061\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.8137\n",
      "Epoch 00011: val_loss improved from 0.34572 to 0.31721, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/011-0.3172.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.6019 - acc: 0.8137 - val_loss: 0.3172 - val_acc: 0.9089\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5769 - acc: 0.8206\n",
      "Epoch 00012: val_loss improved from 0.31721 to 0.29524, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/012-0.2952.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.5769 - acc: 0.8206 - val_loss: 0.2952 - val_acc: 0.9175\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.8331\n",
      "Epoch 00013: val_loss did not improve from 0.29524\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.5404 - acc: 0.8331 - val_loss: 0.3149 - val_acc: 0.9061\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.8427\n",
      "Epoch 00014: val_loss improved from 0.29524 to 0.26505, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/014-0.2650.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.5069 - acc: 0.8427 - val_loss: 0.2650 - val_acc: 0.9262\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4798 - acc: 0.8517\n",
      "Epoch 00015: val_loss improved from 0.26505 to 0.24728, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/015-0.2473.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4798 - acc: 0.8517 - val_loss: 0.2473 - val_acc: 0.9313\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8593\n",
      "Epoch 00016: val_loss improved from 0.24728 to 0.24672, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/016-0.2467.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4572 - acc: 0.8593 - val_loss: 0.2467 - val_acc: 0.9306\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.8667\n",
      "Epoch 00017: val_loss improved from 0.24672 to 0.21851, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/017-0.2185.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4307 - acc: 0.8667 - val_loss: 0.2185 - val_acc: 0.9385\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8701\n",
      "Epoch 00018: val_loss improved from 0.21851 to 0.21470, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/018-0.2147.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.4178 - acc: 0.8701 - val_loss: 0.2147 - val_acc: 0.9366\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8777\n",
      "Epoch 00019: val_loss improved from 0.21470 to 0.21327, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/019-0.2133.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3969 - acc: 0.8777 - val_loss: 0.2133 - val_acc: 0.9392\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.8815\n",
      "Epoch 00020: val_loss did not improve from 0.21327\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3881 - acc: 0.8815 - val_loss: 0.2291 - val_acc: 0.9362\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8873\n",
      "Epoch 00021: val_loss improved from 0.21327 to 0.19096, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/021-0.1910.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3632 - acc: 0.8873 - val_loss: 0.1910 - val_acc: 0.9453\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8867\n",
      "Epoch 00022: val_loss did not improve from 0.19096\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3632 - acc: 0.8867 - val_loss: 0.2234 - val_acc: 0.9334\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.8940\n",
      "Epoch 00023: val_loss improved from 0.19096 to 0.18345, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/023-0.1834.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3425 - acc: 0.8940 - val_loss: 0.1834 - val_acc: 0.9488\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8962\n",
      "Epoch 00024: val_loss did not improve from 0.18345\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3355 - acc: 0.8962 - val_loss: 0.1912 - val_acc: 0.9488\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.9008\n",
      "Epoch 00025: val_loss did not improve from 0.18345\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3227 - acc: 0.9009 - val_loss: 0.1951 - val_acc: 0.9450\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9048\n",
      "Epoch 00026: val_loss improved from 0.18345 to 0.17613, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/026-0.1761.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3065 - acc: 0.9048 - val_loss: 0.1761 - val_acc: 0.9502\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9071\n",
      "Epoch 00027: val_loss did not improve from 0.17613\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3031 - acc: 0.9071 - val_loss: 0.1781 - val_acc: 0.9504\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9096\n",
      "Epoch 00028: val_loss improved from 0.17613 to 0.16375, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/028-0.1638.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2914 - acc: 0.9096 - val_loss: 0.1638 - val_acc: 0.9511\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9125\n",
      "Epoch 00029: val_loss improved from 0.16375 to 0.16035, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/029-0.1603.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2837 - acc: 0.9125 - val_loss: 0.1603 - val_acc: 0.9550\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.9143\n",
      "Epoch 00030: val_loss did not improve from 0.16035\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2769 - acc: 0.9144 - val_loss: 0.1633 - val_acc: 0.9548\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9166\n",
      "Epoch 00031: val_loss improved from 0.16035 to 0.16022, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/031-0.1602.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2708 - acc: 0.9166 - val_loss: 0.1602 - val_acc: 0.9536\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.9204\n",
      "Epoch 00032: val_loss did not improve from 0.16022\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2580 - acc: 0.9204 - val_loss: 0.1664 - val_acc: 0.9515\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9225\n",
      "Epoch 00033: val_loss did not improve from 0.16022\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2528 - acc: 0.9225 - val_loss: 0.1702 - val_acc: 0.9548\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9256\n",
      "Epoch 00034: val_loss improved from 0.16022 to 0.15785, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/034-0.1578.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2441 - acc: 0.9256 - val_loss: 0.1578 - val_acc: 0.9546\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9239\n",
      "Epoch 00035: val_loss did not improve from 0.15785\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2401 - acc: 0.9239 - val_loss: 0.1634 - val_acc: 0.9543\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9250\n",
      "Epoch 00036: val_loss improved from 0.15785 to 0.15131, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/036-0.1513.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2401 - acc: 0.9250 - val_loss: 0.1513 - val_acc: 0.9571\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9274\n",
      "Epoch 00037: val_loss did not improve from 0.15131\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2284 - acc: 0.9274 - val_loss: 0.1638 - val_acc: 0.9534\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9303\n",
      "Epoch 00038: val_loss did not improve from 0.15131\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2239 - acc: 0.9303 - val_loss: 0.1606 - val_acc: 0.9569\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9311\n",
      "Epoch 00039: val_loss did not improve from 0.15131\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2178 - acc: 0.9310 - val_loss: 0.1699 - val_acc: 0.9527\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9332\n",
      "Epoch 00040: val_loss did not improve from 0.15131\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2151 - acc: 0.9332 - val_loss: 0.1598 - val_acc: 0.9567\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9363\n",
      "Epoch 00041: val_loss did not improve from 0.15131\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2046 - acc: 0.9363 - val_loss: 0.1548 - val_acc: 0.9557\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9345\n",
      "Epoch 00042: val_loss did not improve from 0.15131\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2053 - acc: 0.9345 - val_loss: 0.1539 - val_acc: 0.9578\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9393\n",
      "Epoch 00043: val_loss did not improve from 0.15131\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1981 - acc: 0.9393 - val_loss: 0.1555 - val_acc: 0.9604\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9363\n",
      "Epoch 00044: val_loss improved from 0.15131 to 0.14772, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/044-0.1477.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1999 - acc: 0.9363 - val_loss: 0.1477 - val_acc: 0.9634\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9405\n",
      "Epoch 00045: val_loss improved from 0.14772 to 0.14553, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/045-0.1455.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1885 - acc: 0.9405 - val_loss: 0.1455 - val_acc: 0.9588\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9418\n",
      "Epoch 00046: val_loss did not improve from 0.14553\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1842 - acc: 0.9418 - val_loss: 0.1661 - val_acc: 0.9557\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9413\n",
      "Epoch 00047: val_loss did not improve from 0.14553\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1841 - acc: 0.9412 - val_loss: 0.1596 - val_acc: 0.9604\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9425\n",
      "Epoch 00048: val_loss did not improve from 0.14553\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1790 - acc: 0.9425 - val_loss: 0.1527 - val_acc: 0.9630\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9418\n",
      "Epoch 00049: val_loss improved from 0.14553 to 0.13711, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_DO_9_conv_checkpoint/049-0.1371.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1823 - acc: 0.9418 - val_loss: 0.1371 - val_acc: 0.9632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9441\n",
      "Epoch 00050: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1716 - acc: 0.9441 - val_loss: 0.1495 - val_acc: 0.9609\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9477\n",
      "Epoch 00051: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1672 - acc: 0.9477 - val_loss: 0.1601 - val_acc: 0.9632\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9460\n",
      "Epoch 00052: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1659 - acc: 0.9460 - val_loss: 0.1529 - val_acc: 0.9611\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9474\n",
      "Epoch 00053: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1611 - acc: 0.9474 - val_loss: 0.1474 - val_acc: 0.9627\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9479\n",
      "Epoch 00054: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1599 - acc: 0.9479 - val_loss: 0.1435 - val_acc: 0.9609\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9497\n",
      "Epoch 00055: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1537 - acc: 0.9497 - val_loss: 0.1437 - val_acc: 0.9637\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9515\n",
      "Epoch 00056: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1523 - acc: 0.9515 - val_loss: 0.1432 - val_acc: 0.9630\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9510\n",
      "Epoch 00057: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1464 - acc: 0.9510 - val_loss: 0.1422 - val_acc: 0.9639\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9509\n",
      "Epoch 00058: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1518 - acc: 0.9509 - val_loss: 0.1511 - val_acc: 0.9627\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9530\n",
      "Epoch 00059: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1440 - acc: 0.9530 - val_loss: 0.1402 - val_acc: 0.9651\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9525\n",
      "Epoch 00060: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1413 - acc: 0.9525 - val_loss: 0.1527 - val_acc: 0.9637\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9564\n",
      "Epoch 00061: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1370 - acc: 0.9564 - val_loss: 0.1425 - val_acc: 0.9639\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9552\n",
      "Epoch 00062: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1354 - acc: 0.9552 - val_loss: 0.1667 - val_acc: 0.9595\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9555\n",
      "Epoch 00063: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1322 - acc: 0.9555 - val_loss: 0.1470 - val_acc: 0.9658\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9578\n",
      "Epoch 00064: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1293 - acc: 0.9578 - val_loss: 0.1539 - val_acc: 0.9665\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9592\n",
      "Epoch 00065: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1255 - acc: 0.9592 - val_loss: 0.1494 - val_acc: 0.9658\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9584\n",
      "Epoch 00066: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1252 - acc: 0.9584 - val_loss: 0.1467 - val_acc: 0.9646\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9577\n",
      "Epoch 00067: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1247 - acc: 0.9577 - val_loss: 0.1686 - val_acc: 0.9588\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9589\n",
      "Epoch 00068: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1221 - acc: 0.9589 - val_loss: 0.1550 - val_acc: 0.9637\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9620\n",
      "Epoch 00069: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1184 - acc: 0.9620 - val_loss: 0.1666 - val_acc: 0.9639\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9607\n",
      "Epoch 00070: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1191 - acc: 0.9607 - val_loss: 0.1604 - val_acc: 0.9641\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9608\n",
      "Epoch 00071: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1190 - acc: 0.9608 - val_loss: 0.1574 - val_acc: 0.9646\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9616\n",
      "Epoch 00072: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1138 - acc: 0.9616 - val_loss: 0.1493 - val_acc: 0.9676\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9627\n",
      "Epoch 00073: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1126 - acc: 0.9626 - val_loss: 0.1538 - val_acc: 0.9627\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9638\n",
      "Epoch 00074: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1096 - acc: 0.9638 - val_loss: 0.1534 - val_acc: 0.9674\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9636\n",
      "Epoch 00075: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1076 - acc: 0.9636 - val_loss: 0.1645 - val_acc: 0.9660\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9663\n",
      "Epoch 00076: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1027 - acc: 0.9663 - val_loss: 0.1576 - val_acc: 0.9648\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9663\n",
      "Epoch 00077: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1008 - acc: 0.9663 - val_loss: 0.1662 - val_acc: 0.9618\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9629\n",
      "Epoch 00078: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1120 - acc: 0.9629 - val_loss: 0.1588 - val_acc: 0.9651\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9665\n",
      "Epoch 00079: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0997 - acc: 0.9665 - val_loss: 0.1641 - val_acc: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9668\n",
      "Epoch 00080: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0989 - acc: 0.9668 - val_loss: 0.1746 - val_acc: 0.9618\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9674\n",
      "Epoch 00081: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0967 - acc: 0.9674 - val_loss: 0.1578 - val_acc: 0.9679\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9677\n",
      "Epoch 00082: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0945 - acc: 0.9677 - val_loss: 0.1568 - val_acc: 0.9646\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9683\n",
      "Epoch 00083: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0929 - acc: 0.9683 - val_loss: 0.1560 - val_acc: 0.9662\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9692\n",
      "Epoch 00084: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0918 - acc: 0.9692 - val_loss: 0.1525 - val_acc: 0.9658\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9668\n",
      "Epoch 00085: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0966 - acc: 0.9668 - val_loss: 0.1581 - val_acc: 0.9658\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9700\n",
      "Epoch 00086: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0903 - acc: 0.9700 - val_loss: 0.1538 - val_acc: 0.9644\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9699\n",
      "Epoch 00087: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0876 - acc: 0.9699 - val_loss: 0.1703 - val_acc: 0.9646\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9706\n",
      "Epoch 00088: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0866 - acc: 0.9706 - val_loss: 0.1574 - val_acc: 0.9632\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9714\n",
      "Epoch 00089: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0820 - acc: 0.9714 - val_loss: 0.1811 - val_acc: 0.9658\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9721\n",
      "Epoch 00090: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0840 - acc: 0.9721 - val_loss: 0.1462 - val_acc: 0.9646\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9706\n",
      "Epoch 00091: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0871 - acc: 0.9706 - val_loss: 0.1587 - val_acc: 0.9630\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9722\n",
      "Epoch 00092: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0817 - acc: 0.9722 - val_loss: 0.1681 - val_acc: 0.9658\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9713\n",
      "Epoch 00093: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0826 - acc: 0.9713 - val_loss: 0.1578 - val_acc: 0.9660\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9733\n",
      "Epoch 00094: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0794 - acc: 0.9733 - val_loss: 0.1868 - val_acc: 0.9641\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9727\n",
      "Epoch 00095: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0807 - acc: 0.9727 - val_loss: 0.1494 - val_acc: 0.9641\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9756\n",
      "Epoch 00096: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0728 - acc: 0.9756 - val_loss: 0.1813 - val_acc: 0.9644\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9729\n",
      "Epoch 00097: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0790 - acc: 0.9729 - val_loss: 0.1719 - val_acc: 0.9630\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9742\n",
      "Epoch 00098: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0785 - acc: 0.9742 - val_loss: 0.1656 - val_acc: 0.9679\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9742\n",
      "Epoch 00099: val_loss did not improve from 0.13711\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0745 - acc: 0.9742 - val_loss: 0.1763 - val_acc: 0.9662\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1970 - acc: 0.9425\n",
      "Loss: 0.19702063239530612 Accuracy: 0.94247144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_ch_128_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_ch_128_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 375,056\n",
      "Trainable params: 375,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3915 - acc: 0.8995\n",
      "Loss: 0.3914603922164081 Accuracy: 0.89948076\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 350,544\n",
      "Trainable params: 350,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2034 - acc: 0.9421\n",
      "Loss: 0.20344936875539404 Accuracy: 0.94205606\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 356,752\n",
      "Trainable params: 356,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1828 - acc: 0.9516\n",
      "Loss: 0.18280254292958498 Accuracy: 0.95160955\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 32)             10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 360,880\n",
      "Trainable params: 360,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1970 - acc: 0.9425\n",
      "Loss: 0.19702063239530612 Accuracy: 0.94247144\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_ch_128_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_ch_128_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 375,056\n",
      "Trainable params: 375,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.4393 - acc: 0.9022\n",
      "Loss: 0.43931865336398596 Accuracy: 0.9021807\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 350,544\n",
      "Trainable params: 350,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2336 - acc: 0.9414\n",
      "Loss: 0.23356239164358358 Accuracy: 0.941433\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 356,752\n",
      "Trainable params: 356,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2249 - acc: 0.9510\n",
      "Loss: 0.2249433572968876 Accuracy: 0.9509865\n",
      "\n",
      "1D_CNN_custom_2_ch_128_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 32)             10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 360,880\n",
      "Trainable params: 360,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2195 - acc: 0.9504\n",
      "Loss: 0.21948599532180826 Accuracy: 0.95036346\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
