{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_025_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=64, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=64*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,656\n",
      "Trainable params: 16,384,528\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,482,448\n",
      "Trainable params: 5,482,192\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,904\n",
      "Trainable params: 1,861,520\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 669,264\n",
      "Trainable params: 668,752\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 508,112\n",
      "Trainable params: 507,344\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 320,336\n",
      "Trainable params: 319,312\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 312,784\n",
      "Trainable params: 311,504\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 366,672\n",
      "Trainable params: 365,136\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 525,648\n",
      "Trainable params: 523,600\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_025_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.0916 - acc: 0.2195\n",
      "Epoch 00001: val_loss improved from inf to 2.37649, saving model to model/checkpoint/1D_CNN_custom_DO_025_DO_BN_1_conv_checkpoint/001-2.3765.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 4.0916 - acc: 0.2196 - val_loss: 2.3765 - val_acc: 0.2835\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6057 - acc: 0.5325\n",
      "Epoch 00002: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.6058 - acc: 0.5325 - val_loss: 2.5971 - val_acc: 0.3040\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0652 - acc: 0.6860\n",
      "Epoch 00003: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.0652 - acc: 0.6860 - val_loss: 2.8670 - val_acc: 0.2900\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7912 - acc: 0.7718\n",
      "Epoch 00004: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.7913 - acc: 0.7717 - val_loss: 2.9895 - val_acc: 0.2989\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6217 - acc: 0.8246\n",
      "Epoch 00005: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.6217 - acc: 0.8246 - val_loss: 3.3013 - val_acc: 0.2853\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.8623\n",
      "Epoch 00006: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4951 - acc: 0.8622 - val_loss: 3.8336 - val_acc: 0.2695\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8892\n",
      "Epoch 00007: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4079 - acc: 0.8892 - val_loss: 3.8955 - val_acc: 0.2774\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.9079\n",
      "Epoch 00008: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3391 - acc: 0.9079 - val_loss: 3.9749 - val_acc: 0.2979\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9221\n",
      "Epoch 00009: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2997 - acc: 0.9221 - val_loss: 4.7180 - val_acc: 0.2795\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9343\n",
      "Epoch 00010: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2577 - acc: 0.9343 - val_loss: 6.2992 - val_acc: 0.2413\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9452\n",
      "Epoch 00011: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2254 - acc: 0.9452 - val_loss: 4.6574 - val_acc: 0.2977\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9552\n",
      "Epoch 00012: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1900 - acc: 0.9552 - val_loss: 4.6552 - val_acc: 0.2893\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9620\n",
      "Epoch 00013: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1686 - acc: 0.9620 - val_loss: 5.6994 - val_acc: 0.2520\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9624\n",
      "Epoch 00014: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1670 - acc: 0.9624 - val_loss: 5.3958 - val_acc: 0.2821\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9659\n",
      "Epoch 00015: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1497 - acc: 0.9659 - val_loss: 5.7902 - val_acc: 0.2590\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9721\n",
      "Epoch 00016: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1319 - acc: 0.9722 - val_loss: 6.1453 - val_acc: 0.2746\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9749\n",
      "Epoch 00017: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1207 - acc: 0.9749 - val_loss: 5.3499 - val_acc: 0.3028\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9733\n",
      "Epoch 00018: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1290 - acc: 0.9733 - val_loss: 6.0080 - val_acc: 0.2565\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9747\n",
      "Epoch 00019: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1209 - acc: 0.9747 - val_loss: 6.0741 - val_acc: 0.2851\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9799\n",
      "Epoch 00020: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1016 - acc: 0.9799 - val_loss: 5.7989 - val_acc: 0.2891\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9837\n",
      "Epoch 00021: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0904 - acc: 0.9837 - val_loss: 6.2699 - val_acc: 0.2784\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9842\n",
      "Epoch 00022: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0883 - acc: 0.9842 - val_loss: 6.8026 - val_acc: 0.2735\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9819\n",
      "Epoch 00023: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0916 - acc: 0.9819 - val_loss: 6.2299 - val_acc: 0.2795\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9807\n",
      "Epoch 00024: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0962 - acc: 0.9807 - val_loss: 6.2421 - val_acc: 0.2844\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9818\n",
      "Epoch 00025: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0940 - acc: 0.9819 - val_loss: 6.2600 - val_acc: 0.2742\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9849\n",
      "Epoch 00026: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0843 - acc: 0.9849 - val_loss: 6.0948 - val_acc: 0.3033\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9816\n",
      "Epoch 00027: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0932 - acc: 0.9816 - val_loss: 6.8803 - val_acc: 0.2807\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9911\n",
      "Epoch 00028: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0618 - acc: 0.9911 - val_loss: 6.5950 - val_acc: 0.2870\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9920\n",
      "Epoch 00029: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0587 - acc: 0.9920 - val_loss: 5.9322 - val_acc: 0.3089\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9864\n",
      "Epoch 00030: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0767 - acc: 0.9864 - val_loss: 8.2275 - val_acc: 0.2530\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9841\n",
      "Epoch 00031: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0847 - acc: 0.9841 - val_loss: 7.0748 - val_acc: 0.2865\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9843\n",
      "Epoch 00032: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0813 - acc: 0.9843 - val_loss: 6.4739 - val_acc: 0.2954\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9871\n",
      "Epoch 00033: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0720 - acc: 0.9871 - val_loss: 6.5425 - val_acc: 0.2954\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9861\n",
      "Epoch 00034: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0769 - acc: 0.9861 - val_loss: 6.6302 - val_acc: 0.2891\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9906\n",
      "Epoch 00035: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0605 - acc: 0.9906 - val_loss: 6.2471 - val_acc: 0.3028\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9903\n",
      "Epoch 00036: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0645 - acc: 0.9903 - val_loss: 6.5623 - val_acc: 0.2963\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9916\n",
      "Epoch 00037: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0551 - acc: 0.9916 - val_loss: 6.4832 - val_acc: 0.3014\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9899\n",
      "Epoch 00038: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0623 - acc: 0.9899 - val_loss: 6.7199 - val_acc: 0.2823\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9904\n",
      "Epoch 00039: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0627 - acc: 0.9903 - val_loss: 6.8536 - val_acc: 0.2870\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9910\n",
      "Epoch 00040: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0580 - acc: 0.9910 - val_loss: 8.0671 - val_acc: 0.2548\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9935\n",
      "Epoch 00041: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0505 - acc: 0.9935 - val_loss: 6.5429 - val_acc: 0.3070\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9924\n",
      "Epoch 00042: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0510 - acc: 0.9924 - val_loss: 6.9385 - val_acc: 0.2826\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9904\n",
      "Epoch 00043: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0579 - acc: 0.9904 - val_loss: 6.9861 - val_acc: 0.2718\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9905\n",
      "Epoch 00044: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0593 - acc: 0.9905 - val_loss: 6.8061 - val_acc: 0.2944\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9918\n",
      "Epoch 00045: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0541 - acc: 0.9918 - val_loss: 7.6070 - val_acc: 0.2772\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9932\n",
      "Epoch 00046: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0486 - acc: 0.9932 - val_loss: 6.7806 - val_acc: 0.3056\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9941\n",
      "Epoch 00047: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0446 - acc: 0.9941 - val_loss: 7.2440 - val_acc: 0.2930\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9911\n",
      "Epoch 00048: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0594 - acc: 0.9911 - val_loss: 7.2598 - val_acc: 0.2944\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9890\n",
      "Epoch 00049: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0620 - acc: 0.9891 - val_loss: 7.6594 - val_acc: 0.2919\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9906\n",
      "Epoch 00050: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0604 - acc: 0.9906 - val_loss: 7.7069 - val_acc: 0.2723\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9948\n",
      "Epoch 00051: val_loss did not improve from 2.37649\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0443 - acc: 0.9948 - val_loss: 7.0114 - val_acc: 0.2837\n",
      "\n",
      "1D_CNN_custom_DO_025_DO_BN_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 441us/sample - loss: 2.3987 - acc: 0.2690\n",
      "Loss: 2.3986952675218274 Accuracy: 0.2689512\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0756 - acc: 0.2745\n",
      "Epoch 00001: val_loss improved from inf to 3.68035, saving model to model/checkpoint/1D_CNN_custom_DO_025_DO_BN_2_conv_checkpoint/001-3.6803.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 3.0753 - acc: 0.2746 - val_loss: 3.6803 - val_acc: 0.1728\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9406 - acc: 0.4556\n",
      "Epoch 00002: val_loss improved from 3.68035 to 3.39152, saving model to model/checkpoint/1D_CNN_custom_DO_025_DO_BN_2_conv_checkpoint/002-3.3915.hdf5\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.9405 - acc: 0.4556 - val_loss: 3.3915 - val_acc: 0.2516\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4165 - acc: 0.5818\n",
      "Epoch 00003: val_loss improved from 3.39152 to 3.04882, saving model to model/checkpoint/1D_CNN_custom_DO_025_DO_BN_2_conv_checkpoint/003-3.0488.hdf5\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.4165 - acc: 0.5818 - val_loss: 3.0488 - val_acc: 0.3093\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0866 - acc: 0.6725\n",
      "Epoch 00004: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.0865 - acc: 0.6725 - val_loss: 3.1083 - val_acc: 0.3522\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7628 - acc: 0.7640\n",
      "Epoch 00005: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.7628 - acc: 0.7640 - val_loss: 3.3101 - val_acc: 0.3671\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5760 - acc: 0.8211\n",
      "Epoch 00006: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.5760 - acc: 0.8211 - val_loss: 3.1952 - val_acc: 0.3867\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4475 - acc: 0.8593\n",
      "Epoch 00007: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.4476 - acc: 0.8592 - val_loss: 4.5148 - val_acc: 0.2809\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8816\n",
      "Epoch 00008: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3871 - acc: 0.8816 - val_loss: 3.5357 - val_acc: 0.3869\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.9018\n",
      "Epoch 00009: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3222 - acc: 0.9018 - val_loss: 6.3557 - val_acc: 0.2490\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9075\n",
      "Epoch 00010: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3019 - acc: 0.9075 - val_loss: 5.2477 - val_acc: 0.2844\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9106\n",
      "Epoch 00011: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3007 - acc: 0.9106 - val_loss: 4.1422 - val_acc: 0.3736\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9331\n",
      "Epoch 00012: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2215 - acc: 0.9331 - val_loss: 4.6082 - val_acc: 0.3727\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9367\n",
      "Epoch 00013: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2144 - acc: 0.9367 - val_loss: 5.2041 - val_acc: 0.3641\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9345\n",
      "Epoch 00014: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2271 - acc: 0.9345 - val_loss: 5.1154 - val_acc: 0.3571\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9438\n",
      "Epoch 00015: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1957 - acc: 0.9438 - val_loss: 4.3421 - val_acc: 0.4002\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9513\n",
      "Epoch 00016: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1637 - acc: 0.9513 - val_loss: 5.5468 - val_acc: 0.3291\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9596\n",
      "Epoch 00017: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1475 - acc: 0.9595 - val_loss: 4.6037 - val_acc: 0.3804\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9481\n",
      "Epoch 00018: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1871 - acc: 0.9481 - val_loss: 6.5085 - val_acc: 0.3089\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9600\n",
      "Epoch 00019: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1454 - acc: 0.9600 - val_loss: 5.9103 - val_acc: 0.3368\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9576\n",
      "Epoch 00020: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1540 - acc: 0.9575 - val_loss: 5.5964 - val_acc: 0.3618\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9652\n",
      "Epoch 00021: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1297 - acc: 0.9652 - val_loss: 5.4538 - val_acc: 0.3657\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9599\n",
      "Epoch 00022: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1739 - acc: 0.9599 - val_loss: 5.6491 - val_acc: 0.3753\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9641\n",
      "Epoch 00023: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1359 - acc: 0.9641 - val_loss: 5.4415 - val_acc: 0.3783\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9793\n",
      "Epoch 00024: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0801 - acc: 0.9793 - val_loss: 5.4732 - val_acc: 0.3785\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9735\n",
      "Epoch 00025: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1072 - acc: 0.9735 - val_loss: 5.7880 - val_acc: 0.3587\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9683\n",
      "Epoch 00026: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1199 - acc: 0.9683 - val_loss: 5.2945 - val_acc: 0.3832\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9738\n",
      "Epoch 00027: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0993 - acc: 0.9738 - val_loss: 5.6893 - val_acc: 0.3657\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9718\n",
      "Epoch 00028: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1072 - acc: 0.9718 - val_loss: 5.9097 - val_acc: 0.3699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9711\n",
      "Epoch 00029: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1106 - acc: 0.9711 - val_loss: 5.4617 - val_acc: 0.3811\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9782\n",
      "Epoch 00030: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0864 - acc: 0.9782 - val_loss: 6.6258 - val_acc: 0.3480\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9798\n",
      "Epoch 00031: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0817 - acc: 0.9798 - val_loss: 5.7342 - val_acc: 0.3848\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9750\n",
      "Epoch 00032: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0953 - acc: 0.9750 - val_loss: 5.3641 - val_acc: 0.4139\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9811\n",
      "Epoch 00033: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0770 - acc: 0.9811 - val_loss: 5.7146 - val_acc: 0.3729\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9795\n",
      "Epoch 00034: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0824 - acc: 0.9795 - val_loss: 5.2991 - val_acc: 0.4055\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9809\n",
      "Epoch 00035: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0818 - acc: 0.9809 - val_loss: 6.3476 - val_acc: 0.3566\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9827\n",
      "Epoch 00036: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0754 - acc: 0.9827 - val_loss: 5.5438 - val_acc: 0.3955\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9861\n",
      "Epoch 00037: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0623 - acc: 0.9861 - val_loss: 5.4959 - val_acc: 0.4051\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9826\n",
      "Epoch 00038: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0706 - acc: 0.9826 - val_loss: 5.4502 - val_acc: 0.4004\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9839\n",
      "Epoch 00039: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0675 - acc: 0.9839 - val_loss: 5.6190 - val_acc: 0.3899\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9845\n",
      "Epoch 00040: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0650 - acc: 0.9845 - val_loss: 5.8732 - val_acc: 0.3820\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9824\n",
      "Epoch 00041: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0746 - acc: 0.9824 - val_loss: 5.6493 - val_acc: 0.4060\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9859\n",
      "Epoch 00042: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0627 - acc: 0.9859 - val_loss: 6.5027 - val_acc: 0.3466\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9890\n",
      "Epoch 00043: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0524 - acc: 0.9890 - val_loss: 5.9619 - val_acc: 0.3680\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9864\n",
      "Epoch 00044: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0604 - acc: 0.9864 - val_loss: 5.6944 - val_acc: 0.3993\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9857\n",
      "Epoch 00045: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0608 - acc: 0.9857 - val_loss: 5.7027 - val_acc: 0.3939\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9858\n",
      "Epoch 00046: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0577 - acc: 0.9858 - val_loss: 5.6317 - val_acc: 0.3944\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9882\n",
      "Epoch 00047: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0535 - acc: 0.9881 - val_loss: 5.6342 - val_acc: 0.4018\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9855\n",
      "Epoch 00048: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0647 - acc: 0.9855 - val_loss: 5.5438 - val_acc: 0.4167\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9890\n",
      "Epoch 00049: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0516 - acc: 0.9889 - val_loss: 5.7138 - val_acc: 0.3960\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9865\n",
      "Epoch 00050: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0605 - acc: 0.9866 - val_loss: 6.1070 - val_acc: 0.3832\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9914\n",
      "Epoch 00051: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0417 - acc: 0.9914 - val_loss: 6.4515 - val_acc: 0.3625\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9907\n",
      "Epoch 00052: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0415 - acc: 0.9907 - val_loss: 5.5380 - val_acc: 0.4142\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9887\n",
      "Epoch 00053: val_loss did not improve from 3.04882\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0494 - acc: 0.9887 - val_loss: 5.9011 - val_acc: 0.3979\n",
      "\n",
      "1D_CNN_custom_DO_025_DO_BN_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 745us/sample - loss: 3.0891 - acc: 0.2945\n",
      "Loss: 3.0890887787780286 Accuracy: 0.29449636\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2295 - acc: 0.3582\n",
      "Epoch 00001: val_loss improved from inf to 2.55584, saving model to model/checkpoint/1D_CNN_custom_DO_025_DO_BN_3_conv_checkpoint/001-2.5558.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 2.2294 - acc: 0.3583 - val_loss: 2.5558 - val_acc: 0.2471\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4830 - acc: 0.5539\n",
      "Epoch 00002: val_loss improved from 2.55584 to 1.77632, saving model to model/checkpoint/1D_CNN_custom_DO_025_DO_BN_3_conv_checkpoint/002-1.7763.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.4830 - acc: 0.5539 - val_loss: 1.7763 - val_acc: 0.4917\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1206 - acc: 0.6504\n",
      "Epoch 00003: val_loss did not improve from 1.77632\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.1207 - acc: 0.6503 - val_loss: 1.8362 - val_acc: 0.4852\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8751 - acc: 0.7235\n",
      "Epoch 00004: val_loss did not improve from 1.77632\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8751 - acc: 0.7235 - val_loss: 1.8198 - val_acc: 0.5188\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6953 - acc: 0.7776\n",
      "Epoch 00005: val_loss improved from 1.77632 to 1.59981, saving model to model/checkpoint/1D_CNN_custom_DO_025_DO_BN_3_conv_checkpoint/005-1.5998.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6955 - acc: 0.7776 - val_loss: 1.5998 - val_acc: 0.5563\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.8252\n",
      "Epoch 00006: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5460 - acc: 0.8252 - val_loss: 1.7316 - val_acc: 0.5448\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4298 - acc: 0.8665\n",
      "Epoch 00007: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4299 - acc: 0.8665 - val_loss: 2.0277 - val_acc: 0.5106\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8891\n",
      "Epoch 00008: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3552 - acc: 0.8891 - val_loss: 1.8881 - val_acc: 0.5565\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9120\n",
      "Epoch 00009: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2908 - acc: 0.9120 - val_loss: 1.9713 - val_acc: 0.5542\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9212\n",
      "Epoch 00010: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2539 - acc: 0.9213 - val_loss: 2.4008 - val_acc: 0.5206\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9344\n",
      "Epoch 00011: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2191 - acc: 0.9344 - val_loss: 2.1075 - val_acc: 0.5581\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9420\n",
      "Epoch 00012: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1904 - acc: 0.9420 - val_loss: 2.1958 - val_acc: 0.5563\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9492\n",
      "Epoch 00013: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1715 - acc: 0.9492 - val_loss: 2.1668 - val_acc: 0.5577\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9524\n",
      "Epoch 00014: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1644 - acc: 0.9524 - val_loss: 2.1322 - val_acc: 0.5639\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9558\n",
      "Epoch 00015: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1505 - acc: 0.9557 - val_loss: 2.2660 - val_acc: 0.5712\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9575\n",
      "Epoch 00016: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1469 - acc: 0.9575 - val_loss: 2.2700 - val_acc: 0.5623\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9616\n",
      "Epoch 00017: val_loss did not improve from 1.59981\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1339 - acc: 0.9616 - val_loss: 2.1266 - val_acc: 0.5928\n",
      "Epoch 18/500\n",
      "20864/36805 [================>.............] - ETA: 34s - loss: 0.1024 - acc: 0.9734"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model_name = '1D_CNN_custom_DO_025_DO_BN_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_025_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    model_name = '1D_CNN_custom_DO_025_DO_BN_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
