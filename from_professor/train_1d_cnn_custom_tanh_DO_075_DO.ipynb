{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=64, strides=1, padding='same', \n",
    "                      input_shape=input_shape)) \n",
    "    model.add(Activation('tanh'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=64*(2**int((i+1)/4)), strides=1, \n",
    "                          padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7407 - acc: 0.1048\n",
      "Epoch 00001: val_loss improved from inf to 2.73499, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_1_conv_checkpoint/001-2.7350.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 2.7407 - acc: 0.1048 - val_loss: 2.7350 - val_acc: 0.0948\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.5479 - acc: 0.2242\n",
      "Epoch 00002: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 2.5480 - acc: 0.2241 - val_loss: 2.7947 - val_acc: 0.0995\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3671 - acc: 0.2970\n",
      "Epoch 00003: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 2.3670 - acc: 0.2971 - val_loss: 2.8758 - val_acc: 0.1072\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1978 - acc: 0.3564\n",
      "Epoch 00004: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 2.1978 - acc: 0.3563 - val_loss: 2.9538 - val_acc: 0.1137\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0545 - acc: 0.4028\n",
      "Epoch 00005: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 2.0545 - acc: 0.4027 - val_loss: 3.0505 - val_acc: 0.1106\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9367 - acc: 0.4354\n",
      "Epoch 00006: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.9367 - acc: 0.4353 - val_loss: 3.1575 - val_acc: 0.1123\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8418 - acc: 0.4659\n",
      "Epoch 00007: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.8419 - acc: 0.4659 - val_loss: 3.2464 - val_acc: 0.1134\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7595 - acc: 0.4892\n",
      "Epoch 00008: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.7596 - acc: 0.4892 - val_loss: 3.3363 - val_acc: 0.1144\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6858 - acc: 0.5126\n",
      "Epoch 00009: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.6859 - acc: 0.5126 - val_loss: 3.4343 - val_acc: 0.1160\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6278 - acc: 0.5291\n",
      "Epoch 00010: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.6278 - acc: 0.5291 - val_loss: 3.5160 - val_acc: 0.1176\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5682 - acc: 0.5429\n",
      "Epoch 00011: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.5682 - acc: 0.5429 - val_loss: 3.6207 - val_acc: 0.1151\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5142 - acc: 0.5622\n",
      "Epoch 00012: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.5142 - acc: 0.5622 - val_loss: 3.6916 - val_acc: 0.1109\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4676 - acc: 0.5760\n",
      "Epoch 00013: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.4676 - acc: 0.5760 - val_loss: 3.7944 - val_acc: 0.1155\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4280 - acc: 0.5838\n",
      "Epoch 00014: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.4280 - acc: 0.5838 - val_loss: 3.8533 - val_acc: 0.1167\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3820 - acc: 0.6015\n",
      "Epoch 00015: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.3820 - acc: 0.6014 - val_loss: 3.9547 - val_acc: 0.1120\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3458 - acc: 0.6077\n",
      "Epoch 00016: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.3460 - acc: 0.6077 - val_loss: 4.0401 - val_acc: 0.1120\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3150 - acc: 0.6165\n",
      "Epoch 00017: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.3149 - acc: 0.6166 - val_loss: 4.1121 - val_acc: 0.1123\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2811 - acc: 0.6258\n",
      "Epoch 00018: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.2811 - acc: 0.6258 - val_loss: 4.1778 - val_acc: 0.1092\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2481 - acc: 0.6342\n",
      "Epoch 00019: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.2481 - acc: 0.6342 - val_loss: 4.2655 - val_acc: 0.1111\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2169 - acc: 0.6450\n",
      "Epoch 00020: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2170 - acc: 0.6450 - val_loss: 4.3509 - val_acc: 0.1116\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1918 - acc: 0.6516\n",
      "Epoch 00021: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 1.1918 - acc: 0.6516 - val_loss: 4.4117 - val_acc: 0.1127\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1653 - acc: 0.6600\n",
      "Epoch 00022: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1653 - acc: 0.6600 - val_loss: 4.4850 - val_acc: 0.1123\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1394 - acc: 0.6679\n",
      "Epoch 00023: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1394 - acc: 0.6678 - val_loss: 4.5443 - val_acc: 0.1102\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1136 - acc: 0.6742\n",
      "Epoch 00024: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.1136 - acc: 0.6742 - val_loss: 4.6347 - val_acc: 0.1099\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0892 - acc: 0.6795\n",
      "Epoch 00025: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.0892 - acc: 0.6795 - val_loss: 4.7060 - val_acc: 0.1113\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0735 - acc: 0.6878\n",
      "Epoch 00026: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0735 - acc: 0.6878 - val_loss: 4.7741 - val_acc: 0.1078\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0480 - acc: 0.6924\n",
      "Epoch 00027: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.0480 - acc: 0.6925 - val_loss: 4.8409 - val_acc: 0.1113\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0311 - acc: 0.6970\n",
      "Epoch 00028: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.0310 - acc: 0.6970 - val_loss: 4.9076 - val_acc: 0.1104\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0108 - acc: 0.7045\n",
      "Epoch 00029: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.0108 - acc: 0.7044 - val_loss: 4.9856 - val_acc: 0.1085\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9962 - acc: 0.7093\n",
      "Epoch 00030: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.9961 - acc: 0.7093 - val_loss: 5.0555 - val_acc: 0.1106\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9734 - acc: 0.7169\n",
      "Epoch 00031: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.9734 - acc: 0.7169 - val_loss: 5.1119 - val_acc: 0.1095\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9602 - acc: 0.7196\n",
      "Epoch 00032: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.9602 - acc: 0.7195 - val_loss: 5.1728 - val_acc: 0.1125\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9359 - acc: 0.7258\n",
      "Epoch 00033: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.9361 - acc: 0.7258 - val_loss: 5.2550 - val_acc: 0.1116\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9318 - acc: 0.7253\n",
      "Epoch 00034: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.9319 - acc: 0.7253 - val_loss: 5.2934 - val_acc: 0.1078\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9069 - acc: 0.7329\n",
      "Epoch 00035: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.9071 - acc: 0.7329 - val_loss: 5.3699 - val_acc: 0.1104\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8980 - acc: 0.7358\n",
      "Epoch 00036: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.8979 - acc: 0.7358 - val_loss: 5.4423 - val_acc: 0.1090\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8847 - acc: 0.7395\n",
      "Epoch 00037: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.8846 - acc: 0.7395 - val_loss: 5.4933 - val_acc: 0.1104\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8690 - acc: 0.7444\n",
      "Epoch 00038: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.8690 - acc: 0.7444 - val_loss: 5.5459 - val_acc: 0.1078\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8600 - acc: 0.7461\n",
      "Epoch 00039: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.8600 - acc: 0.7461 - val_loss: 5.6152 - val_acc: 0.1083\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8408 - acc: 0.7529\n",
      "Epoch 00040: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.8409 - acc: 0.7529 - val_loss: 5.6921 - val_acc: 0.1106\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8288 - acc: 0.7563\n",
      "Epoch 00041: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.8288 - acc: 0.7563 - val_loss: 5.7300 - val_acc: 0.1111\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8234 - acc: 0.7571\n",
      "Epoch 00042: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.8234 - acc: 0.7571 - val_loss: 5.7903 - val_acc: 0.1062\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8070 - acc: 0.7614\n",
      "Epoch 00043: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.8070 - acc: 0.7614 - val_loss: 5.8425 - val_acc: 0.1095\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7967 - acc: 0.7658\n",
      "Epoch 00044: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.7969 - acc: 0.7658 - val_loss: 5.8972 - val_acc: 0.1104\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7907 - acc: 0.7640\n",
      "Epoch 00045: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.7907 - acc: 0.7640 - val_loss: 5.9529 - val_acc: 0.1088\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7750 - acc: 0.7718\n",
      "Epoch 00046: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.7750 - acc: 0.7718 - val_loss: 6.0136 - val_acc: 0.1074\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7646 - acc: 0.7713\n",
      "Epoch 00047: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.7647 - acc: 0.7713 - val_loss: 6.0475 - val_acc: 0.1095\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7547 - acc: 0.7766\n",
      "Epoch 00048: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.7546 - acc: 0.7766 - val_loss: 6.1119 - val_acc: 0.1095\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7476 - acc: 0.7783\n",
      "Epoch 00049: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.7477 - acc: 0.7782 - val_loss: 6.1788 - val_acc: 0.1109\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7349 - acc: 0.7819\n",
      "Epoch 00050: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.7349 - acc: 0.7819 - val_loss: 6.2140 - val_acc: 0.1118\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7271 - acc: 0.7827\n",
      "Epoch 00051: val_loss did not improve from 2.73499\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.7270 - acc: 0.7827 - val_loss: 6.2515 - val_acc: 0.1116\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VNX5+PHPM1smK1lAQBBZXFgSdhQF0Uq1iBZxxVZbl/7E/rSKX60VbW1t+7O11S7aulHFguJWEcVKxQ2IfhUUEBWFyi5hTQIhCVlmO78/zkwygQABMpnMzPN+ve7r3rlz595zJ5Nnzpx77nPEGINSSqnk54h3AZRSSrUNDfhKKZUiNOArpVSK0ICvlFIpQgO+UkqlCA34SimVIjTgK6VUitCAr5RSKUIDvlJKpQhXvAsQrWPHjqZnz57xLoZSSiWMZcuWlRljOrVk23YV8Hv27MnSpUvjXQyllEoYIrKppdtqk45SSqUIDfhKKZUiNOArpVSKaFdt+M3x+/2UlJRQV1cX76IkJK/XS/fu3XG73fEuilIqztp9wC8pKSE7O5uePXsiIvEuTkIxxlBeXk5JSQm9evWKd3GUUnHW7pt06urqKCgo0GB/BESEgoIC/XWklAISIOADGuyPgr53SqmIdt+ko5RSScnng5UrYdky2L0bfvazmB8yIWr48VRRUcGjjz56RK8dP348FRUVLd7+3nvv5cEHHzyiYyml2rGaGvj4Y3j8cZg8GYYNg6wsO588GR56CNpgfHGt4R9CJODfeOON+z0XCARwuQ78Fs6bNy+WRVNKtUe7d8OSJbBiBXz2mZ1//TWEQvb5vDwb6G+7zc6HDoXevaENml814B/C1KlTWbduHYMHD+acc87h/PPP55577iEvL4/Vq1fz9ddfM3HiRDZv3kxdXR1Tpkxh8uTJQGOqiOrqas477zxGjx7Nhx9+SLdu3XjttddIT08/4HFXrFjBj3/8Y2pqaujTpw/Tp08nLy+Phx9+mMcffxyXy0X//v154YUXWLRoEVOmTAFsm31xcTHZ2dlt8v4opYC9e+H11+G55+DNN8Hvt+t79oRBg2DSJDsfMgSOP75NgntzEirgr1lzK9XVK1p1n1lZgznxxL8e8Pn777+flStXsmKFPe7ChQtZvnw5K1eubOjqOH36dPLz86mtrWXEiBFccsklFBQU7FP2NTz//PP84x//4PLLL2f27NlcddVVBzzuD3/4Q/72t79x5pln8stf/pJf//rX/PWvf+X+++9nw4YNpKWlNTQXPfjggzzyyCOMGjWK6upqvF7v0b4tSqlD8fvh7bdtkH/1VRv0u3WDKVPgggtsgM/NjXcpm9A2/CNwyimnNOnX/vDDDzNo0CBGjhzJ5s2bWbNmzX6v6dWrF4MHDwZg2LBhbNy48YD737NnDxUVFZx55pkAXH311RQXFwMwcOBArrzySp599tmG5qRRo0Zx22238fDDD1NRUXHQZial1FEIBuG99+CGG6BrVzj/fJg3D668EhYuhG++gQcegDPPbHfBHmJcwxeRXOBJoBAwwHXGmI+OdH8Hq4m3pczMzIblhQsX8s477/DRRx+RkZHBWWed1Wy/97S0tIZlp9NJbW3tER37jTfeoLi4mNdff5377ruPL774gqlTp3L++eczb948Ro0axfz58+nbt+8R7V8ptY9QCP73f+HFF+Hll2HHDsjMhO9+F664AsaNg6j/7/Ys1lXBh4A3jTGXiogHyIjx8VpddnY2VVVVB3x+z5495OXlkZGRwerVq1m8ePFRH7NDhw7k5eXx/vvvc8YZZ/DMM89w5plnEgqF2Lx5M9/61rcYPXo0L7zwAtXV1ZSXl1NUVERRURGffPIJq1ev1oCv1JGorYWvvoLPP7fTF1/Yi67l5eD12hr9pEl2npFw4Sx2AV9EOgBjgGsAjDE+wBer48VKQUEBo0aNorCwkPPOO4/zzz+/yfPjxo3j8ccfp1+/fpx88smMHDmyVY47Y8aMhou2vXv35umnnyYYDHLVVVexZ88ejDHccsst5Obmcs8997BgwQIcDgcDBgzgvPPOa5UyKJX0jIFPP4VXXoG5c+HLLxt706SnQ2EhXHghjB1ra/QJ3hlCTIz6forIYGAa8BUwCFgGTDHG7N1nu8nAZIAePXoM27SpaS7/VatW0a9fv5iUMVXoe6hUlGAQPvzQBvk5c2DTJnA6YcwYOOMMGDjQTr172/XtnIgsM8YMb8m2sWzScQFDgZuNMUtE5CFgKnBP9EbGmGnYLwaGDx8e+zsPlFKpp7IS3noL3njDXmTduRM8Hjj3XPjVr2ztvWPHeJcy5mIZ8EuAEmPMkvDjl7EBXymlYssYe7PTG2/Av/8N778PgYDtOTNuHEycCOPHJ3wTzeGKWcA3xmwXkc0icrIx5r/AWGzzjlJKtb7aWts1ct48O61fb9cXFsLtt9sLraedBincbTnWZ34zMCvcQ2c9cG2Mj6eUSiVr18L8+TbAv/ce1NXZi61jxzYG+eOPj3cp242YBnxjzAqgRRcTlFLqkPbsgQULbJB/663GWvwJJ9gkZOPH25ue9G7zZqXubxulVGJYu9b2ppk7Fz76yPayycqCs8+2CcjOPRdOPDHepUwIGvBjICsri+rq6havV0pFMcZmmZwzx3adXLnSrh8yBO680wb4006zvWzUYdGAr5SKr5oaG+CXLbPTokWwYQM4HDB6NPz1r7ZXjbbFHzVNnnYIU6dO5ZFHHml4HBmkpLq6mrFjxzJ06FCKiop47bXXWrxPYwx33HEHhYWFFBUV8eKLLwKwbds2xowZw+DBgyksLOT9998nGAxyzTXXNGz7l7/8pdXPUak25ffbnDTXXANFRbZr5Omnw80324uvhYXw5JOwfbsN/lOmaLBvJYlVw7/1VpvXojUNHmxrEAcwadIkbr31Vm666SYAXnrpJebPn4/X62XOnDnk5ORQVlbGyJEjmTBhQovGkH3llVdYsWIFn332GWVlZYwYMYIxY8bw3HPP8Z3vfIef//znBINBampqWLFiBVu2bGFl+Gft4YygpVS78s03MG0aPPWUDeYdO8Ipp8BFF9mBQIYNs+mFdRzmmEmsgB8HQ4YMYefOnWzdupXS0lLy8vI47rjj8Pv93H333RQXF+NwONiyZQs7duygS5cuh9znBx98wPe+9z2cTiedO3fmzDPP5JNPPmHEiBFcd911+P1+Jk6cyODBg+nduzfr16/n5ptv5vzzz+fcc89tg7NWqpUEg3ZAkMcft7V3Y2xXyR//2N4AlQCpC5JJYgX8g9TEY+myyy7j5ZdfZvv27UyaNAmAWbNmUVpayrJly3C73fTs2bPZtMiHY8yYMRQXF/PGG29wzTXXcNttt/HDH/6Qzz77jPnz5/P444/z0ksvMX369NY4LaViZ+VKmDkTZs2CrVuhSxe4+274P/9Hm2fiKLECfpxMmjSJ66+/nrKyMhYtWgTYtMjHHHMMbrebBQsWsG/St4M544wzeOKJJ7j66qvZtWsXxcXFPPDAA2zatInu3btz/fXXU19fz/Llyxk/fjwej4dLLrmEk08++aCjZCkVV9u329GfnnnGNr26XHDeefDwwzBhArjd8S5hytOA3wIDBgygqqqKbt260bVrVwCuvPJKvvvd71JUVMTw4cMPK//8RRddxEcffcSgQYMQEf74xz/SpUsXZsyYwQMPPIDb7SYrK4uZM2eyZcsWrr32WkLhlK2///3vY3KOSh02Y2zO+Pnz7bRwoU0tPGKEDfJXXAGdOsW7lCpKzNIjH4nhw4ebpUuXNlmnqX2Pnr6HqtWUltpxXCN3um7fbtcXFdmMkz/4AejgO22qvaRHVkolutpa+OADG+Tffruxl1x+Ppxzjr3weu65cOyx8S2nahEN+EqppnbssG3x8+bZtML19bb9/fTT4f/9Pxvohw3THjYJSAO+UsreDPWf/8D06TaHfCBgb4C68UYb4M84w+avUQlNA75SqWz1ahvkZ860NfvOneF//geuvRb0uk/S0YCvVKqpr4fZs+GJJ6C42DbNXHABXHed7Uap3SeTlgZ8pVLFmjU2tcHTT0N5uR2k+/774eqr7Y1RKulp8rRDqKio4NFHHz2i144fP15z36j4MMbmrnnlFXuH65gxcNJJ9m71s86yXSrXrLHphjXYpwyt4R9CJODfeOON+z0XCARwHWR8zHnz5sWyaEo1tXEjzJgBixfD0qVQVmbXu1y2n/x999m2+fDNgyr1aA3/EKZOncq6desYPHgwd9xxBwsXLuSMM85gwoQJ9O/fH4CJEycybNgwBgwYwLRp0xpe27NnT8rKyti4cSP9+vXj+uuvZ8CAAZx77rnU1tbud6zXX3+dU089lSFDhvDtb3+bHTt2AFBdXc21115LUVERAwcOZPbs2QC8+eabDB06lEGDBjF27Ng2eDdUu2OMTSF88cXQpw/85jc2d82ECfDoo7BkCVRVwfLltqavwT6lJVQNPw7Zkbn//vtZuXIlK8IHXrhwIcuXL2flypX06tULgOnTp5Ofn09tbS0jRozgkksuoaCgoMl+1qxZw/PPP88//vEPLr/8cmbPnr1fXpzRo0ezePFiRIQnn3ySP/7xj/zpT3/it7/9LR06dOCLL74AYPfu3ZSWlnL99ddTXFxMr1692LVrVyu+K6rdq6uD55+3KQxWrLA3Qt15p+1G2b17vEun2qmECvjtxSmnnNIQ7AEefvhh5syZA8DmzZtZs2bNfgG/V69eDB48GIBhw4axcePG/fZbUlLCpEmT2LZtGz6fr+EY77zzDi+88ELDdnl5ebz++uuMGTOmYZv8/PxWPUfVDvn9Nl/NK6/YAUTKymDAAHsh9sorISMj3iVU7VxCBfw4ZUfeT2ZmZsPywoULeeedd/joo4/IyMjgrLPOajZNclpaWsOy0+lstknn5ptv5rbbbmPChAksXLiQe++9NyblVwmkttamNJg9G15/HXbvhsxMGD8ebrjBDuStA4aoFtI2/EPIzs6mqqrqgM/v2bOHvLw8MjIyWL16NYsXLz7iY+3Zs4du3boBMGPGjIb155xzTpNhFnfv3s3IkSMpLi5mw4YNANqkk0y2b7c3Q118sc02eeGFMHeuTU726qs2gdlLL8HYsRrs1WGJacAXkY0i8oWIrBCRpYd+RftTUFDAqFGjKCws5I477tjv+XHjxhEIBOjXrx9Tp05l5MiRR3yse++9l8suu4xhw4bRsWPHhvW/+MUv2L17N4WFhQwaNIgFCxbQqVMnpk2bxsUXX8ygQYMaBmZRCcgY+PRT+O1v7ZB/XbvCj34En3xis0++9Rbs3Gl74Fx4IaSnx7vEKkHFND2yiGwEhhtjylqyvaZHjg19D9upjRvh2WdtWoM1a2xt/dRT7V2vF1wAAwdqDV4dkqZHVqq9qqqyF1xnzrQXYMHeCHXnnbbJ5phj4lk6leRiHfAN8JaIGOAJY8y0fTcQkcnAZIAePXrEuDhKxYExtnnmscfgxRfthdgTT7RNOD/4gY7xqtpMrAP+aGPMFhE5BnhbRFYbY4qjNwh/CUwD26QT4/Io1Xb27rV95R97zN74lJlpA/w118DIkdpco9pcTAO+MWZLeL5TROYApwDFB3+VUgnuiy/gySftRdY9e2xe+UcegauugpyceJdOpbCYBXwRyQQcxpiq8PK5wG9idTyl4mrjRlubf+45WLnSphi+9FJ75+uoUVqbV+1CLGv4nYE5Yj/oLuA5Y8ybMTyeUm0r0h/+uefgww/tulGjbG3+sstsH3ql2pGYBXxjzHpgUKz2355lZWVRXV0d72KoWKiqgtdes0H+rbcgGLRNNr//PVxxBfTsGe8SKnVA2i1TqUPx+eDNN22QnzvX9rLp0QPuuAO+9z3bX16pBKCpFQ5h6tSpTdIa3HvvvTz44INUV1czduxYhg4dSlFREa+99toh93WgNMrNpTk+UEpk1UaMgf/9X/i//9fe+XrhhfDuuzaf/AcfwIYNtlavwV4lkISq4d/65q2s2N66+ZEHdxnMX8cdOCvbpEmTuPXWW7npppsAeOmll5g/fz5er5c5c+aQk5NDWVkZI0eOZMKECchBLs41l0Y5FAo1m+a4uZTIqg2sXm3vfp01y16ITU+Hiy6y2SjPOUfHe1UJLaECfjwMGTKEnTt3snXrVkpLS8nLy+O4447D7/dz9913U1xcjMPhYMuWLezYsYMuBxkurrk0yqWlpc2mOW4uJbKKkV27bA+bf/7TjhTlcMC3v20HE5k4EbKz411CpVpFQgX8g9XEY+myyy7j5ZdfZvv27Q1JymbNmkVpaSnLli3D7XbTs2fPZtMiR7Q0jbJqI4GAvej6z3/ai7A+n22e+dOfbLu8jgylkpC24bfApEmTeOGFF3j55Ze57LLLAJvK+JhjjsHtdrNgwQI2bdp00H0cKI3ygdIcN5cSWbWC7dvtUH89esD558OCBbad/tNP4bPP4LbbNNirpKUBvwUGDBhAVVUV3bp1o2s4GFx55ZUsXbqUoqIiZs6cSd++fQ+6jwOlUT5QmuPmUiKro7BxI9x0k+02+Yc/wIgRMGcObNliR9YJj0amVDKLaXrkw6XpkWMjpd/DVavg/vvtRViHw+ax+dnP4IQT4l0ypVqFpkdWqa2iAubNs5kpX3/d9rS55RbbXKMDfKsUpgFfJYctW+zF11dfte3ygQB07mzb66dM0TQHSpEgAd8Yc9D+7erA2lOTXaurqLDdKWfMgCVL7LoTT7Q1+YkT7ehRDr1MpVREuw/4Xq+X8vJyCgoKNOgfJmMM5eXleL3eeBel9RgDixbBU0/ZkaPq6qCoCH73Oxvk+/bVzJRKHUC7D/jdu3enpKSE0tLSeBclIXm9XronQ7v12rU2M+X06bBunc0rf801drDvYcM0yCvVAu0+4Lvd7oa7UFUKMcYOJPLKK3YKp5jgrLPgV7+CSy6BjIy4FlGpRNPuA75KMWvXwrRpNsivW2dr7mecYfvKT5yo478qdRQ04Kv4M8YOIPLgg7anjdNpc9nceSdMmGB72yiljpoGfBU/gYC92/VPf7K9bPLybDfKn/wEDpKETil1ZDTgq7a3dy88/TT8+c82r3yfPvD3v9uLsJmZ8S6dUklLA75qO2VlNrD//e9QXg6nnWZr9xMm2GYcpVRMacBXsbdxo63NP/mkHR7wu9+1+WxGj453yZRKKRrwVWxUVdnxX59/3o4HKwJXXQU//SkMGBDv0imVkjTgq9ZTVwf/+Y8N8v/+t63Nd+9ug/xPfqKJy5SKMw346uhEulQ+/TT8619QWWkTlV17rR056vTTNZ+NUu1EzAO+iDiBpcAWY8wFsT6eaiNbt8LMmTbQf/217V1z6aXw/e/D2WeDS+sSSrU3bfFfOQVYBeS0wbFULIVCtqnmiSdsu3woZO+CnToVLrsMsrLiXUKl1EHENOCLSHfgfOA+4LZYHkvFUE2Nrc3/+c+wZo1ti7/rLttvXkeOUiphxLqG/1fgZ0B2jI+jYmHHDnjkEXj0UdtvfsQIO4rUxRdrk41SCShm/7UicgGw0xizTETOOsh2k4HJAD169IhVcdTh2LoVfvtb2z7v89kbo26/3fab1zTESiWsWFbTRgETRGQ84AVyRORZY8xV0RsZY6YB08AOYh7D8qhDqaiAP/7RZqb0++G662ygP+mkeJdMKdUKYtZfzhhzlzGmuzGmJ3AF8N6+wV61E7W1NlNl797w+9/bNMSrV9uLsxrslUoa2hCbykpKbN75Bx6wy+PG2YA/eHC8S6aUioE2CfjGmIXAwrY4ljoIY+DLL23O+VdfhaVL7fpTT4VnnrGjSSmlkpbW8FPB7t22S+Xzz9tRpABGjmxsvunbN77lU0q1CQ34yayuzqYi/t3v7AXZ73wH7rjD9rrp2jXepVNKtTEN+MkoGIRnn4V77oHNm+G88+D++2HgwHiXTCkVR5rVKpkEg/D66zBkiL0LtnNneO89mDdPg71SSmv4SWH1apgxw1543bLFdq984QWb30YzVSqlwjTgJ6rdu22agxkzYPFiO0TguHH2pqkJE8DjiXcJlVLtjAb8RFNdbfvNP/igTWpWWGiXr7wSunSJd+mUUu2YBvxEEQjA9Onwy1/apGaXX27HhR06VPPbKKVaRAN+e2cMvPGGDe6rVtkEZq+9Zm+WUkqpw6BX9NqrUMgOMnL22fDd79oa/pw5UFyswV4pdUS0ht/eVFXZC7F/+5sdOrBLF3vz1OTJ4HbHu3RKqQSmAb+9WLfOBvbp0+1A4KecArNm2XFitceNUqoVaMCPt7Vr4d574bnnbNfKyy+HW27RZhulVKvTgB8v33zTOKqUxwM//Snceisce2y8S6aUSlItumgrIlNEJEesp0RkuYicG+vCJaXt220N/sQT7cDgN94I69fbkaY02CulYqilvXSuM8ZUAucCecAPgPtjVqpktHmzrcH37m0HBb/6alizBh5+WG+YUkq1iZY26UTu7BkPPGOM+VJE7/Zpka+/hj/8wea5CYXsHbH33AMnnBDvkimlUkxLA/4yEXkL6AXcJSLZQCh2xUoCy5fbAUZmz4a0NLjhBttOf/zx8S6ZUipFtTTg/wgYDKw3xtSISD5wbeyKlaCMgUWLbO75+fMhJwemTrVNOcccE+/SKaVSXEsD/mnACmPMXhG5ChgKPBS7YiWYUAjmzrWBfskSm4f+d7+zF2Q7dIh36ZRSCmj5RdvHgBoRGQTcDqwDZsasVInC77d3xRYWwkUXQWkpPPYYbNgAd92lwV4p1a60NOAHjDEGuBD4uzHmESA7dsVq54yxNfrCQjuylMdjBwj/73/hxz+G9PR4l1AppfbT0oBfJSJ3YbtjviEiDiA1E7usWAHf/jZceKEdTWruXPj0U7jiCnDpfWxKqfarpQF/ElCP7Y+/HegOPHCwF4iIV0Q+FpHPRORLEfn1UZY1vrZtgx/9yOaf/+wzm/fm889tJkvtoaqUSgAtCvjhID8L6CAiFwB1xphDteHXA2cbYwZhe/iME5GRR1XaeKiogF/9yt4Z+8wzcNtt9oapm27S7JVKqYTS0tQKlwMfA5cBlwNLROTSg73GWNXhh+7wZI6irG2rqgruuw969YLf/MaOF/vVV3Y4wby8eJdOKaUOW0sbnX8OjDDG7AQQkU7AO8DLB3uRiDiBZcAJwCPGmCVHUda2sXcvPPKIzW1TXm6bbH79axgyJN4lU0qpo9LSNnxHJNiHlbfktcaYoDFmMLbN/xQRKdx3GxGZLCJLRWRpaWlpC4sTA6WlNgVC795w550wYoTtUz93rgZ7pVRSaGkN/00RmQ88H348CZjX0oMYYypEZAEwDli5z3PTgGkAw4cPb9smH2Pgww9t3/l//Qt8PtsD5957YdSoNi2KUkrFWosCvjHmDhG5BIhEwWnGmDkHe0242ccfDvbpwDnAH46qtK2lqgqefdYG+i++sCkQbrjB9qHv3z/epVNKqZhoccdxY8xsYPZh7LsrMCPcju8AXjLG/Pswy9e61qyx3SmfftoG/SFDYNo0+P73ITMzrkVTSqlYO2jAF5Eqmu9ZI9iOODkHeq0x5nMg/o3foRC89ZbNO/+f/9iulJdfDj/5iR1GUPvQK6VSxEEDvjEmcdMnbNwIr7wCTzxhc9J36WLb5m+4QQccUUqlpKTIBWBMEMFh2+NffRXmzLEpEMDW4mfNgksvtTlvlFIqRSV8wA9U7qT8J8MoKK7HtanUNtGcfrq9QWriROjTJ95FVEqpdiHhA74jM4fcRbvY07UG50+uJveq+7XJRimlmtHSG6/aLYfTi+vrHZQ8cS4rhs9kO2/Hu0hKKdUuJXzAB3CmZVFY+Cq5uWezevU17NjxXLyLpJRS7U5SBHwApzOdoqK55OaOYdWqH7Bz54vxLpJSSrUrSRPwAZzODIqK/k2HDqP46qsrKS09nPvElFIquSVVwAdwOjMpKnqDnJxT+eqrKygtPWgGCKWUShlJF/ABXK5sBg78D9nZw/nyy8vYseP5Q79IKaWSXFIGfACXK4eBA9+iQ4fRrFp1JVu3PhnvIimlVFwlbcCHSE1/Hvn53+Hrr6+npOSheBdJKaXiJqkDPtgLuYWFr9Kx48WsXXsrmzb9Lt5FUkqpuEj6gA/gcKTRv/+LdO58FRs2/Jz16+/GmMQZXlcppVpDwqdWaCmHw0XfvjNwODL45pvfEwhUcuKJD2HT9SulVPJLmYAPIOLgpJMex+nMpqTkT/j9O+jb9xmcTm+8i6aUUjGXUgEfQEQ44YQHSUs7lnXrbsfn20lh4au43XnxLppSSsVUSrThN+e4426jX7/nqaz8iE8/PYO6us3xLpJSSsVUygZ8gM6dr2DgwDepr9/M8uWnUV29Mt5FUkqpmEnpgA+Ql3c2Q4YUAyFWrDiDiopF8S6SUkrFRMoHfICsrEEMHfoRHk8XVqwYyzffPKjdNpVSSUcDfpjXezxDhy6mY8cLWb/+DlauvBC/f1e8i6WUUq1GA34Ul6sDAwa8zAknPMSuXW+ydOlQKis/jnexlFKqVcQs4IvIcSKyQES+EpEvRWRKrI7VmkSE7t1vYciQDwD49NPRlJQ8rE08SqmEF8safgC43RjTHxgJ3CQi/WN4vFaVk3MKw4cvJz9/HGvXTmHlyouor98a72IppdQRi1nAN8ZsM8YsDy9XAauAbrE6Xiy43fkUFr5Gnz5/Yvfu+Xz8cV9KSv6GMcF4F00ppQ5bm7Thi0hPYAiwpC2O15pEhOOOu40RI1aSk3Maa9fewvLlI6mqWh7voiml1GGJecAXkSxgNnCrMaaymecni8hSEVlaWloa6+IcsfT0Pgwc+Cb9+j1PXd1mli0bwdq1/0MgUBXvoimlVItILC9Giogb+Dcw3xjz50NtP3z4cLN06dKYlae1+P0VbNhwN1u3Po7H04UePe6ia9frNQmbUqrNicgyY8zwlmwby146AjwFrGpJsE8kbncuJ530KEOGfEh6+omsXXsLS5b0oaTk7wSDdfEunlJKNSuWTTqjgB8AZ4vIivA0PobHa3MdOoxk8OCFDBr0HunpJ7B27c0a+JVS7VZMm3QOV6I06TTHGENFxUI2bryP6Pa+AAAXJElEQVSXPXuK8Xi60q3bTXTtOhmPp1O8i6eUSlLtokkn1YgIeXnfaqjxZ2YWsWHDL/joo+NYvfpHVFd/Hu8iKqVSXMoNgBJrkcCfl/ct9u5dxZYtD7N9+0y2b59Obu5ZdOs2hYKCC3A49K1XSrUtreHHUGZmP0466TFOO62E3r3/SG3ter788iIWL+7B+vV3UVOzJt5FVEqlEG3Db0OhUIBdu95g27anKC9/AwjRocOZdO36Izp1ugSnMyPeRVRKJZjDacPXgB8n9fVb2b59Btu2PUVd3TqczhwKCs6nY8cLyc8fh8vVId5FVEolAA34CcSYEBUVxezYMZPy8n/j95ci4iY39ywKCibQseMEvN4e8S6mUqqd0oCfoIwJUlm5mLKy1ygrm0tt7X8ByMoaSseOE+nYcSKZmYXYe9qUUkoDftKoqflvOPi/SmXlRwB4vX3o2HEinTpdRE7OSESccS6lUiqeNOAnofr6bZSXz6Ws7FV2734XY/y4XPnk559Lfv548vO/g8dzTLyLqZRqYxrwk1wgsIddu96kvHweu3a9id+/E4Ds7OHk559HXt5YsrNP1WRuSqUADfgpxJgQ1dWfUl7+H3bt+g+VlYuBECJp5OScSm7umXToMIYOHU7D6cyMd3GVUq1MA34K8/t3s2fPB1RULGLPnkXhgVpCiLjIyhpCdvYIsrNHkJNzChkZJ+s1AKUSnAZ81SAQqGTPng/Zs2cRlZVLqKpaSjBoB21xOrPIzh5OTs5p4V8Bp+Ny5cS5xEqpw6EBXx2QMSFqav5LVdXHVFZ+QlXVEqqrV2BMAHCQlTWE3Nwzyc0dQ07OaXohWKl2TgO+OizB4F4qKxdTUVFMRcUiKisXY0w9AB7PsWRlDSErazDZ2UPIyhqC19tL7wVQqp04nICvKRsVTmcmeXljycsbC0AoVB+u/X9CdfWnVFd/yq5dbwLB8PZZZGT0JzOzPxkZ/RqWvd6eiGg+PqXaKw34aj8ORxq5uaPJzR3dsC4YrGPv3pVUV3/K3r1fsHfvV+za9Rbbt/8z6nUZZGYOIDNzIFlZA8nMLCIrayBud0EczkIptS8N+KpFnE4vOTnDyclp+svR76+gpmYVNTVfhb8QvqC8/DW2b3+qYRuPpyuZmUXhqZCsrCIyMvppdlCl2pgGfHVU3O5cOnQ4jQ4dTmtYZ4zB59sR/iXwOdXVn7N370q2bn2EUCgy1q+Qnn5CVNNQZN5XvwiUihEN+KrViQhpaV1IS+tCfv45DeuNCVJbu469e1eGvwxWhpuG3gj3EgIQvN7j8Xr7kJ7em/T0Pni9vUlP743X2xu3Oy8+J6VUEtCAr9qMiJOMjJPIyDiJTp0ublgfCvmprV0bbhb6ipqaVdTWrqes7FX8/tIm+3C58khPPyFq6kN6+gl4vX3weDpr7yGlDkIDvoo7h8NNZmY/MjP70anTJU2eCwSqqKvbQG3teurq1lFba6fKyiXs3PkiEIraT2bDLwH7RdAn/AuhD17v8Tgc7jY+M6XaFw34ql1zubLJyrK9fvYVCvmoq9tEbe1aamvXRX0hrGH37vlR1wsAnHi9PRq+AOwXQy+83p54vb1wuwv014FKejEL+CIyHbgA2GmMKYzVcVTqcjg8ZGScSEbGifs9Z0wIn29bwy+C6C+E0tJ/EQjsarK905mF19sLj+dYPJ5jcLs74XZ3wuOx87S07uFrCLltdXpKtbpY1vD/CfwdmBnDYyjVLBEHaWndSEvrRm7umP2eDwT2UFe3kdraDdTVbaSubgN1dRvw+bZRU7Mav7+UUKhmv9e5XPnhXwf2V0JaWg88ns4Nk9vdGZcrqy1OUanDFrOAb4wpFpGesdq/UkfD5epAVtYgsrIGHXCbYLAGv78Un28n9fWbo64jrKeqaillZbOjehc1cjgy8Hi6hL9wujfMPZ5upKUdi9vdEbe7Iy5Xrt6ZrNqUtuErdQBOZwZO5/F4vccDI/Z7PhQK4PfvwOfbgc+3M2p5Bz7fNurrt1BZ+TH19SUNuYmacuB254e/ADqRltYj3CW1B2lp9rhpacfhdGbq9QXVKuIe8EVkMjAZoEePHnEujVIt53C4GpqNDsYYQyCwi/r6LdTXbyUQKMfvL8Pvj8zL8Pl2sGfPB+zc+QKRnEURIh7c7nxcrvzwPA+3uyD8pdCT9HR78TktrbuOb6AOKu4B3xgzDZgGNltmnIujVKsTEdzuAtzugmZ7G0UzJkh9/Vbq6jZRX7+J+voS/P5dBAK7wvPd1NV9Q1XVMny+bUDjv4yIi7S07jgcmTgcHkQ8TeYez7HhLwfbQyk9vTdudyf99ZBC4h7wlVKNRJx4vcfh9R4HjD7otqFQPXV130RddN5IXd03hEK1GOMjFPJjjA9j6vH5Kqiu/hSfb3uTfTgcmbjdBTidWeEpu2HZ5coNf1FF/7rID6/rFL4GoV8WiSSW3TKfB84COopICfArY8xTB3+VUqqlHI60A3ZLPZBgcG9U76T11NVtwO/fTTBY3TD5/aUEg1UEAnsIBHYT/Ssimoir4fqDnfKjvjiyor448vB4jiUt7Vg8nq76RRFHseyl871Y7VspdWSczsxwCusBLdremFA48O8KX3OITKUNk89n53v3fhX1xVGFMf5m9+lweMP3O3TG5crD5coNT5HlHBwOLw5HGiJpOBxp4cfp4fsiOuuF7COkTTpKqQMSceB25+F255Ge3uewXhsK+cK/GMrDvZa24vNtjVq2PZpqav5LIFBBIFDBvhesD8ThSMftPqbhJjmXqwNOZw4uV3Z4noPTmY3Dkd7wZWHnXpzOdFyuAjyeTjgcaUfwriQuDfhKqZhwODw4HLbt/2DNTsaA3w8+n6G+fi/19VUEg/UEg7595jXU1lawd6+damoqqamppLa2mvp6P/X1lfh8u/D5DH6/h2DQhYjB4QghEsLhCOFwBBEx+Hxeamsz8fvz8fk6hed5QDoOhzv8q8KN02kveAeDafj9Xvz+NHw+Dz6fG5/PRTDoJBQSgkEIhewUDEIgYCe/v+kUDNrzNcZuG1k+5hj46qvY/0004CuV4IwBnw9qa6G+3j4GELFTRH091NTA3r12Hpl8vqZB6kDBKjL5fHaqr2869/sbA170PFK2faf6+sbXhEsMZIWntuVyBUhLq8XhCGGMCb+HgjF2crn8uN31uN1VeDx1uN31eDz1OJ1+HA5wOMDpBIdDcDqFtDTIzHTg8Thwu514PE48HjculwuHw05OZ+Nyhw4uICf25xnzIyiVpILBxuAXHfgiy9FBdd8AGx0sI9O+2+3dawNjINA0IAeDNlDW19vn6+oag3wsiYDbDR6PndLS7BRZdrkiQS86ANrnO3SA9PTGKSPDvsbttpPL1bjsdNpjORyNX1qRx5FjR0/RZYpe53I1/p2ia9+hEHi9kJlpp4wM8HhcQHbDuRpjCIXqCQYrCQT2EAxWhS9kV4bXVREMVhIM7iUUqiUYrCEUqm1Yttvsxu/fTSCwi2Cw6qDvrdt9DLAjdn+8MA34Kin4/Y1BMlKDjQTMSFCMXq6raxpso4NuZB/V1Y3Lke2jA3pjzfTIOZ2NQTM6AGVkQFYWdOzYGAQjATUyT0+3gSt6npbWWKuPNBdEpkigzchoPE7kNS5X08npbAzAkYDqTKF7ukQEp9OL0+nF4znmqPcXCvkJBCrCXxI14S+Kxrn9dRN7GvBVm4k0PUQH0j177FRZ2TivrLSBt7lmgMjr9g3I/uY7hBySw9FYU01LawyGmZmNATczszEwRtciIzXUfddHnosO3pHJ621aM06lIJrKHA43Hk8noFNcy6EBX+3HGKiqgoqKprXjyLymxgbb5qaqqqbzyHIkMAdb1gmjocYaPXm9Ngh37doYkCPBOXqKrsFGguy+teFI4HXpf4BKIfpxT1LG2AC7axeUl9t5JPBGX7iLbLN9e9Opru7Qx4gQaQy+2dl2igTmrKzGKbrmHFnu0KFxysmx86wsW/NWSrUuDfgJJBSyzR6RIL5tG2zZAlu3Np1KS+02Pt+h9+l0Qn4+dOlipxNOaFzOzW1au46eR4J6VpZ9rPfAKNX+acCPM58PSkrgm29gxw7YudNO0cvl5Xbavbv5C4VOpw3Qxx4LvXrBqafaIF5QYKfIck5O0+aOzEzbBq3BWqnUoAE/xurqbDDftAk2brRTZHnTJltD37dLncMBnTpB58523qNH08AdmXftaoN8p0568U8pdWga8FtBIAAbNtg75VatstPXX9uAvm1b022dTjjuODj+eBg71s579rRBvUsXG+Tz87UNWynV+jTgH4ZQCNavh5UrG6cvv7TBPbq9/Nhj4eST4bzzbDDv2bMxsB97rPYMUUrFh4aeg9i2DRYtguJi+OQTG9xraxuf79ULBgywgb1/f+jXD/r2tT1NlFKqvdGAH2XrVnj33cYgv2aNXZ+VZS+E/vjHUFhop/797XqllEoUKR/wV6+GV1+105Ildl1eHpxxBtxwA5x5JgwerM0wSqnEl3JhzBhYvhz+9S8b5P/7X7t+xAi47z4YPx4GDtSLpkqp5JMyAb+sDGbNgunT4fPPbY39W9+CW26BCROge/d4l1AppWIrqQN+MAhvv22D/Guv2Z40I0bAY4/BpEm26UYppVJF0gb8d9+FG2+0XSYLCuzydddBUVG8S6aUUvGRdAG/rAxuvx1mzoQ+feDFF+HCC21mRKWUSmVJE/CNsUH+9tttgrG774Zf/MIm9lJKKZUkAX/NGttH/r334PTT4YknbF95gEAoQI2/pmHa69vbsFwbqKXWX0tdoK5hOWiCuBwu3A43LoerYXKIg5AJETRBQia032SMaVzGNKzbdzl6u+h9BUPN7zdkQjjEgcvhwulw2rnYudvpxu1w43F68Dg9DY9DJkR9sB5f0Ncw1QfqCZpgw/Gjy+V0OBv2keZMa1h2OpxIMyPxGBqT/5ioREAi0uQ9i0xOcTaca+Q8I48jr4+8N9HLzb2f0ceKlE1EcIoThzhwOpw4xYnTYR87pLG7VfS5RO9z3/fa7XQ3+Qy4nW4A/EE/gVAAf8jfsBwyoYayRM8d4tivTA5xICIEQ0GCJthkbjAI0lDmyD4AgqEggVBgvwlo2Db6tZH3PfJ5iP5cBI3dV+TYgVCgyWujp8h7tO8kyH6fx8j7Hf2ZiHxOIn/3QCjQcL6R9y56u8jrRKTJZzHNZeduh7vh3P2h8N8i/Hdo7v/MYPY7p8jfIXqKvNcN/+Ph9yZ6uTnR/98HignRZTHNjEMp4cyF6a50JhVOavY4rSmmAV9ExgEPAU7gSWPM/a19jN27oe9fhkJRFfln+ViXXs+YN3z45vqoD9Y3/GO0Z5F/uOgg1fBhRJr9Rz1SkX1GB4qQCeEPHeGQUUqpo9Y5s3NiB3wRcQKPAOcAJcAnIjLXGPNVax4nLw9OP3EAHTsFyc1Ow+PwNNQK3A436e50Mt2ZZLgzyPTYeWTyuryku9JJd6c3LDsdzmZrVEETbFJjiw7IzdUWomt6+9b+9q1lyGHmJzbGNHwBRNfi/UE/vqCvocYeXUNyO9w4HQdOqRkyoYbXR6aDfVlGlzlSc47+Ymry3oWCTd636PPft6YeWW6u9hV57kC/BvatNUd/MUbXriI1v+b+diETalJzjNTmI79eomv9bocbEWm2Jhf9Ky76V03kF1WkVhyZC9LsLxtjTPO/msJ/y+hfbNGfi8g5RMrvD/kbfymGjxlZjvztoqegCTZb64+UM7rGHvkbR37tRH8mIn/TSJkjv04jn4Xo7SKvC5lQ4y/TYD31Aftr1R/y7/c3iD6f6P+zhl8bUTXwff8OTWrj4XNq7leiU5wH/B890HsU/dnedx79OYzeT1uIZQ3/FGCtMWY9gIi8AFwItGrAB3j/f55p7V22ayKCS+w/vtflbZV9OsRBmiuNNJde3VYqWcXya6UbsDnqcUl4nVJKqTiIewIBEZksIktFZGlpaWm8i6OUUkkrlgF/C3Bc1OPu4XVNGGOmGWOGG2OGd+rUKYbFUUqp1BbLgP8JcKKI9BIRD3AFMDeGx1NKKXUQMbtoa4wJiMhPgPnYbpnTjTFfxup4SimlDi6m/fCNMfOAebE8hlJKqZaJ+0VbpZRSbUMDvlJKpQhpLr9DvIhIKbDpCF/eEShrxeIkAj3n5Jdq5wt6zofreGNMi7o4tquAfzREZKkxZni8y9GW9JyTX6qdL+g5x5I26SilVIrQgK+UUikimQL+tHgXIA70nJNfqp0v6DnHTNK04SullDq4ZKrhK6WUOoiED/giMk5E/isia0VkarzLEwsiMl1EdorIyqh1+SLytoisCc/z4lnG1iYix4nIAhH5SkS+FJEp4fVJe94i4hWRj0Xks/A5/zq8vpeILAl/xl8M56ZKGiLiFJFPReTf4cdJfb4AIrJRRL4QkRUisjS8Luaf7YQO+FGjap0H9Ae+JyL941uqmPgnMG6fdVOBd40xJwLvhh8nkwBwuzGmPzASuCn8t03m864HzjbGDAIGA+NEZCTwB+AvxpgTgN3Aj+JYxliYAqyKepzs5xvxLWPM4KjumDH/bCd0wCdqVC1jjA+IjKqVVIwxxcCufVZfCMwIL88AJrZpoWLMGLPNGLM8vFyFDQjdSOLzNlZ1+KE7PBngbODl8PqkOmcR6Q6cDzwZfiwk8fkeQsw/24ke8FN5VK3Oxpht4eXtQOd4FiaWRKQnMARYQpKfd7h5YwWwE3gbWAdUGGMiAwwn22f8r8DPgFD4cQHJfb4RBnhLRJaJyOTwuph/tmOaLVO1DWOMEZGk7G4lIlnAbOBWY0xl9GDSyXjexpggMFhEcoE5QN84FylmROQCYKcxZpmInBXv8rSx0caYLSJyDPC2iKyOfjJWn+1Er+G3aFStJLVDRLoChOc741yeVicibmywn2WMeSW8OunPG8AYUwEsAE4DckUkUjlLps/4KGCCiGzENseeDTxE8p5vA2PMlvB8J/aL/RTa4LOd6AE/lUfVmgtcHV6+GngtjmVpdeG23KeAVcaYP0c9lbTnLSKdwjV7RCQdOAd77WIBcGl4s6Q5Z2PMXcaY7saYntj/3feMMVeSpOcbISKZIpIdWQbOBVbSBp/thL/xSkTGY9sBI6Nq3RfnIrU6EXkeOAubUW8H8CvgVeAloAc2w+jlxph9L+wmLBEZDbwPfEFj++7d2Hb8pDxvERmIvVjnxFbGXjLG/EZEemNrwPnAp8BVxpj6+JW09YWbdH5qjLkg2c83fH5zwg9dwHPGmPtEpIAYf7YTPuArpZRqmURv0lFKKdVCGvCVUipFaMBXSqkUoQFfKaVShAZ8pZRKERrwlWoFInJWJNujUu2VBnyllEoRGvBVShGRq8I551eIyBPhZGXVIvKXcA76d0WkU3jbwSKyWEQ+F5E5kfzkInKCiLwTzlu/XET6hHefJSIvi8hqEZkl0Yl/lGoHNOCrlCEi/YBJwChjzGAgCFwJZAJLjTEDgEXYO5kBZgJ3GmMGYu/4jayfBTwSzlt/OhDJcDgEuBU7NkNvbK4YpdoNzZapUslYYBjwSbjynY5NUBUCXgxv8yzwioh0AHKNMYvC62cA/wrnQOlmjJkDYIypAwjv72NjTEn48QqgJ/BB7E9LqZbRgK9SiQAzjDF3NVkpcs8+2x1pvpHofC9B9P9LtTPapKNSybvApeEc5JExRI/H/h9EsjN+H/jAGLMH2C0iZ4TX/wBYFB59q0REJob3kSYiGW16FkodIa2BqJRhjPlKRH6BHWnIAfiBm4C9wCnh53Zi2/nBpqh9PBzQ1wPXhtf/AHhCRH4T3sdlbXgaSh0xzZapUp6IVBtjsuJdDqViTZt0lFIqRWgNXymlUoTW8JVSKkVowFdKqRShAV8ppVKEBnyllEoRGvCVUipFaMBXSqkU8f8Bzxi6w+HgIhIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 307us/sample - loss: 2.7278 - acc: 0.1040\n",
      "Loss: 2.7278172813843344 Accuracy: 0.10404985\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4021 - acc: 0.2403\n",
      "Epoch 00001: val_loss improved from inf to 2.18306, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_2_conv_checkpoint/001-2.1831.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 2.4022 - acc: 0.2403 - val_loss: 2.1831 - val_acc: 0.3156\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9221 - acc: 0.4019\n",
      "Epoch 00002: val_loss improved from 2.18306 to 2.05126, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_2_conv_checkpoint/002-2.0513.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.9222 - acc: 0.4019 - val_loss: 2.0513 - val_acc: 0.3559\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6666 - acc: 0.4807\n",
      "Epoch 00003: val_loss improved from 2.05126 to 2.05113, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_2_conv_checkpoint/003-2.0511.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.6665 - acc: 0.4807 - val_loss: 2.0511 - val_acc: 0.3655\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5048 - acc: 0.5292\n",
      "Epoch 00004: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.5050 - acc: 0.5292 - val_loss: 2.0598 - val_acc: 0.3636\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3816 - acc: 0.5651\n",
      "Epoch 00005: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.3816 - acc: 0.5651 - val_loss: 2.0873 - val_acc: 0.3657\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2843 - acc: 0.5951\n",
      "Epoch 00006: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.2843 - acc: 0.5951 - val_loss: 2.1135 - val_acc: 0.3701\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2004 - acc: 0.6194\n",
      "Epoch 00007: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.2005 - acc: 0.6194 - val_loss: 2.1687 - val_acc: 0.3657\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1313 - acc: 0.6383\n",
      "Epoch 00008: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.1313 - acc: 0.6383 - val_loss: 2.2210 - val_acc: 0.3604\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0653 - acc: 0.6590\n",
      "Epoch 00009: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.0652 - acc: 0.6590 - val_loss: 2.2740 - val_acc: 0.3652\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0119 - acc: 0.6774\n",
      "Epoch 00010: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.0118 - acc: 0.6774 - val_loss: 2.3175 - val_acc: 0.3669\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9739 - acc: 0.6866\n",
      "Epoch 00011: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.9740 - acc: 0.6866 - val_loss: 2.3599 - val_acc: 0.3664\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9305 - acc: 0.7012\n",
      "Epoch 00012: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.9305 - acc: 0.7012 - val_loss: 2.4294 - val_acc: 0.3611\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8874 - acc: 0.7164\n",
      "Epoch 00013: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.8874 - acc: 0.7164 - val_loss: 2.4603 - val_acc: 0.3673\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8573 - acc: 0.7230\n",
      "Epoch 00014: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.8573 - acc: 0.7231 - val_loss: 2.4962 - val_acc: 0.3687\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8271 - acc: 0.7296\n",
      "Epoch 00015: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.8270 - acc: 0.7297 - val_loss: 2.5520 - val_acc: 0.3597\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8026 - acc: 0.7360\n",
      "Epoch 00016: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.8025 - acc: 0.7360 - val_loss: 2.6210 - val_acc: 0.3606\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7743 - acc: 0.7466\n",
      "Epoch 00017: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7743 - acc: 0.7466 - val_loss: 2.6261 - val_acc: 0.3662\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7563 - acc: 0.7511\n",
      "Epoch 00018: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7563 - acc: 0.7511 - val_loss: 2.6847 - val_acc: 0.3645\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7362 - acc: 0.7582\n",
      "Epoch 00019: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7361 - acc: 0.7582 - val_loss: 2.7115 - val_acc: 0.3627\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7208 - acc: 0.7638\n",
      "Epoch 00020: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7208 - acc: 0.7639 - val_loss: 2.7543 - val_acc: 0.3615\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7030 - acc: 0.7685\n",
      "Epoch 00021: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7029 - acc: 0.7685 - val_loss: 2.7792 - val_acc: 0.3599\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6842 - acc: 0.7743\n",
      "Epoch 00022: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6843 - acc: 0.7743 - val_loss: 2.8209 - val_acc: 0.3620\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6674 - acc: 0.7773\n",
      "Epoch 00023: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6674 - acc: 0.7773 - val_loss: 2.8555 - val_acc: 0.3676\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6518 - acc: 0.7832\n",
      "Epoch 00024: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6518 - acc: 0.7832 - val_loss: 2.8883 - val_acc: 0.3601\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6419 - acc: 0.7884\n",
      "Epoch 00025: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6419 - acc: 0.7884 - val_loss: 2.9434 - val_acc: 0.3580\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6356 - acc: 0.7882\n",
      "Epoch 00026: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6355 - acc: 0.7883 - val_loss: 2.9414 - val_acc: 0.3685\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6250 - acc: 0.7926\n",
      "Epoch 00027: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6249 - acc: 0.7926 - val_loss: 2.9569 - val_acc: 0.3701\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6129 - acc: 0.7937\n",
      "Epoch 00028: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6130 - acc: 0.7937 - val_loss: 2.9767 - val_acc: 0.3659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5979 - acc: 0.7985\n",
      "Epoch 00029: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5979 - acc: 0.7985 - val_loss: 3.0082 - val_acc: 0.3648\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5832 - acc: 0.8048\n",
      "Epoch 00030: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5833 - acc: 0.8047 - val_loss: 3.0853 - val_acc: 0.3638\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5795 - acc: 0.8063\n",
      "Epoch 00031: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5796 - acc: 0.8062 - val_loss: 3.0947 - val_acc: 0.3573\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5736 - acc: 0.8069\n",
      "Epoch 00032: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5736 - acc: 0.8069 - val_loss: 3.1049 - val_acc: 0.3666\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.8133\n",
      "Epoch 00033: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5601 - acc: 0.8133 - val_loss: 3.1570 - val_acc: 0.3620\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5612 - acc: 0.8114\n",
      "Epoch 00034: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5611 - acc: 0.8114 - val_loss: 3.1790 - val_acc: 0.3643\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5457 - acc: 0.8163\n",
      "Epoch 00035: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5456 - acc: 0.8163 - val_loss: 3.2281 - val_acc: 0.3697\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.8197\n",
      "Epoch 00036: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5390 - acc: 0.8197 - val_loss: 3.2441 - val_acc: 0.3685\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.8214\n",
      "Epoch 00037: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5377 - acc: 0.8214 - val_loss: 3.2336 - val_acc: 0.3666\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.8221\n",
      "Epoch 00038: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5304 - acc: 0.8221 - val_loss: 3.2450 - val_acc: 0.3694\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.8266\n",
      "Epoch 00039: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5153 - acc: 0.8266 - val_loss: 3.2918 - val_acc: 0.3631\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5220 - acc: 0.8245\n",
      "Epoch 00040: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5219 - acc: 0.8245 - val_loss: 3.3043 - val_acc: 0.3662\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.8270\n",
      "Epoch 00041: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5159 - acc: 0.8270 - val_loss: 3.2940 - val_acc: 0.3701\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.8306\n",
      "Epoch 00042: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5013 - acc: 0.8306 - val_loss: 3.3626 - val_acc: 0.3687\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.8296\n",
      "Epoch 00043: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5084 - acc: 0.8296 - val_loss: 3.3556 - val_acc: 0.3659\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4959 - acc: 0.8339\n",
      "Epoch 00044: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4959 - acc: 0.8339 - val_loss: 3.4151 - val_acc: 0.3645\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8363\n",
      "Epoch 00045: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4871 - acc: 0.8363 - val_loss: 3.4230 - val_acc: 0.3669\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.8349\n",
      "Epoch 00046: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4967 - acc: 0.8349 - val_loss: 3.4618 - val_acc: 0.3648\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4750 - acc: 0.8401\n",
      "Epoch 00047: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4751 - acc: 0.8401 - val_loss: 3.4422 - val_acc: 0.3664\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4823 - acc: 0.8390\n",
      "Epoch 00048: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4823 - acc: 0.8390 - val_loss: 3.4691 - val_acc: 0.3611\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4709 - acc: 0.8438\n",
      "Epoch 00049: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4708 - acc: 0.8438 - val_loss: 3.4559 - val_acc: 0.3685\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4683 - acc: 0.8429\n",
      "Epoch 00050: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4684 - acc: 0.8428 - val_loss: 3.5034 - val_acc: 0.3685\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4631 - acc: 0.8449\n",
      "Epoch 00051: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4630 - acc: 0.8449 - val_loss: 3.5010 - val_acc: 0.3699\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8471\n",
      "Epoch 00052: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4562 - acc: 0.8472 - val_loss: 3.5415 - val_acc: 0.3697\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.8489\n",
      "Epoch 00053: val_loss did not improve from 2.05113\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4569 - acc: 0.8489 - val_loss: 3.5278 - val_acc: 0.3629\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 500us/sample - loss: 2.0843 - acc: 0.3524\n",
      "Loss: 2.084329318282265 Accuracy: 0.3524403\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1072 - acc: 0.3287\n",
      "Epoch 00001: val_loss improved from inf to 1.72592, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_3_conv_checkpoint/001-1.7259.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 2.1072 - acc: 0.3287 - val_loss: 1.7259 - val_acc: 0.4587\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5721 - acc: 0.5018\n",
      "Epoch 00002: val_loss improved from 1.72592 to 1.57960, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_3_conv_checkpoint/002-1.5796.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.5721 - acc: 0.5018 - val_loss: 1.5796 - val_acc: 0.5024\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3789 - acc: 0.5645\n",
      "Epoch 00003: val_loss improved from 1.57960 to 1.52620, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_3_conv_checkpoint/003-1.5262.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.3789 - acc: 0.5645 - val_loss: 1.5262 - val_acc: 0.5236\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2661 - acc: 0.5972\n",
      "Epoch 00004: val_loss improved from 1.52620 to 1.52548, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_3_conv_checkpoint/004-1.5255.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.2661 - acc: 0.5972 - val_loss: 1.5255 - val_acc: 0.5232\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1775 - acc: 0.6257\n",
      "Epoch 00005: val_loss improved from 1.52548 to 1.49624, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_3_conv_checkpoint/005-1.4962.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1775 - acc: 0.6257 - val_loss: 1.4962 - val_acc: 0.5437\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1067 - acc: 0.6486\n",
      "Epoch 00006: val_loss improved from 1.49624 to 1.48949, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_3_conv_checkpoint/006-1.4895.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1066 - acc: 0.6486 - val_loss: 1.4895 - val_acc: 0.5411\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0457 - acc: 0.6655\n",
      "Epoch 00007: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0458 - acc: 0.6654 - val_loss: 1.4987 - val_acc: 0.5418\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9934 - acc: 0.6832\n",
      "Epoch 00008: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9934 - acc: 0.6832 - val_loss: 1.4962 - val_acc: 0.5563\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9439 - acc: 0.6982\n",
      "Epoch 00009: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9439 - acc: 0.6982 - val_loss: 1.5198 - val_acc: 0.5451\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8973 - acc: 0.7107\n",
      "Epoch 00010: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8972 - acc: 0.7107 - val_loss: 1.5176 - val_acc: 0.5600\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8642 - acc: 0.7198\n",
      "Epoch 00011: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8642 - acc: 0.7198 - val_loss: 1.5196 - val_acc: 0.5621\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8274 - acc: 0.7340\n",
      "Epoch 00012: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8274 - acc: 0.7340 - val_loss: 1.5340 - val_acc: 0.5691\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7974 - acc: 0.7420\n",
      "Epoch 00013: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7973 - acc: 0.7421 - val_loss: 1.5369 - val_acc: 0.5681\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7596 - acc: 0.7533\n",
      "Epoch 00014: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7598 - acc: 0.7533 - val_loss: 1.5426 - val_acc: 0.5660\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7301 - acc: 0.7626\n",
      "Epoch 00015: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7301 - acc: 0.7626 - val_loss: 1.5250 - val_acc: 0.5786\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6975 - acc: 0.7715\n",
      "Epoch 00016: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6975 - acc: 0.7715 - val_loss: 1.5426 - val_acc: 0.5784\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6728 - acc: 0.7803\n",
      "Epoch 00017: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6728 - acc: 0.7803 - val_loss: 1.5574 - val_acc: 0.5782\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6563 - acc: 0.7854\n",
      "Epoch 00018: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6562 - acc: 0.7855 - val_loss: 1.5591 - val_acc: 0.5786\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6304 - acc: 0.7930\n",
      "Epoch 00019: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6304 - acc: 0.7930 - val_loss: 1.5648 - val_acc: 0.5819\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.8014\n",
      "Epoch 00020: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6035 - acc: 0.8014 - val_loss: 1.5574 - val_acc: 0.5851\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5804 - acc: 0.8088\n",
      "Epoch 00021: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5804 - acc: 0.8088 - val_loss: 1.5867 - val_acc: 0.5861\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5746 - acc: 0.8128\n",
      "Epoch 00022: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5745 - acc: 0.8128 - val_loss: 1.5823 - val_acc: 0.5931\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.8169\n",
      "Epoch 00023: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5556 - acc: 0.8169 - val_loss: 1.5933 - val_acc: 0.5935\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.8267\n",
      "Epoch 00024: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5271 - acc: 0.8268 - val_loss: 1.6050 - val_acc: 0.5982\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.8304\n",
      "Epoch 00025: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5100 - acc: 0.8304 - val_loss: 1.6173 - val_acc: 0.5945\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.8350\n",
      "Epoch 00026: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5012 - acc: 0.8350 - val_loss: 1.6140 - val_acc: 0.6087\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4944 - acc: 0.8362\n",
      "Epoch 00027: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4945 - acc: 0.8362 - val_loss: 1.6385 - val_acc: 0.6028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4725 - acc: 0.8439\n",
      "Epoch 00028: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4726 - acc: 0.8439 - val_loss: 1.6126 - val_acc: 0.6073\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4594 - acc: 0.8488\n",
      "Epoch 00029: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4595 - acc: 0.8488 - val_loss: 1.6328 - val_acc: 0.6091\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4511 - acc: 0.8499\n",
      "Epoch 00030: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4512 - acc: 0.8499 - val_loss: 1.6338 - val_acc: 0.6068\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4351 - acc: 0.8569\n",
      "Epoch 00031: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4352 - acc: 0.8569 - val_loss: 1.6504 - val_acc: 0.6075\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.8611\n",
      "Epoch 00032: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4210 - acc: 0.8611 - val_loss: 1.6650 - val_acc: 0.6108\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8644\n",
      "Epoch 00033: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4119 - acc: 0.8644 - val_loss: 1.6752 - val_acc: 0.6124\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8661\n",
      "Epoch 00034: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4025 - acc: 0.8661 - val_loss: 1.6551 - val_acc: 0.6173\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8721\n",
      "Epoch 00035: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3843 - acc: 0.8721 - val_loss: 1.6987 - val_acc: 0.6143\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8719\n",
      "Epoch 00036: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3859 - acc: 0.8719 - val_loss: 1.6784 - val_acc: 0.6198\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8773\n",
      "Epoch 00037: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3732 - acc: 0.8773 - val_loss: 1.6727 - val_acc: 0.6171\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8776\n",
      "Epoch 00038: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3649 - acc: 0.8776 - val_loss: 1.7084 - val_acc: 0.6131\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8819\n",
      "Epoch 00039: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3566 - acc: 0.8819 - val_loss: 1.7709 - val_acc: 0.6119\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8857\n",
      "Epoch 00040: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3489 - acc: 0.8856 - val_loss: 1.7015 - val_acc: 0.6166\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8880\n",
      "Epoch 00041: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3395 - acc: 0.8880 - val_loss: 1.7670 - val_acc: 0.6143\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8901\n",
      "Epoch 00042: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3311 - acc: 0.8901 - val_loss: 1.7231 - val_acc: 0.6117\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8938\n",
      "Epoch 00043: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3256 - acc: 0.8938 - val_loss: 1.7348 - val_acc: 0.6273\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.8977\n",
      "Epoch 00044: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3122 - acc: 0.8977 - val_loss: 1.7922 - val_acc: 0.6194\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.8989\n",
      "Epoch 00045: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3101 - acc: 0.8989 - val_loss: 1.7537 - val_acc: 0.6289\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9020\n",
      "Epoch 00046: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3011 - acc: 0.9020 - val_loss: 1.8341 - val_acc: 0.6224\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9028\n",
      "Epoch 00047: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2953 - acc: 0.9028 - val_loss: 1.7651 - val_acc: 0.6250\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.9031\n",
      "Epoch 00048: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2985 - acc: 0.9031 - val_loss: 1.7717 - val_acc: 0.6296\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9055\n",
      "Epoch 00049: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2832 - acc: 0.9055 - val_loss: 1.7651 - val_acc: 0.6252\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.9064\n",
      "Epoch 00050: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2869 - acc: 0.9064 - val_loss: 1.7713 - val_acc: 0.6261\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9071\n",
      "Epoch 00051: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2851 - acc: 0.9071 - val_loss: 1.7567 - val_acc: 0.6359\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9079\n",
      "Epoch 00052: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2800 - acc: 0.9079 - val_loss: 1.8000 - val_acc: 0.6310\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9141\n",
      "Epoch 00053: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2674 - acc: 0.9141 - val_loss: 1.7755 - val_acc: 0.6348\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9139\n",
      "Epoch 00054: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2651 - acc: 0.9140 - val_loss: 1.8202 - val_acc: 0.6275\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9153\n",
      "Epoch 00055: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2649 - acc: 0.9153 - val_loss: 1.8253 - val_acc: 0.6327\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.9177\n",
      "Epoch 00056: val_loss did not improve from 1.48949\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2548 - acc: 0.9176 - val_loss: 1.8086 - val_acc: 0.6364\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 585us/sample - loss: 1.5735 - acc: 0.5088\n",
      "Loss: 1.5734667690619748 Accuracy: 0.50882655\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9977 - acc: 0.3514\n",
      "Epoch 00001: val_loss improved from inf to 1.54585, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/001-1.5458.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.9976 - acc: 0.3515 - val_loss: 1.5458 - val_acc: 0.5232\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4979 - acc: 0.5237\n",
      "Epoch 00002: val_loss improved from 1.54585 to 1.37465, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/002-1.3746.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.4980 - acc: 0.5236 - val_loss: 1.3746 - val_acc: 0.5851\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3307 - acc: 0.5841\n",
      "Epoch 00003: val_loss improved from 1.37465 to 1.25840, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/003-1.2584.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3306 - acc: 0.5841 - val_loss: 1.2584 - val_acc: 0.6282\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2157 - acc: 0.6230\n",
      "Epoch 00004: val_loss improved from 1.25840 to 1.20900, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/004-1.2090.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.2159 - acc: 0.6230 - val_loss: 1.2090 - val_acc: 0.6420\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1392 - acc: 0.6470\n",
      "Epoch 00005: val_loss improved from 1.20900 to 1.17737, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/005-1.1774.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1393 - acc: 0.6469 - val_loss: 1.1774 - val_acc: 0.6515\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0659 - acc: 0.6711\n",
      "Epoch 00006: val_loss improved from 1.17737 to 1.14627, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/006-1.1463.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0660 - acc: 0.6711 - val_loss: 1.1463 - val_acc: 0.6597\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0061 - acc: 0.6889\n",
      "Epoch 00007: val_loss improved from 1.14627 to 1.11517, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/007-1.1152.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0062 - acc: 0.6889 - val_loss: 1.1152 - val_acc: 0.6683\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9526 - acc: 0.7071\n",
      "Epoch 00008: val_loss improved from 1.11517 to 1.10794, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/008-1.1079.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9527 - acc: 0.7071 - val_loss: 1.1079 - val_acc: 0.6716\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9013 - acc: 0.7199\n",
      "Epoch 00009: val_loss improved from 1.10794 to 1.08024, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/009-1.0802.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9013 - acc: 0.7199 - val_loss: 1.0802 - val_acc: 0.6788\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8570 - acc: 0.7364\n",
      "Epoch 00010: val_loss did not improve from 1.08024\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8570 - acc: 0.7364 - val_loss: 1.0821 - val_acc: 0.6792\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8237 - acc: 0.7437\n",
      "Epoch 00011: val_loss improved from 1.08024 to 1.06872, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/011-1.0687.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8238 - acc: 0.7436 - val_loss: 1.0687 - val_acc: 0.6865\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7854 - acc: 0.7557\n",
      "Epoch 00012: val_loss improved from 1.06872 to 1.05500, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/012-1.0550.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7854 - acc: 0.7557 - val_loss: 1.0550 - val_acc: 0.6893\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7543 - acc: 0.7640\n",
      "Epoch 00013: val_loss improved from 1.05500 to 1.04264, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/013-1.0426.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7544 - acc: 0.7640 - val_loss: 1.0426 - val_acc: 0.6918\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7196 - acc: 0.7748\n",
      "Epoch 00014: val_loss improved from 1.04264 to 1.01762, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/014-1.0176.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7196 - acc: 0.7748 - val_loss: 1.0176 - val_acc: 0.6993\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6866 - acc: 0.7851\n",
      "Epoch 00015: val_loss did not improve from 1.01762\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6865 - acc: 0.7851 - val_loss: 1.0284 - val_acc: 0.7014\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.7912\n",
      "Epoch 00016: val_loss improved from 1.01762 to 1.00845, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/016-1.0085.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6661 - acc: 0.7913 - val_loss: 1.0085 - val_acc: 0.7056\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6336 - acc: 0.8015\n",
      "Epoch 00017: val_loss did not improve from 1.00845\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6336 - acc: 0.8015 - val_loss: 1.0304 - val_acc: 0.7007\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6128 - acc: 0.8080\n",
      "Epoch 00018: val_loss improved from 1.00845 to 1.00084, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/018-1.0008.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6128 - acc: 0.8080 - val_loss: 1.0008 - val_acc: 0.7121\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5879 - acc: 0.8136\n",
      "Epoch 00019: val_loss did not improve from 1.00084\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5880 - acc: 0.8136 - val_loss: 1.0131 - val_acc: 0.7121\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5757 - acc: 0.8189\n",
      "Epoch 00020: val_loss improved from 1.00084 to 0.99567, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/020-0.9957.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5758 - acc: 0.8189 - val_loss: 0.9957 - val_acc: 0.7149\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.8265\n",
      "Epoch 00021: val_loss improved from 0.99567 to 0.98594, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/021-0.9859.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5527 - acc: 0.8265 - val_loss: 0.9859 - val_acc: 0.7177\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.8262\n",
      "Epoch 00022: val_loss did not improve from 0.98594\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5440 - acc: 0.8262 - val_loss: 0.9863 - val_acc: 0.7233\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.8337\n",
      "Epoch 00023: val_loss improved from 0.98594 to 0.98508, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/023-0.9851.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5213 - acc: 0.8337 - val_loss: 0.9851 - val_acc: 0.7242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.8389\n",
      "Epoch 00024: val_loss improved from 0.98508 to 0.97780, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/024-0.9778.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5050 - acc: 0.8390 - val_loss: 0.9778 - val_acc: 0.7186\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4845 - acc: 0.8445\n",
      "Epoch 00025: val_loss improved from 0.97780 to 0.97130, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/025-0.9713.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4845 - acc: 0.8445 - val_loss: 0.9713 - val_acc: 0.7221\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4761 - acc: 0.8476\n",
      "Epoch 00026: val_loss improved from 0.97130 to 0.96800, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/026-0.9680.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4761 - acc: 0.8476 - val_loss: 0.9680 - val_acc: 0.7254\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4600 - acc: 0.8527\n",
      "Epoch 00027: val_loss did not improve from 0.96800\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4601 - acc: 0.8527 - val_loss: 0.9719 - val_acc: 0.7303\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4479 - acc: 0.8568\n",
      "Epoch 00028: val_loss did not improve from 0.96800\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4479 - acc: 0.8568 - val_loss: 0.9874 - val_acc: 0.7296\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.8587\n",
      "Epoch 00029: val_loss improved from 0.96800 to 0.95300, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/029-0.9530.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4421 - acc: 0.8587 - val_loss: 0.9530 - val_acc: 0.7391\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8642\n",
      "Epoch 00030: val_loss did not improve from 0.95300\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4220 - acc: 0.8642 - val_loss: 0.9666 - val_acc: 0.7321\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8677\n",
      "Epoch 00031: val_loss did not improve from 0.95300\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4128 - acc: 0.8677 - val_loss: 0.9670 - val_acc: 0.7389\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8689\n",
      "Epoch 00032: val_loss did not improve from 0.95300\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4033 - acc: 0.8689 - val_loss: 0.9570 - val_acc: 0.7403\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8712\n",
      "Epoch 00033: val_loss did not improve from 0.95300\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3958 - acc: 0.8712 - val_loss: 0.9587 - val_acc: 0.7412\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3867 - acc: 0.8736\n",
      "Epoch 00034: val_loss did not improve from 0.95300\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3867 - acc: 0.8736 - val_loss: 0.9697 - val_acc: 0.7405\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8775\n",
      "Epoch 00035: val_loss did not improve from 0.95300\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3765 - acc: 0.8775 - val_loss: 0.9850 - val_acc: 0.7342\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8804\n",
      "Epoch 00036: val_loss improved from 0.95300 to 0.94663, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_4_conv_checkpoint/036-0.9466.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3696 - acc: 0.8805 - val_loss: 0.9466 - val_acc: 0.7438\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8823\n",
      "Epoch 00037: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3627 - acc: 0.8823 - val_loss: 0.9581 - val_acc: 0.7473\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.8848\n",
      "Epoch 00038: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3549 - acc: 0.8848 - val_loss: 0.9665 - val_acc: 0.7452\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8869\n",
      "Epoch 00039: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3524 - acc: 0.8869 - val_loss: 0.9749 - val_acc: 0.7421\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8887\n",
      "Epoch 00040: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3392 - acc: 0.8887 - val_loss: 0.9658 - val_acc: 0.7477\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8905\n",
      "Epoch 00041: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3364 - acc: 0.8905 - val_loss: 1.0057 - val_acc: 0.7445\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8913\n",
      "Epoch 00042: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3338 - acc: 0.8913 - val_loss: 0.9523 - val_acc: 0.7536\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3186 - acc: 0.8942\n",
      "Epoch 00043: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3186 - acc: 0.8942 - val_loss: 0.9710 - val_acc: 0.7489\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8930\n",
      "Epoch 00044: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3222 - acc: 0.8930 - val_loss: 0.9668 - val_acc: 0.7580\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.8958\n",
      "Epoch 00045: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3185 - acc: 0.8958 - val_loss: 0.9660 - val_acc: 0.7589\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.8974\n",
      "Epoch 00046: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3122 - acc: 0.8974 - val_loss: 0.9647 - val_acc: 0.7543\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9007\n",
      "Epoch 00047: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3000 - acc: 0.9007 - val_loss: 0.9854 - val_acc: 0.7480\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9032\n",
      "Epoch 00048: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2993 - acc: 0.9032 - val_loss: 0.9920 - val_acc: 0.7498\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9022\n",
      "Epoch 00049: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2988 - acc: 0.9022 - val_loss: 0.9760 - val_acc: 0.7540\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9048\n",
      "Epoch 00050: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2911 - acc: 0.9048 - val_loss: 0.9965 - val_acc: 0.7505\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9046\n",
      "Epoch 00051: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2904 - acc: 0.9046 - val_loss: 0.9994 - val_acc: 0.7477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9083\n",
      "Epoch 00052: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2815 - acc: 0.9083 - val_loss: 0.9722 - val_acc: 0.7591\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9114\n",
      "Epoch 00053: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2736 - acc: 0.9114 - val_loss: 0.9873 - val_acc: 0.7538\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9087\n",
      "Epoch 00054: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2765 - acc: 0.9087 - val_loss: 0.9814 - val_acc: 0.7582\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9101\n",
      "Epoch 00055: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2719 - acc: 0.9101 - val_loss: 0.9960 - val_acc: 0.7561\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9105\n",
      "Epoch 00056: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2747 - acc: 0.9104 - val_loss: 0.9983 - val_acc: 0.7522\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9109\n",
      "Epoch 00057: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2687 - acc: 0.9109 - val_loss: 0.9931 - val_acc: 0.7612\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9151\n",
      "Epoch 00058: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2587 - acc: 0.9151 - val_loss: 1.0166 - val_acc: 0.7584\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9161\n",
      "Epoch 00059: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2569 - acc: 0.9161 - val_loss: 1.0026 - val_acc: 0.7584\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9138\n",
      "Epoch 00060: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2605 - acc: 0.9138 - val_loss: 0.9897 - val_acc: 0.7626\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9168\n",
      "Epoch 00061: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2553 - acc: 0.9168 - val_loss: 1.0117 - val_acc: 0.7543\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9147\n",
      "Epoch 00062: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2553 - acc: 0.9147 - val_loss: 0.9956 - val_acc: 0.7631\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2511 - acc: 0.9172\n",
      "Epoch 00063: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2511 - acc: 0.9172 - val_loss: 0.9951 - val_acc: 0.7608\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9180\n",
      "Epoch 00064: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2485 - acc: 0.9179 - val_loss: 0.9707 - val_acc: 0.7666\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9203\n",
      "Epoch 00065: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2451 - acc: 0.9203 - val_loss: 1.0100 - val_acc: 0.7608\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9184\n",
      "Epoch 00066: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2469 - acc: 0.9184 - val_loss: 0.9819 - val_acc: 0.7652\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9223\n",
      "Epoch 00067: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2335 - acc: 0.9223 - val_loss: 1.0222 - val_acc: 0.7671\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9211\n",
      "Epoch 00068: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2403 - acc: 0.9211 - val_loss: 1.0058 - val_acc: 0.7638\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9221\n",
      "Epoch 00069: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2345 - acc: 0.9220 - val_loss: 0.9985 - val_acc: 0.7701\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9242\n",
      "Epoch 00070: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2303 - acc: 0.9242 - val_loss: 0.9982 - val_acc: 0.7650\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9259\n",
      "Epoch 00071: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2260 - acc: 0.9259 - val_loss: 0.9813 - val_acc: 0.7713\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9241\n",
      "Epoch 00072: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2316 - acc: 0.9241 - val_loss: 1.0146 - val_acc: 0.7659\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9257\n",
      "Epoch 00073: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2247 - acc: 0.9256 - val_loss: 0.9930 - val_acc: 0.7685\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9269\n",
      "Epoch 00074: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2224 - acc: 0.9269 - val_loss: 1.0061 - val_acc: 0.7699\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9264\n",
      "Epoch 00075: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2204 - acc: 0.9264 - val_loss: 1.0014 - val_acc: 0.7713\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9259\n",
      "Epoch 00076: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2218 - acc: 0.9259 - val_loss: 1.0140 - val_acc: 0.7638\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9281\n",
      "Epoch 00077: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2189 - acc: 0.9281 - val_loss: 0.9949 - val_acc: 0.7706\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9283\n",
      "Epoch 00078: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2155 - acc: 0.9283 - val_loss: 1.0069 - val_acc: 0.7731\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9309\n",
      "Epoch 00079: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2099 - acc: 0.9309 - val_loss: 1.0323 - val_acc: 0.7710\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9298\n",
      "Epoch 00080: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2139 - acc: 0.9298 - val_loss: 1.0224 - val_acc: 0.7715\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9292\n",
      "Epoch 00081: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2083 - acc: 0.9291 - val_loss: 1.0163 - val_acc: 0.7685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9321\n",
      "Epoch 00082: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2059 - acc: 0.9321 - val_loss: 1.0029 - val_acc: 0.7734\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9311\n",
      "Epoch 00083: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2059 - acc: 0.9311 - val_loss: 1.0136 - val_acc: 0.7727\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9312\n",
      "Epoch 00084: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2081 - acc: 0.9312 - val_loss: 1.0250 - val_acc: 0.7701\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9303\n",
      "Epoch 00085: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2087 - acc: 0.9303 - val_loss: 1.0233 - val_acc: 0.7687\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9313\n",
      "Epoch 00086: val_loss did not improve from 0.94663\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2058 - acc: 0.9313 - val_loss: 0.9998 - val_acc: 0.7729\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 575us/sample - loss: 1.0243 - acc: 0.7148\n",
      "Loss: 1.0243151929148262 Accuracy: 0.7148494\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9481 - acc: 0.3655\n",
      "Epoch 00001: val_loss improved from inf to 1.46738, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/001-1.4674.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.9480 - acc: 0.3656 - val_loss: 1.4674 - val_acc: 0.5500\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4802 - acc: 0.5283\n",
      "Epoch 00002: val_loss improved from 1.46738 to 1.37628, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/002-1.3763.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.4802 - acc: 0.5283 - val_loss: 1.3763 - val_acc: 0.5674\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3139 - acc: 0.5964\n",
      "Epoch 00003: val_loss improved from 1.37628 to 1.16744, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/003-1.1674.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3138 - acc: 0.5964 - val_loss: 1.1674 - val_acc: 0.6504\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1936 - acc: 0.6377\n",
      "Epoch 00004: val_loss improved from 1.16744 to 1.10500, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/004-1.1050.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1937 - acc: 0.6377 - val_loss: 1.1050 - val_acc: 0.6720\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1139 - acc: 0.6609\n",
      "Epoch 00005: val_loss improved from 1.10500 to 1.03837, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/005-1.0384.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1139 - acc: 0.6609 - val_loss: 1.0384 - val_acc: 0.6865\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0486 - acc: 0.6826\n",
      "Epoch 00006: val_loss improved from 1.03837 to 1.00336, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/006-1.0034.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0486 - acc: 0.6827 - val_loss: 1.0034 - val_acc: 0.6946\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9980 - acc: 0.6985\n",
      "Epoch 00007: val_loss improved from 1.00336 to 0.99144, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/007-0.9914.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9981 - acc: 0.6984 - val_loss: 0.9914 - val_acc: 0.7063\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9468 - acc: 0.7138\n",
      "Epoch 00008: val_loss improved from 0.99144 to 0.93360, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/008-0.9336.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9467 - acc: 0.7138 - val_loss: 0.9336 - val_acc: 0.7235\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8996 - acc: 0.7313\n",
      "Epoch 00009: val_loss improved from 0.93360 to 0.89931, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/009-0.8993.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8997 - acc: 0.7312 - val_loss: 0.8993 - val_acc: 0.7326\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8634 - acc: 0.7427\n",
      "Epoch 00010: val_loss improved from 0.89931 to 0.87168, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/010-0.8717.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8633 - acc: 0.7427 - val_loss: 0.8717 - val_acc: 0.7419\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8257 - acc: 0.7520\n",
      "Epoch 00011: val_loss did not improve from 0.87168\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8257 - acc: 0.7519 - val_loss: 0.8728 - val_acc: 0.7403\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7946 - acc: 0.7592\n",
      "Epoch 00012: val_loss improved from 0.87168 to 0.83997, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/012-0.8400.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7946 - acc: 0.7592 - val_loss: 0.8400 - val_acc: 0.7477\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7595 - acc: 0.7747\n",
      "Epoch 00013: val_loss improved from 0.83997 to 0.81724, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/013-0.8172.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7595 - acc: 0.7747 - val_loss: 0.8172 - val_acc: 0.7605\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7377 - acc: 0.7797\n",
      "Epoch 00014: val_loss improved from 0.81724 to 0.81201, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/014-0.8120.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7378 - acc: 0.7797 - val_loss: 0.8120 - val_acc: 0.7636\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7114 - acc: 0.7881\n",
      "Epoch 00015: val_loss improved from 0.81201 to 0.78090, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/015-0.7809.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7114 - acc: 0.7881 - val_loss: 0.7809 - val_acc: 0.7729\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6827 - acc: 0.7972\n",
      "Epoch 00016: val_loss did not improve from 0.78090\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6826 - acc: 0.7972 - val_loss: 0.8101 - val_acc: 0.7687\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6620 - acc: 0.8039\n",
      "Epoch 00017: val_loss did not improve from 0.78090\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6619 - acc: 0.8039 - val_loss: 0.7987 - val_acc: 0.7694\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6428 - acc: 0.8084\n",
      "Epoch 00018: val_loss improved from 0.78090 to 0.76016, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/018-0.7602.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6427 - acc: 0.8084 - val_loss: 0.7602 - val_acc: 0.7803\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6241 - acc: 0.8156\n",
      "Epoch 00019: val_loss did not improve from 0.76016\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6240 - acc: 0.8157 - val_loss: 0.7741 - val_acc: 0.7771\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5980 - acc: 0.8232\n",
      "Epoch 00020: val_loss improved from 0.76016 to 0.74285, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/020-0.7428.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5979 - acc: 0.8233 - val_loss: 0.7428 - val_acc: 0.7869\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5847 - acc: 0.8246\n",
      "Epoch 00021: val_loss did not improve from 0.74285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5848 - acc: 0.8246 - val_loss: 0.7526 - val_acc: 0.7850\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5687 - acc: 0.8294\n",
      "Epoch 00022: val_loss did not improve from 0.74285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5687 - acc: 0.8294 - val_loss: 0.7626 - val_acc: 0.7869\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.8337\n",
      "Epoch 00023: val_loss did not improve from 0.74285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5547 - acc: 0.8337 - val_loss: 0.7431 - val_acc: 0.7915\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.8396\n",
      "Epoch 00024: val_loss improved from 0.74285 to 0.73811, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/024-0.7381.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5338 - acc: 0.8396 - val_loss: 0.7381 - val_acc: 0.7859\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.8436\n",
      "Epoch 00025: val_loss improved from 0.73811 to 0.73629, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/025-0.7363.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5217 - acc: 0.8436 - val_loss: 0.7363 - val_acc: 0.7880\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.8446\n",
      "Epoch 00026: val_loss improved from 0.73629 to 0.73245, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/026-0.7325.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5150 - acc: 0.8445 - val_loss: 0.7325 - val_acc: 0.7878\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4942 - acc: 0.8503\n",
      "Epoch 00027: val_loss improved from 0.73245 to 0.72799, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/027-0.7280.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4942 - acc: 0.8503 - val_loss: 0.7280 - val_acc: 0.7959\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.8532\n",
      "Epoch 00028: val_loss improved from 0.72799 to 0.72435, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/028-0.7244.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4800 - acc: 0.8532 - val_loss: 0.7244 - val_acc: 0.7973\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4687 - acc: 0.8581\n",
      "Epoch 00029: val_loss did not improve from 0.72435\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4688 - acc: 0.8580 - val_loss: 0.7361 - val_acc: 0.7920\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4565 - acc: 0.8610\n",
      "Epoch 00030: val_loss did not improve from 0.72435\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4565 - acc: 0.8609 - val_loss: 0.7398 - val_acc: 0.7876\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4445 - acc: 0.8635\n",
      "Epoch 00031: val_loss improved from 0.72435 to 0.71550, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/031-0.7155.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4444 - acc: 0.8635 - val_loss: 0.7155 - val_acc: 0.7950\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.8635\n",
      "Epoch 00032: val_loss did not improve from 0.71550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4435 - acc: 0.8636 - val_loss: 0.7284 - val_acc: 0.7918\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8724\n",
      "Epoch 00033: val_loss did not improve from 0.71550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4223 - acc: 0.8724 - val_loss: 0.7291 - val_acc: 0.8015\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4197 - acc: 0.8728\n",
      "Epoch 00034: val_loss did not improve from 0.71550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4197 - acc: 0.8728 - val_loss: 0.7256 - val_acc: 0.7964\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8748\n",
      "Epoch 00035: val_loss did not improve from 0.71550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4071 - acc: 0.8747 - val_loss: 0.7168 - val_acc: 0.7971\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8795\n",
      "Epoch 00036: val_loss did not improve from 0.71550\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3971 - acc: 0.8795 - val_loss: 0.7173 - val_acc: 0.7969\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8805\n",
      "Epoch 00037: val_loss improved from 0.71550 to 0.71449, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/037-0.7145.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3902 - acc: 0.8805 - val_loss: 0.7145 - val_acc: 0.7987\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8808\n",
      "Epoch 00038: val_loss improved from 0.71449 to 0.71123, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/038-0.7112.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3804 - acc: 0.8808 - val_loss: 0.7112 - val_acc: 0.8076\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3761 - acc: 0.8836\n",
      "Epoch 00039: val_loss improved from 0.71123 to 0.70122, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/039-0.7012.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3761 - acc: 0.8836 - val_loss: 0.7012 - val_acc: 0.8069\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8895\n",
      "Epoch 00040: val_loss did not improve from 0.70122\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3642 - acc: 0.8895 - val_loss: 0.7129 - val_acc: 0.8043\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8902\n",
      "Epoch 00041: val_loss did not improve from 0.70122\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3543 - acc: 0.8902 - val_loss: 0.7269 - val_acc: 0.8036\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8897\n",
      "Epoch 00042: val_loss improved from 0.70122 to 0.69797, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_5_conv_checkpoint/042-0.6980.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3532 - acc: 0.8896 - val_loss: 0.6980 - val_acc: 0.8130\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8958\n",
      "Epoch 00043: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3409 - acc: 0.8958 - val_loss: 0.7176 - val_acc: 0.8041\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8953\n",
      "Epoch 00044: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3380 - acc: 0.8953 - val_loss: 0.7171 - val_acc: 0.8041\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8963\n",
      "Epoch 00045: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3325 - acc: 0.8963 - val_loss: 0.7411 - val_acc: 0.8006\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8977\n",
      "Epoch 00046: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3278 - acc: 0.8976 - val_loss: 0.7236 - val_acc: 0.8018\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.8973\n",
      "Epoch 00047: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3224 - acc: 0.8974 - val_loss: 0.7252 - val_acc: 0.8090\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.9026\n",
      "Epoch 00048: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3100 - acc: 0.9026 - val_loss: 0.7093 - val_acc: 0.8078\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.9012\n",
      "Epoch 00049: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3132 - acc: 0.9012 - val_loss: 0.7149 - val_acc: 0.8092\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9039\n",
      "Epoch 00050: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3078 - acc: 0.9039 - val_loss: 0.7083 - val_acc: 0.8099\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9045\n",
      "Epoch 00051: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3013 - acc: 0.9046 - val_loss: 0.7407 - val_acc: 0.8039\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9077\n",
      "Epoch 00052: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2940 - acc: 0.9077 - val_loss: 0.7417 - val_acc: 0.8053\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.9110\n",
      "Epoch 00053: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2864 - acc: 0.9110 - val_loss: 0.7481 - val_acc: 0.8013\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9094\n",
      "Epoch 00054: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2852 - acc: 0.9094 - val_loss: 0.7327 - val_acc: 0.8076\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9115\n",
      "Epoch 00055: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2823 - acc: 0.9115 - val_loss: 0.7350 - val_acc: 0.8083\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9126\n",
      "Epoch 00056: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2764 - acc: 0.9126 - val_loss: 0.7220 - val_acc: 0.8148\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9141\n",
      "Epoch 00057: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2690 - acc: 0.9141 - val_loss: 0.7336 - val_acc: 0.8062\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2711 - acc: 0.9135\n",
      "Epoch 00058: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2712 - acc: 0.9134 - val_loss: 0.7324 - val_acc: 0.8064\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9148\n",
      "Epoch 00059: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2661 - acc: 0.9148 - val_loss: 0.7225 - val_acc: 0.8118\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9168\n",
      "Epoch 00060: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2632 - acc: 0.9168 - val_loss: 0.7140 - val_acc: 0.8146\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9194\n",
      "Epoch 00061: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2566 - acc: 0.9194 - val_loss: 0.7180 - val_acc: 0.8174\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9180\n",
      "Epoch 00062: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2540 - acc: 0.9180 - val_loss: 0.7228 - val_acc: 0.8162\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9200\n",
      "Epoch 00063: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2525 - acc: 0.9200 - val_loss: 0.7310 - val_acc: 0.8106\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9207\n",
      "Epoch 00064: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2515 - acc: 0.9207 - val_loss: 0.7435 - val_acc: 0.8111\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.9202\n",
      "Epoch 00065: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2497 - acc: 0.9201 - val_loss: 0.7454 - val_acc: 0.8099\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9206\n",
      "Epoch 00066: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2454 - acc: 0.9206 - val_loss: 0.7202 - val_acc: 0.8195\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9234\n",
      "Epoch 00067: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2413 - acc: 0.9234 - val_loss: 0.7422 - val_acc: 0.8078\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9249\n",
      "Epoch 00068: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2360 - acc: 0.9249 - val_loss: 0.7262 - val_acc: 0.8146\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9254\n",
      "Epoch 00069: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2326 - acc: 0.9254 - val_loss: 0.7351 - val_acc: 0.8137\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9268\n",
      "Epoch 00070: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2281 - acc: 0.9268 - val_loss: 0.7149 - val_acc: 0.8234\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9257\n",
      "Epoch 00071: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2256 - acc: 0.9257 - val_loss: 0.7425 - val_acc: 0.8150\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9299\n",
      "Epoch 00072: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2231 - acc: 0.9299 - val_loss: 0.7301 - val_acc: 0.8171\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9268\n",
      "Epoch 00073: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2252 - acc: 0.9268 - val_loss: 0.7323 - val_acc: 0.8137\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9288\n",
      "Epoch 00074: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2183 - acc: 0.9288 - val_loss: 0.7551 - val_acc: 0.8109\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9305\n",
      "Epoch 00075: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2182 - acc: 0.9305 - val_loss: 0.7144 - val_acc: 0.8169\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9301\n",
      "Epoch 00076: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2153 - acc: 0.9301 - val_loss: 0.7346 - val_acc: 0.8202\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9324\n",
      "Epoch 00077: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2135 - acc: 0.9324 - val_loss: 0.7298 - val_acc: 0.8160\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9293\n",
      "Epoch 00078: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2152 - acc: 0.9293 - val_loss: 0.7344 - val_acc: 0.8181\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9305\n",
      "Epoch 00079: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2138 - acc: 0.9305 - val_loss: 0.7376 - val_acc: 0.8185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9307\n",
      "Epoch 00080: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2105 - acc: 0.9306 - val_loss: 0.7153 - val_acc: 0.8204\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9337\n",
      "Epoch 00081: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2057 - acc: 0.9337 - val_loss: 0.7346 - val_acc: 0.8218\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9315\n",
      "Epoch 00082: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2117 - acc: 0.9315 - val_loss: 0.7160 - val_acc: 0.8223\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9339\n",
      "Epoch 00083: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2050 - acc: 0.9338 - val_loss: 0.7379 - val_acc: 0.8204\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9334\n",
      "Epoch 00084: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2071 - acc: 0.9334 - val_loss: 0.7282 - val_acc: 0.8234\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9343\n",
      "Epoch 00085: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2033 - acc: 0.9344 - val_loss: 0.7335 - val_acc: 0.8262\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9344\n",
      "Epoch 00086: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1997 - acc: 0.9344 - val_loss: 0.7618 - val_acc: 0.8150\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9371\n",
      "Epoch 00087: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1926 - acc: 0.9371 - val_loss: 0.7532 - val_acc: 0.8223\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9352\n",
      "Epoch 00088: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1996 - acc: 0.9352 - val_loss: 0.7459 - val_acc: 0.8239\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9376\n",
      "Epoch 00089: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1907 - acc: 0.9376 - val_loss: 0.7325 - val_acc: 0.8267\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9380\n",
      "Epoch 00090: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1907 - acc: 0.9380 - val_loss: 0.7451 - val_acc: 0.8209\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9365\n",
      "Epoch 00091: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1956 - acc: 0.9366 - val_loss: 0.7441 - val_acc: 0.8255\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9384\n",
      "Epoch 00092: val_loss did not improve from 0.69797\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1897 - acc: 0.9384 - val_loss: 0.7664 - val_acc: 0.8192\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 606us/sample - loss: 0.7704 - acc: 0.7707\n",
      "Loss: 0.7704321701950002 Accuracy: 0.7707165\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0387 - acc: 0.3228\n",
      "Epoch 00001: val_loss improved from inf to 1.53020, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/001-1.5302.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 2.0386 - acc: 0.3228 - val_loss: 1.5302 - val_acc: 0.5118\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5880 - acc: 0.4806\n",
      "Epoch 00002: val_loss improved from 1.53020 to 1.34415, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/002-1.3442.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.5879 - acc: 0.4806 - val_loss: 1.3442 - val_acc: 0.5940\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4352 - acc: 0.5433\n",
      "Epoch 00003: val_loss improved from 1.34415 to 1.25939, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/003-1.2594.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.4353 - acc: 0.5433 - val_loss: 1.2594 - val_acc: 0.6191\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2985 - acc: 0.5951\n",
      "Epoch 00004: val_loss improved from 1.25939 to 1.13259, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/004-1.1326.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.2985 - acc: 0.5951 - val_loss: 1.1326 - val_acc: 0.6639\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1987 - acc: 0.6304\n",
      "Epoch 00005: val_loss improved from 1.13259 to 1.04996, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/005-1.0500.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.1987 - acc: 0.6304 - val_loss: 1.0500 - val_acc: 0.6888\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1242 - acc: 0.6535\n",
      "Epoch 00006: val_loss improved from 1.04996 to 0.99383, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/006-0.9938.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.1242 - acc: 0.6535 - val_loss: 0.9938 - val_acc: 0.7105\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0717 - acc: 0.6751\n",
      "Epoch 00007: val_loss improved from 0.99383 to 0.96033, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/007-0.9603.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.0716 - acc: 0.6751 - val_loss: 0.9603 - val_acc: 0.7126\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0225 - acc: 0.6862\n",
      "Epoch 00008: val_loss improved from 0.96033 to 0.90438, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/008-0.9044.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.0225 - acc: 0.6862 - val_loss: 0.9044 - val_acc: 0.7361\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9717 - acc: 0.7063\n",
      "Epoch 00009: val_loss improved from 0.90438 to 0.87443, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/009-0.8744.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9718 - acc: 0.7063 - val_loss: 0.8744 - val_acc: 0.7389\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9356 - acc: 0.7155\n",
      "Epoch 00010: val_loss improved from 0.87443 to 0.87014, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/010-0.8701.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9357 - acc: 0.7155 - val_loss: 0.8701 - val_acc: 0.7414\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9018 - acc: 0.7274\n",
      "Epoch 00011: val_loss improved from 0.87014 to 0.80541, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/011-0.8054.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9017 - acc: 0.7274 - val_loss: 0.8054 - val_acc: 0.7675\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8680 - acc: 0.7379\n",
      "Epoch 00012: val_loss improved from 0.80541 to 0.80366, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/012-0.8037.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8680 - acc: 0.7379 - val_loss: 0.8037 - val_acc: 0.7701\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8407 - acc: 0.7491\n",
      "Epoch 00013: val_loss improved from 0.80366 to 0.78302, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/013-0.7830.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8406 - acc: 0.7491 - val_loss: 0.7830 - val_acc: 0.7787\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8175 - acc: 0.7558\n",
      "Epoch 00014: val_loss improved from 0.78302 to 0.73867, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/014-0.7387.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8175 - acc: 0.7558 - val_loss: 0.7387 - val_acc: 0.7901\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7948 - acc: 0.7585\n",
      "Epoch 00015: val_loss improved from 0.73867 to 0.72223, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/015-0.7222.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7947 - acc: 0.7585 - val_loss: 0.7222 - val_acc: 0.7892\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7674 - acc: 0.7695\n",
      "Epoch 00016: val_loss improved from 0.72223 to 0.69724, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/016-0.6972.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7673 - acc: 0.7695 - val_loss: 0.6972 - val_acc: 0.7980\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7474 - acc: 0.7754\n",
      "Epoch 00017: val_loss improved from 0.69724 to 0.68245, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/017-0.6824.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7474 - acc: 0.7754 - val_loss: 0.6824 - val_acc: 0.8074\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7275 - acc: 0.7857\n",
      "Epoch 00018: val_loss did not improve from 0.68245\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7275 - acc: 0.7857 - val_loss: 0.6861 - val_acc: 0.8053\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7079 - acc: 0.7890\n",
      "Epoch 00019: val_loss improved from 0.68245 to 0.66932, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/019-0.6693.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7079 - acc: 0.7890 - val_loss: 0.6693 - val_acc: 0.8074\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6902 - acc: 0.7937\n",
      "Epoch 00020: val_loss did not improve from 0.66932\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6903 - acc: 0.7938 - val_loss: 0.6788 - val_acc: 0.8046\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6768 - acc: 0.7975\n",
      "Epoch 00021: val_loss improved from 0.66932 to 0.65696, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/021-0.6570.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6768 - acc: 0.7976 - val_loss: 0.6570 - val_acc: 0.8123\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6613 - acc: 0.8027\n",
      "Epoch 00022: val_loss improved from 0.65696 to 0.63138, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/022-0.6314.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6613 - acc: 0.8027 - val_loss: 0.6314 - val_acc: 0.8197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6449 - acc: 0.8084\n",
      "Epoch 00023: val_loss improved from 0.63138 to 0.62138, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/023-0.6214.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6448 - acc: 0.8084 - val_loss: 0.6214 - val_acc: 0.8232\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6336 - acc: 0.8125\n",
      "Epoch 00024: val_loss improved from 0.62138 to 0.61085, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/024-0.6108.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6336 - acc: 0.8125 - val_loss: 0.6108 - val_acc: 0.8279\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6171 - acc: 0.8144\n",
      "Epoch 00025: val_loss did not improve from 0.61085\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6170 - acc: 0.8145 - val_loss: 0.6109 - val_acc: 0.8269\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6051 - acc: 0.8196\n",
      "Epoch 00026: val_loss improved from 0.61085 to 0.59290, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/026-0.5929.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6050 - acc: 0.8197 - val_loss: 0.5929 - val_acc: 0.8288\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5969 - acc: 0.8207\n",
      "Epoch 00027: val_loss did not improve from 0.59290\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5969 - acc: 0.8206 - val_loss: 0.6452 - val_acc: 0.8160\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5922 - acc: 0.8253\n",
      "Epoch 00028: val_loss improved from 0.59290 to 0.58321, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/028-0.5832.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5922 - acc: 0.8253 - val_loss: 0.5832 - val_acc: 0.8325\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5715 - acc: 0.8289\n",
      "Epoch 00029: val_loss improved from 0.58321 to 0.57368, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/029-0.5737.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5715 - acc: 0.8289 - val_loss: 0.5737 - val_acc: 0.8383\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.8311\n",
      "Epoch 00030: val_loss did not improve from 0.57368\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5664 - acc: 0.8311 - val_loss: 0.5833 - val_acc: 0.8360\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.8334\n",
      "Epoch 00031: val_loss improved from 0.57368 to 0.56400, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/031-0.5640.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5549 - acc: 0.8333 - val_loss: 0.5640 - val_acc: 0.8395\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8388\n",
      "Epoch 00032: val_loss improved from 0.56400 to 0.55956, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/032-0.5596.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5449 - acc: 0.8387 - val_loss: 0.5596 - val_acc: 0.8458\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.8388\n",
      "Epoch 00033: val_loss did not improve from 0.55956\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5418 - acc: 0.8388 - val_loss: 0.5901 - val_acc: 0.8369\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.8430\n",
      "Epoch 00034: val_loss did not improve from 0.55956\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5304 - acc: 0.8430 - val_loss: 0.5683 - val_acc: 0.8376\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5187 - acc: 0.8440\n",
      "Epoch 00035: val_loss improved from 0.55956 to 0.55128, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/035-0.5513.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5187 - acc: 0.8440 - val_loss: 0.5513 - val_acc: 0.8458\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5147 - acc: 0.8465\n",
      "Epoch 00036: val_loss did not improve from 0.55128\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5147 - acc: 0.8465 - val_loss: 0.5694 - val_acc: 0.8453\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8487\n",
      "Epoch 00037: val_loss improved from 0.55128 to 0.53952, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/037-0.5395.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5066 - acc: 0.8487 - val_loss: 0.5395 - val_acc: 0.8519\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.8497\n",
      "Epoch 00038: val_loss did not improve from 0.53952\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5011 - acc: 0.8497 - val_loss: 0.5403 - val_acc: 0.8479\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4928 - acc: 0.8526\n",
      "Epoch 00039: val_loss did not improve from 0.53952\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4927 - acc: 0.8527 - val_loss: 0.5507 - val_acc: 0.8500\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4842 - acc: 0.8549\n",
      "Epoch 00040: val_loss improved from 0.53952 to 0.53622, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/040-0.5362.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4841 - acc: 0.8549 - val_loss: 0.5362 - val_acc: 0.8516\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4829 - acc: 0.8551\n",
      "Epoch 00041: val_loss improved from 0.53622 to 0.53462, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/041-0.5346.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4829 - acc: 0.8550 - val_loss: 0.5346 - val_acc: 0.8460\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4736 - acc: 0.8577\n",
      "Epoch 00042: val_loss did not improve from 0.53462\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4736 - acc: 0.8577 - val_loss: 0.5387 - val_acc: 0.8521\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4656 - acc: 0.8618\n",
      "Epoch 00043: val_loss improved from 0.53462 to 0.51759, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/043-0.5176.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4656 - acc: 0.8618 - val_loss: 0.5176 - val_acc: 0.8574\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.8624\n",
      "Epoch 00044: val_loss did not improve from 0.51759\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4585 - acc: 0.8623 - val_loss: 0.5245 - val_acc: 0.8542\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4471 - acc: 0.8635\n",
      "Epoch 00045: val_loss did not improve from 0.51759\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4470 - acc: 0.8636 - val_loss: 0.5328 - val_acc: 0.8516\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8650\n",
      "Epoch 00046: val_loss did not improve from 0.51759\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4482 - acc: 0.8650 - val_loss: 0.5268 - val_acc: 0.8509\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8662\n",
      "Epoch 00047: val_loss did not improve from 0.51759\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4418 - acc: 0.8662 - val_loss: 0.5251 - val_acc: 0.8537\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.8687\n",
      "Epoch 00048: val_loss improved from 0.51759 to 0.51530, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/048-0.5153.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4352 - acc: 0.8687 - val_loss: 0.5153 - val_acc: 0.8595\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8689\n",
      "Epoch 00049: val_loss did not improve from 0.51530\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4296 - acc: 0.8689 - val_loss: 0.5173 - val_acc: 0.8570\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.8723\n",
      "Epoch 00050: val_loss did not improve from 0.51530\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4266 - acc: 0.8723 - val_loss: 0.5164 - val_acc: 0.8584\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.8705\n",
      "Epoch 00051: val_loss improved from 0.51530 to 0.51361, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/051-0.5136.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4239 - acc: 0.8705 - val_loss: 0.5136 - val_acc: 0.8581\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4170 - acc: 0.8743\n",
      "Epoch 00052: val_loss did not improve from 0.51361\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4170 - acc: 0.8743 - val_loss: 0.5154 - val_acc: 0.8581\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8752\n",
      "Epoch 00053: val_loss did not improve from 0.51361\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4105 - acc: 0.8752 - val_loss: 0.5201 - val_acc: 0.8553\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8757\n",
      "Epoch 00054: val_loss improved from 0.51361 to 0.50162, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/054-0.5016.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4072 - acc: 0.8756 - val_loss: 0.5016 - val_acc: 0.8630\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8773\n",
      "Epoch 00055: val_loss did not improve from 0.50162\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4023 - acc: 0.8772 - val_loss: 0.5072 - val_acc: 0.8574\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8779\n",
      "Epoch 00056: val_loss did not improve from 0.50162\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4009 - acc: 0.8779 - val_loss: 0.5093 - val_acc: 0.8607\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8825\n",
      "Epoch 00057: val_loss did not improve from 0.50162\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3918 - acc: 0.8825 - val_loss: 0.5102 - val_acc: 0.8644\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3880 - acc: 0.8840\n",
      "Epoch 00058: val_loss did not improve from 0.50162\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3881 - acc: 0.8840 - val_loss: 0.5108 - val_acc: 0.8574\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.8860\n",
      "Epoch 00059: val_loss improved from 0.50162 to 0.50018, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/059-0.5002.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3808 - acc: 0.8860 - val_loss: 0.5002 - val_acc: 0.8668\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8843\n",
      "Epoch 00060: val_loss did not improve from 0.50018\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3787 - acc: 0.8843 - val_loss: 0.5095 - val_acc: 0.8593\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8849\n",
      "Epoch 00061: val_loss did not improve from 0.50018\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3769 - acc: 0.8850 - val_loss: 0.5132 - val_acc: 0.8581\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8868\n",
      "Epoch 00062: val_loss did not improve from 0.50018\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3719 - acc: 0.8868 - val_loss: 0.5172 - val_acc: 0.8647\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8870\n",
      "Epoch 00063: val_loss improved from 0.50018 to 0.49525, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/063-0.4953.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3676 - acc: 0.8869 - val_loss: 0.4953 - val_acc: 0.8707\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8882\n",
      "Epoch 00064: val_loss improved from 0.49525 to 0.49452, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/064-0.4945.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3636 - acc: 0.8881 - val_loss: 0.4945 - val_acc: 0.8684\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.8903\n",
      "Epoch 00065: val_loss did not improve from 0.49452\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3591 - acc: 0.8903 - val_loss: 0.5173 - val_acc: 0.8654\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8894\n",
      "Epoch 00066: val_loss improved from 0.49452 to 0.48702, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/066-0.4870.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3583 - acc: 0.8894 - val_loss: 0.4870 - val_acc: 0.8651\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.8904\n",
      "Epoch 00067: val_loss did not improve from 0.48702\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3554 - acc: 0.8904 - val_loss: 0.4917 - val_acc: 0.8696\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.8898\n",
      "Epoch 00068: val_loss did not improve from 0.48702\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3518 - acc: 0.8898 - val_loss: 0.4907 - val_acc: 0.8679\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8934\n",
      "Epoch 00069: val_loss did not improve from 0.48702\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3475 - acc: 0.8934 - val_loss: 0.5165 - val_acc: 0.8591\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8949\n",
      "Epoch 00070: val_loss did not improve from 0.48702\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3381 - acc: 0.8949 - val_loss: 0.5079 - val_acc: 0.8649\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8949\n",
      "Epoch 00071: val_loss did not improve from 0.48702\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3411 - acc: 0.8949 - val_loss: 0.5077 - val_acc: 0.8663\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3377 - acc: 0.8962\n",
      "Epoch 00072: val_loss improved from 0.48702 to 0.48330, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/072-0.4833.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3377 - acc: 0.8962 - val_loss: 0.4833 - val_acc: 0.8710\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8966\n",
      "Epoch 00073: val_loss did not improve from 0.48330\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3346 - acc: 0.8966 - val_loss: 0.4957 - val_acc: 0.8656\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8992\n",
      "Epoch 00074: val_loss did not improve from 0.48330\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3220 - acc: 0.8992 - val_loss: 0.5041 - val_acc: 0.8607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.9014\n",
      "Epoch 00075: val_loss did not improve from 0.48330\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3227 - acc: 0.9015 - val_loss: 0.5090 - val_acc: 0.8649\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8993\n",
      "Epoch 00076: val_loss improved from 0.48330 to 0.47325, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/076-0.4732.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3289 - acc: 0.8994 - val_loss: 0.4732 - val_acc: 0.8737\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.8996\n",
      "Epoch 00077: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3230 - acc: 0.8996 - val_loss: 0.4938 - val_acc: 0.8684\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8976\n",
      "Epoch 00078: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3391 - acc: 0.8976 - val_loss: 0.5092 - val_acc: 0.8693\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.9005\n",
      "Epoch 00079: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3156 - acc: 0.9005 - val_loss: 0.4804 - val_acc: 0.8721\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9048\n",
      "Epoch 00080: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3069 - acc: 0.9048 - val_loss: 0.4919 - val_acc: 0.8670\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9065\n",
      "Epoch 00081: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3031 - acc: 0.9065 - val_loss: 0.4799 - val_acc: 0.8737\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.9062\n",
      "Epoch 00082: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3053 - acc: 0.9063 - val_loss: 0.4906 - val_acc: 0.8744\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9037\n",
      "Epoch 00083: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3041 - acc: 0.9037 - val_loss: 0.4969 - val_acc: 0.8696\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9087\n",
      "Epoch 00084: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2956 - acc: 0.9087 - val_loss: 0.4791 - val_acc: 0.8712\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9057\n",
      "Epoch 00085: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2977 - acc: 0.9057 - val_loss: 0.5079 - val_acc: 0.8679\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.9085\n",
      "Epoch 00086: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2890 - acc: 0.9085 - val_loss: 0.4992 - val_acc: 0.8705\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9089\n",
      "Epoch 00087: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2917 - acc: 0.9089 - val_loss: 0.4921 - val_acc: 0.8717\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9085\n",
      "Epoch 00088: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2918 - acc: 0.9085 - val_loss: 0.5005 - val_acc: 0.8665\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9114\n",
      "Epoch 00089: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2835 - acc: 0.9114 - val_loss: 0.5192 - val_acc: 0.8703\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9099\n",
      "Epoch 00090: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2848 - acc: 0.9100 - val_loss: 0.5117 - val_acc: 0.8686\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9096\n",
      "Epoch 00091: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2860 - acc: 0.9097 - val_loss: 0.5002 - val_acc: 0.8703\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9093\n",
      "Epoch 00092: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2842 - acc: 0.9094 - val_loss: 0.4982 - val_acc: 0.8749\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9132\n",
      "Epoch 00093: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2740 - acc: 0.9131 - val_loss: 0.5206 - val_acc: 0.8656\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9124\n",
      "Epoch 00094: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2772 - acc: 0.9124 - val_loss: 0.4972 - val_acc: 0.8696\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9139\n",
      "Epoch 00095: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2684 - acc: 0.9140 - val_loss: 0.4773 - val_acc: 0.8754\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9157\n",
      "Epoch 00096: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2653 - acc: 0.9157 - val_loss: 0.5128 - val_acc: 0.8672\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9158\n",
      "Epoch 00097: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2649 - acc: 0.9158 - val_loss: 0.5089 - val_acc: 0.8728\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9168\n",
      "Epoch 00098: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2661 - acc: 0.9168 - val_loss: 0.5202 - val_acc: 0.8707\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9174\n",
      "Epoch 00099: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2621 - acc: 0.9174 - val_loss: 0.4768 - val_acc: 0.8763\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9173\n",
      "Epoch 00100: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2587 - acc: 0.9173 - val_loss: 0.4943 - val_acc: 0.8742\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9170\n",
      "Epoch 00101: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2613 - acc: 0.9170 - val_loss: 0.5000 - val_acc: 0.8775\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9192\n",
      "Epoch 00102: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2542 - acc: 0.9192 - val_loss: 0.4901 - val_acc: 0.8744\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.9190\n",
      "Epoch 00103: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2510 - acc: 0.9190 - val_loss: 0.5014 - val_acc: 0.8707\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9217\n",
      "Epoch 00104: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2446 - acc: 0.9217 - val_loss: 0.4818 - val_acc: 0.8765\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9225\n",
      "Epoch 00105: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2473 - acc: 0.9225 - val_loss: 0.4857 - val_acc: 0.8765\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9223\n",
      "Epoch 00106: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2461 - acc: 0.9223 - val_loss: 0.4936 - val_acc: 0.8763\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9208\n",
      "Epoch 00107: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2445 - acc: 0.9208 - val_loss: 0.4929 - val_acc: 0.8719\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9228\n",
      "Epoch 00108: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2412 - acc: 0.9228 - val_loss: 0.4892 - val_acc: 0.8754\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9245\n",
      "Epoch 00109: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2350 - acc: 0.9245 - val_loss: 0.4851 - val_acc: 0.8789\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9225\n",
      "Epoch 00110: val_loss did not improve from 0.47325\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2413 - acc: 0.9225 - val_loss: 0.4969 - val_acc: 0.8691\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9246\n",
      "Epoch 00111: val_loss improved from 0.47325 to 0.47298, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/111-0.4730.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2354 - acc: 0.9246 - val_loss: 0.4730 - val_acc: 0.8777\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9252\n",
      "Epoch 00112: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2367 - acc: 0.9252 - val_loss: 0.4988 - val_acc: 0.8754\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9251\n",
      "Epoch 00113: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2302 - acc: 0.9251 - val_loss: 0.4914 - val_acc: 0.8751\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9256\n",
      "Epoch 00114: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2308 - acc: 0.9256 - val_loss: 0.4772 - val_acc: 0.8770\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9264\n",
      "Epoch 00115: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2311 - acc: 0.9265 - val_loss: 0.5166 - val_acc: 0.8691\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9255\n",
      "Epoch 00116: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2285 - acc: 0.9255 - val_loss: 0.4873 - val_acc: 0.8749\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9295\n",
      "Epoch 00117: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2251 - acc: 0.9295 - val_loss: 0.4932 - val_acc: 0.8765\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9296\n",
      "Epoch 00118: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2233 - acc: 0.9297 - val_loss: 0.5064 - val_acc: 0.8754\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9296\n",
      "Epoch 00119: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2216 - acc: 0.9296 - val_loss: 0.4735 - val_acc: 0.8819\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9271\n",
      "Epoch 00120: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2250 - acc: 0.9271 - val_loss: 0.5010 - val_acc: 0.8772\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9295\n",
      "Epoch 00121: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2216 - acc: 0.9295 - val_loss: 0.4926 - val_acc: 0.8744\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9261\n",
      "Epoch 00122: val_loss did not improve from 0.47298\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2249 - acc: 0.9261 - val_loss: 0.5087 - val_acc: 0.8758\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9289\n",
      "Epoch 00123: val_loss improved from 0.47298 to 0.47278, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_6_conv_checkpoint/123-0.4728.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2166 - acc: 0.9289 - val_loss: 0.4728 - val_acc: 0.8784\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9320\n",
      "Epoch 00124: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2136 - acc: 0.9319 - val_loss: 0.4989 - val_acc: 0.8756\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9328\n",
      "Epoch 00125: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2135 - acc: 0.9328 - val_loss: 0.4904 - val_acc: 0.8777\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9329\n",
      "Epoch 00126: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2075 - acc: 0.9329 - val_loss: 0.4941 - val_acc: 0.8765\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9340\n",
      "Epoch 00127: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2041 - acc: 0.9340 - val_loss: 0.4836 - val_acc: 0.8791\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9312\n",
      "Epoch 00128: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2086 - acc: 0.9312 - val_loss: 0.4844 - val_acc: 0.8798\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9330\n",
      "Epoch 00129: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2069 - acc: 0.9330 - val_loss: 0.4949 - val_acc: 0.8784\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9340\n",
      "Epoch 00130: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2074 - acc: 0.9340 - val_loss: 0.5065 - val_acc: 0.8793\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9348\n",
      "Epoch 00131: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2006 - acc: 0.9347 - val_loss: 0.4905 - val_acc: 0.8814\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9324\n",
      "Epoch 00132: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2053 - acc: 0.9324 - val_loss: 0.4799 - val_acc: 0.8796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9359\n",
      "Epoch 00133: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1972 - acc: 0.9359 - val_loss: 0.4826 - val_acc: 0.8775\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9329\n",
      "Epoch 00134: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2031 - acc: 0.9329 - val_loss: 0.4785 - val_acc: 0.8791\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9353\n",
      "Epoch 00135: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1988 - acc: 0.9353 - val_loss: 0.4883 - val_acc: 0.8779\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9354\n",
      "Epoch 00136: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1981 - acc: 0.9354 - val_loss: 0.5193 - val_acc: 0.8761\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9352\n",
      "Epoch 00137: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2006 - acc: 0.9351 - val_loss: 0.4914 - val_acc: 0.8770\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9350\n",
      "Epoch 00138: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2030 - acc: 0.9350 - val_loss: 0.5031 - val_acc: 0.8763\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9371\n",
      "Epoch 00139: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1927 - acc: 0.9372 - val_loss: 0.4898 - val_acc: 0.8814\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9372\n",
      "Epoch 00140: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1947 - acc: 0.9372 - val_loss: 0.4927 - val_acc: 0.8814\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9382\n",
      "Epoch 00141: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1894 - acc: 0.9382 - val_loss: 0.5093 - val_acc: 0.8754\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9393\n",
      "Epoch 00142: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1866 - acc: 0.9394 - val_loss: 0.4954 - val_acc: 0.8784\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9390\n",
      "Epoch 00143: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1859 - acc: 0.9390 - val_loss: 0.4999 - val_acc: 0.8784\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9408\n",
      "Epoch 00144: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1827 - acc: 0.9408 - val_loss: 0.5107 - val_acc: 0.8730\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9415\n",
      "Epoch 00145: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1783 - acc: 0.9415 - val_loss: 0.5074 - val_acc: 0.8805\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9404\n",
      "Epoch 00146: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1826 - acc: 0.9404 - val_loss: 0.5049 - val_acc: 0.8803\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9398\n",
      "Epoch 00147: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1872 - acc: 0.9398 - val_loss: 0.5160 - val_acc: 0.8754\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9388\n",
      "Epoch 00148: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1821 - acc: 0.9388 - val_loss: 0.5060 - val_acc: 0.8779\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9430\n",
      "Epoch 00149: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1774 - acc: 0.9430 - val_loss: 0.4785 - val_acc: 0.8840\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9428\n",
      "Epoch 00150: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1746 - acc: 0.9428 - val_loss: 0.5093 - val_acc: 0.8710\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9421\n",
      "Epoch 00151: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1813 - acc: 0.9422 - val_loss: 0.4944 - val_acc: 0.8777\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9450\n",
      "Epoch 00152: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1706 - acc: 0.9450 - val_loss: 0.5096 - val_acc: 0.8758\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9419\n",
      "Epoch 00153: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1796 - acc: 0.9419 - val_loss: 0.5085 - val_acc: 0.8793\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9438\n",
      "Epoch 00154: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1742 - acc: 0.9438 - val_loss: 0.5072 - val_acc: 0.8779\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9432\n",
      "Epoch 00155: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1748 - acc: 0.9432 - val_loss: 0.5109 - val_acc: 0.8758\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9430\n",
      "Epoch 00156: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1753 - acc: 0.9430 - val_loss: 0.5632 - val_acc: 0.8698\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9448\n",
      "Epoch 00157: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1709 - acc: 0.9448 - val_loss: 0.4964 - val_acc: 0.8838\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9447\n",
      "Epoch 00158: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1707 - acc: 0.9447 - val_loss: 0.5168 - val_acc: 0.8779\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9434\n",
      "Epoch 00159: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1716 - acc: 0.9434 - val_loss: 0.4982 - val_acc: 0.8810\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9455\n",
      "Epoch 00160: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1657 - acc: 0.9455 - val_loss: 0.5176 - val_acc: 0.8833\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9447\n",
      "Epoch 00161: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1665 - acc: 0.9447 - val_loss: 0.5218 - val_acc: 0.8744\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9451\n",
      "Epoch 00162: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1658 - acc: 0.9451 - val_loss: 0.5175 - val_acc: 0.8807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9474\n",
      "Epoch 00163: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1658 - acc: 0.9473 - val_loss: 0.5099 - val_acc: 0.8779\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9460\n",
      "Epoch 00164: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1657 - acc: 0.9459 - val_loss: 0.4978 - val_acc: 0.8784\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9452\n",
      "Epoch 00165: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1643 - acc: 0.9453 - val_loss: 0.5134 - val_acc: 0.8768\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9495\n",
      "Epoch 00166: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1562 - acc: 0.9495 - val_loss: 0.5366 - val_acc: 0.8730\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9462\n",
      "Epoch 00167: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1646 - acc: 0.9462 - val_loss: 0.5201 - val_acc: 0.8826\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9493\n",
      "Epoch 00168: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1569 - acc: 0.9493 - val_loss: 0.5094 - val_acc: 0.8784\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9486\n",
      "Epoch 00169: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1577 - acc: 0.9486 - val_loss: 0.5246 - val_acc: 0.8758\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9473\n",
      "Epoch 00170: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1574 - acc: 0.9473 - val_loss: 0.5308 - val_acc: 0.8779\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9480\n",
      "Epoch 00171: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1625 - acc: 0.9480 - val_loss: 0.5091 - val_acc: 0.8817\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9485\n",
      "Epoch 00172: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1593 - acc: 0.9485 - val_loss: 0.5017 - val_acc: 0.8838\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9496\n",
      "Epoch 00173: val_loss did not improve from 0.47278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1545 - acc: 0.9497 - val_loss: 0.5473 - val_acc: 0.8714\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 606us/sample - loss: 0.5956 - acc: 0.8417\n",
      "Loss: 0.5955511114679022 Accuracy: 0.84174454\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1475 - acc: 0.2919\n",
      "Epoch 00001: val_loss improved from inf to 1.56204, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/001-1.5620.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 2.1474 - acc: 0.2919 - val_loss: 1.5620 - val_acc: 0.5195\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6659 - acc: 0.4546\n",
      "Epoch 00002: val_loss improved from 1.56204 to 1.35163, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/002-1.3516.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.6658 - acc: 0.4547 - val_loss: 1.3516 - val_acc: 0.5940\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4786 - acc: 0.5281\n",
      "Epoch 00003: val_loss improved from 1.35163 to 1.16599, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/003-1.1660.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.4786 - acc: 0.5281 - val_loss: 1.1660 - val_acc: 0.6592\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3220 - acc: 0.5857\n",
      "Epoch 00004: val_loss improved from 1.16599 to 1.06684, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/004-1.0668.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.3220 - acc: 0.5857 - val_loss: 1.0668 - val_acc: 0.6939\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1915 - acc: 0.6299\n",
      "Epoch 00005: val_loss improved from 1.06684 to 0.96627, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/005-0.9663.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.1915 - acc: 0.6299 - val_loss: 0.9663 - val_acc: 0.7258\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1012 - acc: 0.6625\n",
      "Epoch 00006: val_loss improved from 0.96627 to 0.89153, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/006-0.8915.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.1011 - acc: 0.6625 - val_loss: 0.8915 - val_acc: 0.7400\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0146 - acc: 0.6895\n",
      "Epoch 00007: val_loss improved from 0.89153 to 0.81353, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/007-0.8135.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.0145 - acc: 0.6895 - val_loss: 0.8135 - val_acc: 0.7701\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9427 - acc: 0.7121\n",
      "Epoch 00008: val_loss improved from 0.81353 to 0.80160, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/008-0.8016.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9430 - acc: 0.7121 - val_loss: 0.8016 - val_acc: 0.7717\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8877 - acc: 0.7334\n",
      "Epoch 00009: val_loss improved from 0.80160 to 0.70737, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/009-0.7074.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8878 - acc: 0.7334 - val_loss: 0.7074 - val_acc: 0.8029\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8287 - acc: 0.7530\n",
      "Epoch 00010: val_loss improved from 0.70737 to 0.67054, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/010-0.6705.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8287 - acc: 0.7530 - val_loss: 0.6705 - val_acc: 0.8130\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7749 - acc: 0.7692\n",
      "Epoch 00011: val_loss improved from 0.67054 to 0.61229, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/011-0.6123.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7751 - acc: 0.7692 - val_loss: 0.6123 - val_acc: 0.8258\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7402 - acc: 0.7806\n",
      "Epoch 00012: val_loss improved from 0.61229 to 0.59892, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/012-0.5989.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7403 - acc: 0.7805 - val_loss: 0.5989 - val_acc: 0.8311\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7008 - acc: 0.7911\n",
      "Epoch 00013: val_loss improved from 0.59892 to 0.56523, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/013-0.5652.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7008 - acc: 0.7911 - val_loss: 0.5652 - val_acc: 0.8453\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6713 - acc: 0.8053\n",
      "Epoch 00014: val_loss improved from 0.56523 to 0.54709, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/014-0.5471.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6713 - acc: 0.8053 - val_loss: 0.5471 - val_acc: 0.8498\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6438 - acc: 0.8109\n",
      "Epoch 00015: val_loss improved from 0.54709 to 0.51890, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/015-0.5189.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6438 - acc: 0.8109 - val_loss: 0.5189 - val_acc: 0.8574\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6194 - acc: 0.8161\n",
      "Epoch 00016: val_loss improved from 0.51890 to 0.49487, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/016-0.4949.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6195 - acc: 0.8161 - val_loss: 0.4949 - val_acc: 0.8637\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5979 - acc: 0.8233\n",
      "Epoch 00017: val_loss improved from 0.49487 to 0.47072, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/017-0.4707.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5978 - acc: 0.8234 - val_loss: 0.4707 - val_acc: 0.8747\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8342\n",
      "Epoch 00018: val_loss improved from 0.47072 to 0.44948, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/018-0.4495.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5732 - acc: 0.8342 - val_loss: 0.4495 - val_acc: 0.8770\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5510 - acc: 0.8380\n",
      "Epoch 00019: val_loss improved from 0.44948 to 0.43608, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/019-0.4361.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5509 - acc: 0.8380 - val_loss: 0.4361 - val_acc: 0.8831\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5362 - acc: 0.8421\n",
      "Epoch 00020: val_loss did not improve from 0.43608\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5362 - acc: 0.8421 - val_loss: 0.4408 - val_acc: 0.8796\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5192 - acc: 0.8491\n",
      "Epoch 00021: val_loss improved from 0.43608 to 0.40876, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/021-0.4088.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5192 - acc: 0.8491 - val_loss: 0.4088 - val_acc: 0.8852\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.8544\n",
      "Epoch 00022: val_loss improved from 0.40876 to 0.40708, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/022-0.4071.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4986 - acc: 0.8544 - val_loss: 0.4071 - val_acc: 0.8880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4834 - acc: 0.8592\n",
      "Epoch 00023: val_loss improved from 0.40708 to 0.39637, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/023-0.3964.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4834 - acc: 0.8592 - val_loss: 0.3964 - val_acc: 0.8910\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4718 - acc: 0.8628\n",
      "Epoch 00024: val_loss improved from 0.39637 to 0.37405, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/024-0.3740.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4718 - acc: 0.8628 - val_loss: 0.3740 - val_acc: 0.8956\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.8639\n",
      "Epoch 00025: val_loss improved from 0.37405 to 0.36694, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/025-0.3669.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4608 - acc: 0.8639 - val_loss: 0.3669 - val_acc: 0.9022\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4424 - acc: 0.8706\n",
      "Epoch 00026: val_loss improved from 0.36694 to 0.36239, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/026-0.3624.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4424 - acc: 0.8706 - val_loss: 0.3624 - val_acc: 0.9015\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.8740\n",
      "Epoch 00027: val_loss improved from 0.36239 to 0.35278, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/027-0.3528.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4315 - acc: 0.8740 - val_loss: 0.3528 - val_acc: 0.9012\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.8776\n",
      "Epoch 00028: val_loss did not improve from 0.35278\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4229 - acc: 0.8775 - val_loss: 0.3578 - val_acc: 0.9029\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8770\n",
      "Epoch 00029: val_loss improved from 0.35278 to 0.33878, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/029-0.3388.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4181 - acc: 0.8770 - val_loss: 0.3388 - val_acc: 0.9075\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8812\n",
      "Epoch 00030: val_loss did not improve from 0.33878\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4064 - acc: 0.8812 - val_loss: 0.3431 - val_acc: 0.8991\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8847\n",
      "Epoch 00031: val_loss did not improve from 0.33878\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3978 - acc: 0.8847 - val_loss: 0.3431 - val_acc: 0.9064\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8874\n",
      "Epoch 00032: val_loss improved from 0.33878 to 0.32687, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/032-0.3269.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3836 - acc: 0.8874 - val_loss: 0.3269 - val_acc: 0.9108\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8887\n",
      "Epoch 00033: val_loss did not improve from 0.32687\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3817 - acc: 0.8887 - val_loss: 0.3316 - val_acc: 0.9110\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3722 - acc: 0.8910\n",
      "Epoch 00034: val_loss improved from 0.32687 to 0.32609, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/034-0.3261.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3723 - acc: 0.8910 - val_loss: 0.3261 - val_acc: 0.9140\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8948\n",
      "Epoch 00035: val_loss improved from 0.32609 to 0.31982, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/035-0.3198.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3647 - acc: 0.8948 - val_loss: 0.3198 - val_acc: 0.9131\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8957\n",
      "Epoch 00036: val_loss improved from 0.31982 to 0.30451, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/036-0.3045.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3570 - acc: 0.8957 - val_loss: 0.3045 - val_acc: 0.9199\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.8961\n",
      "Epoch 00037: val_loss did not improve from 0.30451\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3517 - acc: 0.8961 - val_loss: 0.3078 - val_acc: 0.9154\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8997\n",
      "Epoch 00038: val_loss did not improve from 0.30451\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3434 - acc: 0.8997 - val_loss: 0.3065 - val_acc: 0.9161\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8989\n",
      "Epoch 00039: val_loss did not improve from 0.30451\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3407 - acc: 0.8988 - val_loss: 0.3066 - val_acc: 0.9164\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8987\n",
      "Epoch 00040: val_loss improved from 0.30451 to 0.30236, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/040-0.3024.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3404 - acc: 0.8987 - val_loss: 0.3024 - val_acc: 0.9185\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.9041\n",
      "Epoch 00041: val_loss did not improve from 0.30236\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3246 - acc: 0.9041 - val_loss: 0.3033 - val_acc: 0.9215\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3210 - acc: 0.9063\n",
      "Epoch 00042: val_loss did not improve from 0.30236\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3210 - acc: 0.9063 - val_loss: 0.3053 - val_acc: 0.9215\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.9074\n",
      "Epoch 00043: val_loss improved from 0.30236 to 0.28843, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/043-0.2884.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3151 - acc: 0.9074 - val_loss: 0.2884 - val_acc: 0.9234\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.9065\n",
      "Epoch 00044: val_loss did not improve from 0.28843\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3116 - acc: 0.9066 - val_loss: 0.3205 - val_acc: 0.9159\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9110\n",
      "Epoch 00045: val_loss did not improve from 0.28843\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3015 - acc: 0.9110 - val_loss: 0.2920 - val_acc: 0.9208\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9111\n",
      "Epoch 00046: val_loss improved from 0.28843 to 0.28616, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/046-0.2862.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3014 - acc: 0.9111 - val_loss: 0.2862 - val_acc: 0.9199\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9124\n",
      "Epoch 00047: val_loss did not improve from 0.28616\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2938 - acc: 0.9125 - val_loss: 0.2923 - val_acc: 0.9208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9145\n",
      "Epoch 00048: val_loss did not improve from 0.28616\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2894 - acc: 0.9145 - val_loss: 0.2936 - val_acc: 0.9182\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9142\n",
      "Epoch 00049: val_loss improved from 0.28616 to 0.27209, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/049-0.2721.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2871 - acc: 0.9141 - val_loss: 0.2721 - val_acc: 0.9266\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.9157\n",
      "Epoch 00050: val_loss did not improve from 0.27209\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2864 - acc: 0.9157 - val_loss: 0.2931 - val_acc: 0.9178\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9174\n",
      "Epoch 00051: val_loss did not improve from 0.27209\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2782 - acc: 0.9174 - val_loss: 0.2777 - val_acc: 0.9259\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9196\n",
      "Epoch 00052: val_loss did not improve from 0.27209\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2719 - acc: 0.9196 - val_loss: 0.2946 - val_acc: 0.9213\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9202\n",
      "Epoch 00053: val_loss did not improve from 0.27209\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2680 - acc: 0.9202 - val_loss: 0.2812 - val_acc: 0.9245\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9205\n",
      "Epoch 00054: val_loss did not improve from 0.27209\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2660 - acc: 0.9205 - val_loss: 0.2825 - val_acc: 0.9245\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9227\n",
      "Epoch 00055: val_loss did not improve from 0.27209\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2613 - acc: 0.9227 - val_loss: 0.2773 - val_acc: 0.9250\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9250\n",
      "Epoch 00056: val_loss improved from 0.27209 to 0.26909, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/056-0.2691.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2565 - acc: 0.9250 - val_loss: 0.2691 - val_acc: 0.9255\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9227\n",
      "Epoch 00057: val_loss did not improve from 0.26909\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2553 - acc: 0.9227 - val_loss: 0.2743 - val_acc: 0.9283\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9248\n",
      "Epoch 00058: val_loss improved from 0.26909 to 0.26403, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/058-0.2640.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2525 - acc: 0.9248 - val_loss: 0.2640 - val_acc: 0.9315\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9248\n",
      "Epoch 00059: val_loss did not improve from 0.26403\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2492 - acc: 0.9248 - val_loss: 0.2856 - val_acc: 0.9255\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9260\n",
      "Epoch 00060: val_loss did not improve from 0.26403\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2464 - acc: 0.9260 - val_loss: 0.2663 - val_acc: 0.9299\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9282\n",
      "Epoch 00061: val_loss improved from 0.26403 to 0.26313, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/061-0.2631.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2411 - acc: 0.9282 - val_loss: 0.2631 - val_acc: 0.9280\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9282\n",
      "Epoch 00062: val_loss did not improve from 0.26313\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2363 - acc: 0.9282 - val_loss: 0.2758 - val_acc: 0.9255\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9281\n",
      "Epoch 00063: val_loss did not improve from 0.26313\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2391 - acc: 0.9281 - val_loss: 0.2804 - val_acc: 0.9304\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9289\n",
      "Epoch 00064: val_loss did not improve from 0.26313\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2322 - acc: 0.9289 - val_loss: 0.2870 - val_acc: 0.9278\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9326\n",
      "Epoch 00065: val_loss did not improve from 0.26313\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2246 - acc: 0.9326 - val_loss: 0.2892 - val_acc: 0.9276\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9326\n",
      "Epoch 00066: val_loss did not improve from 0.26313\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2216 - acc: 0.9326 - val_loss: 0.2755 - val_acc: 0.9276\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9328\n",
      "Epoch 00067: val_loss did not improve from 0.26313\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2230 - acc: 0.9328 - val_loss: 0.2822 - val_acc: 0.9271\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9335\n",
      "Epoch 00068: val_loss did not improve from 0.26313\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2173 - acc: 0.9335 - val_loss: 0.2658 - val_acc: 0.9280\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9332\n",
      "Epoch 00069: val_loss did not improve from 0.26313\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2196 - acc: 0.9332 - val_loss: 0.2767 - val_acc: 0.9280\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9362\n",
      "Epoch 00070: val_loss did not improve from 0.26313\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2127 - acc: 0.9362 - val_loss: 0.2651 - val_acc: 0.9269\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9365\n",
      "Epoch 00071: val_loss improved from 0.26313 to 0.25393, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/071-0.2539.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2068 - acc: 0.9365 - val_loss: 0.2539 - val_acc: 0.9329\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9361\n",
      "Epoch 00072: val_loss did not improve from 0.25393\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2130 - acc: 0.9361 - val_loss: 0.2598 - val_acc: 0.9362\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9376\n",
      "Epoch 00073: val_loss improved from 0.25393 to 0.25184, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/073-0.2518.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2025 - acc: 0.9376 - val_loss: 0.2518 - val_acc: 0.9336\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9369\n",
      "Epoch 00074: val_loss did not improve from 0.25184\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2065 - acc: 0.9369 - val_loss: 0.2638 - val_acc: 0.9352\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9364\n",
      "Epoch 00075: val_loss did not improve from 0.25184\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2041 - acc: 0.9364 - val_loss: 0.2641 - val_acc: 0.9336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9394\n",
      "Epoch 00076: val_loss did not improve from 0.25184\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1991 - acc: 0.9394 - val_loss: 0.2536 - val_acc: 0.9331\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9400\n",
      "Epoch 00077: val_loss did not improve from 0.25184\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1965 - acc: 0.9400 - val_loss: 0.2710 - val_acc: 0.9322\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9424\n",
      "Epoch 00078: val_loss did not improve from 0.25184\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1907 - acc: 0.9424 - val_loss: 0.2711 - val_acc: 0.9294\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9421\n",
      "Epoch 00079: val_loss improved from 0.25184 to 0.24460, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/079-0.2446.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1890 - acc: 0.9421 - val_loss: 0.2446 - val_acc: 0.9324\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9423\n",
      "Epoch 00080: val_loss did not improve from 0.24460\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1888 - acc: 0.9423 - val_loss: 0.2653 - val_acc: 0.9320\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9424\n",
      "Epoch 00081: val_loss improved from 0.24460 to 0.24316, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/081-0.2432.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1880 - acc: 0.9424 - val_loss: 0.2432 - val_acc: 0.9348\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9468\n",
      "Epoch 00082: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1798 - acc: 0.9468 - val_loss: 0.2629 - val_acc: 0.9334\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9446\n",
      "Epoch 00083: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1810 - acc: 0.9446 - val_loss: 0.2659 - val_acc: 0.9287\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9441\n",
      "Epoch 00084: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1814 - acc: 0.9441 - val_loss: 0.2692 - val_acc: 0.9306\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9451\n",
      "Epoch 00085: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1742 - acc: 0.9451 - val_loss: 0.2453 - val_acc: 0.9364\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9459\n",
      "Epoch 00086: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1772 - acc: 0.9459 - val_loss: 0.2468 - val_acc: 0.9359\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9474\n",
      "Epoch 00087: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1737 - acc: 0.9474 - val_loss: 0.2582 - val_acc: 0.9357\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9477\n",
      "Epoch 00088: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1692 - acc: 0.9478 - val_loss: 0.2558 - val_acc: 0.9317\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9493\n",
      "Epoch 00089: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1646 - acc: 0.9493 - val_loss: 0.2466 - val_acc: 0.9373\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9498\n",
      "Epoch 00090: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1644 - acc: 0.9498 - val_loss: 0.2490 - val_acc: 0.9366\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9502\n",
      "Epoch 00091: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1641 - acc: 0.9502 - val_loss: 0.2499 - val_acc: 0.9336\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9511\n",
      "Epoch 00092: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1573 - acc: 0.9511 - val_loss: 0.2647 - val_acc: 0.9320\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9511\n",
      "Epoch 00093: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1564 - acc: 0.9511 - val_loss: 0.2816 - val_acc: 0.9262\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9520\n",
      "Epoch 00094: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1569 - acc: 0.9520 - val_loss: 0.2574 - val_acc: 0.9301\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9514\n",
      "Epoch 00095: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1566 - acc: 0.9514 - val_loss: 0.2438 - val_acc: 0.9359\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9536\n",
      "Epoch 00096: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1518 - acc: 0.9536 - val_loss: 0.2751 - val_acc: 0.9348\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9505\n",
      "Epoch 00097: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1591 - acc: 0.9505 - val_loss: 0.2637 - val_acc: 0.9338\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9527\n",
      "Epoch 00098: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1524 - acc: 0.9527 - val_loss: 0.2590 - val_acc: 0.9357\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9541\n",
      "Epoch 00099: val_loss did not improve from 0.24316\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1468 - acc: 0.9541 - val_loss: 0.2769 - val_acc: 0.9338\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9550\n",
      "Epoch 00100: val_loss improved from 0.24316 to 0.24271, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/100-0.2427.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1457 - acc: 0.9550 - val_loss: 0.2427 - val_acc: 0.9397\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9564\n",
      "Epoch 00101: val_loss did not improve from 0.24271\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1444 - acc: 0.9563 - val_loss: 0.2454 - val_acc: 0.9376\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9550\n",
      "Epoch 00102: val_loss did not improve from 0.24271\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1443 - acc: 0.9550 - val_loss: 0.2553 - val_acc: 0.9355\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9577\n",
      "Epoch 00103: val_loss did not improve from 0.24271\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1376 - acc: 0.9577 - val_loss: 0.2654 - val_acc: 0.9345\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9569\n",
      "Epoch 00104: val_loss did not improve from 0.24271\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1379 - acc: 0.9569 - val_loss: 0.2801 - val_acc: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9561\n",
      "Epoch 00105: val_loss did not improve from 0.24271\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1413 - acc: 0.9561 - val_loss: 0.2585 - val_acc: 0.9369\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9592\n",
      "Epoch 00106: val_loss did not improve from 0.24271\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1316 - acc: 0.9592 - val_loss: 0.2719 - val_acc: 0.9355\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9585\n",
      "Epoch 00107: val_loss did not improve from 0.24271\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1337 - acc: 0.9585 - val_loss: 0.2479 - val_acc: 0.9364\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9586\n",
      "Epoch 00108: val_loss did not improve from 0.24271\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1342 - acc: 0.9586 - val_loss: 0.2709 - val_acc: 0.9322\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9594\n",
      "Epoch 00109: val_loss did not improve from 0.24271\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1312 - acc: 0.9594 - val_loss: 0.2593 - val_acc: 0.9390\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9594\n",
      "Epoch 00110: val_loss did not improve from 0.24271\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1287 - acc: 0.9594 - val_loss: 0.2673 - val_acc: 0.9352\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9605\n",
      "Epoch 00111: val_loss improved from 0.24271 to 0.24118, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_7_conv_checkpoint/111-0.2412.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1277 - acc: 0.9605 - val_loss: 0.2412 - val_acc: 0.9406\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9596\n",
      "Epoch 00112: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1281 - acc: 0.9596 - val_loss: 0.2466 - val_acc: 0.9390\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9613\n",
      "Epoch 00113: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1238 - acc: 0.9613 - val_loss: 0.2793 - val_acc: 0.9331\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9620\n",
      "Epoch 00114: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1212 - acc: 0.9619 - val_loss: 0.2754 - val_acc: 0.9331\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9615\n",
      "Epoch 00115: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1246 - acc: 0.9615 - val_loss: 0.2506 - val_acc: 0.9404\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9613\n",
      "Epoch 00116: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1219 - acc: 0.9613 - val_loss: 0.2631 - val_acc: 0.9364\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9611\n",
      "Epoch 00117: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1220 - acc: 0.9611 - val_loss: 0.2929 - val_acc: 0.9338\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9626\n",
      "Epoch 00118: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1172 - acc: 0.9626 - val_loss: 0.2644 - val_acc: 0.9359\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9617\n",
      "Epoch 00119: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1186 - acc: 0.9617 - val_loss: 0.2513 - val_acc: 0.9390\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9645\n",
      "Epoch 00120: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1148 - acc: 0.9645 - val_loss: 0.2903 - val_acc: 0.9304\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9644\n",
      "Epoch 00121: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1126 - acc: 0.9644 - val_loss: 0.2854 - val_acc: 0.9311\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9646\n",
      "Epoch 00122: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1153 - acc: 0.9647 - val_loss: 0.2728 - val_acc: 0.9338\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9656\n",
      "Epoch 00123: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1111 - acc: 0.9656 - val_loss: 0.2648 - val_acc: 0.9392\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9668\n",
      "Epoch 00124: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1076 - acc: 0.9668 - val_loss: 0.2513 - val_acc: 0.9415\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9657\n",
      "Epoch 00125: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1068 - acc: 0.9657 - val_loss: 0.2603 - val_acc: 0.9373\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9668\n",
      "Epoch 00126: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1092 - acc: 0.9668 - val_loss: 0.2550 - val_acc: 0.9371\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9653\n",
      "Epoch 00127: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1096 - acc: 0.9653 - val_loss: 0.2779 - val_acc: 0.9362\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9686\n",
      "Epoch 00128: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1015 - acc: 0.9686 - val_loss: 0.2856 - val_acc: 0.9334\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9674\n",
      "Epoch 00129: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1020 - acc: 0.9674 - val_loss: 0.2885 - val_acc: 0.9320\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9685\n",
      "Epoch 00130: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0981 - acc: 0.9685 - val_loss: 0.2689 - val_acc: 0.9404\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9679\n",
      "Epoch 00131: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1009 - acc: 0.9679 - val_loss: 0.3013 - val_acc: 0.9345\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9693\n",
      "Epoch 00132: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0978 - acc: 0.9693 - val_loss: 0.2817 - val_acc: 0.9357\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9699\n",
      "Epoch 00133: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0980 - acc: 0.9699 - val_loss: 0.2784 - val_acc: 0.9394\n",
      "Epoch 134/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9697\n",
      "Epoch 00134: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0979 - acc: 0.9697 - val_loss: 0.2861 - val_acc: 0.9366\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9693\n",
      "Epoch 00135: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0972 - acc: 0.9694 - val_loss: 0.2814 - val_acc: 0.9369\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9679\n",
      "Epoch 00136: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0982 - acc: 0.9679 - val_loss: 0.2658 - val_acc: 0.9371\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9714\n",
      "Epoch 00137: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0932 - acc: 0.9714 - val_loss: 0.2777 - val_acc: 0.9371\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9704\n",
      "Epoch 00138: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0952 - acc: 0.9703 - val_loss: 0.2529 - val_acc: 0.9408\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9699\n",
      "Epoch 00139: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0964 - acc: 0.9699 - val_loss: 0.2691 - val_acc: 0.9369\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9728\n",
      "Epoch 00140: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0902 - acc: 0.9728 - val_loss: 0.2712 - val_acc: 0.9371\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9714\n",
      "Epoch 00141: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0895 - acc: 0.9714 - val_loss: 0.2804 - val_acc: 0.9376\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9708\n",
      "Epoch 00142: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0896 - acc: 0.9708 - val_loss: 0.2744 - val_acc: 0.9422\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9707\n",
      "Epoch 00143: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0915 - acc: 0.9707 - val_loss: 0.2697 - val_acc: 0.9429\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9751\n",
      "Epoch 00144: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0804 - acc: 0.9751 - val_loss: 0.2864 - val_acc: 0.9357\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9705\n",
      "Epoch 00145: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0959 - acc: 0.9705 - val_loss: 0.2749 - val_acc: 0.9380\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9734\n",
      "Epoch 00146: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0853 - acc: 0.9734 - val_loss: 0.2621 - val_acc: 0.9408\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9738\n",
      "Epoch 00147: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0813 - acc: 0.9738 - val_loss: 0.2662 - val_acc: 0.9399\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9734\n",
      "Epoch 00148: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0828 - acc: 0.9734 - val_loss: 0.2739 - val_acc: 0.9359\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9728\n",
      "Epoch 00149: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0865 - acc: 0.9728 - val_loss: 0.2589 - val_acc: 0.9383\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9745\n",
      "Epoch 00150: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0819 - acc: 0.9745 - val_loss: 0.2857 - val_acc: 0.9338\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9742\n",
      "Epoch 00151: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0811 - acc: 0.9742 - val_loss: 0.2950 - val_acc: 0.9392\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9758\n",
      "Epoch 00152: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0788 - acc: 0.9758 - val_loss: 0.3026 - val_acc: 0.9341\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9746\n",
      "Epoch 00153: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0793 - acc: 0.9746 - val_loss: 0.2865 - val_acc: 0.9387\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9768\n",
      "Epoch 00154: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0755 - acc: 0.9767 - val_loss: 0.2943 - val_acc: 0.9355\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9764\n",
      "Epoch 00155: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0752 - acc: 0.9764 - val_loss: 0.2797 - val_acc: 0.9359\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9754\n",
      "Epoch 00156: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0800 - acc: 0.9754 - val_loss: 0.2765 - val_acc: 0.9380\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9759\n",
      "Epoch 00157: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0751 - acc: 0.9759 - val_loss: 0.2766 - val_acc: 0.9387\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9772\n",
      "Epoch 00158: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0732 - acc: 0.9772 - val_loss: 0.2890 - val_acc: 0.9387\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9764\n",
      "Epoch 00159: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0764 - acc: 0.9764 - val_loss: 0.2705 - val_acc: 0.9408\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9773\n",
      "Epoch 00160: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0725 - acc: 0.9773 - val_loss: 0.2623 - val_acc: 0.9432\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9778\n",
      "Epoch 00161: val_loss did not improve from 0.24118\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0725 - acc: 0.9778 - val_loss: 0.2763 - val_acc: 0.9329\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 637us/sample - loss: 0.3216 - acc: 0.9111\n",
      "Loss: 0.32164603567847583 Accuracy: 0.9111111\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1893 - acc: 0.2773\n",
      "Epoch 00001: val_loss improved from inf to 1.56677, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/001-1.5668.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 2.1893 - acc: 0.2773 - val_loss: 1.5668 - val_acc: 0.5211\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6603 - acc: 0.4605\n",
      "Epoch 00002: val_loss improved from 1.56677 to 1.23150, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/002-1.2315.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.6601 - acc: 0.4606 - val_loss: 1.2315 - val_acc: 0.6513\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3749 - acc: 0.5660\n",
      "Epoch 00003: val_loss improved from 1.23150 to 1.01548, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/003-1.0155.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.3750 - acc: 0.5659 - val_loss: 1.0155 - val_acc: 0.7079\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1551 - acc: 0.6452\n",
      "Epoch 00004: val_loss improved from 1.01548 to 0.81164, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/004-0.8116.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1551 - acc: 0.6452 - val_loss: 0.8116 - val_acc: 0.7794\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9800 - acc: 0.7044\n",
      "Epoch 00005: val_loss improved from 0.81164 to 0.67604, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/005-0.6760.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9800 - acc: 0.7044 - val_loss: 0.6760 - val_acc: 0.8169\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8437 - acc: 0.7485\n",
      "Epoch 00006: val_loss improved from 0.67604 to 0.55643, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/006-0.5564.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8437 - acc: 0.7485 - val_loss: 0.5564 - val_acc: 0.8505\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7334 - acc: 0.7866\n",
      "Epoch 00007: val_loss improved from 0.55643 to 0.48490, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/007-0.4849.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7335 - acc: 0.7866 - val_loss: 0.4849 - val_acc: 0.8717\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6485 - acc: 0.8113\n",
      "Epoch 00008: val_loss improved from 0.48490 to 0.42820, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/008-0.4282.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6485 - acc: 0.8112 - val_loss: 0.4282 - val_acc: 0.8793\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5896 - acc: 0.8280\n",
      "Epoch 00009: val_loss improved from 0.42820 to 0.39069, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/009-0.3907.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5896 - acc: 0.8280 - val_loss: 0.3907 - val_acc: 0.8942\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5332 - acc: 0.8467\n",
      "Epoch 00010: val_loss improved from 0.39069 to 0.35008, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/010-0.3501.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5333 - acc: 0.8467 - val_loss: 0.3501 - val_acc: 0.9029\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.8575\n",
      "Epoch 00011: val_loss improved from 0.35008 to 0.33643, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/011-0.3364.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4953 - acc: 0.8575 - val_loss: 0.3364 - val_acc: 0.9045\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8674\n",
      "Epoch 00012: val_loss improved from 0.33643 to 0.29350, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/012-0.2935.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4595 - acc: 0.8674 - val_loss: 0.2935 - val_acc: 0.9178\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.8790\n",
      "Epoch 00013: val_loss improved from 0.29350 to 0.28372, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/013-0.2837.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4267 - acc: 0.8790 - val_loss: 0.2837 - val_acc: 0.9206\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8848\n",
      "Epoch 00014: val_loss improved from 0.28372 to 0.27348, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/014-0.2735.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3994 - acc: 0.8848 - val_loss: 0.2735 - val_acc: 0.9250\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3822 - acc: 0.8911\n",
      "Epoch 00015: val_loss improved from 0.27348 to 0.26095, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/015-0.2609.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3821 - acc: 0.8911 - val_loss: 0.2609 - val_acc: 0.9262\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8969\n",
      "Epoch 00016: val_loss improved from 0.26095 to 0.24584, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/016-0.2458.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3579 - acc: 0.8969 - val_loss: 0.2458 - val_acc: 0.9278\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.9008\n",
      "Epoch 00017: val_loss improved from 0.24584 to 0.22945, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/017-0.2294.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3466 - acc: 0.9009 - val_loss: 0.2294 - val_acc: 0.9345\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.9055\n",
      "Epoch 00018: val_loss did not improve from 0.22945\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3253 - acc: 0.9056 - val_loss: 0.2408 - val_acc: 0.9364\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.9095\n",
      "Epoch 00019: val_loss improved from 0.22945 to 0.21924, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/019-0.2192.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3127 - acc: 0.9095 - val_loss: 0.2192 - val_acc: 0.9394\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9110\n",
      "Epoch 00020: val_loss improved from 0.21924 to 0.20877, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/020-0.2088.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3029 - acc: 0.9110 - val_loss: 0.2088 - val_acc: 0.9397\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9178\n",
      "Epoch 00021: val_loss improved from 0.20877 to 0.20307, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/021-0.2031.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2880 - acc: 0.9178 - val_loss: 0.2031 - val_acc: 0.9434\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2838 - acc: 0.9183\n",
      "Epoch 00022: val_loss improved from 0.20307 to 0.20015, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/022-0.2002.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2839 - acc: 0.9183 - val_loss: 0.2002 - val_acc: 0.9450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9198\n",
      "Epoch 00023: val_loss improved from 0.20015 to 0.19014, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/023-0.1901.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2778 - acc: 0.9198 - val_loss: 0.1901 - val_acc: 0.9467\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9242\n",
      "Epoch 00024: val_loss improved from 0.19014 to 0.18907, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/024-0.1891.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2617 - acc: 0.9242 - val_loss: 0.1891 - val_acc: 0.9467\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9264\n",
      "Epoch 00025: val_loss improved from 0.18907 to 0.18397, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/025-0.1840.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2523 - acc: 0.9263 - val_loss: 0.1840 - val_acc: 0.9455\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9279\n",
      "Epoch 00026: val_loss improved from 0.18397 to 0.18093, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/026-0.1809.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2508 - acc: 0.9279 - val_loss: 0.1809 - val_acc: 0.9497\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9302\n",
      "Epoch 00027: val_loss improved from 0.18093 to 0.17597, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/027-0.1760.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2395 - acc: 0.9302 - val_loss: 0.1760 - val_acc: 0.9490\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9312\n",
      "Epoch 00028: val_loss did not improve from 0.17597\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2347 - acc: 0.9312 - val_loss: 0.1810 - val_acc: 0.9478\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9332\n",
      "Epoch 00029: val_loss improved from 0.17597 to 0.17040, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/029-0.1704.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2299 - acc: 0.9332 - val_loss: 0.1704 - val_acc: 0.9511\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9345\n",
      "Epoch 00030: val_loss improved from 0.17040 to 0.16649, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/030-0.1665.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2262 - acc: 0.9345 - val_loss: 0.1665 - val_acc: 0.9536\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9369\n",
      "Epoch 00031: val_loss did not improve from 0.16649\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2144 - acc: 0.9369 - val_loss: 0.1723 - val_acc: 0.9509\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9383\n",
      "Epoch 00032: val_loss did not improve from 0.16649\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2128 - acc: 0.9383 - val_loss: 0.1774 - val_acc: 0.9471\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9378\n",
      "Epoch 00033: val_loss did not improve from 0.16649\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2090 - acc: 0.9378 - val_loss: 0.1765 - val_acc: 0.9504\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9399\n",
      "Epoch 00034: val_loss did not improve from 0.16649\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2015 - acc: 0.9399 - val_loss: 0.1702 - val_acc: 0.9529\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9409\n",
      "Epoch 00035: val_loss improved from 0.16649 to 0.16469, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/035-0.1647.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1963 - acc: 0.9409 - val_loss: 0.1647 - val_acc: 0.9539\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9419\n",
      "Epoch 00036: val_loss did not improve from 0.16469\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1942 - acc: 0.9419 - val_loss: 0.1724 - val_acc: 0.9509\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9440\n",
      "Epoch 00037: val_loss did not improve from 0.16469\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1958 - acc: 0.9441 - val_loss: 0.1700 - val_acc: 0.9550\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9457\n",
      "Epoch 00038: val_loss did not improve from 0.16469\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1835 - acc: 0.9457 - val_loss: 0.1673 - val_acc: 0.9536\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9451\n",
      "Epoch 00039: val_loss improved from 0.16469 to 0.16299, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/039-0.1630.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1842 - acc: 0.9451 - val_loss: 0.1630 - val_acc: 0.9532\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9499\n",
      "Epoch 00040: val_loss improved from 0.16299 to 0.14792, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/040-0.1479.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1701 - acc: 0.9499 - val_loss: 0.1479 - val_acc: 0.9555\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9488\n",
      "Epoch 00041: val_loss did not improve from 0.14792\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1689 - acc: 0.9488 - val_loss: 0.1612 - val_acc: 0.9539\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9504\n",
      "Epoch 00042: val_loss did not improve from 0.14792\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1686 - acc: 0.9504 - val_loss: 0.1598 - val_acc: 0.9539\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9511\n",
      "Epoch 00043: val_loss did not improve from 0.14792\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1642 - acc: 0.9511 - val_loss: 0.1644 - val_acc: 0.9506\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9527\n",
      "Epoch 00044: val_loss did not improve from 0.14792\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1581 - acc: 0.9528 - val_loss: 0.1604 - val_acc: 0.9553\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9531\n",
      "Epoch 00045: val_loss did not improve from 0.14792\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1571 - acc: 0.9531 - val_loss: 0.1526 - val_acc: 0.9555\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9549\n",
      "Epoch 00046: val_loss did not improve from 0.14792\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1524 - acc: 0.9549 - val_loss: 0.1613 - val_acc: 0.9557\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9554\n",
      "Epoch 00047: val_loss did not improve from 0.14792\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1508 - acc: 0.9554 - val_loss: 0.1661 - val_acc: 0.9550\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9548\n",
      "Epoch 00048: val_loss did not improve from 0.14792\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1484 - acc: 0.9548 - val_loss: 0.1605 - val_acc: 0.9576\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9580\n",
      "Epoch 00049: val_loss did not improve from 0.14792\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1463 - acc: 0.9579 - val_loss: 0.1698 - val_acc: 0.9497\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9568\n",
      "Epoch 00050: val_loss did not improve from 0.14792\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1431 - acc: 0.9569 - val_loss: 0.1570 - val_acc: 0.9539\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9598\n",
      "Epoch 00051: val_loss improved from 0.14792 to 0.14789, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/051-0.1479.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1364 - acc: 0.9598 - val_loss: 0.1479 - val_acc: 0.9560\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9578\n",
      "Epoch 00052: val_loss did not improve from 0.14789\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1355 - acc: 0.9578 - val_loss: 0.1563 - val_acc: 0.9560\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9596\n",
      "Epoch 00053: val_loss did not improve from 0.14789\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1318 - acc: 0.9596 - val_loss: 0.1489 - val_acc: 0.9578\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9605\n",
      "Epoch 00054: val_loss did not improve from 0.14789\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1310 - acc: 0.9605 - val_loss: 0.1535 - val_acc: 0.9557\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9621\n",
      "Epoch 00055: val_loss did not improve from 0.14789\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1239 - acc: 0.9621 - val_loss: 0.1487 - val_acc: 0.9564\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9615\n",
      "Epoch 00056: val_loss did not improve from 0.14789\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1255 - acc: 0.9615 - val_loss: 0.1531 - val_acc: 0.9567\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9626\n",
      "Epoch 00057: val_loss did not improve from 0.14789\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1230 - acc: 0.9626 - val_loss: 0.1519 - val_acc: 0.9569\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9640\n",
      "Epoch 00058: val_loss did not improve from 0.14789\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1193 - acc: 0.9640 - val_loss: 0.1487 - val_acc: 0.9578\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9652\n",
      "Epoch 00059: val_loss improved from 0.14789 to 0.14783, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/059-0.1478.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1150 - acc: 0.9652 - val_loss: 0.1478 - val_acc: 0.9590\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9643\n",
      "Epoch 00060: val_loss improved from 0.14783 to 0.14306, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/060-0.1431.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1180 - acc: 0.9643 - val_loss: 0.1431 - val_acc: 0.9602\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9660\n",
      "Epoch 00061: val_loss did not improve from 0.14306\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1098 - acc: 0.9660 - val_loss: 0.1512 - val_acc: 0.9599\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9654\n",
      "Epoch 00062: val_loss did not improve from 0.14306\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1128 - acc: 0.9653 - val_loss: 0.1566 - val_acc: 0.9574\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9652\n",
      "Epoch 00063: val_loss did not improve from 0.14306\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1128 - acc: 0.9652 - val_loss: 0.1452 - val_acc: 0.9620\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9680\n",
      "Epoch 00064: val_loss did not improve from 0.14306\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1028 - acc: 0.9680 - val_loss: 0.1539 - val_acc: 0.9569\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9685\n",
      "Epoch 00065: val_loss improved from 0.14306 to 0.14293, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/065-0.1429.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1033 - acc: 0.9685 - val_loss: 0.1429 - val_acc: 0.9602\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9697\n",
      "Epoch 00066: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0997 - acc: 0.9697 - val_loss: 0.1585 - val_acc: 0.9597\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9688\n",
      "Epoch 00067: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1017 - acc: 0.9688 - val_loss: 0.1667 - val_acc: 0.9555\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9691\n",
      "Epoch 00068: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0965 - acc: 0.9691 - val_loss: 0.1598 - val_acc: 0.9564\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9716\n",
      "Epoch 00069: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0937 - acc: 0.9716 - val_loss: 0.1470 - val_acc: 0.9602\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9715\n",
      "Epoch 00070: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0906 - acc: 0.9715 - val_loss: 0.1449 - val_acc: 0.9618\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9715\n",
      "Epoch 00071: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0904 - acc: 0.9716 - val_loss: 0.1499 - val_acc: 0.9562\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9739\n",
      "Epoch 00072: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0847 - acc: 0.9739 - val_loss: 0.1559 - val_acc: 0.9571\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9721\n",
      "Epoch 00073: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0869 - acc: 0.9722 - val_loss: 0.1480 - val_acc: 0.9616\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9737\n",
      "Epoch 00074: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0869 - acc: 0.9738 - val_loss: 0.1552 - val_acc: 0.9567\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9739\n",
      "Epoch 00075: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0834 - acc: 0.9739 - val_loss: 0.1593 - val_acc: 0.9564\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9752\n",
      "Epoch 00076: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0810 - acc: 0.9752 - val_loss: 0.1695 - val_acc: 0.9550\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9740\n",
      "Epoch 00077: val_loss did not improve from 0.14293\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0827 - acc: 0.9740 - val_loss: 0.1492 - val_acc: 0.9571\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9752\n",
      "Epoch 00078: val_loss improved from 0.14293 to 0.14251, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/078-0.1425.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0779 - acc: 0.9752 - val_loss: 0.1425 - val_acc: 0.9606\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9787\n",
      "Epoch 00079: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0714 - acc: 0.9787 - val_loss: 0.1445 - val_acc: 0.9609\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9784\n",
      "Epoch 00080: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0699 - acc: 0.9784 - val_loss: 0.1552 - val_acc: 0.9592\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9758\n",
      "Epoch 00081: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0791 - acc: 0.9758 - val_loss: 0.1525 - val_acc: 0.9578\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9776\n",
      "Epoch 00082: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0706 - acc: 0.9776 - val_loss: 0.1877 - val_acc: 0.9529\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9777\n",
      "Epoch 00083: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0698 - acc: 0.9777 - val_loss: 0.1505 - val_acc: 0.9606\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9789\n",
      "Epoch 00084: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0689 - acc: 0.9789 - val_loss: 0.1767 - val_acc: 0.9574\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9796\n",
      "Epoch 00085: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0677 - acc: 0.9796 - val_loss: 0.1500 - val_acc: 0.9613\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9787\n",
      "Epoch 00086: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0667 - acc: 0.9788 - val_loss: 0.1632 - val_acc: 0.9588\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9814\n",
      "Epoch 00087: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0614 - acc: 0.9814 - val_loss: 0.1479 - val_acc: 0.9616\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9795\n",
      "Epoch 00088: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0642 - acc: 0.9795 - val_loss: 0.1507 - val_acc: 0.9595\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9796\n",
      "Epoch 00089: val_loss did not improve from 0.14251\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0648 - acc: 0.9796 - val_loss: 0.1465 - val_acc: 0.9618\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9811\n",
      "Epoch 00090: val_loss improved from 0.14251 to 0.13031, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_8_conv_checkpoint/090-0.1303.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0614 - acc: 0.9811 - val_loss: 0.1303 - val_acc: 0.9627\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9812\n",
      "Epoch 00091: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0590 - acc: 0.9812 - val_loss: 0.1567 - val_acc: 0.9592\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9813\n",
      "Epoch 00092: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0605 - acc: 0.9813 - val_loss: 0.1543 - val_acc: 0.9590\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9818\n",
      "Epoch 00093: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0568 - acc: 0.9818 - val_loss: 0.1525 - val_acc: 0.9616\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9819\n",
      "Epoch 00094: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0570 - acc: 0.9819 - val_loss: 0.1538 - val_acc: 0.9630\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9827\n",
      "Epoch 00095: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0557 - acc: 0.9827 - val_loss: 0.1431 - val_acc: 0.9620\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9830\n",
      "Epoch 00096: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0532 - acc: 0.9830 - val_loss: 0.1699 - val_acc: 0.9562\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9824\n",
      "Epoch 00097: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0553 - acc: 0.9824 - val_loss: 0.1583 - val_acc: 0.9585\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9842\n",
      "Epoch 00098: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0504 - acc: 0.9842 - val_loss: 0.1557 - val_acc: 0.9588\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9838\n",
      "Epoch 00099: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0512 - acc: 0.9838 - val_loss: 0.1545 - val_acc: 0.9597\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9858\n",
      "Epoch 00100: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0469 - acc: 0.9858 - val_loss: 0.1465 - val_acc: 0.9616\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9845\n",
      "Epoch 00101: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0526 - acc: 0.9845 - val_loss: 0.1529 - val_acc: 0.9620\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9858\n",
      "Epoch 00102: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0486 - acc: 0.9858 - val_loss: 0.1539 - val_acc: 0.9616\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9858\n",
      "Epoch 00103: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0458 - acc: 0.9858 - val_loss: 0.1555 - val_acc: 0.9602\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9862\n",
      "Epoch 00104: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0447 - acc: 0.9862 - val_loss: 0.1508 - val_acc: 0.9634\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9858\n",
      "Epoch 00105: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0433 - acc: 0.9858 - val_loss: 0.1592 - val_acc: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9862\n",
      "Epoch 00106: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0438 - acc: 0.9862 - val_loss: 0.1523 - val_acc: 0.9595\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9878\n",
      "Epoch 00107: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0398 - acc: 0.9878 - val_loss: 0.1677 - val_acc: 0.9583\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9873\n",
      "Epoch 00108: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0424 - acc: 0.9873 - val_loss: 0.1566 - val_acc: 0.9590\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9858\n",
      "Epoch 00109: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0464 - acc: 0.9858 - val_loss: 0.1645 - val_acc: 0.9553\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9866\n",
      "Epoch 00110: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0445 - acc: 0.9866 - val_loss: 0.1602 - val_acc: 0.9592\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9879\n",
      "Epoch 00111: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0402 - acc: 0.9879 - val_loss: 0.1483 - val_acc: 0.9611\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9876\n",
      "Epoch 00112: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0400 - acc: 0.9876 - val_loss: 0.1552 - val_acc: 0.9588\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9890\n",
      "Epoch 00113: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0368 - acc: 0.9890 - val_loss: 0.1589 - val_acc: 0.9609\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9877\n",
      "Epoch 00114: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0391 - acc: 0.9877 - val_loss: 0.1601 - val_acc: 0.9592\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9869\n",
      "Epoch 00115: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0405 - acc: 0.9869 - val_loss: 0.1728 - val_acc: 0.9574\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9871\n",
      "Epoch 00116: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0416 - acc: 0.9871 - val_loss: 0.1680 - val_acc: 0.9597\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9880\n",
      "Epoch 00117: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0408 - acc: 0.9880 - val_loss: 0.1634 - val_acc: 0.9599\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9900\n",
      "Epoch 00118: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0348 - acc: 0.9900 - val_loss: 0.1665 - val_acc: 0.9571\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9899\n",
      "Epoch 00119: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0331 - acc: 0.9899 - val_loss: 0.1668 - val_acc: 0.9630\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9889\n",
      "Epoch 00120: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0361 - acc: 0.9889 - val_loss: 0.1511 - val_acc: 0.9616\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9898\n",
      "Epoch 00121: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0346 - acc: 0.9898 - val_loss: 0.1552 - val_acc: 0.9613\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9891\n",
      "Epoch 00122: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0367 - acc: 0.9891 - val_loss: 0.1684 - val_acc: 0.9644\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9908\n",
      "Epoch 00123: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0305 - acc: 0.9908 - val_loss: 0.1594 - val_acc: 0.9634\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9894\n",
      "Epoch 00124: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0338 - acc: 0.9894 - val_loss: 0.1730 - val_acc: 0.9613\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9905\n",
      "Epoch 00125: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0305 - acc: 0.9905 - val_loss: 0.1623 - val_acc: 0.9632\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9902\n",
      "Epoch 00126: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0328 - acc: 0.9902 - val_loss: 0.1541 - val_acc: 0.9632\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9911\n",
      "Epoch 00127: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0297 - acc: 0.9911 - val_loss: 0.1769 - val_acc: 0.9585\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9906\n",
      "Epoch 00128: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0294 - acc: 0.9906 - val_loss: 0.1691 - val_acc: 0.9567\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9907\n",
      "Epoch 00129: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0302 - acc: 0.9907 - val_loss: 0.1687 - val_acc: 0.9625\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9910\n",
      "Epoch 00130: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0324 - acc: 0.9910 - val_loss: 0.1752 - val_acc: 0.9595\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9906\n",
      "Epoch 00131: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0308 - acc: 0.9906 - val_loss: 0.1650 - val_acc: 0.9583\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9916\n",
      "Epoch 00132: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0261 - acc: 0.9916 - val_loss: 0.1890 - val_acc: 0.9564\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9915\n",
      "Epoch 00133: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0293 - acc: 0.9916 - val_loss: 0.1826 - val_acc: 0.9581\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9923\n",
      "Epoch 00134: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0253 - acc: 0.9923 - val_loss: 0.1819 - val_acc: 0.9595\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9908\n",
      "Epoch 00135: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0293 - acc: 0.9908 - val_loss: 0.1725 - val_acc: 0.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9920\n",
      "Epoch 00136: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0276 - acc: 0.9920 - val_loss: 0.1848 - val_acc: 0.9618\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9926\n",
      "Epoch 00137: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0253 - acc: 0.9926 - val_loss: 0.1661 - val_acc: 0.9613\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9916\n",
      "Epoch 00138: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0266 - acc: 0.9916 - val_loss: 0.1935 - val_acc: 0.9585\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9935\n",
      "Epoch 00139: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0237 - acc: 0.9935 - val_loss: 0.1705 - val_acc: 0.9583\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9921\n",
      "Epoch 00140: val_loss did not improve from 0.13031\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0266 - acc: 0.9921 - val_loss: 0.2099 - val_acc: 0.9555\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 668us/sample - loss: 0.1835 - acc: 0.9485\n",
      "Loss: 0.183520101852568 Accuracy: 0.9484943\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8503 - acc: 0.4063\n",
      "Epoch 00001: val_loss improved from inf to 1.09858, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/001-1.0986.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 1.8501 - acc: 0.4064 - val_loss: 1.0986 - val_acc: 0.6869\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1211 - acc: 0.6570\n",
      "Epoch 00002: val_loss improved from 1.09858 to 0.71458, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/002-0.7146.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1210 - acc: 0.6570 - val_loss: 0.7146 - val_acc: 0.8013\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8019 - acc: 0.7589\n",
      "Epoch 00003: val_loss improved from 0.71458 to 0.51839, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/003-0.5184.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8019 - acc: 0.7589 - val_loss: 0.5184 - val_acc: 0.8486\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6339 - acc: 0.8111\n",
      "Epoch 00004: val_loss improved from 0.51839 to 0.40622, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/004-0.4062.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6339 - acc: 0.8112 - val_loss: 0.4062 - val_acc: 0.8952\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8476\n",
      "Epoch 00005: val_loss improved from 0.40622 to 0.33649, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/005-0.3365.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5143 - acc: 0.8476 - val_loss: 0.3365 - val_acc: 0.9101\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4378 - acc: 0.8714\n",
      "Epoch 00006: val_loss improved from 0.33649 to 0.26909, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/006-0.2691.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4378 - acc: 0.8714 - val_loss: 0.2691 - val_acc: 0.9294\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8863\n",
      "Epoch 00007: val_loss improved from 0.26909 to 0.24324, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/007-0.2432.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3868 - acc: 0.8863 - val_loss: 0.2432 - val_acc: 0.9355\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8988\n",
      "Epoch 00008: val_loss improved from 0.24324 to 0.21412, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/008-0.2141.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3440 - acc: 0.8988 - val_loss: 0.2141 - val_acc: 0.9432\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9085\n",
      "Epoch 00009: val_loss improved from 0.21412 to 0.19728, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/009-0.1973.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3100 - acc: 0.9085 - val_loss: 0.1973 - val_acc: 0.9469\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9136\n",
      "Epoch 00010: val_loss improved from 0.19728 to 0.18604, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/010-0.1860.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2876 - acc: 0.9136 - val_loss: 0.1860 - val_acc: 0.9536\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.9228\n",
      "Epoch 00011: val_loss improved from 0.18604 to 0.17005, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/011-0.1700.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2615 - acc: 0.9228 - val_loss: 0.1700 - val_acc: 0.9529\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9271\n",
      "Epoch 00012: val_loss improved from 0.17005 to 0.16791, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/012-0.1679.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2447 - acc: 0.9271 - val_loss: 0.1679 - val_acc: 0.9557\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9318\n",
      "Epoch 00013: val_loss improved from 0.16791 to 0.16790, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/013-0.1679.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2286 - acc: 0.9318 - val_loss: 0.1679 - val_acc: 0.9509\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9358\n",
      "Epoch 00014: val_loss improved from 0.16790 to 0.15332, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/014-0.1533.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2161 - acc: 0.9358 - val_loss: 0.1533 - val_acc: 0.9562\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9375\n",
      "Epoch 00015: val_loss improved from 0.15332 to 0.15296, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/015-0.1530.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2076 - acc: 0.9375 - val_loss: 0.1530 - val_acc: 0.9557\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9420\n",
      "Epoch 00016: val_loss improved from 0.15296 to 0.14666, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/016-0.1467.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1943 - acc: 0.9420 - val_loss: 0.1467 - val_acc: 0.9555\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9452\n",
      "Epoch 00017: val_loss improved from 0.14666 to 0.13583, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/017-0.1358.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1825 - acc: 0.9451 - val_loss: 0.1358 - val_acc: 0.9623\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9457\n",
      "Epoch 00018: val_loss did not improve from 0.13583\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1801 - acc: 0.9457 - val_loss: 0.1409 - val_acc: 0.9590\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9517\n",
      "Epoch 00019: val_loss improved from 0.13583 to 0.13090, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/019-0.1309.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1615 - acc: 0.9517 - val_loss: 0.1309 - val_acc: 0.9606\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9537\n",
      "Epoch 00020: val_loss did not improve from 0.13090\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1540 - acc: 0.9537 - val_loss: 0.1409 - val_acc: 0.9583\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9562\n",
      "Epoch 00021: val_loss improved from 0.13090 to 0.12961, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/021-0.1296.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1478 - acc: 0.9562 - val_loss: 0.1296 - val_acc: 0.9625\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9579\n",
      "Epoch 00022: val_loss improved from 0.12961 to 0.12852, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/022-0.1285.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1423 - acc: 0.9579 - val_loss: 0.1285 - val_acc: 0.9625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9583\n",
      "Epoch 00023: val_loss improved from 0.12852 to 0.12131, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/023-0.1213.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1353 - acc: 0.9583 - val_loss: 0.1213 - val_acc: 0.9639\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9590\n",
      "Epoch 00024: val_loss did not improve from 0.12131\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1344 - acc: 0.9590 - val_loss: 0.1349 - val_acc: 0.9623\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9616\n",
      "Epoch 00025: val_loss did not improve from 0.12131\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1259 - acc: 0.9616 - val_loss: 0.1299 - val_acc: 0.9646\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9614\n",
      "Epoch 00026: val_loss did not improve from 0.12131\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1239 - acc: 0.9614 - val_loss: 0.1450 - val_acc: 0.9590\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9644\n",
      "Epoch 00027: val_loss did not improve from 0.12131\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1166 - acc: 0.9643 - val_loss: 0.1236 - val_acc: 0.9634\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9654\n",
      "Epoch 00028: val_loss did not improve from 0.12131\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1135 - acc: 0.9654 - val_loss: 0.1214 - val_acc: 0.9648\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9673\n",
      "Epoch 00029: val_loss did not improve from 0.12131\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1073 - acc: 0.9673 - val_loss: 0.1252 - val_acc: 0.9639\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9677\n",
      "Epoch 00030: val_loss did not improve from 0.12131\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1030 - acc: 0.9677 - val_loss: 0.1235 - val_acc: 0.9637\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9695\n",
      "Epoch 00031: val_loss did not improve from 0.12131\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0996 - acc: 0.9695 - val_loss: 0.1223 - val_acc: 0.9639\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9704\n",
      "Epoch 00032: val_loss did not improve from 0.12131\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0962 - acc: 0.9704 - val_loss: 0.1239 - val_acc: 0.9651\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9711\n",
      "Epoch 00033: val_loss improved from 0.12131 to 0.11846, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/033-0.1185.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0927 - acc: 0.9711 - val_loss: 0.1185 - val_acc: 0.9644\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9732\n",
      "Epoch 00034: val_loss improved from 0.11846 to 0.11690, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/034-0.1169.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0907 - acc: 0.9732 - val_loss: 0.1169 - val_acc: 0.9655\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9752\n",
      "Epoch 00035: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0817 - acc: 0.9752 - val_loss: 0.1207 - val_acc: 0.9641\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9747\n",
      "Epoch 00036: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0811 - acc: 0.9747 - val_loss: 0.1221 - val_acc: 0.9634\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9745\n",
      "Epoch 00037: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0828 - acc: 0.9744 - val_loss: 0.1193 - val_acc: 0.9660\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9753\n",
      "Epoch 00038: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0796 - acc: 0.9753 - val_loss: 0.1302 - val_acc: 0.9630\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9784\n",
      "Epoch 00039: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0704 - acc: 0.9784 - val_loss: 0.1219 - val_acc: 0.9651\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9787\n",
      "Epoch 00040: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0686 - acc: 0.9787 - val_loss: 0.1214 - val_acc: 0.9646\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9768\n",
      "Epoch 00041: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0721 - acc: 0.9768 - val_loss: 0.1185 - val_acc: 0.9662\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9790\n",
      "Epoch 00042: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0663 - acc: 0.9789 - val_loss: 0.1198 - val_acc: 0.9660\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9799\n",
      "Epoch 00043: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0650 - acc: 0.9799 - val_loss: 0.1236 - val_acc: 0.9655\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9818\n",
      "Epoch 00044: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0591 - acc: 0.9819 - val_loss: 0.1204 - val_acc: 0.9672\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9824\n",
      "Epoch 00045: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0568 - acc: 0.9824 - val_loss: 0.1253 - val_acc: 0.9646\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9836\n",
      "Epoch 00046: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0537 - acc: 0.9836 - val_loss: 0.1249 - val_acc: 0.9653\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9835\n",
      "Epoch 00047: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0541 - acc: 0.9835 - val_loss: 0.1311 - val_acc: 0.9662\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9829\n",
      "Epoch 00048: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0534 - acc: 0.9829 - val_loss: 0.1237 - val_acc: 0.9679\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9835\n",
      "Epoch 00049: val_loss did not improve from 0.11690\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0508 - acc: 0.9835 - val_loss: 0.1210 - val_acc: 0.9674\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9863\n",
      "Epoch 00050: val_loss improved from 0.11690 to 0.11494, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_075_DO_9_conv_checkpoint/050-0.1149.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0473 - acc: 0.9863 - val_loss: 0.1149 - val_acc: 0.9674\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9858\n",
      "Epoch 00051: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0472 - acc: 0.9858 - val_loss: 0.1307 - val_acc: 0.9653\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9861\n",
      "Epoch 00052: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0461 - acc: 0.9861 - val_loss: 0.1217 - val_acc: 0.9669\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9871\n",
      "Epoch 00053: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0421 - acc: 0.9871 - val_loss: 0.1169 - val_acc: 0.9688\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9869\n",
      "Epoch 00054: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0421 - acc: 0.9869 - val_loss: 0.1314 - val_acc: 0.9644\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9856\n",
      "Epoch 00055: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0446 - acc: 0.9856 - val_loss: 0.1312 - val_acc: 0.9676\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9898\n",
      "Epoch 00056: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0349 - acc: 0.9898 - val_loss: 0.1158 - val_acc: 0.9676\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9892\n",
      "Epoch 00057: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0358 - acc: 0.9892 - val_loss: 0.1259 - val_acc: 0.9674\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9901\n",
      "Epoch 00058: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0344 - acc: 0.9901 - val_loss: 0.1290 - val_acc: 0.9669\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9895\n",
      "Epoch 00059: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0342 - acc: 0.9895 - val_loss: 0.1207 - val_acc: 0.9681\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9900\n",
      "Epoch 00060: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0333 - acc: 0.9900 - val_loss: 0.1345 - val_acc: 0.9655\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9908\n",
      "Epoch 00061: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0311 - acc: 0.9908 - val_loss: 0.1381 - val_acc: 0.9646\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9911\n",
      "Epoch 00062: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0297 - acc: 0.9911 - val_loss: 0.1343 - val_acc: 0.9674\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9914\n",
      "Epoch 00063: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0293 - acc: 0.9914 - val_loss: 0.1484 - val_acc: 0.9658\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9905\n",
      "Epoch 00064: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0316 - acc: 0.9905 - val_loss: 0.1266 - val_acc: 0.9674\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9924\n",
      "Epoch 00065: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0258 - acc: 0.9924 - val_loss: 0.1294 - val_acc: 0.9667\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9921\n",
      "Epoch 00066: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0273 - acc: 0.9921 - val_loss: 0.1444 - val_acc: 0.9634\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9926\n",
      "Epoch 00067: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0261 - acc: 0.9926 - val_loss: 0.1338 - val_acc: 0.9660\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9931\n",
      "Epoch 00068: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0236 - acc: 0.9931 - val_loss: 0.1359 - val_acc: 0.9667\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9927\n",
      "Epoch 00069: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0248 - acc: 0.9927 - val_loss: 0.1479 - val_acc: 0.9634\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9923\n",
      "Epoch 00070: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0247 - acc: 0.9923 - val_loss: 0.1292 - val_acc: 0.9672\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9928\n",
      "Epoch 00071: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0234 - acc: 0.9928 - val_loss: 0.1403 - val_acc: 0.9660\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9934\n",
      "Epoch 00072: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0226 - acc: 0.9934 - val_loss: 0.1409 - val_acc: 0.9648\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9932\n",
      "Epoch 00073: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0231 - acc: 0.9932 - val_loss: 0.1369 - val_acc: 0.9655\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9943\n",
      "Epoch 00074: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0187 - acc: 0.9943 - val_loss: 0.1419 - val_acc: 0.9648\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9918\n",
      "Epoch 00075: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0275 - acc: 0.9918 - val_loss: 0.1404 - val_acc: 0.9679\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9941\n",
      "Epoch 00076: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0196 - acc: 0.9941 - val_loss: 0.1377 - val_acc: 0.9679\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9952\n",
      "Epoch 00077: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0171 - acc: 0.9952 - val_loss: 0.1425 - val_acc: 0.9683\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9933\n",
      "Epoch 00078: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0212 - acc: 0.9933 - val_loss: 0.1419 - val_acc: 0.9681\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9946\n",
      "Epoch 00079: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0171 - acc: 0.9946 - val_loss: 0.1412 - val_acc: 0.9679\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9939\n",
      "Epoch 00080: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0197 - acc: 0.9939 - val_loss: 0.1546 - val_acc: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9942\n",
      "Epoch 00081: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0196 - acc: 0.9942 - val_loss: 0.1395 - val_acc: 0.9674\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9951\n",
      "Epoch 00082: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0156 - acc: 0.9951 - val_loss: 0.1447 - val_acc: 0.9693\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9961\n",
      "Epoch 00083: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0143 - acc: 0.9961 - val_loss: 0.1471 - val_acc: 0.9686\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9949\n",
      "Epoch 00084: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0169 - acc: 0.9949 - val_loss: 0.1555 - val_acc: 0.9658\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9950\n",
      "Epoch 00085: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0168 - acc: 0.9950 - val_loss: 0.1604 - val_acc: 0.9655\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9946\n",
      "Epoch 00086: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0173 - acc: 0.9946 - val_loss: 0.1608 - val_acc: 0.9681\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9953\n",
      "Epoch 00087: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0165 - acc: 0.9953 - val_loss: 0.1598 - val_acc: 0.9662\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9963\n",
      "Epoch 00088: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0128 - acc: 0.9963 - val_loss: 0.1605 - val_acc: 0.9669\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9956\n",
      "Epoch 00089: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0151 - acc: 0.9956 - val_loss: 0.1547 - val_acc: 0.9667\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9959\n",
      "Epoch 00090: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0139 - acc: 0.9959 - val_loss: 0.1674 - val_acc: 0.9651\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9959\n",
      "Epoch 00091: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0142 - acc: 0.9959 - val_loss: 0.1600 - val_acc: 0.9662\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9961\n",
      "Epoch 00092: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0128 - acc: 0.9960 - val_loss: 0.1534 - val_acc: 0.9667\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9952\n",
      "Epoch 00093: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0168 - acc: 0.9952 - val_loss: 0.1472 - val_acc: 0.9681\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9958\n",
      "Epoch 00094: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0139 - acc: 0.9958 - val_loss: 0.1625 - val_acc: 0.9655\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9976\n",
      "Epoch 00095: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0094 - acc: 0.9976 - val_loss: 0.1731 - val_acc: 0.9639\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9955\n",
      "Epoch 00096: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0142 - acc: 0.9955 - val_loss: 0.1552 - val_acc: 0.9667\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9963\n",
      "Epoch 00097: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0128 - acc: 0.9963 - val_loss: 0.1589 - val_acc: 0.9648\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9960\n",
      "Epoch 00098: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0124 - acc: 0.9960 - val_loss: 0.1455 - val_acc: 0.9697\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9965\n",
      "Epoch 00099: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0119 - acc: 0.9965 - val_loss: 0.1624 - val_acc: 0.9669\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9968\n",
      "Epoch 00100: val_loss did not improve from 0.11494\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0106 - acc: 0.9968 - val_loss: 0.1737 - val_acc: 0.9644\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 690us/sample - loss: 0.1663 - acc: 0.9543\n",
      "Loss: 0.16634295977195598 Accuracy: 0.95430946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_custom_tanh_DO_075_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 369us/sample - loss: 2.7278 - acc: 0.1040\n",
      "Loss: 2.7278172813843344 Accuracy: 0.10404985\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 549us/sample - loss: 2.0843 - acc: 0.3524\n",
      "Loss: 2.084329318282265 Accuracy: 0.3524403\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 618us/sample - loss: 1.5735 - acc: 0.5088\n",
      "Loss: 1.5734667690619748 Accuracy: 0.50882655\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 658us/sample - loss: 1.0243 - acc: 0.7148\n",
      "Loss: 1.0243151929148262 Accuracy: 0.7148494\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 673us/sample - loss: 0.7704 - acc: 0.7707\n",
      "Loss: 0.7704321701950002 Accuracy: 0.7707165\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 714us/sample - loss: 0.5956 - acc: 0.8417\n",
      "Loss: 0.5955511114679022 Accuracy: 0.84174454\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 697us/sample - loss: 0.3216 - acc: 0.9111\n",
      "Loss: 0.32164603567847583 Accuracy: 0.9111111\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 738us/sample - loss: 0.1835 - acc: 0.9485\n",
      "Loss: 0.183520101852568 Accuracy: 0.9484943\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 744us/sample - loss: 0.1663 - acc: 0.9543\n",
      "Loss: 0.16634295977195598 Accuracy: 0.95430946\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_tanh_DO_075_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 445us/sample - loss: 6.2113 - acc: 0.1024\n",
      "Loss: 6.2113052562266 Accuracy: 0.10238837\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 619us/sample - loss: 3.6911 - acc: 0.3450\n",
      "Loss: 3.691099198362166 Accuracy: 0.34496367\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 697us/sample - loss: 2.0174 - acc: 0.5952\n",
      "Loss: 2.017392090176496 Accuracy: 0.59522325\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 717us/sample - loss: 1.0698 - acc: 0.7495\n",
      "Loss: 1.069825997246141 Accuracy: 0.7495327\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 747us/sample - loss: 0.8659 - acc: 0.7790\n",
      "Loss: 0.8659318100997592 Accuracy: 0.7790239\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 741us/sample - loss: 0.6439 - acc: 0.8434\n",
      "Loss: 0.6438728602504433 Accuracy: 0.843406\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 768us/sample - loss: 0.3377 - acc: 0.9188\n",
      "Loss: 0.3377062275924508 Accuracy: 0.9187954\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 803us/sample - loss: 0.2849 - acc: 0.9360\n",
      "Loss: 0.2849020960840357 Accuracy: 0.93603325\n",
      "\n",
      "1D_CNN_custom_tanh_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 805us/sample - loss: 0.2150 - acc: 0.9508\n",
      "Loss: 0.21499658276443975 Accuracy: 0.95077884\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_tanh_DO'\n",
    "\n",
    "with open(path.join(log_dir, base+'_last'), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
