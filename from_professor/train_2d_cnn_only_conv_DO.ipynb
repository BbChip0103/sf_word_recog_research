{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(os.path.join(data_dir, 'train_data.npz'))\n",
    "val_data = np.load(os.path.join(data_dir, 'validation_data.npz'))\n",
    "test_data = np.load(os.path.join(data_dir, 'test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 25443),\n",
       " (36805,),\n",
       " (4293, 25443),\n",
       " (4293,),\n",
       " (4815, 25443),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x_data):\n",
    "    x_data = np.reshape(x_data, [x_data.shape[0], 99, 257, 1])\n",
    "    x_data = np.rot90(x_data, 1, (1, 2))\n",
    "    return x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_2d = preprocess(x_train)\n",
    "mean_vals = np.mean(x_train_2d, axis=0)\n",
    "std_val = np.std(x_train_2d)\n",
    "x_train_2d_norm = (x_train_2d-mean_vals) / std_val\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_2d = preprocess(x_val)\n",
    "x_val_2d_norm = (x_val_2d-mean_vals) / std_val\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_2d = preprocess(x_test)\n",
    "x_test_2d_norm = (x_test_2d-mean_vals) / std_val\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test_2d_norm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2d_cnn_only_conv_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D (kernel_size=5, filters=8, strides=(1,1), padding='valid', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=(2,2), padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv2D (kernel_size=5, filters=8*(2**(i+1)), strides=(1,1), padding='valid', \n",
    "                          activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=2, strides=(2,2), padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 48768)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                780304    \n",
      "=================================================================\n",
      "Total params: 780,512\n",
      "Trainable params: 780,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 21824)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 21824)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                349200    \n",
      "=================================================================\n",
      "Total params: 352,624\n",
      "Trainable params: 352,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 58, 18, 32)        12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 29, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8352)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8352)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                133648    \n",
      "=================================================================\n",
      "Total params: 149,904\n",
      "Trainable params: 149,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 58, 18, 32)        12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 29, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 25, 5, 64)         51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2496)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2496)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                39952     \n",
      "=================================================================\n",
      "Total params: 107,472\n",
      "Trainable params: 107,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1, 5):\n",
    "#     model = build_2d_cnn_only_conv(conv_num=i)\n",
    "#     model.summary()\n",
    "#     del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.7079 - acc: 0.4795\n",
      "Epoch 00001: val_loss improved from inf to 1.39176, saving model to model/checkpoint/2D_CNN_1_only_conv_checkpoint/01-1.3918.hdf5\n",
      "36805/36805 [==============================] - 9s 256us/sample - loss: 1.7073 - acc: 0.4797 - val_loss: 1.3918 - val_acc: 0.6080\n",
      "Epoch 2/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3161 - acc: 0.6047\n",
      "Epoch 00002: val_loss improved from 1.39176 to 1.28519, saving model to model/checkpoint/2D_CNN_1_only_conv_checkpoint/02-1.2852.hdf5\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 1.3162 - acc: 0.6046 - val_loss: 1.2852 - val_acc: 0.6406\n",
      "Epoch 3/200\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.1647 - acc: 0.6453\n",
      "Epoch 00003: val_loss improved from 1.28519 to 1.21261, saving model to model/checkpoint/2D_CNN_1_only_conv_checkpoint/03-1.2126.hdf5\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 1.1633 - acc: 0.6458 - val_loss: 1.2126 - val_acc: 0.6508\n",
      "Epoch 4/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0610 - acc: 0.6739\n",
      "Epoch 00004: val_loss improved from 1.21261 to 1.16637, saving model to model/checkpoint/2D_CNN_1_only_conv_checkpoint/04-1.1664.hdf5\n",
      "36805/36805 [==============================] - 9s 245us/sample - loss: 1.0615 - acc: 0.6739 - val_loss: 1.1664 - val_acc: 0.6697\n",
      "Epoch 5/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9865 - acc: 0.6974\n",
      "Epoch 00005: val_loss improved from 1.16637 to 1.12583, saving model to model/checkpoint/2D_CNN_1_only_conv_checkpoint/05-1.1258.hdf5\n",
      "36805/36805 [==============================] - 9s 244us/sample - loss: 0.9868 - acc: 0.6974 - val_loss: 1.1258 - val_acc: 0.6911\n",
      "Epoch 6/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9209 - acc: 0.7168\n",
      "Epoch 00006: val_loss improved from 1.12583 to 1.09510, saving model to model/checkpoint/2D_CNN_1_only_conv_checkpoint/06-1.0951.hdf5\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.9209 - acc: 0.7168 - val_loss: 1.0951 - val_acc: 0.6997\n",
      "Epoch 7/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8837 - acc: 0.7259\n",
      "Epoch 00007: val_loss did not improve from 1.09510\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.8837 - acc: 0.7259 - val_loss: 1.1040 - val_acc: 0.6960\n",
      "Epoch 8/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8370 - acc: 0.7431\n",
      "Epoch 00008: val_loss improved from 1.09510 to 1.07319, saving model to model/checkpoint/2D_CNN_1_only_conv_checkpoint/08-1.0732.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.8367 - acc: 0.7432 - val_loss: 1.0732 - val_acc: 0.7140\n",
      "Epoch 9/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8040 - acc: 0.7528\n",
      "Epoch 00009: val_loss did not improve from 1.07319\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.8035 - acc: 0.7529 - val_loss: 1.0867 - val_acc: 0.7102\n",
      "Epoch 10/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7748 - acc: 0.7609\n",
      "Epoch 00010: val_loss improved from 1.07319 to 1.04759, saving model to model/checkpoint/2D_CNN_1_only_conv_checkpoint/10-1.0476.hdf5\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.7746 - acc: 0.7608 - val_loss: 1.0476 - val_acc: 0.7195\n",
      "Epoch 11/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7432 - acc: 0.7687\n",
      "Epoch 00011: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.7433 - acc: 0.7687 - val_loss: 1.0612 - val_acc: 0.7242\n",
      "Epoch 12/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7194 - acc: 0.7763\n",
      "Epoch 00012: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.7197 - acc: 0.7762 - val_loss: 1.0567 - val_acc: 0.7223\n",
      "Epoch 13/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6985 - acc: 0.7808\n",
      "Epoch 00013: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.6985 - acc: 0.7806 - val_loss: 1.0660 - val_acc: 0.7305\n",
      "Epoch 14/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6753 - acc: 0.7880\n",
      "Epoch 00014: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.6754 - acc: 0.7879 - val_loss: 1.0615 - val_acc: 0.7284\n",
      "Epoch 15/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6553 - acc: 0.7915\n",
      "Epoch 00015: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.6552 - acc: 0.7915 - val_loss: 1.0801 - val_acc: 0.7258\n",
      "Epoch 16/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6385 - acc: 0.8007\n",
      "Epoch 00016: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.6384 - acc: 0.8007 - val_loss: 1.0793 - val_acc: 0.7270\n",
      "Epoch 17/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6241 - acc: 0.8041\n",
      "Epoch 00017: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.6243 - acc: 0.8040 - val_loss: 1.0901 - val_acc: 0.7244\n",
      "Epoch 18/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6063 - acc: 0.8103\n",
      "Epoch 00018: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.6063 - acc: 0.8104 - val_loss: 1.0939 - val_acc: 0.7268\n",
      "Epoch 19/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5934 - acc: 0.8132\n",
      "Epoch 00019: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.5932 - acc: 0.8134 - val_loss: 1.0829 - val_acc: 0.7307\n",
      "Epoch 20/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5824 - acc: 0.8160\n",
      "Epoch 00020: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.5824 - acc: 0.8160 - val_loss: 1.0826 - val_acc: 0.7296\n",
      "Epoch 21/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5715 - acc: 0.8201\n",
      "Epoch 00021: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.5719 - acc: 0.8198 - val_loss: 1.1205 - val_acc: 0.7244\n",
      "Epoch 22/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.8211\n",
      "Epoch 00022: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.5592 - acc: 0.8212 - val_loss: 1.0895 - val_acc: 0.7289\n",
      "Epoch 23/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.8233\n",
      "Epoch 00023: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 243us/sample - loss: 0.5533 - acc: 0.8233 - val_loss: 1.1220 - val_acc: 0.7216\n",
      "Epoch 24/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5471 - acc: 0.8274\n",
      "Epoch 00024: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.5469 - acc: 0.8275 - val_loss: 1.0963 - val_acc: 0.7300\n",
      "Epoch 25/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.8305\n",
      "Epoch 00025: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.5346 - acc: 0.8304 - val_loss: 1.0976 - val_acc: 0.7354\n",
      "Epoch 26/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.8310\n",
      "Epoch 00026: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 242us/sample - loss: 0.5258 - acc: 0.8311 - val_loss: 1.1229 - val_acc: 0.7300\n",
      "Epoch 27/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.8373\n",
      "Epoch 00027: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.5145 - acc: 0.8373 - val_loss: 1.1133 - val_acc: 0.7286\n",
      "Epoch 28/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5050 - acc: 0.8396\n",
      "Epoch 00028: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.5049 - acc: 0.8396 - val_loss: 1.1010 - val_acc: 0.7375\n",
      "Epoch 29/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4985 - acc: 0.8419\n",
      "Epoch 00029: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.4988 - acc: 0.8418 - val_loss: 1.1189 - val_acc: 0.7345\n",
      "Epoch 30/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4915 - acc: 0.8435\n",
      "Epoch 00030: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.4916 - acc: 0.8434 - val_loss: 1.1277 - val_acc: 0.7342\n",
      "Epoch 31/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4865 - acc: 0.8445\n",
      "Epoch 00031: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.4863 - acc: 0.8444 - val_loss: 1.1307 - val_acc: 0.7352\n",
      "Epoch 32/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4771 - acc: 0.8492\n",
      "Epoch 00032: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.4771 - acc: 0.8492 - val_loss: 1.1355 - val_acc: 0.7352\n",
      "Epoch 33/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4675 - acc: 0.8498\n",
      "Epoch 00033: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.4679 - acc: 0.8497 - val_loss: 1.1579 - val_acc: 0.7352\n",
      "Epoch 34/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4660 - acc: 0.8503\n",
      "Epoch 00034: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.4658 - acc: 0.8503 - val_loss: 1.1630 - val_acc: 0.7384\n",
      "Epoch 35/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4603 - acc: 0.8528\n",
      "Epoch 00035: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.4602 - acc: 0.8527 - val_loss: 1.1665 - val_acc: 0.7361\n",
      "Epoch 36/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4586 - acc: 0.8539\n",
      "Epoch 00036: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.4582 - acc: 0.8539 - val_loss: 1.1475 - val_acc: 0.7358\n",
      "Epoch 37/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4512 - acc: 0.8548\n",
      "Epoch 00037: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.4509 - acc: 0.8548 - val_loss: 1.1482 - val_acc: 0.7396\n",
      "Epoch 38/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8587\n",
      "Epoch 00038: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.4427 - acc: 0.8585 - val_loss: 1.1550 - val_acc: 0.7435\n",
      "Epoch 39/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4389 - acc: 0.8595\n",
      "Epoch 00039: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.4392 - acc: 0.8593 - val_loss: 1.1676 - val_acc: 0.7342\n",
      "Epoch 40/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.8605\n",
      "Epoch 00040: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.4304 - acc: 0.8606 - val_loss: 1.1548 - val_acc: 0.7463\n",
      "Epoch 41/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.8643\n",
      "Epoch 00041: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 0.4252 - acc: 0.8643 - val_loss: 1.1676 - val_acc: 0.7433\n",
      "Epoch 42/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.8650\n",
      "Epoch 00042: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.4215 - acc: 0.8650 - val_loss: 1.1547 - val_acc: 0.7417\n",
      "Epoch 43/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.8636\n",
      "Epoch 00043: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.4217 - acc: 0.8636 - val_loss: 1.1651 - val_acc: 0.7396\n",
      "Epoch 44/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4229 - acc: 0.8622\n",
      "Epoch 00044: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.4232 - acc: 0.8621 - val_loss: 1.1951 - val_acc: 0.7375\n",
      "Epoch 45/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8655\n",
      "Epoch 00045: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.4128 - acc: 0.8654 - val_loss: 1.1699 - val_acc: 0.7454\n",
      "Epoch 46/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8667\n",
      "Epoch 00046: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.4089 - acc: 0.8667 - val_loss: 1.1701 - val_acc: 0.7428\n",
      "Epoch 47/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4020 - acc: 0.8694\n",
      "Epoch 00047: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 239us/sample - loss: 0.4021 - acc: 0.8693 - val_loss: 1.1827 - val_acc: 0.7421\n",
      "Epoch 48/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8695\n",
      "Epoch 00048: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.4008 - acc: 0.8695 - val_loss: 1.2000 - val_acc: 0.7480\n",
      "Epoch 49/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3944 - acc: 0.8735\n",
      "Epoch 00049: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 238us/sample - loss: 0.3946 - acc: 0.8734 - val_loss: 1.1842 - val_acc: 0.7482\n",
      "Epoch 50/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8744\n",
      "Epoch 00050: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3916 - acc: 0.8745 - val_loss: 1.1896 - val_acc: 0.7438\n",
      "Epoch 51/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3882 - acc: 0.8758\n",
      "Epoch 00051: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.3881 - acc: 0.8758 - val_loss: 1.1928 - val_acc: 0.7449\n",
      "Epoch 52/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3813 - acc: 0.8772\n",
      "Epoch 00052: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.3810 - acc: 0.8773 - val_loss: 1.2070 - val_acc: 0.7454\n",
      "Epoch 53/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8767\n",
      "Epoch 00053: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.3816 - acc: 0.8767 - val_loss: 1.1987 - val_acc: 0.7433\n",
      "Epoch 54/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8784\n",
      "Epoch 00054: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 236us/sample - loss: 0.3779 - acc: 0.8784 - val_loss: 1.1863 - val_acc: 0.7421\n",
      "Epoch 55/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.8805\n",
      "Epoch 00055: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3718 - acc: 0.8806 - val_loss: 1.2078 - val_acc: 0.7375\n",
      "Epoch 56/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8826\n",
      "Epoch 00056: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.3657 - acc: 0.8825 - val_loss: 1.2066 - val_acc: 0.7473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8802\n",
      "Epoch 00057: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 237us/sample - loss: 0.3708 - acc: 0.8802 - val_loss: 1.2080 - val_acc: 0.7421\n",
      "Epoch 58/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8832\n",
      "Epoch 00058: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 240us/sample - loss: 0.3659 - acc: 0.8832 - val_loss: 1.2207 - val_acc: 0.7438\n",
      "Epoch 59/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.8822\n",
      "Epoch 00059: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 9s 241us/sample - loss: 0.3664 - acc: 0.8822 - val_loss: 1.2411 - val_acc: 0.7403\n",
      "Epoch 60/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3623 - acc: 0.8856\n",
      "Epoch 00060: val_loss did not improve from 1.04759\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.3623 - acc: 0.8856 - val_loss: 1.2241 - val_acc: 0.7433\n",
      "\n",
      "1 Only Conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 193us/sample - loss: 1.1291 - acc: 0.7078\n",
      "Loss: 1.129059188734829 Accuracy: 0.70778817\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7679 - acc: 0.4549\n",
      "Epoch 00001: val_loss improved from inf to 1.32231, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/01-1.3223.hdf5\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 1.7678 - acc: 0.4549 - val_loss: 1.3223 - val_acc: 0.6224\n",
      "Epoch 2/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2485 - acc: 0.6264\n",
      "Epoch 00002: val_loss improved from 1.32231 to 1.04643, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/02-1.0464.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 1.2484 - acc: 0.6264 - val_loss: 1.0464 - val_acc: 0.7195\n",
      "Epoch 3/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9924 - acc: 0.7064\n",
      "Epoch 00003: val_loss improved from 1.04643 to 0.87018, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/03-0.8702.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.9923 - acc: 0.7064 - val_loss: 0.8702 - val_acc: 0.7717\n",
      "Epoch 4/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8350 - acc: 0.7526\n",
      "Epoch 00004: val_loss improved from 0.87018 to 0.76703, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/04-0.7670.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.8351 - acc: 0.7525 - val_loss: 0.7670 - val_acc: 0.7980\n",
      "Epoch 5/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7240 - acc: 0.7847\n",
      "Epoch 00005: val_loss improved from 0.76703 to 0.70155, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/05-0.7016.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.7239 - acc: 0.7847 - val_loss: 0.7016 - val_acc: 0.8246\n",
      "Epoch 6/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6511 - acc: 0.8050\n",
      "Epoch 00006: val_loss improved from 0.70155 to 0.64231, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/06-0.6423.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.6509 - acc: 0.8049 - val_loss: 0.6423 - val_acc: 0.8437\n",
      "Epoch 7/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.5845 - acc: 0.8258\n",
      "Epoch 00007: val_loss improved from 0.64231 to 0.60439, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/07-0.6044.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.5849 - acc: 0.8257 - val_loss: 0.6044 - val_acc: 0.8451\n",
      "Epoch 8/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.8375\n",
      "Epoch 00008: val_loss improved from 0.60439 to 0.57520, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/08-0.5752.hdf5\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.5408 - acc: 0.8375 - val_loss: 0.5752 - val_acc: 0.8542\n",
      "Epoch 9/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8485\n",
      "Epoch 00009: val_loss improved from 0.57520 to 0.55948, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/09-0.5595.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.5019 - acc: 0.8485 - val_loss: 0.5595 - val_acc: 0.8609\n",
      "Epoch 10/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4656 - acc: 0.8589\n",
      "Epoch 00010: val_loss improved from 0.55948 to 0.53661, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/10-0.5366.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.4649 - acc: 0.8591 - val_loss: 0.5366 - val_acc: 0.8693\n",
      "Epoch 11/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.8651\n",
      "Epoch 00011: val_loss improved from 0.53661 to 0.52993, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/11-0.5299.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.4403 - acc: 0.8651 - val_loss: 0.5299 - val_acc: 0.8707\n",
      "Epoch 12/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8752\n",
      "Epoch 00012: val_loss improved from 0.52993 to 0.51312, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/12-0.5131.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.4126 - acc: 0.8753 - val_loss: 0.5131 - val_acc: 0.8758\n",
      "Epoch 13/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8798\n",
      "Epoch 00013: val_loss did not improve from 0.51312\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.3910 - acc: 0.8797 - val_loss: 0.5172 - val_acc: 0.8749\n",
      "Epoch 14/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8853\n",
      "Epoch 00014: val_loss improved from 0.51312 to 0.49348, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/14-0.4935.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.3739 - acc: 0.8854 - val_loss: 0.4935 - val_acc: 0.8796\n",
      "Epoch 15/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8912\n",
      "Epoch 00015: val_loss did not improve from 0.49348\n",
      "36805/36805 [==============================] - 10s 263us/sample - loss: 0.3534 - acc: 0.8910 - val_loss: 0.5020 - val_acc: 0.8833\n",
      "Epoch 16/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8928\n",
      "Epoch 00016: val_loss improved from 0.49348 to 0.49009, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/16-0.4901.hdf5\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.3430 - acc: 0.8928 - val_loss: 0.4901 - val_acc: 0.8828\n",
      "Epoch 17/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.8995\n",
      "Epoch 00017: val_loss improved from 0.49009 to 0.49007, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/17-0.4901.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.3250 - acc: 0.8994 - val_loss: 0.4901 - val_acc: 0.8845\n",
      "Epoch 18/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.9056\n",
      "Epoch 00018: val_loss did not improve from 0.49007\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.3108 - acc: 0.9055 - val_loss: 0.4965 - val_acc: 0.8849\n",
      "Epoch 19/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9053\n",
      "Epoch 00019: val_loss improved from 0.49007 to 0.48230, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/19-0.4823.hdf5\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.3025 - acc: 0.9053 - val_loss: 0.4823 - val_acc: 0.8877\n",
      "Epoch 20/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9107\n",
      "Epoch 00020: val_loss improved from 0.48230 to 0.48138, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/20-0.4814.hdf5\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.2891 - acc: 0.9107 - val_loss: 0.4814 - val_acc: 0.8856\n",
      "Epoch 21/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9134\n",
      "Epoch 00021: val_loss did not improve from 0.48138\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2785 - acc: 0.9135 - val_loss: 0.4849 - val_acc: 0.8866\n",
      "Epoch 22/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9157\n",
      "Epoch 00022: val_loss improved from 0.48138 to 0.48038, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/22-0.4804.hdf5\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2703 - acc: 0.9158 - val_loss: 0.4804 - val_acc: 0.8919\n",
      "Epoch 23/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9166\n",
      "Epoch 00023: val_loss did not improve from 0.48038\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.2670 - acc: 0.9166 - val_loss: 0.4862 - val_acc: 0.8912\n",
      "Epoch 24/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9215\n",
      "Epoch 00024: val_loss improved from 0.48038 to 0.47992, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/24-0.4799.hdf5\n",
      "36805/36805 [==============================] - 10s 272us/sample - loss: 0.2525 - acc: 0.9216 - val_loss: 0.4799 - val_acc: 0.8896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9237\n",
      "Epoch 00025: val_loss improved from 0.47992 to 0.47731, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/25-0.4773.hdf5\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.2432 - acc: 0.9238 - val_loss: 0.4773 - val_acc: 0.8952\n",
      "Epoch 26/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9234\n",
      "Epoch 00026: val_loss did not improve from 0.47731\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.2385 - acc: 0.9237 - val_loss: 0.4793 - val_acc: 0.8915\n",
      "Epoch 27/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9273\n",
      "Epoch 00027: val_loss improved from 0.47731 to 0.47032, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/27-0.4703.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.2347 - acc: 0.9272 - val_loss: 0.4703 - val_acc: 0.8959\n",
      "Epoch 28/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9289\n",
      "Epoch 00028: val_loss improved from 0.47032 to 0.46459, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/28-0.4646.hdf5\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.2245 - acc: 0.9288 - val_loss: 0.4646 - val_acc: 0.8949\n",
      "Epoch 29/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9326\n",
      "Epoch 00029: val_loss did not improve from 0.46459\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2168 - acc: 0.9326 - val_loss: 0.4780 - val_acc: 0.8947\n",
      "Epoch 30/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9324\n",
      "Epoch 00030: val_loss improved from 0.46459 to 0.46377, saving model to model/checkpoint/2D_CNN_2_only_conv_checkpoint/30-0.4638.hdf5\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.2186 - acc: 0.9325 - val_loss: 0.4638 - val_acc: 0.8940\n",
      "Epoch 31/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9333\n",
      "Epoch 00031: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.2114 - acc: 0.9333 - val_loss: 0.4689 - val_acc: 0.8987\n",
      "Epoch 32/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9355\n",
      "Epoch 00032: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.2058 - acc: 0.9354 - val_loss: 0.4775 - val_acc: 0.8940\n",
      "Epoch 33/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9371\n",
      "Epoch 00033: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1995 - acc: 0.9371 - val_loss: 0.4759 - val_acc: 0.8977\n",
      "Epoch 34/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9388\n",
      "Epoch 00034: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1971 - acc: 0.9386 - val_loss: 0.4645 - val_acc: 0.9008\n",
      "Epoch 35/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9394\n",
      "Epoch 00035: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1875 - acc: 0.9393 - val_loss: 0.4758 - val_acc: 0.8956\n",
      "Epoch 36/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9411\n",
      "Epoch 00036: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1877 - acc: 0.9410 - val_loss: 0.4798 - val_acc: 0.8959\n",
      "Epoch 37/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9421\n",
      "Epoch 00037: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1871 - acc: 0.9421 - val_loss: 0.4669 - val_acc: 0.8980\n",
      "Epoch 38/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9422\n",
      "Epoch 00038: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1809 - acc: 0.9422 - val_loss: 0.4794 - val_acc: 0.8977\n",
      "Epoch 39/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9445\n",
      "Epoch 00039: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1746 - acc: 0.9445 - val_loss: 0.4757 - val_acc: 0.8975\n",
      "Epoch 40/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9455\n",
      "Epoch 00040: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1709 - acc: 0.9454 - val_loss: 0.4741 - val_acc: 0.8963\n",
      "Epoch 41/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9477\n",
      "Epoch 00041: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1679 - acc: 0.9476 - val_loss: 0.4734 - val_acc: 0.9003\n",
      "Epoch 42/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9477\n",
      "Epoch 00042: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1644 - acc: 0.9478 - val_loss: 0.4741 - val_acc: 0.8984\n",
      "Epoch 43/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9473\n",
      "Epoch 00043: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.1641 - acc: 0.9472 - val_loss: 0.4768 - val_acc: 0.8977\n",
      "Epoch 44/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9495\n",
      "Epoch 00044: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1593 - acc: 0.9494 - val_loss: 0.4843 - val_acc: 0.9003\n",
      "Epoch 45/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9501\n",
      "Epoch 00045: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1570 - acc: 0.9502 - val_loss: 0.4883 - val_acc: 0.8996\n",
      "Epoch 46/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9512\n",
      "Epoch 00046: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1581 - acc: 0.9512 - val_loss: 0.4900 - val_acc: 0.9012\n",
      "Epoch 47/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9517\n",
      "Epoch 00047: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1531 - acc: 0.9517 - val_loss: 0.4875 - val_acc: 0.9003\n",
      "Epoch 48/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9539\n",
      "Epoch 00048: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1470 - acc: 0.9536 - val_loss: 0.4953 - val_acc: 0.8954\n",
      "Epoch 49/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9556\n",
      "Epoch 00049: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1457 - acc: 0.9556 - val_loss: 0.4870 - val_acc: 0.8994\n",
      "Epoch 50/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9536\n",
      "Epoch 00050: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1463 - acc: 0.9535 - val_loss: 0.4738 - val_acc: 0.9022\n",
      "Epoch 51/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9554\n",
      "Epoch 00051: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1446 - acc: 0.9554 - val_loss: 0.4928 - val_acc: 0.8961\n",
      "Epoch 52/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9555\n",
      "Epoch 00052: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1418 - acc: 0.9556 - val_loss: 0.4901 - val_acc: 0.9008\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9564\n",
      "Epoch 00053: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1390 - acc: 0.9564 - val_loss: 0.4852 - val_acc: 0.8980\n",
      "Epoch 54/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9554\n",
      "Epoch 00054: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1414 - acc: 0.9553 - val_loss: 0.4813 - val_acc: 0.9012\n",
      "Epoch 55/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9569\n",
      "Epoch 00055: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1369 - acc: 0.9568 - val_loss: 0.4924 - val_acc: 0.9012\n",
      "Epoch 56/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9589\n",
      "Epoch 00056: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 271us/sample - loss: 0.1310 - acc: 0.9589 - val_loss: 0.4904 - val_acc: 0.9015\n",
      "Epoch 57/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9598\n",
      "Epoch 00057: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1293 - acc: 0.9598 - val_loss: 0.4902 - val_acc: 0.9012\n",
      "Epoch 58/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9601\n",
      "Epoch 00058: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1279 - acc: 0.9601 - val_loss: 0.4896 - val_acc: 0.8987\n",
      "Epoch 59/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9608\n",
      "Epoch 00059: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1245 - acc: 0.9608 - val_loss: 0.4973 - val_acc: 0.9001\n",
      "Epoch 60/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9603\n",
      "Epoch 00060: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1268 - acc: 0.9603 - val_loss: 0.4908 - val_acc: 0.9015\n",
      "Epoch 61/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9617\n",
      "Epoch 00061: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1232 - acc: 0.9617 - val_loss: 0.4852 - val_acc: 0.9029\n",
      "Epoch 62/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9615\n",
      "Epoch 00062: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 264us/sample - loss: 0.1237 - acc: 0.9615 - val_loss: 0.5010 - val_acc: 0.9022\n",
      "Epoch 63/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9625\n",
      "Epoch 00063: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1190 - acc: 0.9626 - val_loss: 0.4945 - val_acc: 0.9029\n",
      "Epoch 64/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9633\n",
      "Epoch 00064: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1167 - acc: 0.9633 - val_loss: 0.5095 - val_acc: 0.8996\n",
      "Epoch 65/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9631\n",
      "Epoch 00065: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1154 - acc: 0.9631 - val_loss: 0.5026 - val_acc: 0.9026\n",
      "Epoch 66/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9649\n",
      "Epoch 00066: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1152 - acc: 0.9649 - val_loss: 0.4985 - val_acc: 0.9010\n",
      "Epoch 67/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9623\n",
      "Epoch 00067: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.1192 - acc: 0.9623 - val_loss: 0.4959 - val_acc: 0.9005\n",
      "Epoch 68/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9651\n",
      "Epoch 00068: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1106 - acc: 0.9651 - val_loss: 0.5031 - val_acc: 0.9057\n",
      "Epoch 69/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9624\n",
      "Epoch 00069: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 270us/sample - loss: 0.1145 - acc: 0.9624 - val_loss: 0.4992 - val_acc: 0.9017\n",
      "Epoch 70/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9648\n",
      "Epoch 00070: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 268us/sample - loss: 0.1098 - acc: 0.9647 - val_loss: 0.4973 - val_acc: 0.9019\n",
      "Epoch 71/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9652\n",
      "Epoch 00071: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1121 - acc: 0.9652 - val_loss: 0.5019 - val_acc: 0.9033\n",
      "Epoch 72/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9658\n",
      "Epoch 00072: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1087 - acc: 0.9657 - val_loss: 0.4923 - val_acc: 0.9017\n",
      "Epoch 73/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9660\n",
      "Epoch 00073: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 269us/sample - loss: 0.1058 - acc: 0.9661 - val_loss: 0.4970 - val_acc: 0.9057\n",
      "Epoch 74/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9679\n",
      "Epoch 00074: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1051 - acc: 0.9679 - val_loss: 0.5021 - val_acc: 0.9029\n",
      "Epoch 75/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9680\n",
      "Epoch 00075: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1022 - acc: 0.9680 - val_loss: 0.4986 - val_acc: 0.9022\n",
      "Epoch 76/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9692\n",
      "Epoch 00076: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.0980 - acc: 0.9691 - val_loss: 0.5138 - val_acc: 0.8994\n",
      "Epoch 77/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9665\n",
      "Epoch 00077: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1071 - acc: 0.9664 - val_loss: 0.5123 - val_acc: 0.8987\n",
      "Epoch 78/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9677\n",
      "Epoch 00078: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 267us/sample - loss: 0.1006 - acc: 0.9676 - val_loss: 0.4923 - val_acc: 0.9038\n",
      "Epoch 79/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9676\n",
      "Epoch 00079: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 265us/sample - loss: 0.1029 - acc: 0.9676 - val_loss: 0.4922 - val_acc: 0.9043\n",
      "Epoch 80/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9690\n",
      "Epoch 00080: val_loss did not improve from 0.46377\n",
      "36805/36805 [==============================] - 10s 266us/sample - loss: 0.0986 - acc: 0.9690 - val_loss: 0.5072 - val_acc: 0.9043\n",
      "\n",
      "2 Only Conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 225us/sample - loss: 0.5591 - acc: 0.8623\n",
      "Loss: 0.5590534259968457 Accuracy: 0.8623053\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8430 - acc: 0.4189\n",
      "Epoch 00001: val_loss improved from inf to 1.31392, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/01-1.3139.hdf5\n",
      "36805/36805 [==============================] - 12s 328us/sample - loss: 1.8421 - acc: 0.4192 - val_loss: 1.3139 - val_acc: 0.6126\n",
      "Epoch 2/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2784 - acc: 0.6049\n",
      "Epoch 00002: val_loss improved from 1.31392 to 0.98432, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/02-0.9843.hdf5\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 1.2780 - acc: 0.6048 - val_loss: 0.9843 - val_acc: 0.7277\n",
      "Epoch 3/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9880 - acc: 0.7018\n",
      "Epoch 00003: val_loss improved from 0.98432 to 0.77260, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/03-0.7726.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.9879 - acc: 0.7018 - val_loss: 0.7726 - val_acc: 0.8006\n",
      "Epoch 4/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8220 - acc: 0.7506\n",
      "Epoch 00004: val_loss improved from 0.77260 to 0.65907, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/04-0.6591.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.8212 - acc: 0.7509 - val_loss: 0.6591 - val_acc: 0.8300\n",
      "Epoch 5/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7011 - acc: 0.7881\n",
      "Epoch 00005: val_loss improved from 0.65907 to 0.56811, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/05-0.5681.hdf5\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.7010 - acc: 0.7880 - val_loss: 0.5681 - val_acc: 0.8514\n",
      "Epoch 6/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6079 - acc: 0.8165\n",
      "Epoch 00006: val_loss improved from 0.56811 to 0.51722, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/06-0.5172.hdf5\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.6083 - acc: 0.8164 - val_loss: 0.5172 - val_acc: 0.8621\n",
      "Epoch 7/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.8339\n",
      "Epoch 00007: val_loss improved from 0.51722 to 0.46177, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/07-0.4618.hdf5\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.5463 - acc: 0.8338 - val_loss: 0.4618 - val_acc: 0.8803\n",
      "Epoch 8/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4936 - acc: 0.8493\n",
      "Epoch 00008: val_loss improved from 0.46177 to 0.43561, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/08-0.4356.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.4936 - acc: 0.8493 - val_loss: 0.4356 - val_acc: 0.8859\n",
      "Epoch 9/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.8631\n",
      "Epoch 00009: val_loss improved from 0.43561 to 0.40195, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/09-0.4020.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.4503 - acc: 0.8631 - val_loss: 0.4020 - val_acc: 0.8977\n",
      "Epoch 10/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8758\n",
      "Epoch 00010: val_loss improved from 0.40195 to 0.37623, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/10-0.3762.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.4130 - acc: 0.8759 - val_loss: 0.3762 - val_acc: 0.9029\n",
      "Epoch 11/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8808\n",
      "Epoch 00011: val_loss improved from 0.37623 to 0.36165, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/11-0.3616.hdf5\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.3880 - acc: 0.8807 - val_loss: 0.3616 - val_acc: 0.9054\n",
      "Epoch 12/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8890\n",
      "Epoch 00012: val_loss improved from 0.36165 to 0.34588, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/12-0.3459.hdf5\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.3628 - acc: 0.8891 - val_loss: 0.3459 - val_acc: 0.9075\n",
      "Epoch 13/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8949\n",
      "Epoch 00013: val_loss improved from 0.34588 to 0.34077, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/13-0.3408.hdf5\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.3398 - acc: 0.8950 - val_loss: 0.3408 - val_acc: 0.9108\n",
      "Epoch 14/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8993\n",
      "Epoch 00014: val_loss improved from 0.34077 to 0.32568, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/14-0.3257.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.3262 - acc: 0.8992 - val_loss: 0.3257 - val_acc: 0.9147\n",
      "Epoch 15/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.9028\n",
      "Epoch 00015: val_loss improved from 0.32568 to 0.32151, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/15-0.3215.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.3090 - acc: 0.9028 - val_loss: 0.3215 - val_acc: 0.9164\n",
      "Epoch 16/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9094\n",
      "Epoch 00016: val_loss improved from 0.32151 to 0.31573, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/16-0.3157.hdf5\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.2923 - acc: 0.9095 - val_loss: 0.3157 - val_acc: 0.9164\n",
      "Epoch 17/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9121\n",
      "Epoch 00017: val_loss improved from 0.31573 to 0.30490, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/17-0.3049.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.2811 - acc: 0.9122 - val_loss: 0.3049 - val_acc: 0.9203\n",
      "Epoch 18/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9176\n",
      "Epoch 00018: val_loss improved from 0.30490 to 0.29989, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/18-0.2999.hdf5\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.2662 - acc: 0.9176 - val_loss: 0.2999 - val_acc: 0.9231\n",
      "Epoch 19/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.9193\n",
      "Epoch 00019: val_loss improved from 0.29989 to 0.29598, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/19-0.2960.hdf5\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.2560 - acc: 0.9192 - val_loss: 0.2960 - val_acc: 0.9234\n",
      "Epoch 20/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9228\n",
      "Epoch 00020: val_loss did not improve from 0.29598\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.2463 - acc: 0.9228 - val_loss: 0.2969 - val_acc: 0.9252\n",
      "Epoch 21/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9250\n",
      "Epoch 00021: val_loss improved from 0.29598 to 0.29098, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/21-0.2910.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.2374 - acc: 0.9250 - val_loss: 0.2910 - val_acc: 0.9276\n",
      "Epoch 22/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9259\n",
      "Epoch 00022: val_loss improved from 0.29098 to 0.28396, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/22-0.2840.hdf5\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.2334 - acc: 0.9259 - val_loss: 0.2840 - val_acc: 0.9255\n",
      "Epoch 23/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9279\n",
      "Epoch 00023: val_loss did not improve from 0.28396\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.2252 - acc: 0.9280 - val_loss: 0.2860 - val_acc: 0.9269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9322\n",
      "Epoch 00024: val_loss did not improve from 0.28396\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.2149 - acc: 0.9324 - val_loss: 0.2920 - val_acc: 0.9280\n",
      "Epoch 25/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9346\n",
      "Epoch 00025: val_loss improved from 0.28396 to 0.28197, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/25-0.2820.hdf5\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.2081 - acc: 0.9344 - val_loss: 0.2820 - val_acc: 0.9299\n",
      "Epoch 26/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9344\n",
      "Epoch 00026: val_loss did not improve from 0.28197\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.2065 - acc: 0.9344 - val_loss: 0.2844 - val_acc: 0.9299\n",
      "Epoch 27/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9370\n",
      "Epoch 00027: val_loss did not improve from 0.28197\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1976 - acc: 0.9370 - val_loss: 0.2848 - val_acc: 0.9320\n",
      "Epoch 28/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9379\n",
      "Epoch 00028: val_loss did not improve from 0.28197\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.1919 - acc: 0.9379 - val_loss: 0.2841 - val_acc: 0.9334\n",
      "Epoch 29/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9401\n",
      "Epoch 00029: val_loss improved from 0.28197 to 0.27556, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/29-0.2756.hdf5\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1891 - acc: 0.9401 - val_loss: 0.2756 - val_acc: 0.9350\n",
      "Epoch 30/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9399\n",
      "Epoch 00030: val_loss did not improve from 0.27556\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.1882 - acc: 0.9398 - val_loss: 0.2789 - val_acc: 0.9327\n",
      "Epoch 31/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9423\n",
      "Epoch 00031: val_loss improved from 0.27556 to 0.27216, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/31-0.2722.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1811 - acc: 0.9423 - val_loss: 0.2722 - val_acc: 0.9348\n",
      "Epoch 32/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9441\n",
      "Epoch 00032: val_loss did not improve from 0.27216\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1775 - acc: 0.9441 - val_loss: 0.2752 - val_acc: 0.9376\n",
      "Epoch 33/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9439\n",
      "Epoch 00033: val_loss did not improve from 0.27216\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1732 - acc: 0.9439 - val_loss: 0.2749 - val_acc: 0.9362\n",
      "Epoch 34/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9459\n",
      "Epoch 00034: val_loss did not improve from 0.27216\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1690 - acc: 0.9459 - val_loss: 0.2791 - val_acc: 0.9376\n",
      "Epoch 35/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9458\n",
      "Epoch 00035: val_loss did not improve from 0.27216\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1662 - acc: 0.9458 - val_loss: 0.2791 - val_acc: 0.9345\n",
      "Epoch 36/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9491\n",
      "Epoch 00036: val_loss improved from 0.27216 to 0.26722, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/36-0.2672.hdf5\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1617 - acc: 0.9489 - val_loss: 0.2672 - val_acc: 0.9366\n",
      "Epoch 37/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9489\n",
      "Epoch 00037: val_loss did not improve from 0.26722\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1583 - acc: 0.9488 - val_loss: 0.2734 - val_acc: 0.9383\n",
      "Epoch 38/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9484\n",
      "Epoch 00038: val_loss did not improve from 0.26722\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.1575 - acc: 0.9485 - val_loss: 0.2682 - val_acc: 0.9383\n",
      "Epoch 39/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9516\n",
      "Epoch 00039: val_loss improved from 0.26722 to 0.26507, saving model to model/checkpoint/2D_CNN_3_only_conv_checkpoint/39-0.2651.hdf5\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1519 - acc: 0.9516 - val_loss: 0.2651 - val_acc: 0.9373\n",
      "Epoch 40/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9515\n",
      "Epoch 00040: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1494 - acc: 0.9515 - val_loss: 0.2757 - val_acc: 0.9380\n",
      "Epoch 41/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9527\n",
      "Epoch 00041: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1448 - acc: 0.9528 - val_loss: 0.2654 - val_acc: 0.9376\n",
      "Epoch 42/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9530\n",
      "Epoch 00042: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1441 - acc: 0.9529 - val_loss: 0.2776 - val_acc: 0.9355\n",
      "Epoch 43/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9552\n",
      "Epoch 00043: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.1390 - acc: 0.9552 - val_loss: 0.2780 - val_acc: 0.9380\n",
      "Epoch 44/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9536\n",
      "Epoch 00044: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1410 - acc: 0.9536 - val_loss: 0.2860 - val_acc: 0.9352\n",
      "Epoch 45/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9569\n",
      "Epoch 00045: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1347 - acc: 0.9569 - val_loss: 0.2773 - val_acc: 0.9371\n",
      "Epoch 46/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9562\n",
      "Epoch 00046: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1359 - acc: 0.9562 - val_loss: 0.2691 - val_acc: 0.9369\n",
      "Epoch 47/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9568\n",
      "Epoch 00047: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1327 - acc: 0.9568 - val_loss: 0.2680 - val_acc: 0.9352\n",
      "Epoch 48/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9572\n",
      "Epoch 00048: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.1326 - acc: 0.9573 - val_loss: 0.2769 - val_acc: 0.9380\n",
      "Epoch 49/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9597\n",
      "Epoch 00049: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1249 - acc: 0.9597 - val_loss: 0.2794 - val_acc: 0.9373\n",
      "Epoch 50/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9574\n",
      "Epoch 00050: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.1315 - acc: 0.9574 - val_loss: 0.2794 - val_acc: 0.9392\n",
      "Epoch 51/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9595\n",
      "Epoch 00051: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1241 - acc: 0.9595 - val_loss: 0.2745 - val_acc: 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9592\n",
      "Epoch 00052: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1238 - acc: 0.9593 - val_loss: 0.2685 - val_acc: 0.9411\n",
      "Epoch 53/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9596\n",
      "Epoch 00053: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.1251 - acc: 0.9595 - val_loss: 0.2740 - val_acc: 0.9401\n",
      "Epoch 54/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9598\n",
      "Epoch 00054: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.1208 - acc: 0.9598 - val_loss: 0.2771 - val_acc: 0.9390\n",
      "Epoch 55/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9618\n",
      "Epoch 00055: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.1177 - acc: 0.9619 - val_loss: 0.2782 - val_acc: 0.9399\n",
      "Epoch 56/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9616\n",
      "Epoch 00056: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1172 - acc: 0.9615 - val_loss: 0.2704 - val_acc: 0.9394\n",
      "Epoch 57/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9625\n",
      "Epoch 00057: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1158 - acc: 0.9624 - val_loss: 0.2775 - val_acc: 0.9401\n",
      "Epoch 58/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9619\n",
      "Epoch 00058: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1178 - acc: 0.9618 - val_loss: 0.2730 - val_acc: 0.9380\n",
      "Epoch 59/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9621\n",
      "Epoch 00059: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.1131 - acc: 0.9622 - val_loss: 0.2834 - val_acc: 0.9399\n",
      "Epoch 60/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9629\n",
      "Epoch 00060: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.1105 - acc: 0.9630 - val_loss: 0.2898 - val_acc: 0.9390\n",
      "Epoch 61/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9630\n",
      "Epoch 00061: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1121 - acc: 0.9630 - val_loss: 0.2837 - val_acc: 0.9390\n",
      "Epoch 62/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9644\n",
      "Epoch 00062: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1085 - acc: 0.9644 - val_loss: 0.2830 - val_acc: 0.9401\n",
      "Epoch 63/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9644\n",
      "Epoch 00063: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.1103 - acc: 0.9644 - val_loss: 0.2676 - val_acc: 0.9420\n",
      "Epoch 64/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9665\n",
      "Epoch 00064: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1044 - acc: 0.9665 - val_loss: 0.2844 - val_acc: 0.9413\n",
      "Epoch 65/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9649\n",
      "Epoch 00065: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1068 - acc: 0.9649 - val_loss: 0.2878 - val_acc: 0.9385\n",
      "Epoch 66/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9642\n",
      "Epoch 00066: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1053 - acc: 0.9642 - val_loss: 0.2807 - val_acc: 0.9397\n",
      "Epoch 67/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9660\n",
      "Epoch 00067: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1026 - acc: 0.9660 - val_loss: 0.2849 - val_acc: 0.9404\n",
      "Epoch 68/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9671\n",
      "Epoch 00068: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1010 - acc: 0.9672 - val_loss: 0.2760 - val_acc: 0.9406\n",
      "Epoch 69/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9654\n",
      "Epoch 00069: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.1041 - acc: 0.9654 - val_loss: 0.2853 - val_acc: 0.9390\n",
      "Epoch 70/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9651\n",
      "Epoch 00070: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.1049 - acc: 0.9651 - val_loss: 0.2880 - val_acc: 0.9399\n",
      "Epoch 71/200\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9666\n",
      "Epoch 00071: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0992 - acc: 0.9667 - val_loss: 0.2731 - val_acc: 0.9434\n",
      "Epoch 72/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9679\n",
      "Epoch 00072: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0998 - acc: 0.9678 - val_loss: 0.2841 - val_acc: 0.9378\n",
      "Epoch 73/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9676\n",
      "Epoch 00073: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0978 - acc: 0.9676 - val_loss: 0.2808 - val_acc: 0.9434\n",
      "Epoch 74/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9690\n",
      "Epoch 00074: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0936 - acc: 0.9690 - val_loss: 0.2901 - val_acc: 0.9411\n",
      "Epoch 75/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9685\n",
      "Epoch 00075: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0931 - acc: 0.9685 - val_loss: 0.2843 - val_acc: 0.9436\n",
      "Epoch 76/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9689\n",
      "Epoch 00076: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0940 - acc: 0.9689 - val_loss: 0.2912 - val_acc: 0.9408\n",
      "Epoch 77/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9699\n",
      "Epoch 00077: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0955 - acc: 0.9699 - val_loss: 0.2761 - val_acc: 0.9413\n",
      "Epoch 78/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9691\n",
      "Epoch 00078: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0949 - acc: 0.9690 - val_loss: 0.2805 - val_acc: 0.9434\n",
      "Epoch 79/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9708\n",
      "Epoch 00079: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0903 - acc: 0.9708 - val_loss: 0.2881 - val_acc: 0.9422\n",
      "Epoch 80/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9696\n",
      "Epoch 00080: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0930 - acc: 0.9696 - val_loss: 0.2788 - val_acc: 0.9427\n",
      "Epoch 81/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9690\n",
      "Epoch 00081: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0927 - acc: 0.9691 - val_loss: 0.2889 - val_acc: 0.9413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9680\n",
      "Epoch 00082: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0940 - acc: 0.9680 - val_loss: 0.2805 - val_acc: 0.9450\n",
      "Epoch 83/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9718\n",
      "Epoch 00083: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0878 - acc: 0.9718 - val_loss: 0.2821 - val_acc: 0.9427\n",
      "Epoch 84/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9702\n",
      "Epoch 00084: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0884 - acc: 0.9702 - val_loss: 0.2846 - val_acc: 0.9441\n",
      "Epoch 85/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9723\n",
      "Epoch 00085: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 285us/sample - loss: 0.0868 - acc: 0.9723 - val_loss: 0.2853 - val_acc: 0.9420\n",
      "Epoch 86/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9712\n",
      "Epoch 00086: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0871 - acc: 0.9710 - val_loss: 0.2755 - val_acc: 0.9434\n",
      "Epoch 87/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9717\n",
      "Epoch 00087: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0855 - acc: 0.9718 - val_loss: 0.2958 - val_acc: 0.9408\n",
      "Epoch 88/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9718\n",
      "Epoch 00088: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0862 - acc: 0.9717 - val_loss: 0.2926 - val_acc: 0.9443\n",
      "Epoch 89/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9712\n",
      "Epoch 00089: val_loss did not improve from 0.26507\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0889 - acc: 0.9712 - val_loss: 0.2758 - val_acc: 0.9429\n",
      "\n",
      "3 Only Conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 205us/sample - loss: 0.3258 - acc: 0.9136\n",
      "Loss: 0.3258422381286185 Accuracy: 0.9136033\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9698 - acc: 0.3703\n",
      "Epoch 00001: val_loss improved from inf to 1.32066, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/01-1.3207.hdf5\n",
      "36805/36805 [==============================] - 13s 367us/sample - loss: 1.9693 - acc: 0.3704 - val_loss: 1.3207 - val_acc: 0.6212\n",
      "Epoch 2/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2753 - acc: 0.6040\n",
      "Epoch 00002: val_loss improved from 1.32066 to 0.88193, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/02-0.8819.hdf5\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 1.2753 - acc: 0.6040 - val_loss: 0.8819 - val_acc: 0.7577\n",
      "Epoch 3/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9692 - acc: 0.7026\n",
      "Epoch 00003: val_loss improved from 0.88193 to 0.68556, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/03-0.6856.hdf5\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.9689 - acc: 0.7028 - val_loss: 0.6856 - val_acc: 0.8095\n",
      "Epoch 4/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8001 - acc: 0.7547\n",
      "Epoch 00004: val_loss improved from 0.68556 to 0.58725, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/04-0.5873.hdf5\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.8000 - acc: 0.7547 - val_loss: 0.5873 - val_acc: 0.8369\n",
      "Epoch 5/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.7883\n",
      "Epoch 00005: val_loss improved from 0.58725 to 0.51809, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/05-0.5181.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.6913 - acc: 0.7882 - val_loss: 0.5181 - val_acc: 0.8553\n",
      "Epoch 6/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6149 - acc: 0.8123\n",
      "Epoch 00006: val_loss improved from 0.51809 to 0.45681, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/06-0.4568.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.6149 - acc: 0.8122 - val_loss: 0.4568 - val_acc: 0.8714\n",
      "Epoch 7/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.8294\n",
      "Epoch 00007: val_loss improved from 0.45681 to 0.42404, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/07-0.4240.hdf5\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.5576 - acc: 0.8295 - val_loss: 0.4240 - val_acc: 0.8758\n",
      "Epoch 8/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.8415\n",
      "Epoch 00008: val_loss improved from 0.42404 to 0.38769, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/08-0.3877.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.5124 - acc: 0.8415 - val_loss: 0.3877 - val_acc: 0.8915\n",
      "Epoch 9/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.8539\n",
      "Epoch 00009: val_loss improved from 0.38769 to 0.36576, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/09-0.3658.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.4709 - acc: 0.8538 - val_loss: 0.3658 - val_acc: 0.8952\n",
      "Epoch 10/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4460 - acc: 0.8620\n",
      "Epoch 00010: val_loss improved from 0.36576 to 0.35402, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/10-0.3540.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.4461 - acc: 0.8619 - val_loss: 0.3540 - val_acc: 0.8996\n",
      "Epoch 11/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8714\n",
      "Epoch 00011: val_loss improved from 0.35402 to 0.33242, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/11-0.3324.hdf5\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.4143 - acc: 0.8714 - val_loss: 0.3324 - val_acc: 0.9071\n",
      "Epoch 12/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8772\n",
      "Epoch 00012: val_loss improved from 0.33242 to 0.32396, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/12-0.3240.hdf5\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.3893 - acc: 0.8771 - val_loss: 0.3240 - val_acc: 0.9075\n",
      "Epoch 13/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8838\n",
      "Epoch 00013: val_loss improved from 0.32396 to 0.31618, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/13-0.3162.hdf5\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.3742 - acc: 0.8838 - val_loss: 0.3162 - val_acc: 0.9101\n",
      "Epoch 14/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8899\n",
      "Epoch 00014: val_loss improved from 0.31618 to 0.29032, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/14-0.2903.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.3563 - acc: 0.8899 - val_loss: 0.2903 - val_acc: 0.9182\n",
      "Epoch 15/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3368 - acc: 0.8952\n",
      "Epoch 00015: val_loss improved from 0.29032 to 0.28333, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/15-0.2833.hdf5\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.3366 - acc: 0.8953 - val_loss: 0.2833 - val_acc: 0.9194\n",
      "Epoch 16/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3238 - acc: 0.8993\n",
      "Epoch 00016: val_loss improved from 0.28333 to 0.27242, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/16-0.2724.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.3241 - acc: 0.8993 - val_loss: 0.2724 - val_acc: 0.9201\n",
      "Epoch 17/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3166 - acc: 0.9001\n",
      "Epoch 00017: val_loss improved from 0.27242 to 0.26647, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/17-0.2665.hdf5\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.3166 - acc: 0.9001 - val_loss: 0.2665 - val_acc: 0.9234\n",
      "Epoch 18/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9065\n",
      "Epoch 00018: val_loss improved from 0.26647 to 0.26154, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/18-0.2615.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.2989 - acc: 0.9065 - val_loss: 0.2615 - val_acc: 0.9245\n",
      "Epoch 19/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9094\n",
      "Epoch 00019: val_loss improved from 0.26154 to 0.25632, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/19-0.2563.hdf5\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.2877 - acc: 0.9094 - val_loss: 0.2563 - val_acc: 0.9269\n",
      "Epoch 20/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9120\n",
      "Epoch 00020: val_loss improved from 0.25632 to 0.24638, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/20-0.2464.hdf5\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.2806 - acc: 0.9120 - val_loss: 0.2464 - val_acc: 0.9313\n",
      "Epoch 21/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9161\n",
      "Epoch 00021: val_loss improved from 0.24638 to 0.24428, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/21-0.2443.hdf5\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.2679 - acc: 0.9161 - val_loss: 0.2443 - val_acc: 0.9320\n",
      "Epoch 22/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9184\n",
      "Epoch 00022: val_loss improved from 0.24428 to 0.24357, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/22-0.2436.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.2608 - acc: 0.9184 - val_loss: 0.2436 - val_acc: 0.9338\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9217\n",
      "Epoch 00023: val_loss improved from 0.24357 to 0.23941, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/23-0.2394.hdf5\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.2469 - acc: 0.9217 - val_loss: 0.2394 - val_acc: 0.9320\n",
      "Epoch 24/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9229\n",
      "Epoch 00024: val_loss improved from 0.23941 to 0.23589, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/24-0.2359.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.2421 - acc: 0.9229 - val_loss: 0.2359 - val_acc: 0.9350\n",
      "Epoch 25/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9240\n",
      "Epoch 00025: val_loss improved from 0.23589 to 0.22804, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/25-0.2280.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.2381 - acc: 0.9240 - val_loss: 0.2280 - val_acc: 0.9383\n",
      "Epoch 26/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9263\n",
      "Epoch 00026: val_loss improved from 0.22804 to 0.22614, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/26-0.2261.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.2306 - acc: 0.9263 - val_loss: 0.2261 - val_acc: 0.9371\n",
      "Epoch 27/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9270\n",
      "Epoch 00027: val_loss improved from 0.22614 to 0.21951, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/27-0.2195.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.2246 - acc: 0.9270 - val_loss: 0.2195 - val_acc: 0.9411\n",
      "Epoch 28/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9289\n",
      "Epoch 00028: val_loss did not improve from 0.21951\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.2225 - acc: 0.9289 - val_loss: 0.2238 - val_acc: 0.9378\n",
      "Epoch 29/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9327\n",
      "Epoch 00029: val_loss did not improve from 0.21951\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.2126 - acc: 0.9328 - val_loss: 0.2228 - val_acc: 0.9394\n",
      "Epoch 30/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9333\n",
      "Epoch 00030: val_loss improved from 0.21951 to 0.21701, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/30-0.2170.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.2104 - acc: 0.9333 - val_loss: 0.2170 - val_acc: 0.9397\n",
      "Epoch 31/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9361\n",
      "Epoch 00031: val_loss improved from 0.21701 to 0.21048, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/31-0.2105.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.2016 - acc: 0.9361 - val_loss: 0.2105 - val_acc: 0.9408\n",
      "Epoch 32/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9377\n",
      "Epoch 00032: val_loss improved from 0.21048 to 0.20908, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/32-0.2091.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1946 - acc: 0.9377 - val_loss: 0.2091 - val_acc: 0.9441\n",
      "Epoch 33/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9375\n",
      "Epoch 00033: val_loss did not improve from 0.20908\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1943 - acc: 0.9375 - val_loss: 0.2137 - val_acc: 0.9432\n",
      "Epoch 34/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9386\n",
      "Epoch 00034: val_loss did not improve from 0.20908\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1914 - acc: 0.9386 - val_loss: 0.2129 - val_acc: 0.9422\n",
      "Epoch 35/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9393\n",
      "Epoch 00035: val_loss improved from 0.20908 to 0.20704, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/35-0.2070.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1865 - acc: 0.9393 - val_loss: 0.2070 - val_acc: 0.9439\n",
      "Epoch 36/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9407\n",
      "Epoch 00036: val_loss improved from 0.20704 to 0.20159, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/36-0.2016.hdf5\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.1817 - acc: 0.9407 - val_loss: 0.2016 - val_acc: 0.9446\n",
      "Epoch 37/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9415\n",
      "Epoch 00037: val_loss improved from 0.20159 to 0.19991, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/37-0.1999.hdf5\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1813 - acc: 0.9416 - val_loss: 0.1999 - val_acc: 0.9464\n",
      "Epoch 38/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9434\n",
      "Epoch 00038: val_loss did not improve from 0.19991\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1747 - acc: 0.9434 - val_loss: 0.2058 - val_acc: 0.9446\n",
      "Epoch 39/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9451\n",
      "Epoch 00039: val_loss did not improve from 0.19991\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1693 - acc: 0.9451 - val_loss: 0.2087 - val_acc: 0.9469\n",
      "Epoch 40/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9463\n",
      "Epoch 00040: val_loss did not improve from 0.19991\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1666 - acc: 0.9463 - val_loss: 0.2003 - val_acc: 0.9441\n",
      "Epoch 41/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9470\n",
      "Epoch 00041: val_loss improved from 0.19991 to 0.19577, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/41-0.1958.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1625 - acc: 0.9470 - val_loss: 0.1958 - val_acc: 0.9450\n",
      "Epoch 42/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9480\n",
      "Epoch 00042: val_loss improved from 0.19577 to 0.19501, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/42-0.1950.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1589 - acc: 0.9479 - val_loss: 0.1950 - val_acc: 0.9460\n",
      "Epoch 43/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9469\n",
      "Epoch 00043: val_loss did not improve from 0.19501\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1593 - acc: 0.9469 - val_loss: 0.2073 - val_acc: 0.9420\n",
      "Epoch 44/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9488\n",
      "Epoch 00044: val_loss improved from 0.19501 to 0.19432, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/44-0.1943.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1556 - acc: 0.9487 - val_loss: 0.1943 - val_acc: 0.9460\n",
      "Epoch 45/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9451\n",
      "Epoch 00045: val_loss did not improve from 0.19432\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1648 - acc: 0.9450 - val_loss: 0.2013 - val_acc: 0.9448\n",
      "Epoch 46/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9528\n",
      "Epoch 00046: val_loss did not improve from 0.19432\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1471 - acc: 0.9528 - val_loss: 0.2017 - val_acc: 0.9462\n",
      "Epoch 47/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9511\n",
      "Epoch 00047: val_loss improved from 0.19432 to 0.19354, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/47-0.1935.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1514 - acc: 0.9510 - val_loss: 0.1935 - val_acc: 0.9492\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9514\n",
      "Epoch 00048: val_loss did not improve from 0.19354\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1480 - acc: 0.9514 - val_loss: 0.1994 - val_acc: 0.9464\n",
      "Epoch 49/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9533\n",
      "Epoch 00049: val_loss improved from 0.19354 to 0.19018, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/49-0.1902.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1408 - acc: 0.9533 - val_loss: 0.1902 - val_acc: 0.9481\n",
      "Epoch 50/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9529\n",
      "Epoch 00050: val_loss did not improve from 0.19018\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1413 - acc: 0.9529 - val_loss: 0.1925 - val_acc: 0.9490\n",
      "Epoch 51/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9544\n",
      "Epoch 00051: val_loss did not improve from 0.19018\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1383 - acc: 0.9544 - val_loss: 0.1939 - val_acc: 0.9495\n",
      "Epoch 52/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9548\n",
      "Epoch 00052: val_loss did not improve from 0.19018\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1379 - acc: 0.9548 - val_loss: 0.2010 - val_acc: 0.9471\n",
      "Epoch 53/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9563\n",
      "Epoch 00053: val_loss did not improve from 0.19018\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1365 - acc: 0.9563 - val_loss: 0.1943 - val_acc: 0.9476\n",
      "Epoch 54/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9552\n",
      "Epoch 00054: val_loss did not improve from 0.19018\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1349 - acc: 0.9553 - val_loss: 0.1912 - val_acc: 0.9495\n",
      "Epoch 55/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9580\n",
      "Epoch 00055: val_loss improved from 0.19018 to 0.17850, saving model to model/checkpoint/2D_CNN_4_only_conv_checkpoint/55-0.1785.hdf5\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1276 - acc: 0.9580 - val_loss: 0.1785 - val_acc: 0.9497\n",
      "Epoch 56/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9574\n",
      "Epoch 00056: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1287 - acc: 0.9574 - val_loss: 0.2001 - val_acc: 0.9525\n",
      "Epoch 57/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9583\n",
      "Epoch 00057: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.1255 - acc: 0.9583 - val_loss: 0.1916 - val_acc: 0.9492\n",
      "Epoch 58/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9585\n",
      "Epoch 00058: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1239 - acc: 0.9585 - val_loss: 0.1970 - val_acc: 0.9522\n",
      "Epoch 59/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9600\n",
      "Epoch 00059: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1243 - acc: 0.9599 - val_loss: 0.1907 - val_acc: 0.9506\n",
      "Epoch 60/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9617\n",
      "Epoch 00060: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1185 - acc: 0.9617 - val_loss: 0.1937 - val_acc: 0.9509\n",
      "Epoch 61/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9609\n",
      "Epoch 00061: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1208 - acc: 0.9609 - val_loss: 0.1814 - val_acc: 0.9509\n",
      "Epoch 62/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9602\n",
      "Epoch 00062: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1209 - acc: 0.9602 - val_loss: 0.1928 - val_acc: 0.9513\n",
      "Epoch 63/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9617\n",
      "Epoch 00063: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1158 - acc: 0.9617 - val_loss: 0.1857 - val_acc: 0.9529\n",
      "Epoch 64/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9619\n",
      "Epoch 00064: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1163 - acc: 0.9619 - val_loss: 0.1997 - val_acc: 0.9478\n",
      "Epoch 65/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9616\n",
      "Epoch 00065: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1141 - acc: 0.9617 - val_loss: 0.1850 - val_acc: 0.9529\n",
      "Epoch 66/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9618\n",
      "Epoch 00066: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1154 - acc: 0.9618 - val_loss: 0.1890 - val_acc: 0.9502\n",
      "Epoch 67/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9635\n",
      "Epoch 00067: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1127 - acc: 0.9636 - val_loss: 0.1896 - val_acc: 0.9522\n",
      "Epoch 68/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9635\n",
      "Epoch 00068: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1104 - acc: 0.9635 - val_loss: 0.2102 - val_acc: 0.9471\n",
      "Epoch 69/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9650\n",
      "Epoch 00069: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1096 - acc: 0.9650 - val_loss: 0.1986 - val_acc: 0.9488\n",
      "Epoch 70/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9640\n",
      "Epoch 00070: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1072 - acc: 0.9641 - val_loss: 0.1870 - val_acc: 0.9511\n",
      "Epoch 71/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9640\n",
      "Epoch 00071: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1100 - acc: 0.9640 - val_loss: 0.1945 - val_acc: 0.9511\n",
      "Epoch 72/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9627\n",
      "Epoch 00072: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1081 - acc: 0.9626 - val_loss: 0.1814 - val_acc: 0.9518\n",
      "Epoch 73/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9640\n",
      "Epoch 00073: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.1113 - acc: 0.9641 - val_loss: 0.1930 - val_acc: 0.9522\n",
      "Epoch 74/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9662\n",
      "Epoch 00074: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1044 - acc: 0.9661 - val_loss: 0.1895 - val_acc: 0.9525\n",
      "Epoch 75/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9660\n",
      "Epoch 00075: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.1018 - acc: 0.9660 - val_loss: 0.1904 - val_acc: 0.9543\n",
      "Epoch 76/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9675\n",
      "Epoch 00076: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.0991 - acc: 0.9676 - val_loss: 0.2009 - val_acc: 0.9555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9659\n",
      "Epoch 00077: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1027 - acc: 0.9659 - val_loss: 0.1990 - val_acc: 0.9548\n",
      "Epoch 78/200\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9649\n",
      "Epoch 00078: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1054 - acc: 0.9649 - val_loss: 0.2073 - val_acc: 0.9504\n",
      "Epoch 79/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9679\n",
      "Epoch 00079: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0982 - acc: 0.9678 - val_loss: 0.1911 - val_acc: 0.9525\n",
      "Epoch 80/200\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9690\n",
      "Epoch 00080: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0949 - acc: 0.9690 - val_loss: 0.1960 - val_acc: 0.9518\n",
      "Epoch 81/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9697\n",
      "Epoch 00081: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0921 - acc: 0.9697 - val_loss: 0.1978 - val_acc: 0.9522\n",
      "Epoch 82/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9682\n",
      "Epoch 00082: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.0958 - acc: 0.9682 - val_loss: 0.1858 - val_acc: 0.9520\n",
      "Epoch 83/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9690\n",
      "Epoch 00083: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 321us/sample - loss: 0.0945 - acc: 0.9690 - val_loss: 0.1997 - val_acc: 0.9525\n",
      "Epoch 84/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9678\n",
      "Epoch 00084: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.0942 - acc: 0.9677 - val_loss: 0.2041 - val_acc: 0.9529\n",
      "Epoch 85/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9690\n",
      "Epoch 00085: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.0944 - acc: 0.9690 - val_loss: 0.2064 - val_acc: 0.9546\n",
      "Epoch 86/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9691\n",
      "Epoch 00086: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.0928 - acc: 0.9691 - val_loss: 0.2045 - val_acc: 0.9548\n",
      "Epoch 87/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9708\n",
      "Epoch 00087: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.0892 - acc: 0.9708 - val_loss: 0.2037 - val_acc: 0.9557\n",
      "Epoch 88/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9687\n",
      "Epoch 00088: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.0922 - acc: 0.9686 - val_loss: 0.1999 - val_acc: 0.9539\n",
      "Epoch 89/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9714\n",
      "Epoch 00089: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0884 - acc: 0.9714 - val_loss: 0.1985 - val_acc: 0.9541\n",
      "Epoch 90/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9698\n",
      "Epoch 00090: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0923 - acc: 0.9698 - val_loss: 0.2057 - val_acc: 0.9546\n",
      "Epoch 91/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9718\n",
      "Epoch 00091: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.0853 - acc: 0.9718 - val_loss: 0.2004 - val_acc: 0.9541\n",
      "Epoch 92/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9708\n",
      "Epoch 00092: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0873 - acc: 0.9708 - val_loss: 0.1989 - val_acc: 0.9562\n",
      "Epoch 93/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9710\n",
      "Epoch 00093: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0868 - acc: 0.9711 - val_loss: 0.2213 - val_acc: 0.9541\n",
      "Epoch 94/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9711\n",
      "Epoch 00094: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.0863 - acc: 0.9710 - val_loss: 0.2071 - val_acc: 0.9562\n",
      "Epoch 95/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9728\n",
      "Epoch 00095: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.0814 - acc: 0.9728 - val_loss: 0.2141 - val_acc: 0.9518\n",
      "Epoch 96/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9722\n",
      "Epoch 00096: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0853 - acc: 0.9722 - val_loss: 0.1987 - val_acc: 0.9527\n",
      "Epoch 97/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9737\n",
      "Epoch 00097: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.0803 - acc: 0.9737 - val_loss: 0.2040 - val_acc: 0.9550\n",
      "Epoch 98/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9720\n",
      "Epoch 00098: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 317us/sample - loss: 0.0827 - acc: 0.9721 - val_loss: 0.2102 - val_acc: 0.9539\n",
      "Epoch 99/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9727\n",
      "Epoch 00099: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0836 - acc: 0.9727 - val_loss: 0.1915 - val_acc: 0.9532\n",
      "Epoch 100/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9717\n",
      "Epoch 00100: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 319us/sample - loss: 0.0854 - acc: 0.9716 - val_loss: 0.1945 - val_acc: 0.9555\n",
      "Epoch 101/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9750\n",
      "Epoch 00101: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 0.0766 - acc: 0.9750 - val_loss: 0.2108 - val_acc: 0.9532\n",
      "Epoch 102/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9729\n",
      "Epoch 00102: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 320us/sample - loss: 0.0828 - acc: 0.9729 - val_loss: 0.2078 - val_acc: 0.9543\n",
      "Epoch 103/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9729\n",
      "Epoch 00103: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.0782 - acc: 0.9730 - val_loss: 0.2152 - val_acc: 0.9513\n",
      "Epoch 104/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9735\n",
      "Epoch 00104: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 316us/sample - loss: 0.0796 - acc: 0.9735 - val_loss: 0.1934 - val_acc: 0.9564\n",
      "Epoch 105/200\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9749\n",
      "Epoch 00105: val_loss did not improve from 0.17850\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.0770 - acc: 0.9749 - val_loss: 0.2091 - val_acc: 0.9522\n",
      "\n",
      "4 Only Conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 270us/sample - loss: 0.2652 - acc: 0.9313\n",
      "Loss: 0.2652070475825392 Accuracy: 0.9312565\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 5 from 3 for 'conv2d_24/Conv2D' (op: 'Conv2D') with input shapes: [?,13,3,64], [5,5,64,128].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 5 from 3 for 'conv2d_24/Conv2D' (op: 'Conv2D') with input shapes: [?,13,3,64], [5,5,64,128].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-270260656be9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_2d_cnn_only_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#         model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n\u001b[1;32m      5\u001b[0m           metrics=['accuracy'])\n",
      "\u001b[0;32m<ipython-input-10-c91112ddacb1>\u001b[0m in \u001b[0;36mbuild_2d_cnn_only_conv\u001b[0;34m(conv_num)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_num\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         model.add(Conv2D (kernel_size=5, filters=8*(2**(i+1)), strides=(1,1), padding='valid', \n\u001b[0;32m---> 10\u001b[0;31m                           activation='relu'))\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#         model.add(BatchNormalization())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# In graph mode, failure to build the layer's graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;31m# implies a user-side bug. We don't catch exceptions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         name=self.name)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;34m\"Conv2D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m   1027\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1823\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 5 from 3 for 'conv2d_24/Conv2D' (op: 'Conv2D') with input shapes: [?,13,3,64], [5,5,64,128]."
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    model_name = '2D_CNN_{}_only_conv_DO'.format(i)\n",
    "    model = build_2d_cnn_only_conv(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_2d_norm, y_train_onehot, batch_size=64, epochs=200, \n",
    "                     validation_data=[x_val_2d_norm, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print('{} Only Conv Model'.format(i))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_2d_norm, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2D_CNN_1_only_conv_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_78 (Conv2D)           (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_75 (MaxPooling (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 48768)             0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 48768)             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 16)                780304    \n",
      "=================================================================\n",
      "Total params: 780,512\n",
      "Trainable params: 780,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 280us/sample - loss: 1.1291 - acc: 0.7078\n",
      "Loss: 1.129059188734829 Accuracy: 0.70778817\n",
      "\n",
      "2D_CNN_2_only_conv_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 21824)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 21824)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                349200    \n",
      "=================================================================\n",
      "Total params: 352,624\n",
      "Trainable params: 352,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 266us/sample - loss: 0.5591 - acc: 0.8623\n",
      "Loss: 0.5590534259968457 Accuracy: 0.8623053\n",
      "\n",
      "2D_CNN_3_only_conv_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 58, 18, 32)        12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 29, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 8352)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8352)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                133648    \n",
      "=================================================================\n",
      "Total params: 149,904\n",
      "Trainable params: 149,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 235us/sample - loss: 0.3258 - acc: 0.9136\n",
      "Loss: 0.3258422381286185 Accuracy: 0.9136033\n",
      "\n",
      "2D_CNN_4_only_conv_DO Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 253, 95, 8)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 127, 48, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 123, 44, 16)       3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 62, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 58, 18, 32)        12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 29, 9, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 25, 5, 64)         51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 13, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2496)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2496)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                39952     \n",
      "=================================================================\n",
      "Total params: 107,472\n",
      "Trainable params: 107,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 254us/sample - loss: 0.2652 - acc: 0.9313\n",
      "Loss: 0.2652070475825392 Accuracy: 0.9312565\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    model_name = '2D_CNN_{}_only_conv_DO'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "#         model = build_cnn(conv_num=i, fcn_num=j)\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "#         model_filename = model_path + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_2d_norm, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
