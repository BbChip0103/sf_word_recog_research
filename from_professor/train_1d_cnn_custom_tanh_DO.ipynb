{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=64, strides=1, padding='same', \n",
    "                      activation='tanh', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='tanh'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.7442 - acc: 0.1056\n",
      "Epoch 00001: val_loss improved from inf to 2.73636, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_1_conv_checkpoint/001-2.7364.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 2.7439 - acc: 0.1058 - val_loss: 2.7364 - val_acc: 0.0948\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4984 - acc: 0.2538\n",
      "Epoch 00002: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 2.4984 - acc: 0.2537 - val_loss: 2.8121 - val_acc: 0.1088\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2571 - acc: 0.3409\n",
      "Epoch 00003: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 2.2575 - acc: 0.3408 - val_loss: 2.9320 - val_acc: 0.1144\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0482 - acc: 0.4074\n",
      "Epoch 00004: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 2.0483 - acc: 0.4074 - val_loss: 3.0583 - val_acc: 0.1132\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8809 - acc: 0.4601\n",
      "Epoch 00005: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.8811 - acc: 0.4600 - val_loss: 3.2117 - val_acc: 0.1127\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7459 - acc: 0.4994\n",
      "Epoch 00006: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.7459 - acc: 0.4994 - val_loss: 3.3545 - val_acc: 0.1132\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6343 - acc: 0.5340\n",
      "Epoch 00007: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 1.6341 - acc: 0.5340 - val_loss: 3.5052 - val_acc: 0.1125\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5372 - acc: 0.5618\n",
      "Epoch 00008: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.5374 - acc: 0.5618 - val_loss: 3.6275 - val_acc: 0.1193\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4525 - acc: 0.5865\n",
      "Epoch 00009: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.4526 - acc: 0.5863 - val_loss: 3.7907 - val_acc: 0.1165\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3758 - acc: 0.6105\n",
      "Epoch 00010: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.3759 - acc: 0.6104 - val_loss: 3.9360 - val_acc: 0.1167\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3074 - acc: 0.6312\n",
      "Epoch 00011: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 1.3074 - acc: 0.6312 - val_loss: 4.0806 - val_acc: 0.1130\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2485 - acc: 0.6485\n",
      "Epoch 00012: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 1.2485 - acc: 0.6485 - val_loss: 4.2288 - val_acc: 0.1130\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1912 - acc: 0.6649\n",
      "Epoch 00013: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.1912 - acc: 0.6649 - val_loss: 4.3731 - val_acc: 0.1113\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1392 - acc: 0.6820\n",
      "Epoch 00014: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.1391 - acc: 0.6821 - val_loss: 4.5009 - val_acc: 0.1148\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0896 - acc: 0.6979\n",
      "Epoch 00015: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.0897 - acc: 0.6978 - val_loss: 4.6654 - val_acc: 0.1118\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0470 - acc: 0.7101\n",
      "Epoch 00016: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.0470 - acc: 0.7101 - val_loss: 4.8064 - val_acc: 0.1102\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0058 - acc: 0.7235\n",
      "Epoch 00017: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.0057 - acc: 0.7236 - val_loss: 4.9407 - val_acc: 0.1104\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9680 - acc: 0.7340\n",
      "Epoch 00018: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.9680 - acc: 0.7340 - val_loss: 5.1025 - val_acc: 0.1092\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9315 - acc: 0.7453\n",
      "Epoch 00019: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.9314 - acc: 0.7453 - val_loss: 5.2183 - val_acc: 0.1072\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9007 - acc: 0.7537\n",
      "Epoch 00020: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.9008 - acc: 0.7537 - val_loss: 5.3486 - val_acc: 0.1062\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8685 - acc: 0.7605\n",
      "Epoch 00021: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.8684 - acc: 0.7605 - val_loss: 5.4711 - val_acc: 0.1104\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8378 - acc: 0.7723\n",
      "Epoch 00022: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.8379 - acc: 0.7722 - val_loss: 5.6173 - val_acc: 0.1092\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8115 - acc: 0.7788\n",
      "Epoch 00023: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.8118 - acc: 0.7787 - val_loss: 5.7334 - val_acc: 0.1095\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7825 - acc: 0.7883\n",
      "Epoch 00024: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.7827 - acc: 0.7883 - val_loss: 5.8630 - val_acc: 0.1076\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7603 - acc: 0.7941\n",
      "Epoch 00025: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.7602 - acc: 0.7941 - val_loss: 5.9707 - val_acc: 0.1092\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7376 - acc: 0.7980\n",
      "Epoch 00026: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.7376 - acc: 0.7980 - val_loss: 6.0907 - val_acc: 0.1111\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7136 - acc: 0.8087\n",
      "Epoch 00027: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.7136 - acc: 0.8087 - val_loss: 6.1976 - val_acc: 0.1081\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6944 - acc: 0.8122\n",
      "Epoch 00028: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.6944 - acc: 0.8122 - val_loss: 6.3191 - val_acc: 0.1072\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6720 - acc: 0.8191\n",
      "Epoch 00029: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.6721 - acc: 0.8190 - val_loss: 6.4180 - val_acc: 0.1085\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6545 - acc: 0.8241\n",
      "Epoch 00030: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.6546 - acc: 0.8241 - val_loss: 6.5308 - val_acc: 0.1081\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6373 - acc: 0.8273\n",
      "Epoch 00031: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.6373 - acc: 0.8273 - val_loss: 6.6096 - val_acc: 0.1060\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6206 - acc: 0.8329\n",
      "Epoch 00032: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.6206 - acc: 0.8329 - val_loss: 6.7140 - val_acc: 0.1065\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6011 - acc: 0.8387\n",
      "Epoch 00033: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.6011 - acc: 0.8387 - val_loss: 6.8293 - val_acc: 0.1039\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5877 - acc: 0.8434\n",
      "Epoch 00034: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.5878 - acc: 0.8433 - val_loss: 6.9083 - val_acc: 0.1037\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.8468\n",
      "Epoch 00035: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.5743 - acc: 0.8466 - val_loss: 7.0166 - val_acc: 0.1046\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5581 - acc: 0.8518\n",
      "Epoch 00036: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.5582 - acc: 0.8518 - val_loss: 7.0883 - val_acc: 0.1030\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.8564\n",
      "Epoch 00037: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.5412 - acc: 0.8564 - val_loss: 7.1780 - val_acc: 0.1013\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5293 - acc: 0.8600\n",
      "Epoch 00038: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.5293 - acc: 0.8600 - val_loss: 7.2575 - val_acc: 0.1041\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.8620\n",
      "Epoch 00039: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.5181 - acc: 0.8619 - val_loss: 7.3530 - val_acc: 0.1013\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5062 - acc: 0.8648\n",
      "Epoch 00040: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.5062 - acc: 0.8648 - val_loss: 7.4110 - val_acc: 0.1027\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4925 - acc: 0.8695\n",
      "Epoch 00041: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.4929 - acc: 0.8693 - val_loss: 7.4928 - val_acc: 0.1027\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4814 - acc: 0.8736\n",
      "Epoch 00042: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4814 - acc: 0.8736 - val_loss: 7.5815 - val_acc: 0.1016\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4727 - acc: 0.8726\n",
      "Epoch 00043: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.4727 - acc: 0.8726 - val_loss: 7.6659 - val_acc: 0.1011\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4616 - acc: 0.8781\n",
      "Epoch 00044: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.4616 - acc: 0.8781 - val_loss: 7.7212 - val_acc: 0.1002\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8812\n",
      "Epoch 00045: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.4499 - acc: 0.8812 - val_loss: 7.8046 - val_acc: 0.1020\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4412 - acc: 0.8834\n",
      "Epoch 00046: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.4413 - acc: 0.8834 - val_loss: 7.8799 - val_acc: 0.0992\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.8846\n",
      "Epoch 00047: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4331 - acc: 0.8847 - val_loss: 7.9254 - val_acc: 0.1006\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4255 - acc: 0.8872\n",
      "Epoch 00048: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4255 - acc: 0.8871 - val_loss: 7.9974 - val_acc: 0.1034\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.8917\n",
      "Epoch 00049: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.4110 - acc: 0.8917 - val_loss: 8.0553 - val_acc: 0.1018\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8937\n",
      "Epoch 00050: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4028 - acc: 0.8937 - val_loss: 8.1220 - val_acc: 0.1032\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8958\n",
      "Epoch 00051: val_loss did not improve from 2.73636\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.3985 - acc: 0.8958 - val_loss: 8.1780 - val_acc: 0.1032\n",
      "\n",
      "1D_CNN_custom_tanh_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 302us/sample - loss: 2.7258 - acc: 0.0993\n",
      "Loss: 2.7257815188707966 Accuracy: 0.09927311\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3412 - acc: 0.2642\n",
      "Epoch 00001: val_loss improved from inf to 2.12429, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_2_conv_checkpoint/001-2.1243.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 2.3413 - acc: 0.2642 - val_loss: 2.1243 - val_acc: 0.3387\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8159 - acc: 0.4388\n",
      "Epoch 00002: val_loss improved from 2.12429 to 2.06053, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_2_conv_checkpoint/002-2.0605.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.8158 - acc: 0.4389 - val_loss: 2.0605 - val_acc: 0.3524\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5304 - acc: 0.5284\n",
      "Epoch 00003: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.5303 - acc: 0.5285 - val_loss: 2.0818 - val_acc: 0.3508\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3254 - acc: 0.5929\n",
      "Epoch 00004: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.3254 - acc: 0.5929 - val_loss: 2.1609 - val_acc: 0.3566\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1667 - acc: 0.6397\n",
      "Epoch 00005: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.1667 - acc: 0.6397 - val_loss: 2.2439 - val_acc: 0.3585\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0398 - acc: 0.6791\n",
      "Epoch 00006: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.0397 - acc: 0.6792 - val_loss: 2.3694 - val_acc: 0.3461\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9271 - acc: 0.7139\n",
      "Epoch 00007: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.9271 - acc: 0.7139 - val_loss: 2.4687 - val_acc: 0.3587\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8359 - acc: 0.7430\n",
      "Epoch 00008: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8359 - acc: 0.7430 - val_loss: 2.5792 - val_acc: 0.3506\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7518 - acc: 0.7697\n",
      "Epoch 00009: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7517 - acc: 0.7697 - val_loss: 2.6839 - val_acc: 0.3536\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6800 - acc: 0.7927\n",
      "Epoch 00010: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6799 - acc: 0.7927 - val_loss: 2.8030 - val_acc: 0.3457\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.8096\n",
      "Epoch 00011: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6223 - acc: 0.8096 - val_loss: 2.9260 - val_acc: 0.3440\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5692 - acc: 0.8275\n",
      "Epoch 00012: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5692 - acc: 0.8275 - val_loss: 3.0327 - val_acc: 0.3473\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.8418\n",
      "Epoch 00013: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5218 - acc: 0.8418 - val_loss: 3.1260 - val_acc: 0.3394\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8572\n",
      "Epoch 00014: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4789 - acc: 0.8572 - val_loss: 3.2698 - val_acc: 0.3475\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.8588\n",
      "Epoch 00015: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4522 - acc: 0.8589 - val_loss: 3.3393 - val_acc: 0.3433\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8738\n",
      "Epoch 00016: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4132 - acc: 0.8737 - val_loss: 3.4551 - val_acc: 0.3396\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8801\n",
      "Epoch 00017: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3908 - acc: 0.8801 - val_loss: 3.5236 - val_acc: 0.3424\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8892\n",
      "Epoch 00018: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3613 - acc: 0.8892 - val_loss: 3.6455 - val_acc: 0.3443\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8942\n",
      "Epoch 00019: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3405 - acc: 0.8942 - val_loss: 3.7405 - val_acc: 0.3506\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.9030\n",
      "Epoch 00020: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3197 - acc: 0.9030 - val_loss: 3.7907 - val_acc: 0.3478\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9090\n",
      "Epoch 00021: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2985 - acc: 0.9090 - val_loss: 3.8704 - val_acc: 0.3440\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9128\n",
      "Epoch 00022: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2859 - acc: 0.9128 - val_loss: 3.9693 - val_acc: 0.3445\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9156\n",
      "Epoch 00023: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2741 - acc: 0.9156 - val_loss: 4.1346 - val_acc: 0.3436\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9218\n",
      "Epoch 00024: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2552 - acc: 0.9218 - val_loss: 4.0909 - val_acc: 0.3431\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2522 - acc: 0.9212\n",
      "Epoch 00025: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2522 - acc: 0.9212 - val_loss: 4.1563 - val_acc: 0.3496\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9276\n",
      "Epoch 00026: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2359 - acc: 0.9276 - val_loss: 4.2462 - val_acc: 0.3403\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9315\n",
      "Epoch 00027: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2220 - acc: 0.9314 - val_loss: 4.2816 - val_acc: 0.3482\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9322\n",
      "Epoch 00028: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2184 - acc: 0.9322 - val_loss: 4.3363 - val_acc: 0.3429\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9348\n",
      "Epoch 00029: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2072 - acc: 0.9348 - val_loss: 4.3875 - val_acc: 0.3422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9372\n",
      "Epoch 00030: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2019 - acc: 0.9372 - val_loss: 4.4486 - val_acc: 0.3454\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9392\n",
      "Epoch 00031: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1932 - acc: 0.9391 - val_loss: 4.4882 - val_acc: 0.3494\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9389\n",
      "Epoch 00032: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1935 - acc: 0.9389 - val_loss: 4.5684 - val_acc: 0.3494\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9433\n",
      "Epoch 00033: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1855 - acc: 0.9433 - val_loss: 4.6002 - val_acc: 0.3536\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9438\n",
      "Epoch 00034: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1820 - acc: 0.9438 - val_loss: 4.6789 - val_acc: 0.3522\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9470\n",
      "Epoch 00035: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1742 - acc: 0.9470 - val_loss: 4.6958 - val_acc: 0.3501\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9472\n",
      "Epoch 00036: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1697 - acc: 0.9472 - val_loss: 4.7415 - val_acc: 0.3569\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9501\n",
      "Epoch 00037: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1615 - acc: 0.9501 - val_loss: 4.7463 - val_acc: 0.3627\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9518\n",
      "Epoch 00038: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1569 - acc: 0.9518 - val_loss: 4.8010 - val_acc: 0.3510\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9501\n",
      "Epoch 00039: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1608 - acc: 0.9501 - val_loss: 4.8750 - val_acc: 0.3520\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9490\n",
      "Epoch 00040: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1617 - acc: 0.9490 - val_loss: 4.9185 - val_acc: 0.3508\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9533\n",
      "Epoch 00041: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1490 - acc: 0.9533 - val_loss: 4.9375 - val_acc: 0.3517\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9555\n",
      "Epoch 00042: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1441 - acc: 0.9555 - val_loss: 5.0530 - val_acc: 0.3604\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9562\n",
      "Epoch 00043: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1449 - acc: 0.9562 - val_loss: 5.1021 - val_acc: 0.3555\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9551\n",
      "Epoch 00044: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1452 - acc: 0.9551 - val_loss: 5.0373 - val_acc: 0.3531\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9565\n",
      "Epoch 00045: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1372 - acc: 0.9564 - val_loss: 5.1222 - val_acc: 0.3524\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9571\n",
      "Epoch 00046: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1349 - acc: 0.9571 - val_loss: 5.1029 - val_acc: 0.3555\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9586\n",
      "Epoch 00047: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1339 - acc: 0.9586 - val_loss: 5.2424 - val_acc: 0.3576\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9607\n",
      "Epoch 00048: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1242 - acc: 0.9607 - val_loss: 5.1851 - val_acc: 0.3611\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9594\n",
      "Epoch 00049: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1297 - acc: 0.9594 - val_loss: 5.1932 - val_acc: 0.3625\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9621\n",
      "Epoch 00050: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1263 - acc: 0.9622 - val_loss: 5.2359 - val_acc: 0.3545\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9622\n",
      "Epoch 00051: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1220 - acc: 0.9622 - val_loss: 5.2424 - val_acc: 0.3631\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9614\n",
      "Epoch 00052: val_loss did not improve from 2.06053\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1241 - acc: 0.9614 - val_loss: 5.2437 - val_acc: 0.3643\n",
      "\n",
      "1D_CNN_custom_tanh_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 498us/sample - loss: 2.0970 - acc: 0.3396\n",
      "Loss: 2.096959860401609 Accuracy: 0.33956388\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0410 - acc: 0.3536\n",
      "Epoch 00001: val_loss improved from inf to 1.65322, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_3_conv_checkpoint/001-1.6532.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 2.0410 - acc: 0.3536 - val_loss: 1.6532 - val_acc: 0.4952\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4554 - acc: 0.5478\n",
      "Epoch 00002: val_loss improved from 1.65322 to 1.54845, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_3_conv_checkpoint/002-1.5484.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.4554 - acc: 0.5478 - val_loss: 1.5484 - val_acc: 0.5206\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2355 - acc: 0.6172\n",
      "Epoch 00003: val_loss improved from 1.54845 to 1.50752, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_3_conv_checkpoint/003-1.5075.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.2354 - acc: 0.6172 - val_loss: 1.5075 - val_acc: 0.5330\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0806 - acc: 0.6632\n",
      "Epoch 00004: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0805 - acc: 0.6632 - val_loss: 1.5354 - val_acc: 0.5292\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9618 - acc: 0.7045\n",
      "Epoch 00005: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9618 - acc: 0.7045 - val_loss: 1.5534 - val_acc: 0.5311\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8498 - acc: 0.7362\n",
      "Epoch 00006: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8498 - acc: 0.7361 - val_loss: 1.5964 - val_acc: 0.5386\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7605 - acc: 0.7663\n",
      "Epoch 00007: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7605 - acc: 0.7663 - val_loss: 1.5972 - val_acc: 0.5469\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6758 - acc: 0.7942\n",
      "Epoch 00008: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6758 - acc: 0.7942 - val_loss: 1.6530 - val_acc: 0.5418\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.8160\n",
      "Epoch 00009: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6016 - acc: 0.8160 - val_loss: 1.6422 - val_acc: 0.5558\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8365\n",
      "Epoch 00010: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5374 - acc: 0.8365 - val_loss: 1.7002 - val_acc: 0.5490\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4843 - acc: 0.8520\n",
      "Epoch 00011: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4843 - acc: 0.8520 - val_loss: 1.7033 - val_acc: 0.5518\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8734\n",
      "Epoch 00012: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4235 - acc: 0.8734 - val_loss: 1.7300 - val_acc: 0.5609\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8863\n",
      "Epoch 00013: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3860 - acc: 0.8863 - val_loss: 1.7728 - val_acc: 0.5705\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8967\n",
      "Epoch 00014: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3501 - acc: 0.8966 - val_loss: 1.7818 - val_acc: 0.5707\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3200 - acc: 0.9038\n",
      "Epoch 00015: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3200 - acc: 0.9038 - val_loss: 1.8317 - val_acc: 0.5777\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9123\n",
      "Epoch 00016: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2935 - acc: 0.9122 - val_loss: 1.8573 - val_acc: 0.5702\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9220\n",
      "Epoch 00017: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2665 - acc: 0.9220 - val_loss: 1.9131 - val_acc: 0.5751\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9289\n",
      "Epoch 00018: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2435 - acc: 0.9289 - val_loss: 1.9028 - val_acc: 0.5754\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9351\n",
      "Epoch 00019: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2222 - acc: 0.9350 - val_loss: 1.9406 - val_acc: 0.5840\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9397\n",
      "Epoch 00020: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2098 - acc: 0.9397 - val_loss: 1.9886 - val_acc: 0.5779\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9423\n",
      "Epoch 00021: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1996 - acc: 0.9423 - val_loss: 2.0156 - val_acc: 0.5872\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9493\n",
      "Epoch 00022: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1784 - acc: 0.9493 - val_loss: 1.9994 - val_acc: 0.5903\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9545\n",
      "Epoch 00023: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1640 - acc: 0.9545 - val_loss: 2.0454 - val_acc: 0.5935\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9537\n",
      "Epoch 00024: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1658 - acc: 0.9536 - val_loss: 2.0631 - val_acc: 0.5931\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9547\n",
      "Epoch 00025: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1604 - acc: 0.9547 - val_loss: 2.0828 - val_acc: 0.5935\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9586\n",
      "Epoch 00026: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1446 - acc: 0.9586 - val_loss: 2.1142 - val_acc: 0.5910\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9613\n",
      "Epoch 00027: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1382 - acc: 0.9613 - val_loss: 2.1116 - val_acc: 0.5952\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9614\n",
      "Epoch 00028: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1359 - acc: 0.9614 - val_loss: 2.1145 - val_acc: 0.5996\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9647\n",
      "Epoch 00029: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1249 - acc: 0.9647 - val_loss: 2.1464 - val_acc: 0.5949\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9650\n",
      "Epoch 00030: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1261 - acc: 0.9650 - val_loss: 2.1477 - val_acc: 0.5984\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9684\n",
      "Epoch 00031: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1178 - acc: 0.9684 - val_loss: 2.2226 - val_acc: 0.5984\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9682\n",
      "Epoch 00032: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1151 - acc: 0.9682 - val_loss: 2.2922 - val_acc: 0.5907\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9696\n",
      "Epoch 00033: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1122 - acc: 0.9696 - val_loss: 2.2432 - val_acc: 0.5945\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9719\n",
      "Epoch 00034: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1065 - acc: 0.9719 - val_loss: 2.2425 - val_acc: 0.6000\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9708\n",
      "Epoch 00035: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1124 - acc: 0.9708 - val_loss: 2.2772 - val_acc: 0.6040\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9723\n",
      "Epoch 00036: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0977 - acc: 0.9723 - val_loss: 2.2732 - val_acc: 0.6084\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9717\n",
      "Epoch 00037: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1030 - acc: 0.9717 - val_loss: 2.3268 - val_acc: 0.6026\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9757\n",
      "Epoch 00038: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0894 - acc: 0.9757 - val_loss: 2.3177 - val_acc: 0.6019\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9760\n",
      "Epoch 00039: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0904 - acc: 0.9760 - val_loss: 2.3108 - val_acc: 0.6066\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9749\n",
      "Epoch 00040: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0893 - acc: 0.9749 - val_loss: 2.3749 - val_acc: 0.6021\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9762\n",
      "Epoch 00041: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0899 - acc: 0.9762 - val_loss: 2.3924 - val_acc: 0.6070\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9769\n",
      "Epoch 00042: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0848 - acc: 0.9769 - val_loss: 2.3605 - val_acc: 0.6110\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9787\n",
      "Epoch 00043: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0808 - acc: 0.9787 - val_loss: 2.3769 - val_acc: 0.6098\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9762\n",
      "Epoch 00044: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0842 - acc: 0.9762 - val_loss: 2.3976 - val_acc: 0.6091\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9773\n",
      "Epoch 00045: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0837 - acc: 0.9773 - val_loss: 2.4683 - val_acc: 0.5947\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9779\n",
      "Epoch 00046: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0824 - acc: 0.9779 - val_loss: 2.4063 - val_acc: 0.6091\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9789\n",
      "Epoch 00047: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0757 - acc: 0.9789 - val_loss: 2.4846 - val_acc: 0.6070\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9785\n",
      "Epoch 00048: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0795 - acc: 0.9785 - val_loss: 2.4923 - val_acc: 0.5993\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9790\n",
      "Epoch 00049: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0773 - acc: 0.9790 - val_loss: 2.5544 - val_acc: 0.6019\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9797\n",
      "Epoch 00050: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0767 - acc: 0.9797 - val_loss: 2.4813 - val_acc: 0.6096\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9812\n",
      "Epoch 00051: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0705 - acc: 0.9812 - val_loss: 2.4793 - val_acc: 0.6096\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9816\n",
      "Epoch 00052: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0741 - acc: 0.9816 - val_loss: 2.4848 - val_acc: 0.6138\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9801\n",
      "Epoch 00053: val_loss did not improve from 1.50752\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0736 - acc: 0.9801 - val_loss: 2.5909 - val_acc: 0.6047\n",
      "\n",
      "1D_CNN_custom_tanh_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 559us/sample - loss: 1.5901 - acc: 0.5009\n",
      "Loss: 1.5901038102519351 Accuracy: 0.5009346\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8793 - acc: 0.3954\n",
      "Epoch 00001: val_loss improved from inf to 1.47811, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/001-1.4781.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.8791 - acc: 0.3955 - val_loss: 1.4781 - val_acc: 0.5465\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3848 - acc: 0.5701\n",
      "Epoch 00002: val_loss improved from 1.47811 to 1.30199, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/002-1.3020.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3847 - acc: 0.5701 - val_loss: 1.3020 - val_acc: 0.6108\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1871 - acc: 0.6393\n",
      "Epoch 00003: val_loss improved from 1.30199 to 1.22339, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/003-1.2234.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1872 - acc: 0.6393 - val_loss: 1.2234 - val_acc: 0.6280\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0624 - acc: 0.6782\n",
      "Epoch 00004: val_loss improved from 1.22339 to 1.21156, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/004-1.2116.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0624 - acc: 0.6782 - val_loss: 1.2116 - val_acc: 0.6303\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9601 - acc: 0.7067\n",
      "Epoch 00005: val_loss improved from 1.21156 to 1.16884, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/005-1.1688.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9602 - acc: 0.7067 - val_loss: 1.1688 - val_acc: 0.6408\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8828 - acc: 0.7330\n",
      "Epoch 00006: val_loss improved from 1.16884 to 1.16054, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/006-1.1605.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8827 - acc: 0.7330 - val_loss: 1.1605 - val_acc: 0.6522\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8025 - acc: 0.7605\n",
      "Epoch 00007: val_loss did not improve from 1.16054\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8027 - acc: 0.7605 - val_loss: 1.1795 - val_acc: 0.6490\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7380 - acc: 0.7793\n",
      "Epoch 00008: val_loss improved from 1.16054 to 1.13917, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/008-1.1392.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7379 - acc: 0.7793 - val_loss: 1.1392 - val_acc: 0.6571\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6815 - acc: 0.7964\n",
      "Epoch 00009: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6815 - acc: 0.7964 - val_loss: 1.1580 - val_acc: 0.6594\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6262 - acc: 0.8122\n",
      "Epoch 00010: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6262 - acc: 0.8122 - val_loss: 1.1566 - val_acc: 0.6627\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5791 - acc: 0.8277\n",
      "Epoch 00011: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5791 - acc: 0.8278 - val_loss: 1.1617 - val_acc: 0.6653\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.8444\n",
      "Epoch 00012: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5305 - acc: 0.8445 - val_loss: 1.1483 - val_acc: 0.6667\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4925 - acc: 0.8543\n",
      "Epoch 00013: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4925 - acc: 0.8543 - val_loss: 1.1693 - val_acc: 0.6657\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.8651\n",
      "Epoch 00014: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4584 - acc: 0.8652 - val_loss: 1.1713 - val_acc: 0.6725\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4238 - acc: 0.8757\n",
      "Epoch 00015: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4238 - acc: 0.8757 - val_loss: 1.1597 - val_acc: 0.6765\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3974 - acc: 0.8866\n",
      "Epoch 00016: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3975 - acc: 0.8866 - val_loss: 1.1687 - val_acc: 0.6778\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8917\n",
      "Epoch 00017: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3717 - acc: 0.8917 - val_loss: 1.1887 - val_acc: 0.6816\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8993\n",
      "Epoch 00018: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3438 - acc: 0.8993 - val_loss: 1.1905 - val_acc: 0.6806\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.9077\n",
      "Epoch 00019: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3219 - acc: 0.9077 - val_loss: 1.1773 - val_acc: 0.6832\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.9130\n",
      "Epoch 00020: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3008 - acc: 0.9130 - val_loss: 1.1914 - val_acc: 0.6827\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9186\n",
      "Epoch 00021: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2838 - acc: 0.9186 - val_loss: 1.1971 - val_acc: 0.6858\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9250\n",
      "Epoch 00022: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2655 - acc: 0.9250 - val_loss: 1.2411 - val_acc: 0.6853\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9305\n",
      "Epoch 00023: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2496 - acc: 0.9305 - val_loss: 1.2248 - val_acc: 0.6874\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9326\n",
      "Epoch 00024: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2360 - acc: 0.9326 - val_loss: 1.2514 - val_acc: 0.6869\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9355\n",
      "Epoch 00025: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2263 - acc: 0.9355 - val_loss: 1.2202 - val_acc: 0.6949\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9399\n",
      "Epoch 00026: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2177 - acc: 0.9398 - val_loss: 1.2399 - val_acc: 0.6949\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9440\n",
      "Epoch 00027: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2008 - acc: 0.9440 - val_loss: 1.2459 - val_acc: 0.6960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9455\n",
      "Epoch 00028: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1973 - acc: 0.9455 - val_loss: 1.2572 - val_acc: 0.6937\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9500\n",
      "Epoch 00029: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1806 - acc: 0.9500 - val_loss: 1.2681 - val_acc: 0.6951\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9518\n",
      "Epoch 00030: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1788 - acc: 0.9518 - val_loss: 1.2733 - val_acc: 0.6981\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9539\n",
      "Epoch 00031: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1683 - acc: 0.9539 - val_loss: 1.2634 - val_acc: 0.7032\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9538\n",
      "Epoch 00032: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1659 - acc: 0.9538 - val_loss: 1.2653 - val_acc: 0.7046\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9568\n",
      "Epoch 00033: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1563 - acc: 0.9569 - val_loss: 1.2786 - val_acc: 0.7014\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9579\n",
      "Epoch 00034: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1535 - acc: 0.9579 - val_loss: 1.2920 - val_acc: 0.7014\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9597\n",
      "Epoch 00035: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1474 - acc: 0.9597 - val_loss: 1.3136 - val_acc: 0.7000\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9609\n",
      "Epoch 00036: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1400 - acc: 0.9609 - val_loss: 1.3141 - val_acc: 0.6993\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9638\n",
      "Epoch 00037: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1351 - acc: 0.9638 - val_loss: 1.3016 - val_acc: 0.7070\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9645\n",
      "Epoch 00038: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1320 - acc: 0.9645 - val_loss: 1.3368 - val_acc: 0.7051\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9647\n",
      "Epoch 00039: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1296 - acc: 0.9647 - val_loss: 1.3119 - val_acc: 0.7077\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9659\n",
      "Epoch 00040: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1233 - acc: 0.9659 - val_loss: 1.3195 - val_acc: 0.7105\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9678\n",
      "Epoch 00041: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1197 - acc: 0.9678 - val_loss: 1.3423 - val_acc: 0.7074\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9697\n",
      "Epoch 00042: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1152 - acc: 0.9697 - val_loss: 1.3444 - val_acc: 0.7058\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9694\n",
      "Epoch 00043: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1164 - acc: 0.9694 - val_loss: 1.3608 - val_acc: 0.7086\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9686\n",
      "Epoch 00044: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1126 - acc: 0.9686 - val_loss: 1.3513 - val_acc: 0.7058\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9711\n",
      "Epoch 00045: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1088 - acc: 0.9711 - val_loss: 1.3362 - val_acc: 0.7109\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9721\n",
      "Epoch 00046: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1048 - acc: 0.9721 - val_loss: 1.3748 - val_acc: 0.7051\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9717\n",
      "Epoch 00047: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1062 - acc: 0.9717 - val_loss: 1.3770 - val_acc: 0.7088\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9735\n",
      "Epoch 00048: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1009 - acc: 0.9735 - val_loss: 1.3440 - val_acc: 0.7230\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9726\n",
      "Epoch 00049: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1006 - acc: 0.9726 - val_loss: 1.3805 - val_acc: 0.7100\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9748\n",
      "Epoch 00050: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0947 - acc: 0.9748 - val_loss: 1.3595 - val_acc: 0.7184\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9744\n",
      "Epoch 00051: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0966 - acc: 0.9744 - val_loss: 1.3553 - val_acc: 0.7198\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9765\n",
      "Epoch 00052: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0924 - acc: 0.9765 - val_loss: 1.3818 - val_acc: 0.7135\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9745\n",
      "Epoch 00053: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0951 - acc: 0.9745 - val_loss: 1.3968 - val_acc: 0.7174\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9756\n",
      "Epoch 00054: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0910 - acc: 0.9756 - val_loss: 1.3950 - val_acc: 0.7163\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9771\n",
      "Epoch 00055: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0874 - acc: 0.9770 - val_loss: 1.3813 - val_acc: 0.7261\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9766\n",
      "Epoch 00056: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0884 - acc: 0.9766 - val_loss: 1.3749 - val_acc: 0.7226\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9783\n",
      "Epoch 00057: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0849 - acc: 0.9783 - val_loss: 1.4287 - val_acc: 0.7156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9767\n",
      "Epoch 00058: val_loss did not improve from 1.13917\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0873 - acc: 0.9767 - val_loss: 1.4085 - val_acc: 0.7195\n",
      "\n",
      "1D_CNN_custom_tanh_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 594us/sample - loss: 1.1904 - acc: 0.6422\n",
      "Loss: 1.1904275558695492 Accuracy: 0.64215994\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8785 - acc: 0.3927\n",
      "Epoch 00001: val_loss improved from inf to 1.50087, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/001-1.5009.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.8785 - acc: 0.3926 - val_loss: 1.5009 - val_acc: 0.5239\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4125 - acc: 0.5590\n",
      "Epoch 00002: val_loss improved from 1.50087 to 1.31584, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/002-1.3158.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.4124 - acc: 0.5591 - val_loss: 1.3158 - val_acc: 0.6000\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2256 - acc: 0.6275\n",
      "Epoch 00003: val_loss improved from 1.31584 to 1.17591, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/003-1.1759.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.2256 - acc: 0.6275 - val_loss: 1.1759 - val_acc: 0.6534\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1083 - acc: 0.6674\n",
      "Epoch 00004: val_loss improved from 1.17591 to 1.09897, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/004-1.0990.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1084 - acc: 0.6674 - val_loss: 1.0990 - val_acc: 0.6732\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0270 - acc: 0.6922\n",
      "Epoch 00005: val_loss improved from 1.09897 to 1.05456, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/005-1.0546.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.0269 - acc: 0.6922 - val_loss: 1.0546 - val_acc: 0.6890\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9582 - acc: 0.7136\n",
      "Epoch 00006: val_loss improved from 1.05456 to 1.01871, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/006-1.0187.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9582 - acc: 0.7136 - val_loss: 1.0187 - val_acc: 0.6956\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8999 - acc: 0.7345\n",
      "Epoch 00007: val_loss improved from 1.01871 to 0.99649, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/007-0.9965.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8999 - acc: 0.7345 - val_loss: 0.9965 - val_acc: 0.6988\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8417 - acc: 0.7518\n",
      "Epoch 00008: val_loss improved from 0.99649 to 0.96273, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/008-0.9627.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8418 - acc: 0.7517 - val_loss: 0.9627 - val_acc: 0.7086\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7897 - acc: 0.7681\n",
      "Epoch 00009: val_loss improved from 0.96273 to 0.93418, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/009-0.9342.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7897 - acc: 0.7681 - val_loss: 0.9342 - val_acc: 0.7358\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7423 - acc: 0.7827\n",
      "Epoch 00010: val_loss improved from 0.93418 to 0.90077, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/010-0.9008.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7423 - acc: 0.7827 - val_loss: 0.9008 - val_acc: 0.7277\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6942 - acc: 0.7968\n",
      "Epoch 00011: val_loss did not improve from 0.90077\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6942 - acc: 0.7968 - val_loss: 0.9028 - val_acc: 0.7419\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6552 - acc: 0.8087\n",
      "Epoch 00012: val_loss improved from 0.90077 to 0.87280, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/012-0.8728.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6552 - acc: 0.8087 - val_loss: 0.8728 - val_acc: 0.7379\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6173 - acc: 0.8195\n",
      "Epoch 00013: val_loss improved from 0.87280 to 0.87027, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/013-0.8703.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6175 - acc: 0.8195 - val_loss: 0.8703 - val_acc: 0.7468\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5847 - acc: 0.8309\n",
      "Epoch 00014: val_loss improved from 0.87027 to 0.85809, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/014-0.8581.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5847 - acc: 0.8309 - val_loss: 0.8581 - val_acc: 0.7547\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.8418\n",
      "Epoch 00015: val_loss did not improve from 0.85809\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5513 - acc: 0.8418 - val_loss: 0.8761 - val_acc: 0.7489\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.8496\n",
      "Epoch 00016: val_loss improved from 0.85809 to 0.82616, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/016-0.8262.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5209 - acc: 0.8496 - val_loss: 0.8262 - val_acc: 0.7575\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4910 - acc: 0.8575\n",
      "Epoch 00017: val_loss improved from 0.82616 to 0.82276, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/017-0.8228.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4910 - acc: 0.8575 - val_loss: 0.8228 - val_acc: 0.7612\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4668 - acc: 0.8658\n",
      "Epoch 00018: val_loss did not improve from 0.82276\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4667 - acc: 0.8659 - val_loss: 0.8260 - val_acc: 0.7622\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4391 - acc: 0.8742\n",
      "Epoch 00019: val_loss did not improve from 0.82276\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4392 - acc: 0.8742 - val_loss: 0.8492 - val_acc: 0.7568\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8809\n",
      "Epoch 00020: val_loss improved from 0.82276 to 0.81165, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/020-0.8117.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4161 - acc: 0.8809 - val_loss: 0.8117 - val_acc: 0.7657\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8865\n",
      "Epoch 00021: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3968 - acc: 0.8865 - val_loss: 0.8264 - val_acc: 0.7654\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8914\n",
      "Epoch 00022: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3777 - acc: 0.8914 - val_loss: 0.8260 - val_acc: 0.7652\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3585 - acc: 0.8992\n",
      "Epoch 00023: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3585 - acc: 0.8992 - val_loss: 0.8192 - val_acc: 0.7680\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.9052\n",
      "Epoch 00024: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3381 - acc: 0.9052 - val_loss: 0.8369 - val_acc: 0.7633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.9080\n",
      "Epoch 00025: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3230 - acc: 0.9080 - val_loss: 0.8306 - val_acc: 0.7589\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9159\n",
      "Epoch 00026: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3025 - acc: 0.9159 - val_loss: 0.8285 - val_acc: 0.7661\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.9160\n",
      "Epoch 00027: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2929 - acc: 0.9160 - val_loss: 0.8405 - val_acc: 0.7682\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9235\n",
      "Epoch 00028: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2761 - acc: 0.9235 - val_loss: 0.8228 - val_acc: 0.7708\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9262\n",
      "Epoch 00029: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2618 - acc: 0.9262 - val_loss: 0.8456 - val_acc: 0.7654\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9263\n",
      "Epoch 00030: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2658 - acc: 0.9263 - val_loss: 0.8373 - val_acc: 0.7645\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9339\n",
      "Epoch 00031: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2397 - acc: 0.9339 - val_loss: 0.8658 - val_acc: 0.7638\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9367\n",
      "Epoch 00032: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2278 - acc: 0.9367 - val_loss: 0.8619 - val_acc: 0.7722\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9397\n",
      "Epoch 00033: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2186 - acc: 0.9397 - val_loss: 0.8637 - val_acc: 0.7661\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9432\n",
      "Epoch 00034: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2118 - acc: 0.9432 - val_loss: 0.8566 - val_acc: 0.7708\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9448\n",
      "Epoch 00035: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2011 - acc: 0.9448 - val_loss: 0.8548 - val_acc: 0.7731\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9446\n",
      "Epoch 00036: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1963 - acc: 0.9446 - val_loss: 0.8961 - val_acc: 0.7633\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9501\n",
      "Epoch 00037: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1848 - acc: 0.9501 - val_loss: 0.8757 - val_acc: 0.7724\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9519\n",
      "Epoch 00038: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1799 - acc: 0.9519 - val_loss: 0.8748 - val_acc: 0.7680\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9537\n",
      "Epoch 00039: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1745 - acc: 0.9537 - val_loss: 0.8842 - val_acc: 0.7689\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9568\n",
      "Epoch 00040: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1625 - acc: 0.9568 - val_loss: 0.8880 - val_acc: 0.7715\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9570\n",
      "Epoch 00041: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1589 - acc: 0.9570 - val_loss: 0.8750 - val_acc: 0.7720\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9613\n",
      "Epoch 00042: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1501 - acc: 0.9613 - val_loss: 0.9026 - val_acc: 0.7724\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9603\n",
      "Epoch 00043: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1474 - acc: 0.9603 - val_loss: 0.8959 - val_acc: 0.7699\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9620\n",
      "Epoch 00044: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1413 - acc: 0.9619 - val_loss: 0.9248 - val_acc: 0.7678\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9636\n",
      "Epoch 00045: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1404 - acc: 0.9636 - val_loss: 0.9095 - val_acc: 0.7747\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9634\n",
      "Epoch 00046: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1369 - acc: 0.9634 - val_loss: 0.9102 - val_acc: 0.7768\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9658\n",
      "Epoch 00047: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1288 - acc: 0.9658 - val_loss: 0.9135 - val_acc: 0.7745\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9681\n",
      "Epoch 00048: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1234 - acc: 0.9681 - val_loss: 0.9323 - val_acc: 0.7710\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9669\n",
      "Epoch 00049: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1227 - acc: 0.9669 - val_loss: 0.9292 - val_acc: 0.7745\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9683\n",
      "Epoch 00050: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1210 - acc: 0.9683 - val_loss: 0.9659 - val_acc: 0.7657\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9704\n",
      "Epoch 00051: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1147 - acc: 0.9704 - val_loss: 0.9373 - val_acc: 0.7750\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9716\n",
      "Epoch 00052: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1132 - acc: 0.9716 - val_loss: 0.9361 - val_acc: 0.7759\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9714\n",
      "Epoch 00053: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1098 - acc: 0.9714 - val_loss: 0.9498 - val_acc: 0.7743\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9714\n",
      "Epoch 00054: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1089 - acc: 0.9714 - val_loss: 0.9534 - val_acc: 0.7713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9715\n",
      "Epoch 00055: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1079 - acc: 0.9716 - val_loss: 0.9407 - val_acc: 0.7708\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9743\n",
      "Epoch 00056: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0995 - acc: 0.9742 - val_loss: 0.9685 - val_acc: 0.7727\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9681\n",
      "Epoch 00057: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1296 - acc: 0.9681 - val_loss: 0.9393 - val_acc: 0.7775\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9751\n",
      "Epoch 00058: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0960 - acc: 0.9751 - val_loss: 0.9617 - val_acc: 0.7745\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9758\n",
      "Epoch 00059: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0955 - acc: 0.9758 - val_loss: 0.9706 - val_acc: 0.7722\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9760\n",
      "Epoch 00060: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0915 - acc: 0.9760 - val_loss: 0.9721 - val_acc: 0.7724\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9747\n",
      "Epoch 00061: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0918 - acc: 0.9747 - val_loss: 0.9756 - val_acc: 0.7792\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9760\n",
      "Epoch 00062: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0920 - acc: 0.9760 - val_loss: 0.9839 - val_acc: 0.7787\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9769\n",
      "Epoch 00063: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0887 - acc: 0.9769 - val_loss: 0.9828 - val_acc: 0.7750\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9786\n",
      "Epoch 00064: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0839 - acc: 0.9786 - val_loss: 0.9923 - val_acc: 0.7785\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9773\n",
      "Epoch 00065: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0854 - acc: 0.9773 - val_loss: 0.9966 - val_acc: 0.7701\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9780\n",
      "Epoch 00066: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0847 - acc: 0.9780 - val_loss: 1.0117 - val_acc: 0.7778\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9784\n",
      "Epoch 00067: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0818 - acc: 0.9784 - val_loss: 1.0135 - val_acc: 0.7743\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9798\n",
      "Epoch 00068: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0780 - acc: 0.9798 - val_loss: 0.9949 - val_acc: 0.7820\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9792\n",
      "Epoch 00069: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0796 - acc: 0.9792 - val_loss: 1.0133 - val_acc: 0.7743\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9796\n",
      "Epoch 00070: val_loss did not improve from 0.81165\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0794 - acc: 0.9796 - val_loss: 0.9990 - val_acc: 0.7785\n",
      "\n",
      "1D_CNN_custom_tanh_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 626us/sample - loss: 0.8895 - acc: 0.7429\n",
      "Loss: 0.8895337905591646 Accuracy: 0.74288684\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9172 - acc: 0.3724\n",
      "Epoch 00001: val_loss improved from inf to 1.45125, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/001-1.4512.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.9171 - acc: 0.3725 - val_loss: 1.4512 - val_acc: 0.5395\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4531 - acc: 0.5390\n",
      "Epoch 00002: val_loss improved from 1.45125 to 1.28225, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/002-1.2822.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.4530 - acc: 0.5390 - val_loss: 1.2822 - val_acc: 0.6101\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2917 - acc: 0.6025\n",
      "Epoch 00003: val_loss improved from 1.28225 to 1.16209, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/003-1.1621.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2916 - acc: 0.6026 - val_loss: 1.1621 - val_acc: 0.6487\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1595 - acc: 0.6458\n",
      "Epoch 00004: val_loss improved from 1.16209 to 1.02269, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/004-1.0227.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1595 - acc: 0.6458 - val_loss: 1.0227 - val_acc: 0.6960\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0621 - acc: 0.6797\n",
      "Epoch 00005: val_loss improved from 1.02269 to 0.96024, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/005-0.9602.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0621 - acc: 0.6797 - val_loss: 0.9602 - val_acc: 0.7198\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9863 - acc: 0.7036\n",
      "Epoch 00006: val_loss improved from 0.96024 to 0.89392, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/006-0.8939.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9863 - acc: 0.7036 - val_loss: 0.8939 - val_acc: 0.7317\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9247 - acc: 0.7218\n",
      "Epoch 00007: val_loss improved from 0.89392 to 0.86747, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/007-0.8675.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9247 - acc: 0.7219 - val_loss: 0.8675 - val_acc: 0.7414\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8760 - acc: 0.7399\n",
      "Epoch 00008: val_loss improved from 0.86747 to 0.82192, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/008-0.8219.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8761 - acc: 0.7399 - val_loss: 0.8219 - val_acc: 0.7638\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8334 - acc: 0.7525\n",
      "Epoch 00009: val_loss improved from 0.82192 to 0.80776, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/009-0.8078.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8333 - acc: 0.7525 - val_loss: 0.8078 - val_acc: 0.7631\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7998 - acc: 0.7627\n",
      "Epoch 00010: val_loss improved from 0.80776 to 0.75779, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/010-0.7578.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7999 - acc: 0.7627 - val_loss: 0.7578 - val_acc: 0.7827\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7672 - acc: 0.7706\n",
      "Epoch 00011: val_loss improved from 0.75779 to 0.72340, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/011-0.7234.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7671 - acc: 0.7706 - val_loss: 0.7234 - val_acc: 0.7950\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7393 - acc: 0.7815\n",
      "Epoch 00012: val_loss improved from 0.72340 to 0.71878, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/012-0.7188.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7392 - acc: 0.7815 - val_loss: 0.7188 - val_acc: 0.7950\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7085 - acc: 0.7901\n",
      "Epoch 00013: val_loss improved from 0.71878 to 0.70577, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/013-0.7058.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7085 - acc: 0.7900 - val_loss: 0.7058 - val_acc: 0.8039\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6884 - acc: 0.7962\n",
      "Epoch 00014: val_loss improved from 0.70577 to 0.67259, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/014-0.6726.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6884 - acc: 0.7963 - val_loss: 0.6726 - val_acc: 0.8046\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6594 - acc: 0.8051\n",
      "Epoch 00015: val_loss improved from 0.67259 to 0.66322, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/015-0.6632.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6594 - acc: 0.8051 - val_loss: 0.6632 - val_acc: 0.8123\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6367 - acc: 0.8141\n",
      "Epoch 00016: val_loss improved from 0.66322 to 0.64935, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/016-0.6493.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6367 - acc: 0.8141 - val_loss: 0.6493 - val_acc: 0.8167\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6231 - acc: 0.8167\n",
      "Epoch 00017: val_loss improved from 0.64935 to 0.63350, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/017-0.6335.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6231 - acc: 0.8167 - val_loss: 0.6335 - val_acc: 0.8262\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5993 - acc: 0.8216\n",
      "Epoch 00018: val_loss did not improve from 0.63350\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5995 - acc: 0.8216 - val_loss: 0.6450 - val_acc: 0.8185\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5865 - acc: 0.8263\n",
      "Epoch 00019: val_loss improved from 0.63350 to 0.60875, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/019-0.6088.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5865 - acc: 0.8263 - val_loss: 0.6088 - val_acc: 0.8244\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.8335\n",
      "Epoch 00020: val_loss improved from 0.60875 to 0.58649, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/020-0.5865.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5636 - acc: 0.8335 - val_loss: 0.5865 - val_acc: 0.8325\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5571 - acc: 0.8348\n",
      "Epoch 00021: val_loss did not improve from 0.58649\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5571 - acc: 0.8348 - val_loss: 0.5866 - val_acc: 0.8355\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.8421\n",
      "Epoch 00022: val_loss did not improve from 0.58649\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5353 - acc: 0.8421 - val_loss: 0.5878 - val_acc: 0.8383\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.8464\n",
      "Epoch 00023: val_loss improved from 0.58649 to 0.56732, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/023-0.5673.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5219 - acc: 0.8464 - val_loss: 0.5673 - val_acc: 0.8362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.8510\n",
      "Epoch 00024: val_loss improved from 0.56732 to 0.54712, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/024-0.5471.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5054 - acc: 0.8510 - val_loss: 0.5471 - val_acc: 0.8437\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4947 - acc: 0.8528\n",
      "Epoch 00025: val_loss did not improve from 0.54712\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4947 - acc: 0.8528 - val_loss: 0.5602 - val_acc: 0.8444\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4868 - acc: 0.8569\n",
      "Epoch 00026: val_loss improved from 0.54712 to 0.54325, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/026-0.5432.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4868 - acc: 0.8569 - val_loss: 0.5432 - val_acc: 0.8486\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4682 - acc: 0.8618\n",
      "Epoch 00027: val_loss improved from 0.54325 to 0.53756, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/027-0.5376.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4683 - acc: 0.8618 - val_loss: 0.5376 - val_acc: 0.8495\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.8661\n",
      "Epoch 00028: val_loss improved from 0.53756 to 0.52869, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/028-0.5287.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4562 - acc: 0.8661 - val_loss: 0.5287 - val_acc: 0.8572\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4471 - acc: 0.8676\n",
      "Epoch 00029: val_loss did not improve from 0.52869\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4470 - acc: 0.8676 - val_loss: 0.5318 - val_acc: 0.8532\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.8711\n",
      "Epoch 00030: val_loss did not improve from 0.52869\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4367 - acc: 0.8711 - val_loss: 0.5374 - val_acc: 0.8477\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.8745\n",
      "Epoch 00031: val_loss improved from 0.52869 to 0.50620, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/031-0.5062.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4216 - acc: 0.8745 - val_loss: 0.5062 - val_acc: 0.8586\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8761\n",
      "Epoch 00032: val_loss did not improve from 0.50620\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4160 - acc: 0.8762 - val_loss: 0.5301 - val_acc: 0.8509\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.8797\n",
      "Epoch 00033: val_loss did not improve from 0.50620\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4063 - acc: 0.8797 - val_loss: 0.5118 - val_acc: 0.8577\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8857\n",
      "Epoch 00034: val_loss did not improve from 0.50620\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3948 - acc: 0.8857 - val_loss: 0.5063 - val_acc: 0.8605\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8854\n",
      "Epoch 00035: val_loss improved from 0.50620 to 0.50486, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/035-0.5049.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3809 - acc: 0.8853 - val_loss: 0.5049 - val_acc: 0.8584\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3829 - acc: 0.8865\n",
      "Epoch 00036: val_loss did not improve from 0.50486\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3830 - acc: 0.8865 - val_loss: 0.5078 - val_acc: 0.8640\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8880\n",
      "Epoch 00037: val_loss did not improve from 0.50486\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3731 - acc: 0.8880 - val_loss: 0.5055 - val_acc: 0.8619\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3644 - acc: 0.8922\n",
      "Epoch 00038: val_loss improved from 0.50486 to 0.49983, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/038-0.4998.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3644 - acc: 0.8922 - val_loss: 0.4998 - val_acc: 0.8649\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8939\n",
      "Epoch 00039: val_loss did not improve from 0.49983\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3539 - acc: 0.8939 - val_loss: 0.5011 - val_acc: 0.8668\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8964\n",
      "Epoch 00040: val_loss improved from 0.49983 to 0.48589, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/040-0.4859.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3472 - acc: 0.8964 - val_loss: 0.4859 - val_acc: 0.8719\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8998\n",
      "Epoch 00041: val_loss did not improve from 0.48589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3397 - acc: 0.8998 - val_loss: 0.5099 - val_acc: 0.8663\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.9013\n",
      "Epoch 00042: val_loss did not improve from 0.48589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3342 - acc: 0.9013 - val_loss: 0.5058 - val_acc: 0.8588\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.9038\n",
      "Epoch 00043: val_loss did not improve from 0.48589\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3247 - acc: 0.9038 - val_loss: 0.5007 - val_acc: 0.8696\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.9057\n",
      "Epoch 00044: val_loss improved from 0.48589 to 0.48052, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/044-0.4805.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3167 - acc: 0.9057 - val_loss: 0.4805 - val_acc: 0.8742\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.9076\n",
      "Epoch 00045: val_loss did not improve from 0.48052\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3108 - acc: 0.9076 - val_loss: 0.4869 - val_acc: 0.8696\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.9083\n",
      "Epoch 00046: val_loss did not improve from 0.48052\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3044 - acc: 0.9083 - val_loss: 0.4831 - val_acc: 0.8698\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9118\n",
      "Epoch 00047: val_loss did not improve from 0.48052\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2965 - acc: 0.9118 - val_loss: 0.5156 - val_acc: 0.8677\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.9130\n",
      "Epoch 00048: val_loss did not improve from 0.48052\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2929 - acc: 0.9130 - val_loss: 0.4816 - val_acc: 0.8749\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.9147\n",
      "Epoch 00049: val_loss did not improve from 0.48052\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2863 - acc: 0.9147 - val_loss: 0.4885 - val_acc: 0.8728\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.9164\n",
      "Epoch 00050: val_loss did not improve from 0.48052\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2789 - acc: 0.9164 - val_loss: 0.4919 - val_acc: 0.8724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.9168\n",
      "Epoch 00051: val_loss did not improve from 0.48052\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2763 - acc: 0.9168 - val_loss: 0.4925 - val_acc: 0.8698\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2725 - acc: 0.9185\n",
      "Epoch 00052: val_loss improved from 0.48052 to 0.47932, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/052-0.4793.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2724 - acc: 0.9185 - val_loss: 0.4793 - val_acc: 0.8712\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9211\n",
      "Epoch 00053: val_loss improved from 0.47932 to 0.45959, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/053-0.4596.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2647 - acc: 0.9211 - val_loss: 0.4596 - val_acc: 0.8821\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9224\n",
      "Epoch 00054: val_loss did not improve from 0.45959\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2573 - acc: 0.9224 - val_loss: 0.4772 - val_acc: 0.8754\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9240\n",
      "Epoch 00055: val_loss did not improve from 0.45959\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2539 - acc: 0.9239 - val_loss: 0.4872 - val_acc: 0.8726\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9218\n",
      "Epoch 00056: val_loss did not improve from 0.45959\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2562 - acc: 0.9218 - val_loss: 0.4923 - val_acc: 0.8721\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9248\n",
      "Epoch 00057: val_loss improved from 0.45959 to 0.45941, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/057-0.4594.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2435 - acc: 0.9248 - val_loss: 0.4594 - val_acc: 0.8803\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9285\n",
      "Epoch 00058: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2369 - acc: 0.9285 - val_loss: 0.4824 - val_acc: 0.8742\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9277\n",
      "Epoch 00059: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2376 - acc: 0.9277 - val_loss: 0.4989 - val_acc: 0.8719\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9298\n",
      "Epoch 00060: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2317 - acc: 0.9298 - val_loss: 0.4779 - val_acc: 0.8775\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9314\n",
      "Epoch 00061: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2252 - acc: 0.9314 - val_loss: 0.4943 - val_acc: 0.8735\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9321\n",
      "Epoch 00062: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2205 - acc: 0.9321 - val_loss: 0.4799 - val_acc: 0.8803\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9327\n",
      "Epoch 00063: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2197 - acc: 0.9327 - val_loss: 0.4791 - val_acc: 0.8777\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9351\n",
      "Epoch 00064: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2169 - acc: 0.9351 - val_loss: 0.4865 - val_acc: 0.8786\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9352\n",
      "Epoch 00065: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2112 - acc: 0.9352 - val_loss: 0.4723 - val_acc: 0.8786\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9367\n",
      "Epoch 00066: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2106 - acc: 0.9367 - val_loss: 0.4638 - val_acc: 0.8831\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9386\n",
      "Epoch 00067: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2016 - acc: 0.9386 - val_loss: 0.4689 - val_acc: 0.8810\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9404\n",
      "Epoch 00068: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1980 - acc: 0.9404 - val_loss: 0.4775 - val_acc: 0.8782\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9369\n",
      "Epoch 00069: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2018 - acc: 0.9369 - val_loss: 0.4836 - val_acc: 0.8819\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9389\n",
      "Epoch 00070: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1976 - acc: 0.9389 - val_loss: 0.4882 - val_acc: 0.8775\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9417\n",
      "Epoch 00071: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1898 - acc: 0.9417 - val_loss: 0.4847 - val_acc: 0.8805\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9413\n",
      "Epoch 00072: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1893 - acc: 0.9413 - val_loss: 0.4699 - val_acc: 0.8833\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9431\n",
      "Epoch 00073: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1861 - acc: 0.9431 - val_loss: 0.5059 - val_acc: 0.8821\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9435\n",
      "Epoch 00074: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1834 - acc: 0.9435 - val_loss: 0.4921 - val_acc: 0.8784\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9448\n",
      "Epoch 00075: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1777 - acc: 0.9448 - val_loss: 0.4754 - val_acc: 0.8812\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9445\n",
      "Epoch 00076: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1786 - acc: 0.9445 - val_loss: 0.4828 - val_acc: 0.8821\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9466\n",
      "Epoch 00077: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1749 - acc: 0.9466 - val_loss: 0.4838 - val_acc: 0.8817\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9484\n",
      "Epoch 00078: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1693 - acc: 0.9484 - val_loss: 0.4733 - val_acc: 0.8833\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9496\n",
      "Epoch 00079: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1632 - acc: 0.9496 - val_loss: 0.4843 - val_acc: 0.8824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9491\n",
      "Epoch 00080: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1624 - acc: 0.9491 - val_loss: 0.5007 - val_acc: 0.8812\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9518\n",
      "Epoch 00081: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1583 - acc: 0.9518 - val_loss: 0.4982 - val_acc: 0.8786\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9513\n",
      "Epoch 00082: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1599 - acc: 0.9513 - val_loss: 0.4803 - val_acc: 0.8791\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9520\n",
      "Epoch 00083: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1577 - acc: 0.9520 - val_loss: 0.4686 - val_acc: 0.8835\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9525\n",
      "Epoch 00084: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1534 - acc: 0.9525 - val_loss: 0.4871 - val_acc: 0.8826\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9517\n",
      "Epoch 00085: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1570 - acc: 0.9517 - val_loss: 0.5006 - val_acc: 0.8775\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9529\n",
      "Epoch 00086: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1479 - acc: 0.9529 - val_loss: 0.4967 - val_acc: 0.8810\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9529\n",
      "Epoch 00087: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1481 - acc: 0.9529 - val_loss: 0.4951 - val_acc: 0.8833\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9558\n",
      "Epoch 00088: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1439 - acc: 0.9557 - val_loss: 0.5080 - val_acc: 0.8772\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9558\n",
      "Epoch 00089: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1461 - acc: 0.9557 - val_loss: 0.4907 - val_acc: 0.8805\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9564\n",
      "Epoch 00090: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1425 - acc: 0.9564 - val_loss: 0.5106 - val_acc: 0.8789\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9554\n",
      "Epoch 00091: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1433 - acc: 0.9553 - val_loss: 0.4912 - val_acc: 0.8838\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9574\n",
      "Epoch 00092: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1370 - acc: 0.9574 - val_loss: 0.4953 - val_acc: 0.8810\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9606\n",
      "Epoch 00093: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1306 - acc: 0.9606 - val_loss: 0.4924 - val_acc: 0.8833\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9595\n",
      "Epoch 00094: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1313 - acc: 0.9595 - val_loss: 0.4950 - val_acc: 0.8838\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9598\n",
      "Epoch 00095: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1289 - acc: 0.9598 - val_loss: 0.4946 - val_acc: 0.8821\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9614\n",
      "Epoch 00096: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1246 - acc: 0.9614 - val_loss: 0.5015 - val_acc: 0.8807\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9633\n",
      "Epoch 00097: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1226 - acc: 0.9633 - val_loss: 0.4904 - val_acc: 0.8828\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9611\n",
      "Epoch 00098: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1268 - acc: 0.9611 - val_loss: 0.5084 - val_acc: 0.8838\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9616\n",
      "Epoch 00099: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1243 - acc: 0.9616 - val_loss: 0.5104 - val_acc: 0.8847\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9600\n",
      "Epoch 00100: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1262 - acc: 0.9600 - val_loss: 0.5205 - val_acc: 0.8824\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9620\n",
      "Epoch 00101: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1217 - acc: 0.9620 - val_loss: 0.4863 - val_acc: 0.8868\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9624\n",
      "Epoch 00102: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1201 - acc: 0.9624 - val_loss: 0.5005 - val_acc: 0.8817\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9644\n",
      "Epoch 00103: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1172 - acc: 0.9644 - val_loss: 0.5089 - val_acc: 0.8833\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9654\n",
      "Epoch 00104: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1141 - acc: 0.9654 - val_loss: 0.4994 - val_acc: 0.8826\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9638\n",
      "Epoch 00105: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1159 - acc: 0.9638 - val_loss: 0.4941 - val_acc: 0.8854\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9649\n",
      "Epoch 00106: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1142 - acc: 0.9649 - val_loss: 0.5222 - val_acc: 0.8859\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9661\n",
      "Epoch 00107: val_loss did not improve from 0.45941\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1097 - acc: 0.9661 - val_loss: 0.5033 - val_acc: 0.8821\n",
      "\n",
      "1D_CNN_custom_tanh_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 596us/sample - loss: 0.5498 - acc: 0.8474\n",
      "Loss: 0.5498361694726122 Accuracy: 0.847352\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9706 - acc: 0.3595\n",
      "Epoch 00001: val_loss improved from inf to 1.43462, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/001-1.4346.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 1.9705 - acc: 0.3595 - val_loss: 1.4346 - val_acc: 0.5602\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4727 - acc: 0.5340\n",
      "Epoch 00002: val_loss improved from 1.43462 to 1.21566, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/002-1.2157.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.4726 - acc: 0.5340 - val_loss: 1.2157 - val_acc: 0.6320\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2491 - acc: 0.6155\n",
      "Epoch 00003: val_loss improved from 1.21566 to 1.04951, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/003-1.0495.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2490 - acc: 0.6155 - val_loss: 1.0495 - val_acc: 0.6960\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1055 - acc: 0.6660\n",
      "Epoch 00004: val_loss improved from 1.04951 to 0.91405, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/004-0.9140.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1055 - acc: 0.6660 - val_loss: 0.9140 - val_acc: 0.7361\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9924 - acc: 0.7025\n",
      "Epoch 00005: val_loss improved from 0.91405 to 0.82513, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/005-0.8251.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9923 - acc: 0.7025 - val_loss: 0.8251 - val_acc: 0.7706\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8890 - acc: 0.7380\n",
      "Epoch 00006: val_loss improved from 0.82513 to 0.78315, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/006-0.7831.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8890 - acc: 0.7380 - val_loss: 0.7831 - val_acc: 0.7752\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8091 - acc: 0.7631\n",
      "Epoch 00007: val_loss improved from 0.78315 to 0.66061, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/007-0.6606.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8094 - acc: 0.7630 - val_loss: 0.6606 - val_acc: 0.8160\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7452 - acc: 0.7833\n",
      "Epoch 00008: val_loss improved from 0.66061 to 0.62526, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/008-0.6253.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7454 - acc: 0.7832 - val_loss: 0.6253 - val_acc: 0.8311\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6907 - acc: 0.8002\n",
      "Epoch 00009: val_loss improved from 0.62526 to 0.58460, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/009-0.5846.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6907 - acc: 0.8002 - val_loss: 0.5846 - val_acc: 0.8369\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6380 - acc: 0.8182\n",
      "Epoch 00010: val_loss improved from 0.58460 to 0.54695, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/010-0.5470.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6382 - acc: 0.8181 - val_loss: 0.5470 - val_acc: 0.8453\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6052 - acc: 0.8231\n",
      "Epoch 00011: val_loss improved from 0.54695 to 0.50044, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/011-0.5004.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6051 - acc: 0.8231 - val_loss: 0.5004 - val_acc: 0.8628\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5686 - acc: 0.8372\n",
      "Epoch 00012: val_loss improved from 0.50044 to 0.48953, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/012-0.4895.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5685 - acc: 0.8372 - val_loss: 0.4895 - val_acc: 0.8654\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.8437\n",
      "Epoch 00013: val_loss improved from 0.48953 to 0.44849, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/013-0.4485.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5409 - acc: 0.8436 - val_loss: 0.4485 - val_acc: 0.8800\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.8499\n",
      "Epoch 00014: val_loss improved from 0.44849 to 0.43187, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/014-0.4319.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5147 - acc: 0.8499 - val_loss: 0.4319 - val_acc: 0.8842\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4877 - acc: 0.8608\n",
      "Epoch 00015: val_loss did not improve from 0.43187\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4877 - acc: 0.8608 - val_loss: 0.4671 - val_acc: 0.8740\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.8648\n",
      "Epoch 00016: val_loss improved from 0.43187 to 0.39317, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/016-0.3932.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4715 - acc: 0.8649 - val_loss: 0.3932 - val_acc: 0.8910\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8715\n",
      "Epoch 00017: val_loss improved from 0.39317 to 0.38540, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/017-0.3854.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4492 - acc: 0.8715 - val_loss: 0.3854 - val_acc: 0.8970\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4322 - acc: 0.8735\n",
      "Epoch 00018: val_loss did not improve from 0.38540\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4321 - acc: 0.8735 - val_loss: 0.3894 - val_acc: 0.8910\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4163 - acc: 0.8802\n",
      "Epoch 00019: val_loss improved from 0.38540 to 0.36331, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/019-0.3633.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4163 - acc: 0.8802 - val_loss: 0.3633 - val_acc: 0.9012\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4034 - acc: 0.8834\n",
      "Epoch 00020: val_loss improved from 0.36331 to 0.35878, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/020-0.3588.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4034 - acc: 0.8834 - val_loss: 0.3588 - val_acc: 0.9052\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8886\n",
      "Epoch 00021: val_loss improved from 0.35878 to 0.35090, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/021-0.3509.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3877 - acc: 0.8886 - val_loss: 0.3509 - val_acc: 0.9012\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8926\n",
      "Epoch 00022: val_loss improved from 0.35090 to 0.33088, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/022-0.3309.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3754 - acc: 0.8926 - val_loss: 0.3309 - val_acc: 0.9052\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8960\n",
      "Epoch 00023: val_loss improved from 0.33088 to 0.32686, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/023-0.3269.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3643 - acc: 0.8960 - val_loss: 0.3269 - val_acc: 0.9096\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8982\n",
      "Epoch 00024: val_loss did not improve from 0.32686\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3531 - acc: 0.8981 - val_loss: 0.3271 - val_acc: 0.9096\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.9002\n",
      "Epoch 00025: val_loss improved from 0.32686 to 0.31267, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/025-0.3127.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3417 - acc: 0.9002 - val_loss: 0.3127 - val_acc: 0.9113\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.9044\n",
      "Epoch 00026: val_loss improved from 0.31267 to 0.30943, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/026-0.3094.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3331 - acc: 0.9044 - val_loss: 0.3094 - val_acc: 0.9113\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.9061\n",
      "Epoch 00027: val_loss improved from 0.30943 to 0.29677, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/027-0.2968.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3256 - acc: 0.9062 - val_loss: 0.2968 - val_acc: 0.9215\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.9089\n",
      "Epoch 00028: val_loss did not improve from 0.29677\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3144 - acc: 0.9090 - val_loss: 0.3010 - val_acc: 0.9154\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9099\n",
      "Epoch 00029: val_loss improved from 0.29677 to 0.28239, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/029-0.2824.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3078 - acc: 0.9099 - val_loss: 0.2824 - val_acc: 0.9203\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9116\n",
      "Epoch 00030: val_loss did not improve from 0.28239\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3015 - acc: 0.9116 - val_loss: 0.2896 - val_acc: 0.9199\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.9148\n",
      "Epoch 00031: val_loss improved from 0.28239 to 0.27743, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/031-0.2774.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2970 - acc: 0.9148 - val_loss: 0.2774 - val_acc: 0.9234\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9165\n",
      "Epoch 00032: val_loss improved from 0.27743 to 0.26379, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/032-0.2638.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2878 - acc: 0.9165 - val_loss: 0.2638 - val_acc: 0.9276\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9181\n",
      "Epoch 00033: val_loss did not improve from 0.26379\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2813 - acc: 0.9181 - val_loss: 0.2852 - val_acc: 0.9222\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9203\n",
      "Epoch 00034: val_loss did not improve from 0.26379\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2767 - acc: 0.9203 - val_loss: 0.2643 - val_acc: 0.9255\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9223\n",
      "Epoch 00035: val_loss did not improve from 0.26379\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2672 - acc: 0.9223 - val_loss: 0.2711 - val_acc: 0.9257\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2629 - acc: 0.9226\n",
      "Epoch 00036: val_loss did not improve from 0.26379\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2629 - acc: 0.9226 - val_loss: 0.2810 - val_acc: 0.9227\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9249\n",
      "Epoch 00037: val_loss did not improve from 0.26379\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2528 - acc: 0.9249 - val_loss: 0.2916 - val_acc: 0.9215\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9219\n",
      "Epoch 00038: val_loss did not improve from 0.26379\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2670 - acc: 0.9219 - val_loss: 0.2713 - val_acc: 0.9241\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9289\n",
      "Epoch 00039: val_loss improved from 0.26379 to 0.25418, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/039-0.2542.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2444 - acc: 0.9288 - val_loss: 0.2542 - val_acc: 0.9304\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9284\n",
      "Epoch 00040: val_loss did not improve from 0.25418\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2423 - acc: 0.9284 - val_loss: 0.2653 - val_acc: 0.9285\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9297\n",
      "Epoch 00041: val_loss did not improve from 0.25418\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2368 - acc: 0.9297 - val_loss: 0.2621 - val_acc: 0.9278\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9314\n",
      "Epoch 00042: val_loss did not improve from 0.25418\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2351 - acc: 0.9314 - val_loss: 0.2604 - val_acc: 0.9315\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9307\n",
      "Epoch 00043: val_loss improved from 0.25418 to 0.24944, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/043-0.2494.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2313 - acc: 0.9307 - val_loss: 0.2494 - val_acc: 0.9329\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9333\n",
      "Epoch 00044: val_loss did not improve from 0.24944\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2242 - acc: 0.9332 - val_loss: 0.2516 - val_acc: 0.9308\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9335\n",
      "Epoch 00045: val_loss did not improve from 0.24944\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2209 - acc: 0.9334 - val_loss: 0.2602 - val_acc: 0.9322\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9344\n",
      "Epoch 00046: val_loss improved from 0.24944 to 0.24931, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/046-0.2493.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2193 - acc: 0.9344 - val_loss: 0.2493 - val_acc: 0.9327\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9372\n",
      "Epoch 00047: val_loss improved from 0.24931 to 0.24864, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/047-0.2486.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2136 - acc: 0.9372 - val_loss: 0.2486 - val_acc: 0.9327\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9386\n",
      "Epoch 00048: val_loss improved from 0.24864 to 0.24795, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/048-0.2479.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2076 - acc: 0.9386 - val_loss: 0.2479 - val_acc: 0.9334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9365\n",
      "Epoch 00049: val_loss improved from 0.24795 to 0.23845, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/049-0.2384.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2084 - acc: 0.9365 - val_loss: 0.2384 - val_acc: 0.9369\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9404\n",
      "Epoch 00050: val_loss improved from 0.23845 to 0.23126, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/050-0.2313.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2024 - acc: 0.9404 - val_loss: 0.2313 - val_acc: 0.9345\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9413\n",
      "Epoch 00051: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1973 - acc: 0.9413 - val_loss: 0.2426 - val_acc: 0.9331\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9418\n",
      "Epoch 00052: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1917 - acc: 0.9419 - val_loss: 0.2314 - val_acc: 0.9355\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9421\n",
      "Epoch 00053: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1915 - acc: 0.9421 - val_loss: 0.2396 - val_acc: 0.9341\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9435\n",
      "Epoch 00054: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1854 - acc: 0.9435 - val_loss: 0.2425 - val_acc: 0.9341\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9458\n",
      "Epoch 00055: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1826 - acc: 0.9458 - val_loss: 0.2378 - val_acc: 0.9383\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9445\n",
      "Epoch 00056: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1827 - acc: 0.9445 - val_loss: 0.2412 - val_acc: 0.9348\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9460\n",
      "Epoch 00057: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1789 - acc: 0.9460 - val_loss: 0.2454 - val_acc: 0.9336\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9473\n",
      "Epoch 00058: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1753 - acc: 0.9473 - val_loss: 0.2417 - val_acc: 0.9343\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9468\n",
      "Epoch 00059: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1776 - acc: 0.9468 - val_loss: 0.2488 - val_acc: 0.9327\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9493\n",
      "Epoch 00060: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1692 - acc: 0.9494 - val_loss: 0.2352 - val_acc: 0.9357\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9510\n",
      "Epoch 00061: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1652 - acc: 0.9510 - val_loss: 0.2406 - val_acc: 0.9366\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9504\n",
      "Epoch 00062: val_loss did not improve from 0.23126\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1664 - acc: 0.9504 - val_loss: 0.2332 - val_acc: 0.9345\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9496\n",
      "Epoch 00063: val_loss improved from 0.23126 to 0.23107, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/063-0.2311.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1634 - acc: 0.9496 - val_loss: 0.2311 - val_acc: 0.9404\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9519\n",
      "Epoch 00064: val_loss did not improve from 0.23107\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1578 - acc: 0.9519 - val_loss: 0.2330 - val_acc: 0.9401\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9533\n",
      "Epoch 00065: val_loss did not improve from 0.23107\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1553 - acc: 0.9533 - val_loss: 0.2341 - val_acc: 0.9359\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9524\n",
      "Epoch 00066: val_loss did not improve from 0.23107\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1556 - acc: 0.9524 - val_loss: 0.2329 - val_acc: 0.9359\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9520\n",
      "Epoch 00067: val_loss did not improve from 0.23107\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1553 - acc: 0.9520 - val_loss: 0.2352 - val_acc: 0.9380\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9530\n",
      "Epoch 00068: val_loss improved from 0.23107 to 0.22930, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/068-0.2293.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1531 - acc: 0.9529 - val_loss: 0.2293 - val_acc: 0.9394\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9548\n",
      "Epoch 00069: val_loss did not improve from 0.22930\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1472 - acc: 0.9548 - val_loss: 0.2355 - val_acc: 0.9380\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9563\n",
      "Epoch 00070: val_loss did not improve from 0.22930\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1444 - acc: 0.9563 - val_loss: 0.2294 - val_acc: 0.9397\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9573\n",
      "Epoch 00071: val_loss did not improve from 0.22930\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1418 - acc: 0.9573 - val_loss: 0.2325 - val_acc: 0.9373\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9551\n",
      "Epoch 00072: val_loss did not improve from 0.22930\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1411 - acc: 0.9551 - val_loss: 0.2411 - val_acc: 0.9355\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9571\n",
      "Epoch 00073: val_loss improved from 0.22930 to 0.22770, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/073-0.2277.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1408 - acc: 0.9571 - val_loss: 0.2277 - val_acc: 0.9418\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9588\n",
      "Epoch 00074: val_loss did not improve from 0.22770\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1335 - acc: 0.9588 - val_loss: 0.2340 - val_acc: 0.9413\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9591\n",
      "Epoch 00075: val_loss did not improve from 0.22770\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1338 - acc: 0.9591 - val_loss: 0.2354 - val_acc: 0.9369\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9598\n",
      "Epoch 00076: val_loss did not improve from 0.22770\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1294 - acc: 0.9598 - val_loss: 0.2332 - val_acc: 0.9390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9613\n",
      "Epoch 00077: val_loss did not improve from 0.22770\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1251 - acc: 0.9613 - val_loss: 0.2349 - val_acc: 0.9357\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9616\n",
      "Epoch 00078: val_loss did not improve from 0.22770\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1258 - acc: 0.9616 - val_loss: 0.2417 - val_acc: 0.9371\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9596\n",
      "Epoch 00079: val_loss improved from 0.22770 to 0.22326, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/079-0.2233.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1261 - acc: 0.9596 - val_loss: 0.2233 - val_acc: 0.9406\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9608\n",
      "Epoch 00080: val_loss did not improve from 0.22326\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1241 - acc: 0.9608 - val_loss: 0.2428 - val_acc: 0.9373\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9624\n",
      "Epoch 00081: val_loss did not improve from 0.22326\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1216 - acc: 0.9625 - val_loss: 0.2300 - val_acc: 0.9406\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9645\n",
      "Epoch 00082: val_loss did not improve from 0.22326\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1182 - acc: 0.9645 - val_loss: 0.2316 - val_acc: 0.9408\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9647\n",
      "Epoch 00083: val_loss did not improve from 0.22326\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1143 - acc: 0.9647 - val_loss: 0.2260 - val_acc: 0.9364\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9639\n",
      "Epoch 00084: val_loss did not improve from 0.22326\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1175 - acc: 0.9639 - val_loss: 0.2296 - val_acc: 0.9408\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9630\n",
      "Epoch 00085: val_loss improved from 0.22326 to 0.21809, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/085-0.2181.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1164 - acc: 0.9630 - val_loss: 0.2181 - val_acc: 0.9427\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9651\n",
      "Epoch 00086: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1152 - acc: 0.9651 - val_loss: 0.2463 - val_acc: 0.9364\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9649\n",
      "Epoch 00087: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1134 - acc: 0.9649 - val_loss: 0.2571 - val_acc: 0.9364\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9661\n",
      "Epoch 00088: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1102 - acc: 0.9661 - val_loss: 0.2348 - val_acc: 0.9399\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9670\n",
      "Epoch 00089: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1082 - acc: 0.9670 - val_loss: 0.2394 - val_acc: 0.9376\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9660\n",
      "Epoch 00090: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1085 - acc: 0.9660 - val_loss: 0.2287 - val_acc: 0.9427\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9677\n",
      "Epoch 00091: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1046 - acc: 0.9677 - val_loss: 0.2336 - val_acc: 0.9408\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9682\n",
      "Epoch 00092: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1038 - acc: 0.9682 - val_loss: 0.2596 - val_acc: 0.9343\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9657\n",
      "Epoch 00093: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1163 - acc: 0.9657 - val_loss: 0.2239 - val_acc: 0.9427\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9674\n",
      "Epoch 00094: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1039 - acc: 0.9674 - val_loss: 0.2460 - val_acc: 0.9355\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9705\n",
      "Epoch 00095: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0977 - acc: 0.9705 - val_loss: 0.2301 - val_acc: 0.9408\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9696\n",
      "Epoch 00096: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0967 - acc: 0.9696 - val_loss: 0.2390 - val_acc: 0.9376\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9700\n",
      "Epoch 00097: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0982 - acc: 0.9700 - val_loss: 0.2319 - val_acc: 0.9397\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9712\n",
      "Epoch 00098: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0930 - acc: 0.9713 - val_loss: 0.2466 - val_acc: 0.9378\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9726\n",
      "Epoch 00099: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0903 - acc: 0.9726 - val_loss: 0.2367 - val_acc: 0.9427\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9706\n",
      "Epoch 00100: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0919 - acc: 0.9706 - val_loss: 0.2343 - val_acc: 0.9415\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9707\n",
      "Epoch 00101: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0947 - acc: 0.9707 - val_loss: 0.2579 - val_acc: 0.9378\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9717\n",
      "Epoch 00102: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0900 - acc: 0.9717 - val_loss: 0.2345 - val_acc: 0.9448\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9723\n",
      "Epoch 00103: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0877 - acc: 0.9723 - val_loss: 0.2317 - val_acc: 0.9420\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9730\n",
      "Epoch 00104: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0888 - acc: 0.9730 - val_loss: 0.2524 - val_acc: 0.9371\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9736\n",
      "Epoch 00105: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0866 - acc: 0.9736 - val_loss: 0.2382 - val_acc: 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9721\n",
      "Epoch 00106: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0880 - acc: 0.9722 - val_loss: 0.2482 - val_acc: 0.9404\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9743\n",
      "Epoch 00107: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0825 - acc: 0.9743 - val_loss: 0.2352 - val_acc: 0.9425\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9746\n",
      "Epoch 00108: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0823 - acc: 0.9747 - val_loss: 0.2351 - val_acc: 0.9429\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9757\n",
      "Epoch 00109: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0787 - acc: 0.9757 - val_loss: 0.2330 - val_acc: 0.9422\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9748\n",
      "Epoch 00110: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0813 - acc: 0.9748 - val_loss: 0.2542 - val_acc: 0.9378\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9760\n",
      "Epoch 00111: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0781 - acc: 0.9760 - val_loss: 0.2399 - val_acc: 0.9401\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9755\n",
      "Epoch 00112: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0784 - acc: 0.9755 - val_loss: 0.2464 - val_acc: 0.9415\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9755\n",
      "Epoch 00113: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0777 - acc: 0.9755 - val_loss: 0.2478 - val_acc: 0.9406\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9772\n",
      "Epoch 00114: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0742 - acc: 0.9772 - val_loss: 0.2401 - val_acc: 0.9425\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9781\n",
      "Epoch 00115: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0733 - acc: 0.9781 - val_loss: 0.2412 - val_acc: 0.9422\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9768\n",
      "Epoch 00116: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0745 - acc: 0.9768 - val_loss: 0.2360 - val_acc: 0.9420\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9759\n",
      "Epoch 00117: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0754 - acc: 0.9759 - val_loss: 0.2483 - val_acc: 0.9415\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9762\n",
      "Epoch 00118: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0739 - acc: 0.9762 - val_loss: 0.2479 - val_acc: 0.9394\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9784\n",
      "Epoch 00119: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0702 - acc: 0.9784 - val_loss: 0.2437 - val_acc: 0.9397\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9795\n",
      "Epoch 00120: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0686 - acc: 0.9795 - val_loss: 0.2545 - val_acc: 0.9408\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9807\n",
      "Epoch 00121: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0659 - acc: 0.9807 - val_loss: 0.2562 - val_acc: 0.9413\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9774\n",
      "Epoch 00122: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0715 - acc: 0.9774 - val_loss: 0.2514 - val_acc: 0.9418\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9793\n",
      "Epoch 00123: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0694 - acc: 0.9793 - val_loss: 0.2623 - val_acc: 0.9383\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9800\n",
      "Epoch 00124: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0655 - acc: 0.9800 - val_loss: 0.2589 - val_acc: 0.9392\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9797\n",
      "Epoch 00125: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0669 - acc: 0.9797 - val_loss: 0.2477 - val_acc: 0.9422\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9815\n",
      "Epoch 00126: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0616 - acc: 0.9815 - val_loss: 0.2582 - val_acc: 0.9394\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9810\n",
      "Epoch 00127: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0632 - acc: 0.9810 - val_loss: 0.2626 - val_acc: 0.9385\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9810\n",
      "Epoch 00128: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0639 - acc: 0.9810 - val_loss: 0.2712 - val_acc: 0.9364\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9823\n",
      "Epoch 00129: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0596 - acc: 0.9823 - val_loss: 0.2375 - val_acc: 0.9450\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9808\n",
      "Epoch 00130: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0615 - acc: 0.9808 - val_loss: 0.2408 - val_acc: 0.9436\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9811\n",
      "Epoch 00131: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0607 - acc: 0.9811 - val_loss: 0.2481 - val_acc: 0.9422\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9806\n",
      "Epoch 00132: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0639 - acc: 0.9806 - val_loss: 0.2495 - val_acc: 0.9415\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9822\n",
      "Epoch 00133: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0586 - acc: 0.9822 - val_loss: 0.2509 - val_acc: 0.9411\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9818\n",
      "Epoch 00134: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0575 - acc: 0.9818 - val_loss: 0.2584 - val_acc: 0.9404\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9827\n",
      "Epoch 00135: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0584 - acc: 0.9827 - val_loss: 0.2733 - val_acc: 0.9385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_tanh_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 634us/sample - loss: 0.2675 - acc: 0.9173\n",
      "Loss: 0.26745054775371235 Accuracy: 0.91734165\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9800 - acc: 0.3546\n",
      "Epoch 00001: val_loss improved from inf to 1.41437, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/001-1.4144.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.9800 - acc: 0.3546 - val_loss: 1.4144 - val_acc: 0.5768\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4170 - acc: 0.5518\n",
      "Epoch 00002: val_loss improved from 1.41437 to 1.09531, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/002-1.0953.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4169 - acc: 0.5518 - val_loss: 1.0953 - val_acc: 0.6881\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1408 - acc: 0.6521\n",
      "Epoch 00003: val_loss improved from 1.09531 to 0.86731, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/003-0.8673.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1409 - acc: 0.6521 - val_loss: 0.8673 - val_acc: 0.7512\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9273 - acc: 0.7249\n",
      "Epoch 00004: val_loss improved from 0.86731 to 0.68897, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/004-0.6890.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9273 - acc: 0.7249 - val_loss: 0.6890 - val_acc: 0.8036\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7723 - acc: 0.7726\n",
      "Epoch 00005: val_loss improved from 0.68897 to 0.58344, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/005-0.5834.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7723 - acc: 0.7726 - val_loss: 0.5834 - val_acc: 0.8488\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6577 - acc: 0.8107\n",
      "Epoch 00006: val_loss improved from 0.58344 to 0.48611, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/006-0.4861.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6577 - acc: 0.8107 - val_loss: 0.4861 - val_acc: 0.8730\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5676 - acc: 0.8361\n",
      "Epoch 00007: val_loss improved from 0.48611 to 0.40450, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/007-0.4045.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5677 - acc: 0.8361 - val_loss: 0.4045 - val_acc: 0.8926\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.8541\n",
      "Epoch 00008: val_loss improved from 0.40450 to 0.35552, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/008-0.3555.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5076 - acc: 0.8541 - val_loss: 0.3555 - val_acc: 0.9068\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4534 - acc: 0.8696\n",
      "Epoch 00009: val_loss improved from 0.35552 to 0.32517, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/009-0.3252.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4533 - acc: 0.8696 - val_loss: 0.3252 - val_acc: 0.9131\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8820\n",
      "Epoch 00010: val_loss improved from 0.32517 to 0.31145, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/010-0.3115.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4136 - acc: 0.8820 - val_loss: 0.3115 - val_acc: 0.9150\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8886\n",
      "Epoch 00011: val_loss improved from 0.31145 to 0.27661, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/011-0.2766.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3858 - acc: 0.8886 - val_loss: 0.2766 - val_acc: 0.9252\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8993\n",
      "Epoch 00012: val_loss improved from 0.27661 to 0.24659, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/012-0.2466.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3525 - acc: 0.8993 - val_loss: 0.2466 - val_acc: 0.9355\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3283 - acc: 0.9051\n",
      "Epoch 00013: val_loss did not improve from 0.24659\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3284 - acc: 0.9050 - val_loss: 0.2524 - val_acc: 0.9359\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9124\n",
      "Epoch 00014: val_loss improved from 0.24659 to 0.22545, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/014-0.2255.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3061 - acc: 0.9124 - val_loss: 0.2255 - val_acc: 0.9408\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9185\n",
      "Epoch 00015: val_loss improved from 0.22545 to 0.22496, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/015-0.2250.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2888 - acc: 0.9185 - val_loss: 0.2250 - val_acc: 0.9392\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9192\n",
      "Epoch 00016: val_loss improved from 0.22496 to 0.20806, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/016-0.2081.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2772 - acc: 0.9192 - val_loss: 0.2081 - val_acc: 0.9427\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9242\n",
      "Epoch 00017: val_loss improved from 0.20806 to 0.20499, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/017-0.2050.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2622 - acc: 0.9242 - val_loss: 0.2050 - val_acc: 0.9427\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9273\n",
      "Epoch 00018: val_loss improved from 0.20499 to 0.20120, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/018-0.2012.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2501 - acc: 0.9272 - val_loss: 0.2012 - val_acc: 0.9474\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9302\n",
      "Epoch 00019: val_loss improved from 0.20120 to 0.19456, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/019-0.1946.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2394 - acc: 0.9302 - val_loss: 0.1946 - val_acc: 0.9460\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9338\n",
      "Epoch 00020: val_loss improved from 0.19456 to 0.19325, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/020-0.1932.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2292 - acc: 0.9338 - val_loss: 0.1932 - val_acc: 0.9464\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9376\n",
      "Epoch 00021: val_loss improved from 0.19325 to 0.18205, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/021-0.1820.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2197 - acc: 0.9376 - val_loss: 0.1820 - val_acc: 0.9481\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9382\n",
      "Epoch 00022: val_loss improved from 0.18205 to 0.17333, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/022-0.1733.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2107 - acc: 0.9382 - val_loss: 0.1733 - val_acc: 0.9504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9405\n",
      "Epoch 00023: val_loss did not improve from 0.17333\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2047 - acc: 0.9405 - val_loss: 0.1776 - val_acc: 0.9490\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9437\n",
      "Epoch 00024: val_loss did not improve from 0.17333\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1966 - acc: 0.9437 - val_loss: 0.1863 - val_acc: 0.9495\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9448\n",
      "Epoch 00025: val_loss improved from 0.17333 to 0.16321, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/025-0.1632.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1893 - acc: 0.9448 - val_loss: 0.1632 - val_acc: 0.9532\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9474\n",
      "Epoch 00026: val_loss did not improve from 0.16321\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1834 - acc: 0.9474 - val_loss: 0.1714 - val_acc: 0.9520\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9483\n",
      "Epoch 00027: val_loss did not improve from 0.16321\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1794 - acc: 0.9483 - val_loss: 0.1717 - val_acc: 0.9518\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9488\n",
      "Epoch 00028: val_loss did not improve from 0.16321\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1735 - acc: 0.9488 - val_loss: 0.1820 - val_acc: 0.9483\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9521\n",
      "Epoch 00029: val_loss did not improve from 0.16321\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1653 - acc: 0.9520 - val_loss: 0.1689 - val_acc: 0.9525\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9518\n",
      "Epoch 00030: val_loss improved from 0.16321 to 0.15679, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/030-0.1568.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1646 - acc: 0.9518 - val_loss: 0.1568 - val_acc: 0.9560\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9545\n",
      "Epoch 00031: val_loss did not improve from 0.15679\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1560 - acc: 0.9545 - val_loss: 0.1791 - val_acc: 0.9462\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9554\n",
      "Epoch 00032: val_loss improved from 0.15679 to 0.15618, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/032-0.1562.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1519 - acc: 0.9554 - val_loss: 0.1562 - val_acc: 0.9560\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9567\n",
      "Epoch 00033: val_loss did not improve from 0.15618\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1472 - acc: 0.9567 - val_loss: 0.1654 - val_acc: 0.9522\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9585\n",
      "Epoch 00034: val_loss improved from 0.15618 to 0.15534, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/034-0.1553.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1433 - acc: 0.9584 - val_loss: 0.1553 - val_acc: 0.9555\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9576\n",
      "Epoch 00035: val_loss did not improve from 0.15534\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1436 - acc: 0.9576 - val_loss: 0.1556 - val_acc: 0.9585\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9601\n",
      "Epoch 00036: val_loss improved from 0.15534 to 0.15164, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/036-0.1516.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1361 - acc: 0.9601 - val_loss: 0.1516 - val_acc: 0.9555\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9620\n",
      "Epoch 00037: val_loss did not improve from 0.15164\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1288 - acc: 0.9620 - val_loss: 0.1660 - val_acc: 0.9562\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9624\n",
      "Epoch 00038: val_loss did not improve from 0.15164\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1275 - acc: 0.9625 - val_loss: 0.1628 - val_acc: 0.9567\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9638\n",
      "Epoch 00039: val_loss improved from 0.15164 to 0.14965, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/039-0.1497.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1243 - acc: 0.9638 - val_loss: 0.1497 - val_acc: 0.9557\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9640\n",
      "Epoch 00040: val_loss did not improve from 0.14965\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1224 - acc: 0.9640 - val_loss: 0.1577 - val_acc: 0.9543\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9662\n",
      "Epoch 00041: val_loss did not improve from 0.14965\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1148 - acc: 0.9661 - val_loss: 0.1596 - val_acc: 0.9560\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9650\n",
      "Epoch 00042: val_loss improved from 0.14965 to 0.14742, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/042-0.1474.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1196 - acc: 0.9650 - val_loss: 0.1474 - val_acc: 0.9564\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9669\n",
      "Epoch 00043: val_loss did not improve from 0.14742\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1134 - acc: 0.9669 - val_loss: 0.1667 - val_acc: 0.9543\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9685\n",
      "Epoch 00044: val_loss did not improve from 0.14742\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1050 - acc: 0.9684 - val_loss: 0.1517 - val_acc: 0.9564\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9665\n",
      "Epoch 00045: val_loss did not improve from 0.14742\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1114 - acc: 0.9665 - val_loss: 0.1480 - val_acc: 0.9557\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9696\n",
      "Epoch 00046: val_loss did not improve from 0.14742\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1010 - acc: 0.9695 - val_loss: 0.1619 - val_acc: 0.9532\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9705\n",
      "Epoch 00047: val_loss did not improve from 0.14742\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0988 - acc: 0.9705 - val_loss: 0.1591 - val_acc: 0.9588\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9719\n",
      "Epoch 00048: val_loss did not improve from 0.14742\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0953 - acc: 0.9719 - val_loss: 0.1608 - val_acc: 0.9541\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9735\n",
      "Epoch 00049: val_loss improved from 0.14742 to 0.14533, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/049-0.1453.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0917 - acc: 0.9735 - val_loss: 0.1453 - val_acc: 0.9590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9714\n",
      "Epoch 00050: val_loss did not improve from 0.14533\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0909 - acc: 0.9714 - val_loss: 0.1524 - val_acc: 0.9595\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9742\n",
      "Epoch 00051: val_loss did not improve from 0.14533\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0883 - acc: 0.9742 - val_loss: 0.1563 - val_acc: 0.9557\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9745\n",
      "Epoch 00052: val_loss did not improve from 0.14533\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0888 - acc: 0.9745 - val_loss: 0.1578 - val_acc: 0.9574\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9748\n",
      "Epoch 00053: val_loss did not improve from 0.14533\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0843 - acc: 0.9747 - val_loss: 0.1518 - val_acc: 0.9576\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9753\n",
      "Epoch 00054: val_loss did not improve from 0.14533\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0848 - acc: 0.9753 - val_loss: 0.1595 - val_acc: 0.9557\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9758\n",
      "Epoch 00055: val_loss did not improve from 0.14533\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0809 - acc: 0.9758 - val_loss: 0.1523 - val_acc: 0.9581\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9769\n",
      "Epoch 00056: val_loss did not improve from 0.14533\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0776 - acc: 0.9769 - val_loss: 0.1669 - val_acc: 0.9567\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9771\n",
      "Epoch 00057: val_loss did not improve from 0.14533\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0751 - acc: 0.9771 - val_loss: 0.1558 - val_acc: 0.9557\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9778\n",
      "Epoch 00058: val_loss did not improve from 0.14533\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0763 - acc: 0.9778 - val_loss: 0.1611 - val_acc: 0.9555\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9774\n",
      "Epoch 00059: val_loss improved from 0.14533 to 0.14076, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/059-0.1408.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0726 - acc: 0.9774 - val_loss: 0.1408 - val_acc: 0.9595\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9783\n",
      "Epoch 00060: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0705 - acc: 0.9783 - val_loss: 0.1552 - val_acc: 0.9576\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9797\n",
      "Epoch 00061: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0711 - acc: 0.9797 - val_loss: 0.1547 - val_acc: 0.9576\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9793\n",
      "Epoch 00062: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0715 - acc: 0.9793 - val_loss: 0.1614 - val_acc: 0.9574\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9810\n",
      "Epoch 00063: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0638 - acc: 0.9810 - val_loss: 0.1664 - val_acc: 0.9548\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9801\n",
      "Epoch 00064: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0662 - acc: 0.9801 - val_loss: 0.1514 - val_acc: 0.9597\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9806\n",
      "Epoch 00065: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0653 - acc: 0.9806 - val_loss: 0.1464 - val_acc: 0.9564\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9826\n",
      "Epoch 00066: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0583 - acc: 0.9826 - val_loss: 0.1564 - val_acc: 0.9555\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9830\n",
      "Epoch 00067: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0587 - acc: 0.9830 - val_loss: 0.1492 - val_acc: 0.9590\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9839\n",
      "Epoch 00068: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0547 - acc: 0.9839 - val_loss: 0.1587 - val_acc: 0.9592\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9820\n",
      "Epoch 00069: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0604 - acc: 0.9820 - val_loss: 0.1630 - val_acc: 0.9576\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9843\n",
      "Epoch 00070: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0537 - acc: 0.9843 - val_loss: 0.1620 - val_acc: 0.9574\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9845\n",
      "Epoch 00071: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0529 - acc: 0.9845 - val_loss: 0.1584 - val_acc: 0.9578\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9850\n",
      "Epoch 00072: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0510 - acc: 0.9850 - val_loss: 0.1516 - val_acc: 0.9585\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9842\n",
      "Epoch 00073: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0526 - acc: 0.9842 - val_loss: 0.1492 - val_acc: 0.9599\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9850\n",
      "Epoch 00074: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0502 - acc: 0.9850 - val_loss: 0.1460 - val_acc: 0.9595\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9852\n",
      "Epoch 00075: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0494 - acc: 0.9852 - val_loss: 0.1752 - val_acc: 0.9564\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9867\n",
      "Epoch 00076: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0453 - acc: 0.9867 - val_loss: 0.1711 - val_acc: 0.9560\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9866\n",
      "Epoch 00077: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0448 - acc: 0.9866 - val_loss: 0.1733 - val_acc: 0.9574\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9867\n",
      "Epoch 00078: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0454 - acc: 0.9867 - val_loss: 0.1590 - val_acc: 0.9632\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9885\n",
      "Epoch 00079: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0414 - acc: 0.9885 - val_loss: 0.1790 - val_acc: 0.9548\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9884\n",
      "Epoch 00080: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0420 - acc: 0.9884 - val_loss: 0.1512 - val_acc: 0.9604\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9883\n",
      "Epoch 00081: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0405 - acc: 0.9883 - val_loss: 0.1708 - val_acc: 0.9592\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9869\n",
      "Epoch 00082: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0442 - acc: 0.9869 - val_loss: 0.1492 - val_acc: 0.9630\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9887\n",
      "Epoch 00083: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0380 - acc: 0.9887 - val_loss: 0.1660 - val_acc: 0.9581\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9894\n",
      "Epoch 00084: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0382 - acc: 0.9894 - val_loss: 0.1543 - val_acc: 0.9585\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9886\n",
      "Epoch 00085: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0387 - acc: 0.9886 - val_loss: 0.1576 - val_acc: 0.9592\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9894\n",
      "Epoch 00086: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0373 - acc: 0.9894 - val_loss: 0.1614 - val_acc: 0.9623\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9898\n",
      "Epoch 00087: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0357 - acc: 0.9898 - val_loss: 0.1574 - val_acc: 0.9606\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9896\n",
      "Epoch 00088: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0376 - acc: 0.9896 - val_loss: 0.1753 - val_acc: 0.9585\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9894\n",
      "Epoch 00089: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0359 - acc: 0.9894 - val_loss: 0.1802 - val_acc: 0.9539\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9899\n",
      "Epoch 00090: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0336 - acc: 0.9899 - val_loss: 0.1857 - val_acc: 0.9525\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9915\n",
      "Epoch 00091: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0315 - acc: 0.9915 - val_loss: 0.1708 - val_acc: 0.9597\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9909\n",
      "Epoch 00092: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0319 - acc: 0.9909 - val_loss: 0.1692 - val_acc: 0.9588\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9877\n",
      "Epoch 00093: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0414 - acc: 0.9877 - val_loss: 0.1656 - val_acc: 0.9585\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9921\n",
      "Epoch 00094: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0294 - acc: 0.9921 - val_loss: 0.1698 - val_acc: 0.9590\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9911\n",
      "Epoch 00095: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0315 - acc: 0.9911 - val_loss: 0.1738 - val_acc: 0.9590\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9939\n",
      "Epoch 00096: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0245 - acc: 0.9939 - val_loss: 0.1730 - val_acc: 0.9595\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9911\n",
      "Epoch 00097: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0314 - acc: 0.9911 - val_loss: 0.1757 - val_acc: 0.9581\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9905\n",
      "Epoch 00098: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0329 - acc: 0.9905 - val_loss: 0.1647 - val_acc: 0.9569\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9917\n",
      "Epoch 00099: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0294 - acc: 0.9917 - val_loss: 0.1745 - val_acc: 0.9599\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9934\n",
      "Epoch 00100: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0247 - acc: 0.9934 - val_loss: 0.1863 - val_acc: 0.9567\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9923\n",
      "Epoch 00101: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0270 - acc: 0.9923 - val_loss: 0.1806 - val_acc: 0.9581\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9929\n",
      "Epoch 00102: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0258 - acc: 0.9929 - val_loss: 0.1758 - val_acc: 0.9574\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9921\n",
      "Epoch 00103: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0258 - acc: 0.9921 - val_loss: 0.1606 - val_acc: 0.9604\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9918\n",
      "Epoch 00104: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0277 - acc: 0.9918 - val_loss: 0.1733 - val_acc: 0.9592\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9935\n",
      "Epoch 00105: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0236 - acc: 0.9935 - val_loss: 0.1747 - val_acc: 0.9604\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9930\n",
      "Epoch 00106: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0254 - acc: 0.9930 - val_loss: 0.1664 - val_acc: 0.9611\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9941\n",
      "Epoch 00107: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0211 - acc: 0.9941 - val_loss: 0.1733 - val_acc: 0.9604\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9931\n",
      "Epoch 00108: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0245 - acc: 0.9931 - val_loss: 0.1894 - val_acc: 0.9571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9924\n",
      "Epoch 00109: val_loss did not improve from 0.14076\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0265 - acc: 0.9924 - val_loss: 0.1950 - val_acc: 0.9553\n",
      "\n",
      "1D_CNN_custom_tanh_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 657us/sample - loss: 0.1836 - acc: 0.9452\n",
      "Loss: 0.183581630432222 Accuracy: 0.94517136\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6067 - acc: 0.4963\n",
      "Epoch 00001: val_loss improved from inf to 0.91797, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/001-0.9180.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.6068 - acc: 0.4963 - val_loss: 0.9180 - val_acc: 0.7282\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8523 - acc: 0.7440\n",
      "Epoch 00002: val_loss improved from 0.91797 to 0.57724, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/002-0.5772.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8523 - acc: 0.7440 - val_loss: 0.5772 - val_acc: 0.8383\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.8222\n",
      "Epoch 00003: val_loss improved from 0.57724 to 0.40690, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/003-0.4069.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6026 - acc: 0.8223 - val_loss: 0.4069 - val_acc: 0.8847\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.8604\n",
      "Epoch 00004: val_loss improved from 0.40690 to 0.33326, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/004-0.3333.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4644 - acc: 0.8604 - val_loss: 0.3333 - val_acc: 0.9066\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3856 - acc: 0.8864\n",
      "Epoch 00005: val_loss improved from 0.33326 to 0.28133, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/005-0.2813.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3856 - acc: 0.8864 - val_loss: 0.2813 - val_acc: 0.9210\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.9010\n",
      "Epoch 00006: val_loss improved from 0.28133 to 0.24158, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/006-0.2416.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3324 - acc: 0.9010 - val_loss: 0.2416 - val_acc: 0.9287\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9127\n",
      "Epoch 00007: val_loss improved from 0.24158 to 0.22146, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/007-0.2215.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2910 - acc: 0.9127 - val_loss: 0.2215 - val_acc: 0.9378\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9218\n",
      "Epoch 00008: val_loss improved from 0.22146 to 0.19661, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/008-0.1966.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2628 - acc: 0.9217 - val_loss: 0.1966 - val_acc: 0.9432\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9278\n",
      "Epoch 00009: val_loss improved from 0.19661 to 0.18952, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/009-0.1895.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2422 - acc: 0.9278 - val_loss: 0.1895 - val_acc: 0.9443\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9361\n",
      "Epoch 00010: val_loss improved from 0.18952 to 0.17448, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/010-0.1745.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2157 - acc: 0.9360 - val_loss: 0.1745 - val_acc: 0.9476\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9392\n",
      "Epoch 00011: val_loss improved from 0.17448 to 0.16103, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/011-0.1610.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2035 - acc: 0.9392 - val_loss: 0.1610 - val_acc: 0.9555\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9443\n",
      "Epoch 00012: val_loss did not improve from 0.16103\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1884 - acc: 0.9443 - val_loss: 0.1747 - val_acc: 0.9492\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9483\n",
      "Epoch 00013: val_loss improved from 0.16103 to 0.16078, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/013-0.1608.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1762 - acc: 0.9483 - val_loss: 0.1608 - val_acc: 0.9515\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9515\n",
      "Epoch 00014: val_loss improved from 0.16078 to 0.14668, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/014-0.1467.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1630 - acc: 0.9515 - val_loss: 0.1467 - val_acc: 0.9527\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9535\n",
      "Epoch 00015: val_loss did not improve from 0.14668\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1517 - acc: 0.9535 - val_loss: 0.1579 - val_acc: 0.9536\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9575\n",
      "Epoch 00016: val_loss improved from 0.14668 to 0.14236, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/016-0.1424.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1433 - acc: 0.9575 - val_loss: 0.1424 - val_acc: 0.9588\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9591\n",
      "Epoch 00017: val_loss improved from 0.14236 to 0.14105, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/017-0.1411.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1341 - acc: 0.9591 - val_loss: 0.1411 - val_acc: 0.9583\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9612\n",
      "Epoch 00018: val_loss did not improve from 0.14105\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1286 - acc: 0.9612 - val_loss: 0.1435 - val_acc: 0.9557\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9643\n",
      "Epoch 00019: val_loss improved from 0.14105 to 0.13988, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/019-0.1399.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1191 - acc: 0.9643 - val_loss: 0.1399 - val_acc: 0.9550\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9658\n",
      "Epoch 00020: val_loss improved from 0.13988 to 0.13037, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/020-0.1304.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1151 - acc: 0.9658 - val_loss: 0.1304 - val_acc: 0.9595\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9686\n",
      "Epoch 00021: val_loss did not improve from 0.13037\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1067 - acc: 0.9686 - val_loss: 0.1314 - val_acc: 0.9597\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9696\n",
      "Epoch 00022: val_loss improved from 0.13037 to 0.12882, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/022-0.1288.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1016 - acc: 0.9697 - val_loss: 0.1288 - val_acc: 0.9604\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9723\n",
      "Epoch 00023: val_loss did not improve from 0.12882\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0958 - acc: 0.9722 - val_loss: 0.1341 - val_acc: 0.9623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9728\n",
      "Epoch 00024: val_loss improved from 0.12882 to 0.12668, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/024-0.1267.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0917 - acc: 0.9728 - val_loss: 0.1267 - val_acc: 0.9618\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9749\n",
      "Epoch 00025: val_loss improved from 0.12668 to 0.12601, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/025-0.1260.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0853 - acc: 0.9749 - val_loss: 0.1260 - val_acc: 0.9618\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9741\n",
      "Epoch 00026: val_loss did not improve from 0.12601\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0851 - acc: 0.9741 - val_loss: 0.1269 - val_acc: 0.9627\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9768\n",
      "Epoch 00027: val_loss did not improve from 0.12601\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0781 - acc: 0.9768 - val_loss: 0.1335 - val_acc: 0.9606\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9786\n",
      "Epoch 00028: val_loss did not improve from 0.12601\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0723 - acc: 0.9786 - val_loss: 0.1383 - val_acc: 0.9595\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9779\n",
      "Epoch 00029: val_loss did not improve from 0.12601\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0743 - acc: 0.9779 - val_loss: 0.1303 - val_acc: 0.9604\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9811\n",
      "Epoch 00030: val_loss improved from 0.12601 to 0.12361, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/030-0.1236.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0661 - acc: 0.9811 - val_loss: 0.1236 - val_acc: 0.9606\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9829\n",
      "Epoch 00031: val_loss did not improve from 0.12361\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0616 - acc: 0.9829 - val_loss: 0.1252 - val_acc: 0.9634\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9825\n",
      "Epoch 00032: val_loss improved from 0.12361 to 0.12257, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/032-0.1226.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0595 - acc: 0.9825 - val_loss: 0.1226 - val_acc: 0.9625\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9845\n",
      "Epoch 00033: val_loss improved from 0.12257 to 0.11929, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/033-0.1193.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0553 - acc: 0.9845 - val_loss: 0.1193 - val_acc: 0.9653\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9842\n",
      "Epoch 00034: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0538 - acc: 0.9842 - val_loss: 0.1379 - val_acc: 0.9599\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9854\n",
      "Epoch 00035: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0520 - acc: 0.9854 - val_loss: 0.1304 - val_acc: 0.9637\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9853\n",
      "Epoch 00036: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0497 - acc: 0.9853 - val_loss: 0.1290 - val_acc: 0.9623\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9850\n",
      "Epoch 00037: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0495 - acc: 0.9850 - val_loss: 0.1242 - val_acc: 0.9641\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9871\n",
      "Epoch 00038: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0446 - acc: 0.9871 - val_loss: 0.1235 - val_acc: 0.9646\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9874\n",
      "Epoch 00039: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0429 - acc: 0.9874 - val_loss: 0.1229 - val_acc: 0.9637\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9889\n",
      "Epoch 00040: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0391 - acc: 0.9888 - val_loss: 0.1347 - val_acc: 0.9620\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9887\n",
      "Epoch 00041: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0392 - acc: 0.9887 - val_loss: 0.1651 - val_acc: 0.9509\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9896\n",
      "Epoch 00042: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0365 - acc: 0.9896 - val_loss: 0.1290 - val_acc: 0.9651\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9901\n",
      "Epoch 00043: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0342 - acc: 0.9900 - val_loss: 0.1358 - val_acc: 0.9630\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9882\n",
      "Epoch 00044: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0403 - acc: 0.9882 - val_loss: 0.1301 - val_acc: 0.9632\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9921\n",
      "Epoch 00045: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0294 - acc: 0.9921 - val_loss: 0.1266 - val_acc: 0.9630\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9921\n",
      "Epoch 00046: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0293 - acc: 0.9921 - val_loss: 0.1307 - val_acc: 0.9634\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9928\n",
      "Epoch 00047: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0269 - acc: 0.9928 - val_loss: 0.1291 - val_acc: 0.9632\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9925\n",
      "Epoch 00048: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0275 - acc: 0.9925 - val_loss: 0.1280 - val_acc: 0.9653\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9930\n",
      "Epoch 00049: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0257 - acc: 0.9930 - val_loss: 0.1499 - val_acc: 0.9602\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9937\n",
      "Epoch 00050: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0242 - acc: 0.9937 - val_loss: 0.1340 - val_acc: 0.9658\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9934\n",
      "Epoch 00051: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0237 - acc: 0.9934 - val_loss: 0.1298 - val_acc: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9934\n",
      "Epoch 00052: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0232 - acc: 0.9934 - val_loss: 0.1383 - val_acc: 0.9655\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9950\n",
      "Epoch 00053: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0196 - acc: 0.9950 - val_loss: 0.1518 - val_acc: 0.9613\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9950\n",
      "Epoch 00054: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0201 - acc: 0.9950 - val_loss: 0.1307 - val_acc: 0.9644\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9940\n",
      "Epoch 00055: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0220 - acc: 0.9940 - val_loss: 0.1416 - val_acc: 0.9641\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9949\n",
      "Epoch 00056: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0189 - acc: 0.9949 - val_loss: 0.1492 - val_acc: 0.9613\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9949\n",
      "Epoch 00057: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0190 - acc: 0.9949 - val_loss: 0.1293 - val_acc: 0.9674\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9961\n",
      "Epoch 00058: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0158 - acc: 0.9961 - val_loss: 0.1392 - val_acc: 0.9648\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9961\n",
      "Epoch 00059: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0163 - acc: 0.9960 - val_loss: 0.1465 - val_acc: 0.9623\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9920\n",
      "Epoch 00060: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0278 - acc: 0.9920 - val_loss: 0.1459 - val_acc: 0.9655\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9978\n",
      "Epoch 00061: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0111 - acc: 0.9978 - val_loss: 0.1377 - val_acc: 0.9658\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9965\n",
      "Epoch 00062: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0145 - acc: 0.9965 - val_loss: 0.1376 - val_acc: 0.9646\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9962\n",
      "Epoch 00063: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0149 - acc: 0.9962 - val_loss: 0.1452 - val_acc: 0.9651\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9966\n",
      "Epoch 00064: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0133 - acc: 0.9966 - val_loss: 0.1415 - val_acc: 0.9674\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9960\n",
      "Epoch 00065: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0148 - acc: 0.9960 - val_loss: 0.1455 - val_acc: 0.9623\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9967\n",
      "Epoch 00066: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0125 - acc: 0.9967 - val_loss: 0.1499 - val_acc: 0.9637\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9964\n",
      "Epoch 00067: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0139 - acc: 0.9964 - val_loss: 0.1451 - val_acc: 0.9648\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9971\n",
      "Epoch 00068: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0120 - acc: 0.9971 - val_loss: 0.1482 - val_acc: 0.9658\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9969\n",
      "Epoch 00069: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0125 - acc: 0.9969 - val_loss: 0.1507 - val_acc: 0.9627\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9967\n",
      "Epoch 00070: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0122 - acc: 0.9967 - val_loss: 0.1499 - val_acc: 0.9632\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9968\n",
      "Epoch 00071: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0114 - acc: 0.9968 - val_loss: 0.1493 - val_acc: 0.9660\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9973\n",
      "Epoch 00072: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0110 - acc: 0.9973 - val_loss: 0.1534 - val_acc: 0.9644\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9970\n",
      "Epoch 00073: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0115 - acc: 0.9970 - val_loss: 0.1792 - val_acc: 0.9592\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9954\n",
      "Epoch 00074: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0175 - acc: 0.9954 - val_loss: 0.1365 - val_acc: 0.9700\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9983\n",
      "Epoch 00075: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0081 - acc: 0.9983 - val_loss: 0.1587 - val_acc: 0.9616\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9971\n",
      "Epoch 00076: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0112 - acc: 0.9971 - val_loss: 0.1543 - val_acc: 0.9646\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9972\n",
      "Epoch 00077: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0099 - acc: 0.9972 - val_loss: 0.1581 - val_acc: 0.9627\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9975\n",
      "Epoch 00078: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0098 - acc: 0.9975 - val_loss: 0.1591 - val_acc: 0.9609\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9983\n",
      "Epoch 00079: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0073 - acc: 0.9983 - val_loss: 0.1389 - val_acc: 0.9688\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9986\n",
      "Epoch 00080: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0069 - acc: 0.9986 - val_loss: 0.1546 - val_acc: 0.9658\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9975\n",
      "Epoch 00081: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0098 - acc: 0.9975 - val_loss: 0.1767 - val_acc: 0.9618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9977\n",
      "Epoch 00082: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0086 - acc: 0.9977 - val_loss: 0.1681 - val_acc: 0.9620\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9980\n",
      "Epoch 00083: val_loss did not improve from 0.11929\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0078 - acc: 0.9980 - val_loss: 0.1556 - val_acc: 0.9658\n",
      "\n",
      "1D_CNN_custom_tanh_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 672us/sample - loss: 0.1752 - acc: 0.9485\n",
      "Loss: 0.1751945678535404 Accuracy: 0.9484943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_custom_tanh_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_tanh_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 513us/sample - loss: 2.7258 - acc: 0.0993\n",
      "Loss: 2.7257815188707966 Accuracy: 0.09927311\n",
      "\n",
      "1D_CNN_custom_tanh_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 653us/sample - loss: 2.0970 - acc: 0.3396\n",
      "Loss: 2.096959860401609 Accuracy: 0.33956388\n",
      "\n",
      "1D_CNN_custom_tanh_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 736us/sample - loss: 1.5901 - acc: 0.5009\n",
      "Loss: 1.5901038102519351 Accuracy: 0.5009346\n",
      "\n",
      "1D_CNN_custom_tanh_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 745us/sample - loss: 1.1904 - acc: 0.6422\n",
      "Loss: 1.1904275558695492 Accuracy: 0.64215994\n",
      "\n",
      "1D_CNN_custom_tanh_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 805us/sample - loss: 0.8895 - acc: 0.7429\n",
      "Loss: 0.8895337905591646 Accuracy: 0.74288684\n",
      "\n",
      "1D_CNN_custom_tanh_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 815us/sample - loss: 0.5498 - acc: 0.8474\n",
      "Loss: 0.5498361694726122 Accuracy: 0.847352\n",
      "\n",
      "1D_CNN_custom_tanh_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 813us/sample - loss: 0.2675 - acc: 0.9173\n",
      "Loss: 0.26745054775371235 Accuracy: 0.91734165\n",
      "\n",
      "1D_CNN_custom_tanh_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 815us/sample - loss: 0.1836 - acc: 0.9452\n",
      "Loss: 0.183581630432222 Accuracy: 0.94517136\n",
      "\n",
      "1D_CNN_custom_tanh_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 856us/sample - loss: 0.1752 - acc: 0.9485\n",
      "Loss: 0.1751945678535404 Accuracy: 0.9484943\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_tanh_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_tanh_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 574us/sample - loss: 8.1900 - acc: 0.0970\n",
      "Loss: 8.190012983023069 Accuracy: 0.09698857\n",
      "\n",
      "1D_CNN_custom_tanh_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 755us/sample - loss: 5.4953 - acc: 0.3294\n",
      "Loss: 5.495278421443571 Accuracy: 0.32938734\n",
      "\n",
      "1D_CNN_custom_tanh_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 824us/sample - loss: 2.7775 - acc: 0.5751\n",
      "Loss: 2.7775485998868694 Accuracy: 0.5750779\n",
      "\n",
      "1D_CNN_custom_tanh_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 837us/sample - loss: 1.5347 - acc: 0.6945\n",
      "Loss: 1.5347417957933769 Accuracy: 0.6944964\n",
      "\n",
      "1D_CNN_custom_tanh_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 868us/sample - loss: 1.1325 - acc: 0.7462\n",
      "Loss: 1.1325290969351494 Accuracy: 0.74620974\n",
      "\n",
      "1D_CNN_custom_tanh_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 848us/sample - loss: 0.5991 - acc: 0.8492\n",
      "Loss: 0.5990967267522683 Accuracy: 0.84922117\n",
      "\n",
      "1D_CNN_custom_tanh_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 910us/sample - loss: 0.3099 - acc: 0.9198\n",
      "Loss: 0.3099434817629564 Accuracy: 0.91983384\n",
      "\n",
      "1D_CNN_custom_tanh_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 905us/sample - loss: 0.2267 - acc: 0.9481\n",
      "Loss: 0.22667840392960317 Accuracy: 0.94807893\n",
      "\n",
      "1D_CNN_custom_tanh_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 938us/sample - loss: 0.2030 - acc: 0.9558\n",
      "Loss: 0.20299070183601645 Accuracy: 0.9557632\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_tanh_DO'\n",
    "\n",
    "with open(path.join(log_dir, base+'_last'), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
