{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=64, strides=1, padding='same', \n",
    "                      input_shape=input_shape)) \n",
    "    model.add(Activation('tanh'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=64*(2**int((i+1)/4)), strides=1, \n",
    "                          padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.7472 - acc: 0.1051\n",
      "Epoch 00001: val_loss improved from inf to 2.73355, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_1_conv_checkpoint/001-2.7335.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 2.7472 - acc: 0.1051 - val_loss: 2.7335 - val_acc: 0.1020\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.5341 - acc: 0.2329\n",
      "Epoch 00002: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 2.5339 - acc: 0.2329 - val_loss: 2.8047 - val_acc: 0.1048\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3182 - acc: 0.3169\n",
      "Epoch 00003: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 2.3183 - acc: 0.3169 - val_loss: 2.9142 - val_acc: 0.1090\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1113 - acc: 0.3870\n",
      "Epoch 00004: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 2.1112 - acc: 0.3871 - val_loss: 3.0279 - val_acc: 0.1141\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9371 - acc: 0.4399\n",
      "Epoch 00005: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 1.9371 - acc: 0.4399 - val_loss: 3.1562 - val_acc: 0.1104\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7948 - acc: 0.4851\n",
      "Epoch 00006: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.7947 - acc: 0.4852 - val_loss: 3.3070 - val_acc: 0.1137\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6736 - acc: 0.5219\n",
      "Epoch 00007: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 1.6738 - acc: 0.5217 - val_loss: 3.4550 - val_acc: 0.1141\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5741 - acc: 0.5497\n",
      "Epoch 00008: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.5741 - acc: 0.5497 - val_loss: 3.5931 - val_acc: 0.1132\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4853 - acc: 0.5775\n",
      "Epoch 00009: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.4853 - acc: 0.5775 - val_loss: 3.7413 - val_acc: 0.1148\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4059 - acc: 0.6019\n",
      "Epoch 00010: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.4058 - acc: 0.6019 - val_loss: 3.8846 - val_acc: 0.1127\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3374 - acc: 0.6216\n",
      "Epoch 00011: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 1.3374 - acc: 0.6216 - val_loss: 4.0098 - val_acc: 0.1155\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2717 - acc: 0.6429\n",
      "Epoch 00012: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 1.2717 - acc: 0.6429 - val_loss: 4.1619 - val_acc: 0.1088\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2123 - acc: 0.6606\n",
      "Epoch 00013: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 1.2123 - acc: 0.6606 - val_loss: 4.3204 - val_acc: 0.1123\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1599 - acc: 0.6754\n",
      "Epoch 00014: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 1.1598 - acc: 0.6754 - val_loss: 4.4559 - val_acc: 0.1111\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1094 - acc: 0.6917\n",
      "Epoch 00015: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.1093 - acc: 0.6917 - val_loss: 4.5952 - val_acc: 0.1083\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0656 - acc: 0.7033\n",
      "Epoch 00016: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.0658 - acc: 0.7033 - val_loss: 4.7296 - val_acc: 0.1088\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0247 - acc: 0.7145\n",
      "Epoch 00017: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 1.0246 - acc: 0.7145 - val_loss: 4.8877 - val_acc: 0.1137\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9834 - acc: 0.7275\n",
      "Epoch 00018: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.9834 - acc: 0.7275 - val_loss: 5.0197 - val_acc: 0.1123\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9459 - acc: 0.7410\n",
      "Epoch 00019: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.9460 - acc: 0.7410 - val_loss: 5.1197 - val_acc: 0.1102\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9117 - acc: 0.7517\n",
      "Epoch 00020: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.9119 - acc: 0.7517 - val_loss: 5.2693 - val_acc: 0.1097\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8783 - acc: 0.7581\n",
      "Epoch 00021: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.8782 - acc: 0.7581 - val_loss: 5.4170 - val_acc: 0.1111\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8504 - acc: 0.7677\n",
      "Epoch 00022: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.8505 - acc: 0.7677 - val_loss: 5.5356 - val_acc: 0.1081\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8206 - acc: 0.7769\n",
      "Epoch 00023: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.8206 - acc: 0.7769 - val_loss: 5.6784 - val_acc: 0.1055\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7932 - acc: 0.7833\n",
      "Epoch 00024: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.7934 - acc: 0.7832 - val_loss: 5.7957 - val_acc: 0.1076\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7680 - acc: 0.7909\n",
      "Epoch 00025: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.7680 - acc: 0.7909 - val_loss: 5.9357 - val_acc: 0.1097\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7454 - acc: 0.7966\n",
      "Epoch 00026: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.7453 - acc: 0.7966 - val_loss: 6.0364 - val_acc: 0.1092\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7219 - acc: 0.8051\n",
      "Epoch 00027: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.7221 - acc: 0.8050 - val_loss: 6.1519 - val_acc: 0.1095\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7007 - acc: 0.8112\n",
      "Epoch 00028: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.7006 - acc: 0.8112 - val_loss: 6.2713 - val_acc: 0.1081\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.8170\n",
      "Epoch 00029: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.6805 - acc: 0.8170 - val_loss: 6.3625 - val_acc: 0.1055\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6623 - acc: 0.8213\n",
      "Epoch 00030: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.6623 - acc: 0.8214 - val_loss: 6.4796 - val_acc: 0.1060\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6432 - acc: 0.8262\n",
      "Epoch 00031: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.6432 - acc: 0.8262 - val_loss: 6.5696 - val_acc: 0.1060\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6244 - acc: 0.8324\n",
      "Epoch 00032: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.6245 - acc: 0.8324 - val_loss: 6.6875 - val_acc: 0.1051\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6062 - acc: 0.8387\n",
      "Epoch 00033: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.6062 - acc: 0.8387 - val_loss: 6.7828 - val_acc: 0.1046\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5942 - acc: 0.8410\n",
      "Epoch 00034: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.5941 - acc: 0.8410 - val_loss: 6.8900 - val_acc: 0.1072\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5768 - acc: 0.8450\n",
      "Epoch 00035: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.5768 - acc: 0.8450 - val_loss: 6.9653 - val_acc: 0.1025\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5621 - acc: 0.8493\n",
      "Epoch 00036: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.5621 - acc: 0.8493 - val_loss: 7.0692 - val_acc: 0.0999\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.8542\n",
      "Epoch 00037: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.5483 - acc: 0.8542 - val_loss: 7.1288 - val_acc: 0.1011\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.8580\n",
      "Epoch 00038: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.5335 - acc: 0.8580 - val_loss: 7.2194 - val_acc: 0.1023\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.8590\n",
      "Epoch 00039: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.5229 - acc: 0.8591 - val_loss: 7.2939 - val_acc: 0.1020\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.8651\n",
      "Epoch 00040: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.5097 - acc: 0.8650 - val_loss: 7.3904 - val_acc: 0.1025\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4958 - acc: 0.8680\n",
      "Epoch 00041: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.4958 - acc: 0.8680 - val_loss: 7.4545 - val_acc: 0.1023\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.8711\n",
      "Epoch 00042: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.4864 - acc: 0.8711 - val_loss: 7.5431 - val_acc: 0.1025\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4736 - acc: 0.8750\n",
      "Epoch 00043: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.4736 - acc: 0.8750 - val_loss: 7.6025 - val_acc: 0.1034\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4630 - acc: 0.8785\n",
      "Epoch 00044: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.4630 - acc: 0.8785 - val_loss: 7.6784 - val_acc: 0.1051\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4549 - acc: 0.8789\n",
      "Epoch 00045: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.4549 - acc: 0.8789 - val_loss: 7.7578 - val_acc: 0.1062\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8831\n",
      "Epoch 00046: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.4425 - acc: 0.8831 - val_loss: 7.8158 - val_acc: 0.1016\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4309 - acc: 0.8891\n",
      "Epoch 00047: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.4309 - acc: 0.8891 - val_loss: 7.8870 - val_acc: 0.1034\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8878\n",
      "Epoch 00048: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.4235 - acc: 0.8878 - val_loss: 7.9496 - val_acc: 0.1020\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4169 - acc: 0.8903\n",
      "Epoch 00049: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.4169 - acc: 0.8903 - val_loss: 8.0107 - val_acc: 0.1011\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4052 - acc: 0.8929\n",
      "Epoch 00050: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.4053 - acc: 0.8929 - val_loss: 8.0703 - val_acc: 0.1004\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8958\n",
      "Epoch 00051: val_loss did not improve from 2.73355\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.3995 - acc: 0.8958 - val_loss: 8.1296 - val_acc: 0.0976\n",
      "\n",
      "1D_CNN_custom_tanh_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 337us/sample - loss: 2.7252 - acc: 0.0953\n",
      "Loss: 2.7251594565243984 Accuracy: 0.0953271\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3742 - acc: 0.2516\n",
      "Epoch 00001: val_loss improved from inf to 2.17916, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_2_conv_checkpoint/001-2.1792.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 2.3742 - acc: 0.2516 - val_loss: 2.1792 - val_acc: 0.3249\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8823 - acc: 0.4165\n",
      "Epoch 00002: val_loss improved from 2.17916 to 2.06719, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_2_conv_checkpoint/002-2.0672.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.8824 - acc: 0.4165 - val_loss: 2.0672 - val_acc: 0.3534\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5938 - acc: 0.5090\n",
      "Epoch 00003: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.5938 - acc: 0.5090 - val_loss: 2.0789 - val_acc: 0.3492\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3879 - acc: 0.5736\n",
      "Epoch 00004: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.3880 - acc: 0.5736 - val_loss: 2.1344 - val_acc: 0.3443\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2272 - acc: 0.6241\n",
      "Epoch 00005: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.2272 - acc: 0.6241 - val_loss: 2.1945 - val_acc: 0.3571\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0913 - acc: 0.6671\n",
      "Epoch 00006: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.0912 - acc: 0.6671 - val_loss: 2.2989 - val_acc: 0.3564\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9825 - acc: 0.7006\n",
      "Epoch 00007: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9824 - acc: 0.7006 - val_loss: 2.3770 - val_acc: 0.3587\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8796 - acc: 0.7315\n",
      "Epoch 00008: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8796 - acc: 0.7315 - val_loss: 2.4935 - val_acc: 0.3578\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7956 - acc: 0.7603\n",
      "Epoch 00009: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7956 - acc: 0.7602 - val_loss: 2.5966 - val_acc: 0.3552\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7153 - acc: 0.7823\n",
      "Epoch 00010: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7153 - acc: 0.7822 - val_loss: 2.7145 - val_acc: 0.3452\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6579 - acc: 0.7994\n",
      "Epoch 00011: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6579 - acc: 0.7995 - val_loss: 2.8182 - val_acc: 0.3466\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5975 - acc: 0.8176\n",
      "Epoch 00012: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5975 - acc: 0.8176 - val_loss: 2.9256 - val_acc: 0.3552\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.8343\n",
      "Epoch 00013: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5474 - acc: 0.8343 - val_loss: 3.0317 - val_acc: 0.3564\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8480\n",
      "Epoch 00014: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5019 - acc: 0.8480 - val_loss: 3.1616 - val_acc: 0.3415\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8582\n",
      "Epoch 00015: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4647 - acc: 0.8581 - val_loss: 3.2448 - val_acc: 0.3527\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.8695\n",
      "Epoch 00016: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4301 - acc: 0.8695 - val_loss: 3.3389 - val_acc: 0.3459\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8794\n",
      "Epoch 00017: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3965 - acc: 0.8794 - val_loss: 3.3966 - val_acc: 0.3459\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8871\n",
      "Epoch 00018: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3731 - acc: 0.8871 - val_loss: 3.5205 - val_acc: 0.3524\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8928\n",
      "Epoch 00019: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3520 - acc: 0.8928 - val_loss: 3.6346 - val_acc: 0.3427\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.9009\n",
      "Epoch 00020: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3247 - acc: 0.9009 - val_loss: 3.7205 - val_acc: 0.3401\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9066\n",
      "Epoch 00021: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3102 - acc: 0.9066 - val_loss: 3.7572 - val_acc: 0.3489\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9130\n",
      "Epoch 00022: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2884 - acc: 0.9130 - val_loss: 3.8732 - val_acc: 0.3531\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9181\n",
      "Epoch 00023: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2726 - acc: 0.9181 - val_loss: 3.9500 - val_acc: 0.3494\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9191\n",
      "Epoch 00024: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2637 - acc: 0.9190 - val_loss: 3.9965 - val_acc: 0.3534\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.9204\n",
      "Epoch 00025: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2531 - acc: 0.9204 - val_loss: 4.0426 - val_acc: 0.3557\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9294\n",
      "Epoch 00026: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2329 - acc: 0.9294 - val_loss: 4.1604 - val_acc: 0.3468\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9299\n",
      "Epoch 00027: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2283 - acc: 0.9299 - val_loss: 4.1795 - val_acc: 0.3578\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9345\n",
      "Epoch 00028: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2148 - acc: 0.9345 - val_loss: 4.2719 - val_acc: 0.3557\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9371\n",
      "Epoch 00029: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2069 - acc: 0.9371 - val_loss: 4.2980 - val_acc: 0.3578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9387\n",
      "Epoch 00030: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2006 - acc: 0.9387 - val_loss: 4.3774 - val_acc: 0.3618\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9392\n",
      "Epoch 00031: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1950 - acc: 0.9392 - val_loss: 4.3502 - val_acc: 0.3536\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9409\n",
      "Epoch 00032: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1894 - acc: 0.9409 - val_loss: 4.4700 - val_acc: 0.3645\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9477\n",
      "Epoch 00033: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1723 - acc: 0.9478 - val_loss: 4.5266 - val_acc: 0.3615\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9480\n",
      "Epoch 00034: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1711 - acc: 0.9480 - val_loss: 4.6455 - val_acc: 0.3478\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9457\n",
      "Epoch 00035: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1773 - acc: 0.9457 - val_loss: 4.6085 - val_acc: 0.3573\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9481\n",
      "Epoch 00036: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1704 - acc: 0.9481 - val_loss: 4.6362 - val_acc: 0.3636\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9509\n",
      "Epoch 00037: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1614 - acc: 0.9508 - val_loss: 4.6937 - val_acc: 0.3608\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9502\n",
      "Epoch 00038: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1591 - acc: 0.9502 - val_loss: 4.6917 - val_acc: 0.3583\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9526\n",
      "Epoch 00039: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1557 - acc: 0.9526 - val_loss: 4.7573 - val_acc: 0.3604\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9547\n",
      "Epoch 00040: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1484 - acc: 0.9547 - val_loss: 4.7749 - val_acc: 0.3638\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9550\n",
      "Epoch 00041: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1498 - acc: 0.9550 - val_loss: 4.8106 - val_acc: 0.3611\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9581\n",
      "Epoch 00042: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1397 - acc: 0.9581 - val_loss: 4.8686 - val_acc: 0.3594\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9587\n",
      "Epoch 00043: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1376 - acc: 0.9587 - val_loss: 4.8984 - val_acc: 0.3597\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9581\n",
      "Epoch 00044: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1371 - acc: 0.9581 - val_loss: 4.9340 - val_acc: 0.3620\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9605\n",
      "Epoch 00045: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1305 - acc: 0.9605 - val_loss: 4.9303 - val_acc: 0.3629\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9600\n",
      "Epoch 00046: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1294 - acc: 0.9600 - val_loss: 4.9935 - val_acc: 0.3559\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9616\n",
      "Epoch 00047: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1254 - acc: 0.9616 - val_loss: 5.0019 - val_acc: 0.3625\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9595\n",
      "Epoch 00048: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1306 - acc: 0.9595 - val_loss: 5.0768 - val_acc: 0.3643\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9623\n",
      "Epoch 00049: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1233 - acc: 0.9623 - val_loss: 5.0745 - val_acc: 0.3652\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9612\n",
      "Epoch 00050: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1255 - acc: 0.9612 - val_loss: 5.0664 - val_acc: 0.3692\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9621\n",
      "Epoch 00051: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1222 - acc: 0.9622 - val_loss: 5.1111 - val_acc: 0.3685\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9660\n",
      "Epoch 00052: val_loss did not improve from 2.06719\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1130 - acc: 0.9660 - val_loss: 5.1603 - val_acc: 0.3618\n",
      "\n",
      "1D_CNN_custom_tanh_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 484us/sample - loss: 2.0949 - acc: 0.3421\n",
      "Loss: 2.0949046964833546 Accuracy: 0.34205607\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0257 - acc: 0.3560\n",
      "Epoch 00001: val_loss improved from inf to 1.66671, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_3_conv_checkpoint/001-1.6667.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 2.0256 - acc: 0.3560 - val_loss: 1.6667 - val_acc: 0.4752\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4626 - acc: 0.5434\n",
      "Epoch 00002: val_loss improved from 1.66671 to 1.57152, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_3_conv_checkpoint/002-1.5715.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.4626 - acc: 0.5433 - val_loss: 1.5715 - val_acc: 0.5041\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2407 - acc: 0.6118\n",
      "Epoch 00003: val_loss improved from 1.57152 to 1.52505, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_3_conv_checkpoint/003-1.5251.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.2407 - acc: 0.6118 - val_loss: 1.5251 - val_acc: 0.5225\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0878 - acc: 0.6624\n",
      "Epoch 00004: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0878 - acc: 0.6624 - val_loss: 1.5530 - val_acc: 0.5229\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9606 - acc: 0.7033\n",
      "Epoch 00005: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9605 - acc: 0.7033 - val_loss: 1.5747 - val_acc: 0.5274\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8592 - acc: 0.7342\n",
      "Epoch 00006: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8592 - acc: 0.7342 - val_loss: 1.6118 - val_acc: 0.5297\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7681 - acc: 0.7671\n",
      "Epoch 00007: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7680 - acc: 0.7671 - val_loss: 1.6182 - val_acc: 0.5413\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6815 - acc: 0.7908\n",
      "Epoch 00008: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6815 - acc: 0.7907 - val_loss: 1.6621 - val_acc: 0.5497\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6046 - acc: 0.8160\n",
      "Epoch 00009: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6049 - acc: 0.8159 - val_loss: 1.6705 - val_acc: 0.5514\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.8335\n",
      "Epoch 00010: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5503 - acc: 0.8335 - val_loss: 1.7084 - val_acc: 0.5639\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.8492\n",
      "Epoch 00011: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4956 - acc: 0.8493 - val_loss: 1.7414 - val_acc: 0.5621\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.8646\n",
      "Epoch 00012: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4429 - acc: 0.8646 - val_loss: 1.7613 - val_acc: 0.5600\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3989 - acc: 0.8792\n",
      "Epoch 00013: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3989 - acc: 0.8792 - val_loss: 1.7667 - val_acc: 0.5667\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3640 - acc: 0.8914\n",
      "Epoch 00014: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3640 - acc: 0.8914 - val_loss: 1.7996 - val_acc: 0.5705\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.9039\n",
      "Epoch 00015: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3252 - acc: 0.9038 - val_loss: 1.8358 - val_acc: 0.5684\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9085\n",
      "Epoch 00016: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3023 - acc: 0.9085 - val_loss: 1.8750 - val_acc: 0.5793\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9182\n",
      "Epoch 00017: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2740 - acc: 0.9181 - val_loss: 1.8958 - val_acc: 0.5726\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9217\n",
      "Epoch 00018: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2589 - acc: 0.9217 - val_loss: 1.9055 - val_acc: 0.5758\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9322\n",
      "Epoch 00019: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2348 - acc: 0.9322 - val_loss: 1.9287 - val_acc: 0.5786\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9370\n",
      "Epoch 00020: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2173 - acc: 0.9370 - val_loss: 1.9729 - val_acc: 0.5814\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9399\n",
      "Epoch 00021: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2073 - acc: 0.9398 - val_loss: 2.0120 - val_acc: 0.5788\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9464\n",
      "Epoch 00022: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1877 - acc: 0.9464 - val_loss: 1.9784 - val_acc: 0.5879\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9476\n",
      "Epoch 00023: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1810 - acc: 0.9476 - val_loss: 2.0579 - val_acc: 0.5833\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9543\n",
      "Epoch 00024: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1646 - acc: 0.9544 - val_loss: 2.0861 - val_acc: 0.5854\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9551\n",
      "Epoch 00025: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1587 - acc: 0.9551 - val_loss: 2.1031 - val_acc: 0.5868\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9584\n",
      "Epoch 00026: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1455 - acc: 0.9584 - val_loss: 2.0992 - val_acc: 0.5863\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9592\n",
      "Epoch 00027: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1456 - acc: 0.9592 - val_loss: 2.1626 - val_acc: 0.5875\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9614\n",
      "Epoch 00028: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1409 - acc: 0.9614 - val_loss: 2.1056 - val_acc: 0.5945\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9615\n",
      "Epoch 00029: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1357 - acc: 0.9615 - val_loss: 2.1497 - val_acc: 0.5926\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9631\n",
      "Epoch 00030: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1313 - acc: 0.9631 - val_loss: 2.1826 - val_acc: 0.5942\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9657\n",
      "Epoch 00031: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1212 - acc: 0.9657 - val_loss: 2.1693 - val_acc: 0.5924\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9679\n",
      "Epoch 00032: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1191 - acc: 0.9679 - val_loss: 2.1880 - val_acc: 0.5954\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9691\n",
      "Epoch 00033: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1112 - acc: 0.9691 - val_loss: 2.1892 - val_acc: 0.5945\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9702\n",
      "Epoch 00034: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1080 - acc: 0.9702 - val_loss: 2.2213 - val_acc: 0.5982\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9697\n",
      "Epoch 00035: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1130 - acc: 0.9697 - val_loss: 2.2928 - val_acc: 0.5947\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9721\n",
      "Epoch 00036: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1043 - acc: 0.9721 - val_loss: 2.2591 - val_acc: 0.6000\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9730\n",
      "Epoch 00037: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0964 - acc: 0.9730 - val_loss: 2.2579 - val_acc: 0.6045\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9733\n",
      "Epoch 00038: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0977 - acc: 0.9732 - val_loss: 2.3229 - val_acc: 0.6038\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9743\n",
      "Epoch 00039: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0945 - acc: 0.9743 - val_loss: 2.3320 - val_acc: 0.6000\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9737\n",
      "Epoch 00040: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0970 - acc: 0.9738 - val_loss: 2.3284 - val_acc: 0.6000\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9756\n",
      "Epoch 00041: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0894 - acc: 0.9756 - val_loss: 2.3346 - val_acc: 0.6031\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9755\n",
      "Epoch 00042: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0912 - acc: 0.9755 - val_loss: 2.3600 - val_acc: 0.6014\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9767\n",
      "Epoch 00043: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0901 - acc: 0.9767 - val_loss: 2.3423 - val_acc: 0.6038\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9766\n",
      "Epoch 00044: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0885 - acc: 0.9766 - val_loss: 2.3525 - val_acc: 0.6094\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9772\n",
      "Epoch 00045: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0847 - acc: 0.9772 - val_loss: 2.3761 - val_acc: 0.6024\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9790\n",
      "Epoch 00046: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0777 - acc: 0.9790 - val_loss: 2.3729 - val_acc: 0.6096\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9773\n",
      "Epoch 00047: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0887 - acc: 0.9773 - val_loss: 2.3994 - val_acc: 0.6047\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9780\n",
      "Epoch 00048: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0814 - acc: 0.9779 - val_loss: 2.4352 - val_acc: 0.6019\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9787\n",
      "Epoch 00049: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0783 - acc: 0.9786 - val_loss: 2.4508 - val_acc: 0.6080\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9788\n",
      "Epoch 00050: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0797 - acc: 0.9788 - val_loss: 2.4091 - val_acc: 0.6068\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9813\n",
      "Epoch 00051: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0701 - acc: 0.9813 - val_loss: 2.4696 - val_acc: 0.6026\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9795\n",
      "Epoch 00052: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0750 - acc: 0.9795 - val_loss: 2.4587 - val_acc: 0.6129\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9800\n",
      "Epoch 00053: val_loss did not improve from 1.52505\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0737 - acc: 0.9799 - val_loss: 2.4969 - val_acc: 0.5986\n",
      "\n",
      "1D_CNN_custom_tanh_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 551us/sample - loss: 1.6062 - acc: 0.5001\n",
      "Loss: 1.6061557545716394 Accuracy: 0.50010383\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8688 - acc: 0.3964\n",
      "Epoch 00001: val_loss improved from inf to 1.47487, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/001-1.4749.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.8687 - acc: 0.3964 - val_loss: 1.4749 - val_acc: 0.5465\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3836 - acc: 0.5684\n",
      "Epoch 00002: val_loss improved from 1.47487 to 1.31403, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/002-1.3140.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3835 - acc: 0.5685 - val_loss: 1.3140 - val_acc: 0.6047\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1865 - acc: 0.6386\n",
      "Epoch 00003: val_loss improved from 1.31403 to 1.22083, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/003-1.2208.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1868 - acc: 0.6386 - val_loss: 1.2208 - val_acc: 0.6373\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0551 - acc: 0.6828\n",
      "Epoch 00004: val_loss improved from 1.22083 to 1.19004, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/004-1.1900.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0551 - acc: 0.6827 - val_loss: 1.1900 - val_acc: 0.6366\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9534 - acc: 0.7139\n",
      "Epoch 00005: val_loss improved from 1.19004 to 1.16416, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/005-1.1642.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9533 - acc: 0.7140 - val_loss: 1.1642 - val_acc: 0.6497\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8659 - acc: 0.7396\n",
      "Epoch 00006: val_loss improved from 1.16416 to 1.12629, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/006-1.1263.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8659 - acc: 0.7396 - val_loss: 1.1263 - val_acc: 0.6599\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7862 - acc: 0.7664\n",
      "Epoch 00007: val_loss did not improve from 1.12629\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7862 - acc: 0.7664 - val_loss: 1.1558 - val_acc: 0.6518\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7171 - acc: 0.7884\n",
      "Epoch 00008: val_loss improved from 1.12629 to 1.11034, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/008-1.1103.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7171 - acc: 0.7884 - val_loss: 1.1103 - val_acc: 0.6706\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6569 - acc: 0.8081\n",
      "Epoch 00009: val_loss did not improve from 1.11034\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6569 - acc: 0.8082 - val_loss: 1.1177 - val_acc: 0.6646\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6002 - acc: 0.8250\n",
      "Epoch 00010: val_loss improved from 1.11034 to 1.10338, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_4_conv_checkpoint/010-1.1034.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6003 - acc: 0.8249 - val_loss: 1.1034 - val_acc: 0.6778\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.8366\n",
      "Epoch 00011: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5556 - acc: 0.8366 - val_loss: 1.1094 - val_acc: 0.6739\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.8522\n",
      "Epoch 00012: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5106 - acc: 0.8522 - val_loss: 1.1194 - val_acc: 0.6737\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4726 - acc: 0.8637\n",
      "Epoch 00013: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4726 - acc: 0.8637 - val_loss: 1.1250 - val_acc: 0.6725\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4310 - acc: 0.8750\n",
      "Epoch 00014: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4310 - acc: 0.8750 - val_loss: 1.1310 - val_acc: 0.6778\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8840\n",
      "Epoch 00015: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4041 - acc: 0.8840 - val_loss: 1.1258 - val_acc: 0.6862\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3761 - acc: 0.8920\n",
      "Epoch 00016: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3761 - acc: 0.8920 - val_loss: 1.1403 - val_acc: 0.6820\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.9000\n",
      "Epoch 00017: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3485 - acc: 0.9000 - val_loss: 1.1252 - val_acc: 0.6872\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.9106\n",
      "Epoch 00018: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3215 - acc: 0.9106 - val_loss: 1.1309 - val_acc: 0.6900\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3018 - acc: 0.9146\n",
      "Epoch 00019: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3018 - acc: 0.9146 - val_loss: 1.1733 - val_acc: 0.6914\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9198\n",
      "Epoch 00020: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2824 - acc: 0.9198 - val_loss: 1.1567 - val_acc: 0.6953\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9249\n",
      "Epoch 00021: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2686 - acc: 0.9250 - val_loss: 1.1699 - val_acc: 0.6923\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9301\n",
      "Epoch 00022: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2504 - acc: 0.9301 - val_loss: 1.2009 - val_acc: 0.6872\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9362\n",
      "Epoch 00023: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2343 - acc: 0.9362 - val_loss: 1.1731 - val_acc: 0.6953\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9368\n",
      "Epoch 00024: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2243 - acc: 0.9367 - val_loss: 1.1905 - val_acc: 0.6953\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9413\n",
      "Epoch 00025: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2112 - acc: 0.9413 - val_loss: 1.1925 - val_acc: 0.6988\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9471\n",
      "Epoch 00026: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1989 - acc: 0.9472 - val_loss: 1.2008 - val_acc: 0.7018\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9475\n",
      "Epoch 00027: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1905 - acc: 0.9475 - val_loss: 1.2517 - val_acc: 0.6946\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9495\n",
      "Epoch 00028: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1825 - acc: 0.9495 - val_loss: 1.2215 - val_acc: 0.7023\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9527\n",
      "Epoch 00029: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1720 - acc: 0.9527 - val_loss: 1.2051 - val_acc: 0.7072\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9543\n",
      "Epoch 00030: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1666 - acc: 0.9542 - val_loss: 1.2285 - val_acc: 0.7023\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9564\n",
      "Epoch 00031: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1612 - acc: 0.9564 - val_loss: 1.2265 - val_acc: 0.7067\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9589\n",
      "Epoch 00032: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1500 - acc: 0.9589 - val_loss: 1.2588 - val_acc: 0.7028\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9612\n",
      "Epoch 00033: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1460 - acc: 0.9612 - val_loss: 1.2338 - val_acc: 0.7116\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9604\n",
      "Epoch 00034: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1473 - acc: 0.9604 - val_loss: 1.2610 - val_acc: 0.7091\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9621\n",
      "Epoch 00035: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1395 - acc: 0.9621 - val_loss: 1.2558 - val_acc: 0.7158\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9648\n",
      "Epoch 00036: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1319 - acc: 0.9648 - val_loss: 1.2730 - val_acc: 0.7137\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9646\n",
      "Epoch 00037: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1304 - acc: 0.9646 - val_loss: 1.2937 - val_acc: 0.7116\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9661\n",
      "Epoch 00038: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1234 - acc: 0.9661 - val_loss: 1.2898 - val_acc: 0.7065\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9673\n",
      "Epoch 00039: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1219 - acc: 0.9673 - val_loss: 1.2840 - val_acc: 0.7172\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9681\n",
      "Epoch 00040: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1178 - acc: 0.9681 - val_loss: 1.2911 - val_acc: 0.7219\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9686\n",
      "Epoch 00041: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1155 - acc: 0.9686 - val_loss: 1.3106 - val_acc: 0.7079\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9708\n",
      "Epoch 00042: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1108 - acc: 0.9708 - val_loss: 1.3051 - val_acc: 0.7163\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9694\n",
      "Epoch 00043: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1086 - acc: 0.9694 - val_loss: 1.3191 - val_acc: 0.7144\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9725\n",
      "Epoch 00044: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1029 - acc: 0.9725 - val_loss: 1.3166 - val_acc: 0.7156\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9726\n",
      "Epoch 00045: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1039 - acc: 0.9726 - val_loss: 1.3273 - val_acc: 0.7149\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9723\n",
      "Epoch 00046: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1030 - acc: 0.9723 - val_loss: 1.3369 - val_acc: 0.7128\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9727\n",
      "Epoch 00047: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1000 - acc: 0.9727 - val_loss: 1.3278 - val_acc: 0.7163\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9741\n",
      "Epoch 00048: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0982 - acc: 0.9741 - val_loss: 1.3349 - val_acc: 0.7165\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9760\n",
      "Epoch 00049: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0930 - acc: 0.9760 - val_loss: 1.3545 - val_acc: 0.7147\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9754\n",
      "Epoch 00050: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0894 - acc: 0.9754 - val_loss: 1.3507 - val_acc: 0.7214\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9759\n",
      "Epoch 00051: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0895 - acc: 0.9759 - val_loss: 1.4011 - val_acc: 0.7112\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9766\n",
      "Epoch 00052: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0889 - acc: 0.9766 - val_loss: 1.3615 - val_acc: 0.7172\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9773\n",
      "Epoch 00053: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0859 - acc: 0.9773 - val_loss: 1.3604 - val_acc: 0.7228\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9771\n",
      "Epoch 00054: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0861 - acc: 0.9771 - val_loss: 1.3840 - val_acc: 0.7207\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9777\n",
      "Epoch 00055: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0852 - acc: 0.9777 - val_loss: 1.3575 - val_acc: 0.7277\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9771\n",
      "Epoch 00056: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0852 - acc: 0.9771 - val_loss: 1.3658 - val_acc: 0.7195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9780\n",
      "Epoch 00057: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0812 - acc: 0.9780 - val_loss: 1.3761 - val_acc: 0.7223\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9791\n",
      "Epoch 00058: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0832 - acc: 0.9791 - val_loss: 1.3985 - val_acc: 0.7179\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9802\n",
      "Epoch 00059: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0740 - acc: 0.9802 - val_loss: 1.4026 - val_acc: 0.7198\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9793\n",
      "Epoch 00060: val_loss did not improve from 1.10338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0795 - acc: 0.9793 - val_loss: 1.3716 - val_acc: 0.7242\n",
      "\n",
      "1D_CNN_custom_tanh_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 589us/sample - loss: 1.2182 - acc: 0.6370\n",
      "Loss: 1.2181774709826318 Accuracy: 0.63696784\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8787 - acc: 0.3919\n",
      "Epoch 00001: val_loss improved from inf to 1.48990, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/001-1.4899.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.8786 - acc: 0.3919 - val_loss: 1.4899 - val_acc: 0.5332\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4276 - acc: 0.5520\n",
      "Epoch 00002: val_loss improved from 1.48990 to 1.29452, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/002-1.2945.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.4275 - acc: 0.5520 - val_loss: 1.2945 - val_acc: 0.6070\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2353 - acc: 0.6238\n",
      "Epoch 00003: val_loss improved from 1.29452 to 1.16028, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/003-1.1603.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.2352 - acc: 0.6238 - val_loss: 1.1603 - val_acc: 0.6541\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1094 - acc: 0.6684\n",
      "Epoch 00004: val_loss improved from 1.16028 to 1.08298, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/004-1.0830.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1095 - acc: 0.6684 - val_loss: 1.0830 - val_acc: 0.6820\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0442 - acc: 0.6904\n",
      "Epoch 00005: val_loss improved from 1.08298 to 1.04889, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/005-1.0489.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0441 - acc: 0.6904 - val_loss: 1.0489 - val_acc: 0.6925\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9505 - acc: 0.7150\n",
      "Epoch 00006: val_loss improved from 1.04889 to 0.99427, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/006-0.9943.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9505 - acc: 0.7150 - val_loss: 0.9943 - val_acc: 0.7042\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8888 - acc: 0.7368\n",
      "Epoch 00007: val_loss improved from 0.99427 to 0.98447, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/007-0.9845.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8889 - acc: 0.7368 - val_loss: 0.9845 - val_acc: 0.7035\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8303 - acc: 0.7542\n",
      "Epoch 00008: val_loss improved from 0.98447 to 0.92581, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/008-0.9258.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8304 - acc: 0.7542 - val_loss: 0.9258 - val_acc: 0.7235\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7750 - acc: 0.7723\n",
      "Epoch 00009: val_loss improved from 0.92581 to 0.90660, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/009-0.9066.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7750 - acc: 0.7722 - val_loss: 0.9066 - val_acc: 0.7270\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7255 - acc: 0.7866\n",
      "Epoch 00010: val_loss did not improve from 0.90660\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7256 - acc: 0.7866 - val_loss: 0.9131 - val_acc: 0.7321\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6893 - acc: 0.7981\n",
      "Epoch 00011: val_loss improved from 0.90660 to 0.87678, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/011-0.8768.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6894 - acc: 0.7980 - val_loss: 0.8768 - val_acc: 0.7410\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6486 - acc: 0.8096\n",
      "Epoch 00012: val_loss improved from 0.87678 to 0.86560, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/012-0.8656.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6485 - acc: 0.8097 - val_loss: 0.8656 - val_acc: 0.7503\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6129 - acc: 0.8195\n",
      "Epoch 00013: val_loss improved from 0.86560 to 0.85728, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/013-0.8573.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6129 - acc: 0.8195 - val_loss: 0.8573 - val_acc: 0.7508\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5755 - acc: 0.8325\n",
      "Epoch 00014: val_loss improved from 0.85728 to 0.84701, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/014-0.8470.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5755 - acc: 0.8325 - val_loss: 0.8470 - val_acc: 0.7556\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.8420\n",
      "Epoch 00015: val_loss improved from 0.84701 to 0.83819, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/015-0.8382.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5453 - acc: 0.8421 - val_loss: 0.8382 - val_acc: 0.7598\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.8498\n",
      "Epoch 00016: val_loss improved from 0.83819 to 0.82805, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/016-0.8281.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5201 - acc: 0.8497 - val_loss: 0.8281 - val_acc: 0.7561\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8610\n",
      "Epoch 00017: val_loss did not improve from 0.82805\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4852 - acc: 0.8611 - val_loss: 0.8374 - val_acc: 0.7631\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.8667\n",
      "Epoch 00018: val_loss did not improve from 0.82805\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4608 - acc: 0.8667 - val_loss: 0.8342 - val_acc: 0.7575\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4377 - acc: 0.8739\n",
      "Epoch 00019: val_loss did not improve from 0.82805\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4377 - acc: 0.8739 - val_loss: 0.8365 - val_acc: 0.7682\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8808\n",
      "Epoch 00020: val_loss improved from 0.82805 to 0.80678, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_5_conv_checkpoint/020-0.8068.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4131 - acc: 0.8808 - val_loss: 0.8068 - val_acc: 0.7720\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8869\n",
      "Epoch 00021: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3921 - acc: 0.8869 - val_loss: 0.8281 - val_acc: 0.7734\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8959\n",
      "Epoch 00022: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3697 - acc: 0.8959 - val_loss: 0.8221 - val_acc: 0.7703\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.9016\n",
      "Epoch 00023: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3503 - acc: 0.9016 - val_loss: 0.8207 - val_acc: 0.7706\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.9059\n",
      "Epoch 00024: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3348 - acc: 0.9059 - val_loss: 0.8313 - val_acc: 0.7643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.9083\n",
      "Epoch 00025: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3388 - acc: 0.9083 - val_loss: 0.8359 - val_acc: 0.7659\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9168\n",
      "Epoch 00026: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2973 - acc: 0.9168 - val_loss: 0.8213 - val_acc: 0.7699\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9191\n",
      "Epoch 00027: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2897 - acc: 0.9191 - val_loss: 0.8444 - val_acc: 0.7706\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9252\n",
      "Epoch 00028: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2726 - acc: 0.9252 - val_loss: 0.8287 - val_acc: 0.7720\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9273\n",
      "Epoch 00029: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2634 - acc: 0.9273 - val_loss: 0.8434 - val_acc: 0.7694\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9323\n",
      "Epoch 00030: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2477 - acc: 0.9323 - val_loss: 0.8507 - val_acc: 0.7645\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9354\n",
      "Epoch 00031: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2372 - acc: 0.9354 - val_loss: 0.8284 - val_acc: 0.7775\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9380\n",
      "Epoch 00032: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2279 - acc: 0.9380 - val_loss: 0.8391 - val_acc: 0.7741\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9421\n",
      "Epoch 00033: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2134 - acc: 0.9422 - val_loss: 0.8463 - val_acc: 0.7775\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9447\n",
      "Epoch 00034: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2063 - acc: 0.9447 - val_loss: 0.8721 - val_acc: 0.7694\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9470\n",
      "Epoch 00035: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1984 - acc: 0.9470 - val_loss: 0.8419 - val_acc: 0.7761\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9471\n",
      "Epoch 00036: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1929 - acc: 0.9472 - val_loss: 0.9211 - val_acc: 0.7612\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9506\n",
      "Epoch 00037: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1849 - acc: 0.9506 - val_loss: 0.8505 - val_acc: 0.7817\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9544\n",
      "Epoch 00038: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1710 - acc: 0.9544 - val_loss: 0.9088 - val_acc: 0.7722\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9519\n",
      "Epoch 00039: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1749 - acc: 0.9519 - val_loss: 0.9011 - val_acc: 0.7752\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9579\n",
      "Epoch 00040: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1621 - acc: 0.9579 - val_loss: 0.8777 - val_acc: 0.7729\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9584\n",
      "Epoch 00041: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1600 - acc: 0.9584 - val_loss: 0.8631 - val_acc: 0.7806\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9600\n",
      "Epoch 00042: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1504 - acc: 0.9600 - val_loss: 0.8727 - val_acc: 0.7782\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9608\n",
      "Epoch 00043: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1476 - acc: 0.9608 - val_loss: 0.8766 - val_acc: 0.7838\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9627\n",
      "Epoch 00044: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1424 - acc: 0.9627 - val_loss: 0.8708 - val_acc: 0.7850\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9641\n",
      "Epoch 00045: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1362 - acc: 0.9641 - val_loss: 0.8691 - val_acc: 0.7806\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9652\n",
      "Epoch 00046: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1314 - acc: 0.9652 - val_loss: 0.8762 - val_acc: 0.7848\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9652\n",
      "Epoch 00047: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1315 - acc: 0.9652 - val_loss: 0.8838 - val_acc: 0.7836\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9672\n",
      "Epoch 00048: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1253 - acc: 0.9672 - val_loss: 0.8843 - val_acc: 0.7834\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9670\n",
      "Epoch 00049: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1233 - acc: 0.9670 - val_loss: 0.8996 - val_acc: 0.7817\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9695\n",
      "Epoch 00050: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1175 - acc: 0.9695 - val_loss: 0.9084 - val_acc: 0.7829\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9687\n",
      "Epoch 00051: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1166 - acc: 0.9687 - val_loss: 0.8835 - val_acc: 0.7855\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9724\n",
      "Epoch 00052: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1095 - acc: 0.9724 - val_loss: 0.9116 - val_acc: 0.7773\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9693\n",
      "Epoch 00053: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1117 - acc: 0.9694 - val_loss: 0.9250 - val_acc: 0.7822\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9729\n",
      "Epoch 00054: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1077 - acc: 0.9729 - val_loss: 0.8938 - val_acc: 0.7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9713\n",
      "Epoch 00055: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1065 - acc: 0.9713 - val_loss: 0.9170 - val_acc: 0.7857\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9744\n",
      "Epoch 00056: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0986 - acc: 0.9744 - val_loss: 0.9114 - val_acc: 0.7894\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9738\n",
      "Epoch 00057: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0989 - acc: 0.9738 - val_loss: 0.9070 - val_acc: 0.7876\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9747\n",
      "Epoch 00058: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0975 - acc: 0.9747 - val_loss: 0.9320 - val_acc: 0.7869\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9759\n",
      "Epoch 00059: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0929 - acc: 0.9759 - val_loss: 0.9269 - val_acc: 0.7913\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9751\n",
      "Epoch 00060: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0972 - acc: 0.9751 - val_loss: 0.9273 - val_acc: 0.7871\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9762\n",
      "Epoch 00061: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0907 - acc: 0.9762 - val_loss: 0.9510 - val_acc: 0.7880\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9762\n",
      "Epoch 00062: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0925 - acc: 0.9763 - val_loss: 0.9383 - val_acc: 0.7817\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9767\n",
      "Epoch 00063: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0913 - acc: 0.9767 - val_loss: 0.9277 - val_acc: 0.7894\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9788\n",
      "Epoch 00064: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0829 - acc: 0.9788 - val_loss: 0.9164 - val_acc: 0.7927\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9787\n",
      "Epoch 00065: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0849 - acc: 0.9788 - val_loss: 0.9687 - val_acc: 0.7890\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9778\n",
      "Epoch 00066: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0843 - acc: 0.9777 - val_loss: 0.9341 - val_acc: 0.7964\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9788\n",
      "Epoch 00067: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0828 - acc: 0.9788 - val_loss: 0.9252 - val_acc: 0.7922\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9797\n",
      "Epoch 00068: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0777 - acc: 0.9797 - val_loss: 0.9404 - val_acc: 0.7878\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9784\n",
      "Epoch 00069: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0823 - acc: 0.9784 - val_loss: 0.9336 - val_acc: 0.7920\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9807\n",
      "Epoch 00070: val_loss did not improve from 0.80678\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0751 - acc: 0.9807 - val_loss: 0.9503 - val_acc: 0.7848\n",
      "\n",
      "1D_CNN_custom_tanh_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 616us/sample - loss: 0.9044 - acc: 0.7423\n",
      "Loss: 0.90441693653075 Accuracy: 0.74226373\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9244 - acc: 0.3708\n",
      "Epoch 00001: val_loss improved from inf to 1.47005, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/001-1.4700.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.9243 - acc: 0.3708 - val_loss: 1.4700 - val_acc: 0.5383\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4793 - acc: 0.5311\n",
      "Epoch 00002: val_loss improved from 1.47005 to 1.26668, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/002-1.2667.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.4792 - acc: 0.5311 - val_loss: 1.2667 - val_acc: 0.6159\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2985 - acc: 0.6020\n",
      "Epoch 00003: val_loss improved from 1.26668 to 1.11592, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/003-1.1159.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2987 - acc: 0.6020 - val_loss: 1.1159 - val_acc: 0.6746\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1651 - acc: 0.6476\n",
      "Epoch 00004: val_loss improved from 1.11592 to 1.03219, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/004-1.0322.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1650 - acc: 0.6476 - val_loss: 1.0322 - val_acc: 0.7056\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0650 - acc: 0.6798\n",
      "Epoch 00005: val_loss improved from 1.03219 to 0.96898, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/005-0.9690.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0650 - acc: 0.6798 - val_loss: 0.9690 - val_acc: 0.7142\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9923 - acc: 0.6990\n",
      "Epoch 00006: val_loss improved from 0.96898 to 0.91681, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/006-0.9168.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9922 - acc: 0.6990 - val_loss: 0.9168 - val_acc: 0.7284\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9355 - acc: 0.7196\n",
      "Epoch 00007: val_loss improved from 0.91681 to 0.86874, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/007-0.8687.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9355 - acc: 0.7196 - val_loss: 0.8687 - val_acc: 0.7508\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8811 - acc: 0.7379\n",
      "Epoch 00008: val_loss improved from 0.86874 to 0.81920, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/008-0.8192.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8810 - acc: 0.7379 - val_loss: 0.8192 - val_acc: 0.7696\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8333 - acc: 0.7513\n",
      "Epoch 00009: val_loss improved from 0.81920 to 0.79250, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/009-0.7925.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8333 - acc: 0.7513 - val_loss: 0.7925 - val_acc: 0.7631\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7968 - acc: 0.7630\n",
      "Epoch 00010: val_loss improved from 0.79250 to 0.75350, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/010-0.7535.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7968 - acc: 0.7630 - val_loss: 0.7535 - val_acc: 0.7759\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7615 - acc: 0.7759\n",
      "Epoch 00011: val_loss improved from 0.75350 to 0.72424, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/011-0.7242.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7614 - acc: 0.7759 - val_loss: 0.7242 - val_acc: 0.7943\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7256 - acc: 0.7848\n",
      "Epoch 00012: val_loss improved from 0.72424 to 0.71486, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/012-0.7149.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7256 - acc: 0.7848 - val_loss: 0.7149 - val_acc: 0.7959\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7054 - acc: 0.7895\n",
      "Epoch 00013: val_loss improved from 0.71486 to 0.69493, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/013-0.6949.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7053 - acc: 0.7895 - val_loss: 0.6949 - val_acc: 0.8032\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6744 - acc: 0.8008\n",
      "Epoch 00014: val_loss improved from 0.69493 to 0.67178, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/014-0.6718.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6743 - acc: 0.8008 - val_loss: 0.6718 - val_acc: 0.8053\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6508 - acc: 0.8073\n",
      "Epoch 00015: val_loss improved from 0.67178 to 0.64850, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/015-0.6485.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6508 - acc: 0.8073 - val_loss: 0.6485 - val_acc: 0.8111\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6280 - acc: 0.8174\n",
      "Epoch 00016: val_loss improved from 0.64850 to 0.62562, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/016-0.6256.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6280 - acc: 0.8174 - val_loss: 0.6256 - val_acc: 0.8255\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6071 - acc: 0.8216\n",
      "Epoch 00017: val_loss did not improve from 0.62562\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6071 - acc: 0.8216 - val_loss: 0.6662 - val_acc: 0.8092\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5910 - acc: 0.8261\n",
      "Epoch 00018: val_loss improved from 0.62562 to 0.61740, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/018-0.6174.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5909 - acc: 0.8262 - val_loss: 0.6174 - val_acc: 0.8304\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.8324\n",
      "Epoch 00019: val_loss improved from 0.61740 to 0.59644, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/019-0.5964.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5747 - acc: 0.8324 - val_loss: 0.5964 - val_acc: 0.8269\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.8349\n",
      "Epoch 00020: val_loss did not improve from 0.59644\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5559 - acc: 0.8349 - val_loss: 0.6186 - val_acc: 0.8241\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.8426\n",
      "Epoch 00021: val_loss improved from 0.59644 to 0.56423, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/021-0.5642.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5366 - acc: 0.8426 - val_loss: 0.5642 - val_acc: 0.8418\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.8448\n",
      "Epoch 00022: val_loss did not improve from 0.56423\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5261 - acc: 0.8448 - val_loss: 0.5688 - val_acc: 0.8381\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5088 - acc: 0.8507\n",
      "Epoch 00023: val_loss did not improve from 0.56423\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5088 - acc: 0.8507 - val_loss: 0.5755 - val_acc: 0.8383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4947 - acc: 0.8545\n",
      "Epoch 00024: val_loss improved from 0.56423 to 0.55882, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/024-0.5588.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4946 - acc: 0.8545 - val_loss: 0.5588 - val_acc: 0.8421\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4841 - acc: 0.8582\n",
      "Epoch 00025: val_loss improved from 0.55882 to 0.53920, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/025-0.5392.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4842 - acc: 0.8581 - val_loss: 0.5392 - val_acc: 0.8505\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4697 - acc: 0.8620\n",
      "Epoch 00026: val_loss improved from 0.53920 to 0.53391, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/026-0.5339.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4697 - acc: 0.8620 - val_loss: 0.5339 - val_acc: 0.8549\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4566 - acc: 0.8656\n",
      "Epoch 00027: val_loss improved from 0.53391 to 0.52371, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/027-0.5237.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4566 - acc: 0.8656 - val_loss: 0.5237 - val_acc: 0.8542\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.8687\n",
      "Epoch 00028: val_loss did not improve from 0.52371\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4436 - acc: 0.8688 - val_loss: 0.5347 - val_acc: 0.8546\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8709\n",
      "Epoch 00029: val_loss did not improve from 0.52371\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4362 - acc: 0.8709 - val_loss: 0.5342 - val_acc: 0.8542\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4223 - acc: 0.8763\n",
      "Epoch 00030: val_loss improved from 0.52371 to 0.52190, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/030-0.5219.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4223 - acc: 0.8763 - val_loss: 0.5219 - val_acc: 0.8558\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8799\n",
      "Epoch 00031: val_loss improved from 0.52190 to 0.51495, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/031-0.5149.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4112 - acc: 0.8799 - val_loss: 0.5149 - val_acc: 0.8572\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8813\n",
      "Epoch 00032: val_loss improved from 0.51495 to 0.50493, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/032-0.5049.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4040 - acc: 0.8813 - val_loss: 0.5049 - val_acc: 0.8614\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8831\n",
      "Epoch 00033: val_loss did not improve from 0.50493\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3962 - acc: 0.8831 - val_loss: 0.5217 - val_acc: 0.8549\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8877\n",
      "Epoch 00034: val_loss improved from 0.50493 to 0.49895, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/034-0.4990.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3802 - acc: 0.8877 - val_loss: 0.4990 - val_acc: 0.8612\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8884\n",
      "Epoch 00035: val_loss improved from 0.49895 to 0.49646, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/035-0.4965.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3748 - acc: 0.8884 - val_loss: 0.4965 - val_acc: 0.8605\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3685 - acc: 0.8888\n",
      "Epoch 00036: val_loss improved from 0.49646 to 0.48337, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/036-0.4834.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3685 - acc: 0.8888 - val_loss: 0.4834 - val_acc: 0.8696\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8943\n",
      "Epoch 00037: val_loss did not improve from 0.48337\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3515 - acc: 0.8943 - val_loss: 0.5087 - val_acc: 0.8640\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8930\n",
      "Epoch 00038: val_loss improved from 0.48337 to 0.47716, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/038-0.4772.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3527 - acc: 0.8930 - val_loss: 0.4772 - val_acc: 0.8654\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8980\n",
      "Epoch 00039: val_loss did not improve from 0.47716\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3419 - acc: 0.8980 - val_loss: 0.5019 - val_acc: 0.8600\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.9025\n",
      "Epoch 00040: val_loss did not improve from 0.47716\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3281 - acc: 0.9025 - val_loss: 0.4777 - val_acc: 0.8679\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.9021\n",
      "Epoch 00041: val_loss improved from 0.47716 to 0.47452, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/041-0.4745.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3284 - acc: 0.9021 - val_loss: 0.4745 - val_acc: 0.8710\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.9048\n",
      "Epoch 00042: val_loss did not improve from 0.47452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3199 - acc: 0.9047 - val_loss: 0.4839 - val_acc: 0.8684\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.9069\n",
      "Epoch 00043: val_loss did not improve from 0.47452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3144 - acc: 0.9069 - val_loss: 0.4799 - val_acc: 0.8670\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9089\n",
      "Epoch 00044: val_loss did not improve from 0.47452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3061 - acc: 0.9089 - val_loss: 0.4825 - val_acc: 0.8698\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9102\n",
      "Epoch 00045: val_loss did not improve from 0.47452\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3013 - acc: 0.9102 - val_loss: 0.4845 - val_acc: 0.8742\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9121\n",
      "Epoch 00046: val_loss did not improve from 0.47452\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2920 - acc: 0.9121 - val_loss: 0.4774 - val_acc: 0.8740\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.9137\n",
      "Epoch 00047: val_loss improved from 0.47452 to 0.47437, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/047-0.4744.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2856 - acc: 0.9138 - val_loss: 0.4744 - val_acc: 0.8712\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9136\n",
      "Epoch 00048: val_loss improved from 0.47437 to 0.46888, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/048-0.4689.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2843 - acc: 0.9136 - val_loss: 0.4689 - val_acc: 0.8733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9168\n",
      "Epoch 00049: val_loss did not improve from 0.46888\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2737 - acc: 0.9168 - val_loss: 0.4774 - val_acc: 0.8689\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2680 - acc: 0.9199\n",
      "Epoch 00050: val_loss did not improve from 0.46888\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2679 - acc: 0.9200 - val_loss: 0.4769 - val_acc: 0.8677\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9184\n",
      "Epoch 00051: val_loss did not improve from 0.46888\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2675 - acc: 0.9184 - val_loss: 0.4938 - val_acc: 0.8735\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9208\n",
      "Epoch 00052: val_loss did not improve from 0.46888\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2638 - acc: 0.9208 - val_loss: 0.4718 - val_acc: 0.8721\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9229\n",
      "Epoch 00053: val_loss did not improve from 0.46888\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2535 - acc: 0.9229 - val_loss: 0.4860 - val_acc: 0.8730\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2495 - acc: 0.9236\n",
      "Epoch 00054: val_loss improved from 0.46888 to 0.45860, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_6_conv_checkpoint/054-0.4586.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2495 - acc: 0.9236 - val_loss: 0.4586 - val_acc: 0.8744\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9257\n",
      "Epoch 00055: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2446 - acc: 0.9257 - val_loss: 0.4663 - val_acc: 0.8707\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9262\n",
      "Epoch 00056: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2413 - acc: 0.9262 - val_loss: 0.4719 - val_acc: 0.8775\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9275\n",
      "Epoch 00057: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2357 - acc: 0.9274 - val_loss: 0.4758 - val_acc: 0.8703\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9302\n",
      "Epoch 00058: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2313 - acc: 0.9302 - val_loss: 0.4687 - val_acc: 0.8749\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9331\n",
      "Epoch 00059: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2245 - acc: 0.9331 - val_loss: 0.4728 - val_acc: 0.8763\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9307\n",
      "Epoch 00060: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2253 - acc: 0.9307 - val_loss: 0.4589 - val_acc: 0.8763\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9327\n",
      "Epoch 00061: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2185 - acc: 0.9328 - val_loss: 0.4880 - val_acc: 0.8768\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9370\n",
      "Epoch 00062: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2109 - acc: 0.9370 - val_loss: 0.4777 - val_acc: 0.8765\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9336\n",
      "Epoch 00063: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2150 - acc: 0.9336 - val_loss: 0.4799 - val_acc: 0.8744\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 0.9383\n",
      "Epoch 00064: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2051 - acc: 0.9383 - val_loss: 0.4618 - val_acc: 0.8751\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9354\n",
      "Epoch 00065: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2042 - acc: 0.9354 - val_loss: 0.4636 - val_acc: 0.8791\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9386\n",
      "Epoch 00066: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1981 - acc: 0.9386 - val_loss: 0.4762 - val_acc: 0.8735\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9410\n",
      "Epoch 00067: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1922 - acc: 0.9410 - val_loss: 0.4783 - val_acc: 0.8756\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9396\n",
      "Epoch 00068: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1931 - acc: 0.9395 - val_loss: 0.4974 - val_acc: 0.8721\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9404\n",
      "Epoch 00069: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2022 - acc: 0.9404 - val_loss: 0.4721 - val_acc: 0.8817\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9436\n",
      "Epoch 00070: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1836 - acc: 0.9436 - val_loss: 0.4723 - val_acc: 0.8777\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9423\n",
      "Epoch 00071: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1858 - acc: 0.9423 - val_loss: 0.4770 - val_acc: 0.8758\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9434\n",
      "Epoch 00072: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1803 - acc: 0.9434 - val_loss: 0.4994 - val_acc: 0.8754\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9456\n",
      "Epoch 00073: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1766 - acc: 0.9456 - val_loss: 0.4681 - val_acc: 0.8826\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9474\n",
      "Epoch 00074: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1749 - acc: 0.9474 - val_loss: 0.4707 - val_acc: 0.8779\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9481\n",
      "Epoch 00075: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1697 - acc: 0.9481 - val_loss: 0.4723 - val_acc: 0.8810\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9496\n",
      "Epoch 00076: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1646 - acc: 0.9497 - val_loss: 0.4906 - val_acc: 0.8747\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9478\n",
      "Epoch 00077: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1686 - acc: 0.9478 - val_loss: 0.4671 - val_acc: 0.8812\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9511\n",
      "Epoch 00078: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1605 - acc: 0.9511 - val_loss: 0.4677 - val_acc: 0.8838\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9531\n",
      "Epoch 00079: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1561 - acc: 0.9530 - val_loss: 0.4853 - val_acc: 0.8761\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9515\n",
      "Epoch 00080: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1572 - acc: 0.9515 - val_loss: 0.4626 - val_acc: 0.8845\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9531\n",
      "Epoch 00081: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1517 - acc: 0.9531 - val_loss: 0.4696 - val_acc: 0.8821\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9536\n",
      "Epoch 00082: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1536 - acc: 0.9536 - val_loss: 0.4880 - val_acc: 0.8807\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9523\n",
      "Epoch 00083: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1532 - acc: 0.9523 - val_loss: 0.5013 - val_acc: 0.8758\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9555\n",
      "Epoch 00084: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1487 - acc: 0.9555 - val_loss: 0.4806 - val_acc: 0.8852\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9536\n",
      "Epoch 00085: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1488 - acc: 0.9536 - val_loss: 0.5062 - val_acc: 0.8784\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9561\n",
      "Epoch 00086: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1449 - acc: 0.9561 - val_loss: 0.4864 - val_acc: 0.8779\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9558\n",
      "Epoch 00087: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1442 - acc: 0.9558 - val_loss: 0.4755 - val_acc: 0.8831\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9563\n",
      "Epoch 00088: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1403 - acc: 0.9563 - val_loss: 0.4739 - val_acc: 0.8789\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9586\n",
      "Epoch 00089: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1370 - acc: 0.9586 - val_loss: 0.4879 - val_acc: 0.8814\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9595\n",
      "Epoch 00090: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1335 - acc: 0.9595 - val_loss: 0.4858 - val_acc: 0.8777\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9582\n",
      "Epoch 00091: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1352 - acc: 0.9582 - val_loss: 0.4719 - val_acc: 0.8828\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9594\n",
      "Epoch 00092: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1317 - acc: 0.9594 - val_loss: 0.5092 - val_acc: 0.8770\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9621\n",
      "Epoch 00093: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1249 - acc: 0.9621 - val_loss: 0.4825 - val_acc: 0.8796\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9603\n",
      "Epoch 00094: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1285 - acc: 0.9603 - val_loss: 0.4907 - val_acc: 0.8798\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9617\n",
      "Epoch 00095: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1248 - acc: 0.9617 - val_loss: 0.4772 - val_acc: 0.8826\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9618\n",
      "Epoch 00096: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1213 - acc: 0.9618 - val_loss: 0.4989 - val_acc: 0.8824\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9619\n",
      "Epoch 00097: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1235 - acc: 0.9619 - val_loss: 0.5054 - val_acc: 0.8793\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9633\n",
      "Epoch 00098: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1188 - acc: 0.9633 - val_loss: 0.5014 - val_acc: 0.8800\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9639\n",
      "Epoch 00099: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1176 - acc: 0.9638 - val_loss: 0.4805 - val_acc: 0.8849\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9645\n",
      "Epoch 00100: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1181 - acc: 0.9645 - val_loss: 0.4924 - val_acc: 0.8840\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9653\n",
      "Epoch 00101: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1121 - acc: 0.9653 - val_loss: 0.4952 - val_acc: 0.8826\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9648\n",
      "Epoch 00102: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1141 - acc: 0.9648 - val_loss: 0.4880 - val_acc: 0.8817\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9652\n",
      "Epoch 00103: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1144 - acc: 0.9652 - val_loss: 0.4929 - val_acc: 0.8810\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9649\n",
      "Epoch 00104: val_loss did not improve from 0.45860\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1121 - acc: 0.9649 - val_loss: 0.4998 - val_acc: 0.8821\n",
      "\n",
      "1D_CNN_custom_tanh_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 634us/sample - loss: 0.5394 - acc: 0.8455\n",
      "Loss: 0.539416510850841 Accuracy: 0.8454829\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0034 - acc: 0.3442\n",
      "Epoch 00001: val_loss improved from inf to 1.50943, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/001-1.5094.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 2.0034 - acc: 0.3442 - val_loss: 1.5094 - val_acc: 0.5297\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5377 - acc: 0.5048\n",
      "Epoch 00002: val_loss improved from 1.50943 to 1.28353, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/002-1.2835.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.5377 - acc: 0.5048 - val_loss: 1.2835 - val_acc: 0.6152\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3268 - acc: 0.5899\n",
      "Epoch 00003: val_loss improved from 1.28353 to 1.12869, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/003-1.1287.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.3269 - acc: 0.5899 - val_loss: 1.1287 - val_acc: 0.6657\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1546 - acc: 0.6487\n",
      "Epoch 00004: val_loss improved from 1.12869 to 0.96924, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/004-0.9692.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1545 - acc: 0.6488 - val_loss: 0.9692 - val_acc: 0.7233\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0381 - acc: 0.6846\n",
      "Epoch 00005: val_loss improved from 0.96924 to 0.87324, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/005-0.8732.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0381 - acc: 0.6846 - val_loss: 0.8732 - val_acc: 0.7384\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9366 - acc: 0.7186\n",
      "Epoch 00006: val_loss improved from 0.87324 to 0.79057, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/006-0.7906.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9367 - acc: 0.7185 - val_loss: 0.7906 - val_acc: 0.7720\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8556 - acc: 0.7443\n",
      "Epoch 00007: val_loss improved from 0.79057 to 0.70780, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/007-0.7078.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8555 - acc: 0.7444 - val_loss: 0.7078 - val_acc: 0.7992\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7845 - acc: 0.7685\n",
      "Epoch 00008: val_loss improved from 0.70780 to 0.65357, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/008-0.6536.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7845 - acc: 0.7685 - val_loss: 0.6536 - val_acc: 0.8160\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7301 - acc: 0.7849\n",
      "Epoch 00009: val_loss improved from 0.65357 to 0.61794, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/009-0.6179.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7300 - acc: 0.7849 - val_loss: 0.6179 - val_acc: 0.8272\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6767 - acc: 0.8016\n",
      "Epoch 00010: val_loss improved from 0.61794 to 0.56110, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/010-0.5611.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6767 - acc: 0.8016 - val_loss: 0.5611 - val_acc: 0.8446\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6295 - acc: 0.8181\n",
      "Epoch 00011: val_loss improved from 0.56110 to 0.52937, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/011-0.5294.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6296 - acc: 0.8181 - val_loss: 0.5294 - val_acc: 0.8532\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.8258\n",
      "Epoch 00012: val_loss improved from 0.52937 to 0.50577, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/012-0.5058.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5941 - acc: 0.8258 - val_loss: 0.5058 - val_acc: 0.8609\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.8375\n",
      "Epoch 00013: val_loss improved from 0.50577 to 0.47448, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/013-0.4745.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5637 - acc: 0.8375 - val_loss: 0.4745 - val_acc: 0.8686\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8451\n",
      "Epoch 00014: val_loss improved from 0.47448 to 0.46287, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/014-0.4629.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5348 - acc: 0.8452 - val_loss: 0.4629 - val_acc: 0.8721\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.8507\n",
      "Epoch 00015: val_loss improved from 0.46287 to 0.43673, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/015-0.4367.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5122 - acc: 0.8507 - val_loss: 0.4367 - val_acc: 0.8819\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4856 - acc: 0.8602\n",
      "Epoch 00016: val_loss improved from 0.43673 to 0.40971, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/016-0.4097.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4856 - acc: 0.8603 - val_loss: 0.4097 - val_acc: 0.8877\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8667\n",
      "Epoch 00017: val_loss improved from 0.40971 to 0.40066, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/017-0.4007.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4626 - acc: 0.8667 - val_loss: 0.4007 - val_acc: 0.8882\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8696\n",
      "Epoch 00018: val_loss improved from 0.40066 to 0.38990, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/018-0.3899.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4508 - acc: 0.8696 - val_loss: 0.3899 - val_acc: 0.8905\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8756\n",
      "Epoch 00019: val_loss did not improve from 0.38990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4300 - acc: 0.8756 - val_loss: 0.3951 - val_acc: 0.8882\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4176 - acc: 0.8778\n",
      "Epoch 00020: val_loss improved from 0.38990 to 0.37391, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/020-0.3739.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4176 - acc: 0.8778 - val_loss: 0.3739 - val_acc: 0.8949\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8839\n",
      "Epoch 00021: val_loss improved from 0.37391 to 0.37222, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/021-0.3722.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4029 - acc: 0.8839 - val_loss: 0.3722 - val_acc: 0.8952\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3880 - acc: 0.8869\n",
      "Epoch 00022: val_loss improved from 0.37222 to 0.35668, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/022-0.3567.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3879 - acc: 0.8869 - val_loss: 0.3567 - val_acc: 0.8975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8918\n",
      "Epoch 00023: val_loss did not improve from 0.35668\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3766 - acc: 0.8918 - val_loss: 0.3639 - val_acc: 0.9008\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8938\n",
      "Epoch 00024: val_loss improved from 0.35668 to 0.33453, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/024-0.3345.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3641 - acc: 0.8938 - val_loss: 0.3345 - val_acc: 0.9101\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3528 - acc: 0.8989\n",
      "Epoch 00025: val_loss improved from 0.33453 to 0.31716, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/025-0.3172.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3527 - acc: 0.8989 - val_loss: 0.3172 - val_acc: 0.9122\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.9014\n",
      "Epoch 00026: val_loss improved from 0.31716 to 0.31198, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/026-0.3120.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3417 - acc: 0.9015 - val_loss: 0.3120 - val_acc: 0.9129\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3352 - acc: 0.9018\n",
      "Epoch 00027: val_loss did not improve from 0.31198\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3352 - acc: 0.9018 - val_loss: 0.3152 - val_acc: 0.9126\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.9064\n",
      "Epoch 00028: val_loss improved from 0.31198 to 0.29437, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/028-0.2944.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3251 - acc: 0.9064 - val_loss: 0.2944 - val_acc: 0.9182\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.9073\n",
      "Epoch 00029: val_loss did not improve from 0.29437\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3189 - acc: 0.9073 - val_loss: 0.2951 - val_acc: 0.9171\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9075\n",
      "Epoch 00030: val_loss did not improve from 0.29437\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3136 - acc: 0.9075 - val_loss: 0.3041 - val_acc: 0.9187\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.9117\n",
      "Epoch 00031: val_loss improved from 0.29437 to 0.28364, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/031-0.2836.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3052 - acc: 0.9117 - val_loss: 0.2836 - val_acc: 0.9203\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9145\n",
      "Epoch 00032: val_loss improved from 0.28364 to 0.28257, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/032-0.2826.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2927 - acc: 0.9145 - val_loss: 0.2826 - val_acc: 0.9208\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9153\n",
      "Epoch 00033: val_loss did not improve from 0.28257\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2885 - acc: 0.9153 - val_loss: 0.2881 - val_acc: 0.9187\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9171\n",
      "Epoch 00034: val_loss improved from 0.28257 to 0.26886, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/034-0.2689.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2824 - acc: 0.9172 - val_loss: 0.2689 - val_acc: 0.9231\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9190\n",
      "Epoch 00035: val_loss did not improve from 0.26886\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2755 - acc: 0.9190 - val_loss: 0.2720 - val_acc: 0.9238\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9204\n",
      "Epoch 00036: val_loss did not improve from 0.26886\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2690 - acc: 0.9204 - val_loss: 0.2928 - val_acc: 0.9208\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9217\n",
      "Epoch 00037: val_loss improved from 0.26886 to 0.26779, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/037-0.2678.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2686 - acc: 0.9217 - val_loss: 0.2678 - val_acc: 0.9266\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9224\n",
      "Epoch 00038: val_loss did not improve from 0.26779\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2595 - acc: 0.9223 - val_loss: 0.2742 - val_acc: 0.9241\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9248\n",
      "Epoch 00039: val_loss improved from 0.26779 to 0.25962, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/039-0.2596.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2538 - acc: 0.9248 - val_loss: 0.2596 - val_acc: 0.9255\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9275\n",
      "Epoch 00040: val_loss improved from 0.25962 to 0.25757, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/040-0.2576.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2455 - acc: 0.9275 - val_loss: 0.2576 - val_acc: 0.9271\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9269\n",
      "Epoch 00041: val_loss improved from 0.25757 to 0.25492, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/041-0.2549.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2464 - acc: 0.9269 - val_loss: 0.2549 - val_acc: 0.9283\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9289\n",
      "Epoch 00042: val_loss did not improve from 0.25492\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2386 - acc: 0.9289 - val_loss: 0.2569 - val_acc: 0.9306\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9288\n",
      "Epoch 00043: val_loss improved from 0.25492 to 0.25385, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/043-0.2539.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2370 - acc: 0.9288 - val_loss: 0.2539 - val_acc: 0.9283\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9315\n",
      "Epoch 00044: val_loss improved from 0.25385 to 0.25055, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/044-0.2506.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2281 - acc: 0.9316 - val_loss: 0.2506 - val_acc: 0.9320\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9339\n",
      "Epoch 00045: val_loss did not improve from 0.25055\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2237 - acc: 0.9339 - val_loss: 0.2546 - val_acc: 0.9315\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9340\n",
      "Epoch 00046: val_loss improved from 0.25055 to 0.23968, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/046-0.2397.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2189 - acc: 0.9340 - val_loss: 0.2397 - val_acc: 0.9334\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9342\n",
      "Epoch 00047: val_loss did not improve from 0.23968\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2220 - acc: 0.9342 - val_loss: 0.2515 - val_acc: 0.9317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9368\n",
      "Epoch 00048: val_loss did not improve from 0.23968\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2131 - acc: 0.9368 - val_loss: 0.2598 - val_acc: 0.9287\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9342\n",
      "Epoch 00049: val_loss did not improve from 0.23968\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2172 - acc: 0.9341 - val_loss: 0.2603 - val_acc: 0.9280\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9377\n",
      "Epoch 00050: val_loss improved from 0.23968 to 0.23861, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/050-0.2386.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2104 - acc: 0.9377 - val_loss: 0.2386 - val_acc: 0.9315\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9386\n",
      "Epoch 00051: val_loss did not improve from 0.23861\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2031 - acc: 0.9386 - val_loss: 0.2499 - val_acc: 0.9297\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9404\n",
      "Epoch 00052: val_loss did not improve from 0.23861\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1981 - acc: 0.9404 - val_loss: 0.2483 - val_acc: 0.9317\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9423\n",
      "Epoch 00053: val_loss improved from 0.23861 to 0.23633, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/053-0.2363.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1954 - acc: 0.9423 - val_loss: 0.2363 - val_acc: 0.9336\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9423\n",
      "Epoch 00054: val_loss improved from 0.23633 to 0.23528, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/054-0.2353.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1949 - acc: 0.9423 - val_loss: 0.2353 - val_acc: 0.9376\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9422\n",
      "Epoch 00055: val_loss did not improve from 0.23528\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1911 - acc: 0.9422 - val_loss: 0.2401 - val_acc: 0.9343\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9438\n",
      "Epoch 00056: val_loss did not improve from 0.23528\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1858 - acc: 0.9438 - val_loss: 0.2444 - val_acc: 0.9338\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9416\n",
      "Epoch 00057: val_loss did not improve from 0.23528\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1916 - acc: 0.9415 - val_loss: 0.2476 - val_acc: 0.9357\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9455\n",
      "Epoch 00058: val_loss did not improve from 0.23528\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1840 - acc: 0.9455 - val_loss: 0.2471 - val_acc: 0.9334\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9459\n",
      "Epoch 00059: val_loss improved from 0.23528 to 0.23457, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/059-0.2346.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1767 - acc: 0.9459 - val_loss: 0.2346 - val_acc: 0.9345\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9461\n",
      "Epoch 00060: val_loss improved from 0.23457 to 0.23131, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/060-0.2313.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1758 - acc: 0.9461 - val_loss: 0.2313 - val_acc: 0.9357\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9476\n",
      "Epoch 00061: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1709 - acc: 0.9475 - val_loss: 0.2443 - val_acc: 0.9357\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9482\n",
      "Epoch 00062: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1720 - acc: 0.9481 - val_loss: 0.2331 - val_acc: 0.9343\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9496\n",
      "Epoch 00063: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1673 - acc: 0.9496 - val_loss: 0.2432 - val_acc: 0.9320\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9495\n",
      "Epoch 00064: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1654 - acc: 0.9495 - val_loss: 0.2368 - val_acc: 0.9392\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9513\n",
      "Epoch 00065: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1607 - acc: 0.9513 - val_loss: 0.2360 - val_acc: 0.9376\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9517\n",
      "Epoch 00066: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1582 - acc: 0.9517 - val_loss: 0.2368 - val_acc: 0.9373\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9533\n",
      "Epoch 00067: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1533 - acc: 0.9533 - val_loss: 0.2449 - val_acc: 0.9336\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9527\n",
      "Epoch 00068: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1551 - acc: 0.9528 - val_loss: 0.2465 - val_acc: 0.9334\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9536\n",
      "Epoch 00069: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1514 - acc: 0.9536 - val_loss: 0.2582 - val_acc: 0.9317\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9559\n",
      "Epoch 00070: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1469 - acc: 0.9559 - val_loss: 0.2496 - val_acc: 0.9334\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9523\n",
      "Epoch 00071: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1539 - acc: 0.9523 - val_loss: 0.2349 - val_acc: 0.9357\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9527\n",
      "Epoch 00072: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1510 - acc: 0.9527 - val_loss: 0.2446 - val_acc: 0.9376\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9567\n",
      "Epoch 00073: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1421 - acc: 0.9567 - val_loss: 0.2375 - val_acc: 0.9359\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9564\n",
      "Epoch 00074: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1425 - acc: 0.9564 - val_loss: 0.2431 - val_acc: 0.9348\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9572\n",
      "Epoch 00075: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1391 - acc: 0.9572 - val_loss: 0.2357 - val_acc: 0.9366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9579\n",
      "Epoch 00076: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1345 - acc: 0.9579 - val_loss: 0.2326 - val_acc: 0.9385\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9583\n",
      "Epoch 00077: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1364 - acc: 0.9583 - val_loss: 0.2333 - val_acc: 0.9378\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9573\n",
      "Epoch 00078: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1376 - acc: 0.9573 - val_loss: 0.2420 - val_acc: 0.9348\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9579\n",
      "Epoch 00079: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1339 - acc: 0.9579 - val_loss: 0.2364 - val_acc: 0.9366\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9595\n",
      "Epoch 00080: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1318 - acc: 0.9595 - val_loss: 0.2330 - val_acc: 0.9406\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9604\n",
      "Epoch 00081: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1276 - acc: 0.9604 - val_loss: 0.2415 - val_acc: 0.9366\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9616\n",
      "Epoch 00082: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1245 - acc: 0.9616 - val_loss: 0.2431 - val_acc: 0.9362\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9610\n",
      "Epoch 00083: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1275 - acc: 0.9610 - val_loss: 0.2356 - val_acc: 0.9383\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.9623\n",
      "Epoch 00084: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1204 - acc: 0.9623 - val_loss: 0.2379 - val_acc: 0.9401\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9617\n",
      "Epoch 00085: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1224 - acc: 0.9617 - val_loss: 0.2421 - val_acc: 0.9364\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9616\n",
      "Epoch 00086: val_loss did not improve from 0.23131\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1246 - acc: 0.9616 - val_loss: 0.2532 - val_acc: 0.9348\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9646\n",
      "Epoch 00087: val_loss improved from 0.23131 to 0.23102, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_7_conv_checkpoint/087-0.2310.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1146 - acc: 0.9647 - val_loss: 0.2310 - val_acc: 0.9380\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9646\n",
      "Epoch 00088: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1145 - acc: 0.9646 - val_loss: 0.2430 - val_acc: 0.9364\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9646\n",
      "Epoch 00089: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1135 - acc: 0.9647 - val_loss: 0.2419 - val_acc: 0.9371\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9645\n",
      "Epoch 00090: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1121 - acc: 0.9645 - val_loss: 0.2468 - val_acc: 0.9355\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9676\n",
      "Epoch 00091: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1087 - acc: 0.9676 - val_loss: 0.2540 - val_acc: 0.9345\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9642\n",
      "Epoch 00092: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1126 - acc: 0.9642 - val_loss: 0.2514 - val_acc: 0.9336\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9662\n",
      "Epoch 00093: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1066 - acc: 0.9662 - val_loss: 0.2475 - val_acc: 0.9366\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9665\n",
      "Epoch 00094: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1048 - acc: 0.9665 - val_loss: 0.2416 - val_acc: 0.9390\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9673\n",
      "Epoch 00095: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1049 - acc: 0.9672 - val_loss: 0.2376 - val_acc: 0.9390\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9645\n",
      "Epoch 00096: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1123 - acc: 0.9645 - val_loss: 0.2411 - val_acc: 0.9385\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9691\n",
      "Epoch 00097: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1009 - acc: 0.9691 - val_loss: 0.2406 - val_acc: 0.9406\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9708\n",
      "Epoch 00098: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0965 - acc: 0.9708 - val_loss: 0.2333 - val_acc: 0.9399\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9709\n",
      "Epoch 00099: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0963 - acc: 0.9709 - val_loss: 0.2569 - val_acc: 0.9345\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9701\n",
      "Epoch 00100: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0973 - acc: 0.9701 - val_loss: 0.2477 - val_acc: 0.9408\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9708\n",
      "Epoch 00101: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0957 - acc: 0.9708 - val_loss: 0.2717 - val_acc: 0.9350\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9709\n",
      "Epoch 00102: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0949 - acc: 0.9709 - val_loss: 0.2600 - val_acc: 0.9387\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9720\n",
      "Epoch 00103: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0913 - acc: 0.9720 - val_loss: 0.2461 - val_acc: 0.9406\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9709\n",
      "Epoch 00104: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0964 - acc: 0.9708 - val_loss: 0.2561 - val_acc: 0.9350\n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9692\n",
      "Epoch 00105: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0948 - acc: 0.9692 - val_loss: 0.2469 - val_acc: 0.9404\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9739\n",
      "Epoch 00106: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0896 - acc: 0.9739 - val_loss: 0.2433 - val_acc: 0.9383\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9728\n",
      "Epoch 00107: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0908 - acc: 0.9728 - val_loss: 0.2433 - val_acc: 0.9425\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9725\n",
      "Epoch 00108: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0881 - acc: 0.9725 - val_loss: 0.2529 - val_acc: 0.9394\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9735\n",
      "Epoch 00109: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0852 - acc: 0.9735 - val_loss: 0.2445 - val_acc: 0.9439\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9746\n",
      "Epoch 00110: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0867 - acc: 0.9746 - val_loss: 0.2365 - val_acc: 0.9399\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9731\n",
      "Epoch 00111: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0864 - acc: 0.9731 - val_loss: 0.2542 - val_acc: 0.9390\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9719\n",
      "Epoch 00112: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0876 - acc: 0.9719 - val_loss: 0.2647 - val_acc: 0.9392\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9737\n",
      "Epoch 00113: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0824 - acc: 0.9737 - val_loss: 0.2626 - val_acc: 0.9357\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9757\n",
      "Epoch 00114: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0780 - acc: 0.9757 - val_loss: 0.2459 - val_acc: 0.9420\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9751\n",
      "Epoch 00115: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0805 - acc: 0.9751 - val_loss: 0.2520 - val_acc: 0.9378\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9746\n",
      "Epoch 00116: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0803 - acc: 0.9746 - val_loss: 0.2535 - val_acc: 0.9420\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9768\n",
      "Epoch 00117: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0765 - acc: 0.9768 - val_loss: 0.2580 - val_acc: 0.9378\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9764\n",
      "Epoch 00118: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0768 - acc: 0.9764 - val_loss: 0.2319 - val_acc: 0.9471\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9762\n",
      "Epoch 00119: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0747 - acc: 0.9762 - val_loss: 0.2512 - val_acc: 0.9378\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9763\n",
      "Epoch 00120: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0760 - acc: 0.9763 - val_loss: 0.2489 - val_acc: 0.9394\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9770\n",
      "Epoch 00121: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0748 - acc: 0.9770 - val_loss: 0.2493 - val_acc: 0.9420\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9768\n",
      "Epoch 00122: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0743 - acc: 0.9768 - val_loss: 0.2467 - val_acc: 0.9420\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9767\n",
      "Epoch 00123: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0748 - acc: 0.9767 - val_loss: 0.2607 - val_acc: 0.9422\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9786\n",
      "Epoch 00124: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0698 - acc: 0.9786 - val_loss: 0.2687 - val_acc: 0.9366\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9786\n",
      "Epoch 00125: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0707 - acc: 0.9786 - val_loss: 0.2690 - val_acc: 0.9404\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9782\n",
      "Epoch 00126: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0687 - acc: 0.9782 - val_loss: 0.2723 - val_acc: 0.9366\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9775\n",
      "Epoch 00127: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0756 - acc: 0.9775 - val_loss: 0.2543 - val_acc: 0.9441\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9805\n",
      "Epoch 00128: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0650 - acc: 0.9805 - val_loss: 0.2462 - val_acc: 0.9415\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9795\n",
      "Epoch 00129: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0649 - acc: 0.9795 - val_loss: 0.2719 - val_acc: 0.9357\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9803\n",
      "Epoch 00130: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0659 - acc: 0.9803 - val_loss: 0.2406 - val_acc: 0.9415\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9787\n",
      "Epoch 00131: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0685 - acc: 0.9788 - val_loss: 0.2710 - val_acc: 0.9378\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9797\n",
      "Epoch 00132: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0645 - acc: 0.9797 - val_loss: 0.2502 - val_acc: 0.9411\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9810\n",
      "Epoch 00133: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0616 - acc: 0.9810 - val_loss: 0.2758 - val_acc: 0.9378\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9803\n",
      "Epoch 00134: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0645 - acc: 0.9802 - val_loss: 0.2632 - val_acc: 0.9406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9819\n",
      "Epoch 00135: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0608 - acc: 0.9819 - val_loss: 0.2557 - val_acc: 0.9411\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9792\n",
      "Epoch 00136: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0647 - acc: 0.9792 - val_loss: 0.2613 - val_acc: 0.9406\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9824\n",
      "Epoch 00137: val_loss did not improve from 0.23102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0606 - acc: 0.9824 - val_loss: 0.2485 - val_acc: 0.9422\n",
      "\n",
      "1D_CNN_custom_tanh_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 642us/sample - loss: 0.2755 - acc: 0.9192\n",
      "Loss: 0.2754717957317396 Accuracy: 0.9192108\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9409 - acc: 0.3702\n",
      "Epoch 00001: val_loss improved from inf to 1.40669, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/001-1.4067.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.9407 - acc: 0.3702 - val_loss: 1.4067 - val_acc: 0.5849\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3818 - acc: 0.5686\n",
      "Epoch 00002: val_loss improved from 1.40669 to 1.06608, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/002-1.0661.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3818 - acc: 0.5685 - val_loss: 1.0661 - val_acc: 0.6816\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1057 - acc: 0.6665\n",
      "Epoch 00003: val_loss improved from 1.06608 to 0.83743, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/003-0.8374.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1056 - acc: 0.6665 - val_loss: 0.8374 - val_acc: 0.7657\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9055 - acc: 0.7310\n",
      "Epoch 00004: val_loss improved from 0.83743 to 0.66379, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/004-0.6638.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9055 - acc: 0.7310 - val_loss: 0.6638 - val_acc: 0.8204\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7505 - acc: 0.7800\n",
      "Epoch 00005: val_loss improved from 0.66379 to 0.53598, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/005-0.5360.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7505 - acc: 0.7800 - val_loss: 0.5360 - val_acc: 0.8574\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6322 - acc: 0.8158\n",
      "Epoch 00006: val_loss improved from 0.53598 to 0.45070, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/006-0.4507.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6322 - acc: 0.8158 - val_loss: 0.4507 - val_acc: 0.8828\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.8388\n",
      "Epoch 00007: val_loss improved from 0.45070 to 0.39302, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/007-0.3930.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5569 - acc: 0.8388 - val_loss: 0.3930 - val_acc: 0.8977\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8572\n",
      "Epoch 00008: val_loss improved from 0.39302 to 0.35437, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/008-0.3544.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4938 - acc: 0.8572 - val_loss: 0.3544 - val_acc: 0.9054\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4451 - acc: 0.8715\n",
      "Epoch 00009: val_loss improved from 0.35437 to 0.32224, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/009-0.3222.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4451 - acc: 0.8715 - val_loss: 0.3222 - val_acc: 0.9145\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8821\n",
      "Epoch 00010: val_loss improved from 0.32224 to 0.29780, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/010-0.2978.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4080 - acc: 0.8821 - val_loss: 0.2978 - val_acc: 0.9227\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8892\n",
      "Epoch 00011: val_loss improved from 0.29780 to 0.27038, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/011-0.2704.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3822 - acc: 0.8892 - val_loss: 0.2704 - val_acc: 0.9264\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.8970\n",
      "Epoch 00012: val_loss improved from 0.27038 to 0.25475, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/012-0.2548.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3552 - acc: 0.8970 - val_loss: 0.2548 - val_acc: 0.9280\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.9048\n",
      "Epoch 00013: val_loss improved from 0.25475 to 0.25106, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/013-0.2511.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3317 - acc: 0.9048 - val_loss: 0.2511 - val_acc: 0.9317\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9108\n",
      "Epoch 00014: val_loss improved from 0.25106 to 0.24952, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/014-0.2495.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3102 - acc: 0.9108 - val_loss: 0.2495 - val_acc: 0.9324\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2909 - acc: 0.9162\n",
      "Epoch 00015: val_loss improved from 0.24952 to 0.22282, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/015-0.2228.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2908 - acc: 0.9163 - val_loss: 0.2228 - val_acc: 0.9399\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9198\n",
      "Epoch 00016: val_loss improved from 0.22282 to 0.20604, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/016-0.2060.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2804 - acc: 0.9197 - val_loss: 0.2060 - val_acc: 0.9415\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9242\n",
      "Epoch 00017: val_loss improved from 0.20604 to 0.20110, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/017-0.2011.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2639 - acc: 0.9241 - val_loss: 0.2011 - val_acc: 0.9464\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9265\n",
      "Epoch 00018: val_loss improved from 0.20110 to 0.19731, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/018-0.1973.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2561 - acc: 0.9266 - val_loss: 0.1973 - val_acc: 0.9453\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9295\n",
      "Epoch 00019: val_loss did not improve from 0.19731\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2416 - acc: 0.9295 - val_loss: 0.2042 - val_acc: 0.9397\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9301\n",
      "Epoch 00020: val_loss improved from 0.19731 to 0.18996, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/020-0.1900.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2379 - acc: 0.9301 - val_loss: 0.1900 - val_acc: 0.9474\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9337\n",
      "Epoch 00021: val_loss improved from 0.18996 to 0.18480, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/021-0.1848.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2276 - acc: 0.9337 - val_loss: 0.1848 - val_acc: 0.9471\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9358\n",
      "Epoch 00022: val_loss did not improve from 0.18480\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2169 - acc: 0.9358 - val_loss: 0.1881 - val_acc: 0.9455\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9414\n",
      "Epoch 00023: val_loss improved from 0.18480 to 0.18086, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/023-0.1809.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2046 - acc: 0.9414 - val_loss: 0.1809 - val_acc: 0.9490\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9429\n",
      "Epoch 00024: val_loss improved from 0.18086 to 0.17383, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/024-0.1738.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1996 - acc: 0.9429 - val_loss: 0.1738 - val_acc: 0.9532\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9435\n",
      "Epoch 00025: val_loss improved from 0.17383 to 0.17339, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/025-0.1734.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1953 - acc: 0.9435 - val_loss: 0.1734 - val_acc: 0.9513\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9435\n",
      "Epoch 00026: val_loss improved from 0.17339 to 0.16987, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/026-0.1699.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1923 - acc: 0.9435 - val_loss: 0.1699 - val_acc: 0.9499\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9462\n",
      "Epoch 00027: val_loss improved from 0.16987 to 0.16116, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/027-0.1612.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1817 - acc: 0.9462 - val_loss: 0.1612 - val_acc: 0.9532\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9470\n",
      "Epoch 00028: val_loss improved from 0.16116 to 0.15830, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/028-0.1583.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1788 - acc: 0.9470 - val_loss: 0.1583 - val_acc: 0.9536\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9507\n",
      "Epoch 00029: val_loss did not improve from 0.15830\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1679 - acc: 0.9507 - val_loss: 0.1617 - val_acc: 0.9548\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9514\n",
      "Epoch 00030: val_loss did not improve from 0.15830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1665 - acc: 0.9514 - val_loss: 0.1600 - val_acc: 0.9513\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9532\n",
      "Epoch 00031: val_loss did not improve from 0.15830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1578 - acc: 0.9532 - val_loss: 0.1598 - val_acc: 0.9581\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9531\n",
      "Epoch 00032: val_loss did not improve from 0.15830\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1588 - acc: 0.9531 - val_loss: 0.1594 - val_acc: 0.9536\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9548\n",
      "Epoch 00033: val_loss improved from 0.15830 to 0.14365, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/033-0.1437.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1537 - acc: 0.9548 - val_loss: 0.1437 - val_acc: 0.9581\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9575\n",
      "Epoch 00034: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1456 - acc: 0.9575 - val_loss: 0.1560 - val_acc: 0.9534\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9574\n",
      "Epoch 00035: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1423 - acc: 0.9574 - val_loss: 0.1600 - val_acc: 0.9527\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9585\n",
      "Epoch 00036: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1422 - acc: 0.9585 - val_loss: 0.1459 - val_acc: 0.9574\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9603\n",
      "Epoch 00037: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1345 - acc: 0.9603 - val_loss: 0.1498 - val_acc: 0.9562\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9608\n",
      "Epoch 00038: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1304 - acc: 0.9608 - val_loss: 0.1453 - val_acc: 0.9553\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9624\n",
      "Epoch 00039: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1277 - acc: 0.9624 - val_loss: 0.1507 - val_acc: 0.9571\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9613\n",
      "Epoch 00040: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1264 - acc: 0.9613 - val_loss: 0.1535 - val_acc: 0.9541\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9644\n",
      "Epoch 00041: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1216 - acc: 0.9644 - val_loss: 0.1446 - val_acc: 0.9578\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9651\n",
      "Epoch 00042: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1147 - acc: 0.9651 - val_loss: 0.1475 - val_acc: 0.9564\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9660\n",
      "Epoch 00043: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1129 - acc: 0.9660 - val_loss: 0.1494 - val_acc: 0.9557\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9673\n",
      "Epoch 00044: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1104 - acc: 0.9673 - val_loss: 0.1535 - val_acc: 0.9548\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9670\n",
      "Epoch 00045: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1084 - acc: 0.9670 - val_loss: 0.1588 - val_acc: 0.9550\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9682\n",
      "Epoch 00046: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1071 - acc: 0.9682 - val_loss: 0.1456 - val_acc: 0.9574\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9685\n",
      "Epoch 00047: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1037 - acc: 0.9685 - val_loss: 0.1597 - val_acc: 0.9553\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9706\n",
      "Epoch 00048: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1003 - acc: 0.9706 - val_loss: 0.1472 - val_acc: 0.9574\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9695\n",
      "Epoch 00049: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0985 - acc: 0.9695 - val_loss: 0.1625 - val_acc: 0.9541\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9709\n",
      "Epoch 00050: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0974 - acc: 0.9709 - val_loss: 0.1490 - val_acc: 0.9595\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9723\n",
      "Epoch 00051: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0922 - acc: 0.9723 - val_loss: 0.1571 - val_acc: 0.9574\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9728\n",
      "Epoch 00052: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0895 - acc: 0.9728 - val_loss: 0.1531 - val_acc: 0.9571\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9735\n",
      "Epoch 00053: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0879 - acc: 0.9735 - val_loss: 0.1554 - val_acc: 0.9571\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9737\n",
      "Epoch 00054: val_loss improved from 0.14365 to 0.13754, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_8_conv_checkpoint/054-0.1375.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0876 - acc: 0.9737 - val_loss: 0.1375 - val_acc: 0.9595\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9754\n",
      "Epoch 00055: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0833 - acc: 0.9754 - val_loss: 0.1395 - val_acc: 0.9599\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9753\n",
      "Epoch 00056: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0805 - acc: 0.9753 - val_loss: 0.1447 - val_acc: 0.9595\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9764\n",
      "Epoch 00057: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0805 - acc: 0.9764 - val_loss: 0.1456 - val_acc: 0.9613\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9767\n",
      "Epoch 00058: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0761 - acc: 0.9767 - val_loss: 0.1538 - val_acc: 0.9576\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9776\n",
      "Epoch 00059: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0746 - acc: 0.9776 - val_loss: 0.1574 - val_acc: 0.9576\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9777\n",
      "Epoch 00060: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0750 - acc: 0.9777 - val_loss: 0.1589 - val_acc: 0.9562\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9793\n",
      "Epoch 00061: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0679 - acc: 0.9794 - val_loss: 0.1561 - val_acc: 0.9571\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9782\n",
      "Epoch 00062: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0724 - acc: 0.9782 - val_loss: 0.1591 - val_acc: 0.9578\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9799\n",
      "Epoch 00063: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0660 - acc: 0.9799 - val_loss: 0.1587 - val_acc: 0.9546\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9796\n",
      "Epoch 00064: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0663 - acc: 0.9796 - val_loss: 0.1495 - val_acc: 0.9588\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9814\n",
      "Epoch 00065: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0629 - acc: 0.9814 - val_loss: 0.1558 - val_acc: 0.9555\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9823\n",
      "Epoch 00066: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0621 - acc: 0.9823 - val_loss: 0.1396 - val_acc: 0.9597\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9826\n",
      "Epoch 00067: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0603 - acc: 0.9826 - val_loss: 0.1472 - val_acc: 0.9569\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9825\n",
      "Epoch 00068: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0595 - acc: 0.9825 - val_loss: 0.1581 - val_acc: 0.9578\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9832\n",
      "Epoch 00069: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0572 - acc: 0.9832 - val_loss: 0.1499 - val_acc: 0.9569\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9845\n",
      "Epoch 00070: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0525 - acc: 0.9845 - val_loss: 0.1562 - val_acc: 0.9550\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9839\n",
      "Epoch 00071: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0539 - acc: 0.9839 - val_loss: 0.1545 - val_acc: 0.9616\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9839\n",
      "Epoch 00072: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0540 - acc: 0.9839 - val_loss: 0.1617 - val_acc: 0.9569\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9844\n",
      "Epoch 00073: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0520 - acc: 0.9843 - val_loss: 0.1572 - val_acc: 0.9576\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9829\n",
      "Epoch 00074: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0595 - acc: 0.9829 - val_loss: 0.1712 - val_acc: 0.9543\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9861\n",
      "Epoch 00075: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0490 - acc: 0.9861 - val_loss: 0.1650 - val_acc: 0.9564\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9857\n",
      "Epoch 00076: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0483 - acc: 0.9857 - val_loss: 0.1551 - val_acc: 0.9599\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9866\n",
      "Epoch 00077: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0467 - acc: 0.9866 - val_loss: 0.1534 - val_acc: 0.9588\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9869\n",
      "Epoch 00078: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0454 - acc: 0.9869 - val_loss: 0.1466 - val_acc: 0.9602\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9881\n",
      "Epoch 00079: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0415 - acc: 0.9881 - val_loss: 0.1490 - val_acc: 0.9623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9875\n",
      "Epoch 00080: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0442 - acc: 0.9875 - val_loss: 0.1673 - val_acc: 0.9541\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9870\n",
      "Epoch 00081: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0449 - acc: 0.9870 - val_loss: 0.1555 - val_acc: 0.9597\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9868\n",
      "Epoch 00082: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0422 - acc: 0.9868 - val_loss: 0.1486 - val_acc: 0.9599\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9880\n",
      "Epoch 00083: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0398 - acc: 0.9880 - val_loss: 0.1616 - val_acc: 0.9592\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9882\n",
      "Epoch 00084: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0423 - acc: 0.9882 - val_loss: 0.1663 - val_acc: 0.9604\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9892\n",
      "Epoch 00085: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0371 - acc: 0.9892 - val_loss: 0.1565 - val_acc: 0.9569\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9879\n",
      "Epoch 00086: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0396 - acc: 0.9879 - val_loss: 0.1564 - val_acc: 0.9606\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9896\n",
      "Epoch 00087: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0359 - acc: 0.9896 - val_loss: 0.1625 - val_acc: 0.9585\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9908\n",
      "Epoch 00088: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0337 - acc: 0.9908 - val_loss: 0.1584 - val_acc: 0.9599\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9900\n",
      "Epoch 00089: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0352 - acc: 0.9900 - val_loss: 0.1687 - val_acc: 0.9569\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9898\n",
      "Epoch 00090: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0352 - acc: 0.9898 - val_loss: 0.1627 - val_acc: 0.9623\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9905\n",
      "Epoch 00091: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0337 - acc: 0.9905 - val_loss: 0.1533 - val_acc: 0.9627\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9901\n",
      "Epoch 00092: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0347 - acc: 0.9901 - val_loss: 0.1660 - val_acc: 0.9557\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9910\n",
      "Epoch 00093: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0316 - acc: 0.9910 - val_loss: 0.1566 - val_acc: 0.9609\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9907\n",
      "Epoch 00094: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0320 - acc: 0.9907 - val_loss: 0.1654 - val_acc: 0.9585\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9903\n",
      "Epoch 00095: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0313 - acc: 0.9903 - val_loss: 0.1631 - val_acc: 0.9574\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9917\n",
      "Epoch 00096: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0302 - acc: 0.9917 - val_loss: 0.1563 - val_acc: 0.9602\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9913\n",
      "Epoch 00097: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0299 - acc: 0.9913 - val_loss: 0.1536 - val_acc: 0.9625\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9925\n",
      "Epoch 00098: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0251 - acc: 0.9925 - val_loss: 0.1853 - val_acc: 0.9562\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9915\n",
      "Epoch 00099: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0297 - acc: 0.9915 - val_loss: 0.1749 - val_acc: 0.9613\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9926\n",
      "Epoch 00100: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0267 - acc: 0.9926 - val_loss: 0.1513 - val_acc: 0.9618\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9912\n",
      "Epoch 00101: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0302 - acc: 0.9912 - val_loss: 0.1684 - val_acc: 0.9602\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9931\n",
      "Epoch 00102: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0248 - acc: 0.9931 - val_loss: 0.1627 - val_acc: 0.9576\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9916\n",
      "Epoch 00103: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0287 - acc: 0.9916 - val_loss: 0.1618 - val_acc: 0.9576\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9935\n",
      "Epoch 00104: val_loss did not improve from 0.13754\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0236 - acc: 0.9935 - val_loss: 0.1700 - val_acc: 0.9604\n",
      "\n",
      "1D_CNN_custom_tanh_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 678us/sample - loss: 0.1873 - acc: 0.9433\n",
      "Loss: 0.18725951010069247 Accuracy: 0.94330215\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5903 - acc: 0.4965\n",
      "Epoch 00001: val_loss improved from inf to 0.93548, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/001-0.9355.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 1.5903 - acc: 0.4965 - val_loss: 0.9355 - val_acc: 0.7263\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8652 - acc: 0.7410\n",
      "Epoch 00002: val_loss improved from 0.93548 to 0.56262, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/002-0.5626.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8651 - acc: 0.7410 - val_loss: 0.5626 - val_acc: 0.8430\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6122 - acc: 0.8182\n",
      "Epoch 00003: val_loss improved from 0.56262 to 0.41389, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/003-0.4139.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6123 - acc: 0.8182 - val_loss: 0.4139 - val_acc: 0.8831\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4760 - acc: 0.8603\n",
      "Epoch 00004: val_loss improved from 0.41389 to 0.33351, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/004-0.3335.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4760 - acc: 0.8603 - val_loss: 0.3335 - val_acc: 0.9045\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8850\n",
      "Epoch 00005: val_loss improved from 0.33351 to 0.27787, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/005-0.2779.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3924 - acc: 0.8850 - val_loss: 0.2779 - val_acc: 0.9271\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3375 - acc: 0.9011\n",
      "Epoch 00006: val_loss improved from 0.27787 to 0.25773, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/006-0.2577.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3374 - acc: 0.9011 - val_loss: 0.2577 - val_acc: 0.9264\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9146\n",
      "Epoch 00007: val_loss improved from 0.25773 to 0.21688, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/007-0.2169.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2926 - acc: 0.9146 - val_loss: 0.2169 - val_acc: 0.9418\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.9226\n",
      "Epoch 00008: val_loss improved from 0.21688 to 0.21273, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/008-0.2127.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2600 - acc: 0.9226 - val_loss: 0.2127 - val_acc: 0.9373\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9289\n",
      "Epoch 00009: val_loss improved from 0.21273 to 0.19428, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/009-0.1943.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2415 - acc: 0.9289 - val_loss: 0.1943 - val_acc: 0.9404\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9341\n",
      "Epoch 00010: val_loss improved from 0.19428 to 0.17771, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/010-0.1777.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2190 - acc: 0.9341 - val_loss: 0.1777 - val_acc: 0.9490\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9398\n",
      "Epoch 00011: val_loss improved from 0.17771 to 0.17305, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/011-0.1731.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2016 - acc: 0.9398 - val_loss: 0.1731 - val_acc: 0.9448\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9444\n",
      "Epoch 00012: val_loss improved from 0.17305 to 0.16809, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/012-0.1681.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1861 - acc: 0.9444 - val_loss: 0.1681 - val_acc: 0.9529\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9489\n",
      "Epoch 00013: val_loss did not improve from 0.16809\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1743 - acc: 0.9489 - val_loss: 0.1776 - val_acc: 0.9485\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9494\n",
      "Epoch 00014: val_loss improved from 0.16809 to 0.15874, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/014-0.1587.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1667 - acc: 0.9494 - val_loss: 0.1587 - val_acc: 0.9527\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9538\n",
      "Epoch 00015: val_loss improved from 0.15874 to 0.14476, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/015-0.1448.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1527 - acc: 0.9538 - val_loss: 0.1448 - val_acc: 0.9574\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9579\n",
      "Epoch 00016: val_loss improved from 0.14476 to 0.14351, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/016-0.1435.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1420 - acc: 0.9579 - val_loss: 0.1435 - val_acc: 0.9581\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9585\n",
      "Epoch 00017: val_loss did not improve from 0.14351\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1353 - acc: 0.9585 - val_loss: 0.1438 - val_acc: 0.9581\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9617\n",
      "Epoch 00018: val_loss did not improve from 0.14351\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1260 - acc: 0.9617 - val_loss: 0.1437 - val_acc: 0.9581\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9635\n",
      "Epoch 00019: val_loss improved from 0.14351 to 0.13919, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/019-0.1392.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1218 - acc: 0.9635 - val_loss: 0.1392 - val_acc: 0.9585\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9657\n",
      "Epoch 00020: val_loss did not improve from 0.13919\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1144 - acc: 0.9657 - val_loss: 0.1457 - val_acc: 0.9576\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9677\n",
      "Epoch 00021: val_loss improved from 0.13919 to 0.12880, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/021-0.1288.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1067 - acc: 0.9677 - val_loss: 0.1288 - val_acc: 0.9611\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9683\n",
      "Epoch 00022: val_loss did not improve from 0.12880\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1041 - acc: 0.9683 - val_loss: 0.1306 - val_acc: 0.9599\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9710\n",
      "Epoch 00023: val_loss did not improve from 0.12880\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0958 - acc: 0.9710 - val_loss: 0.1343 - val_acc: 0.9574\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9736\n",
      "Epoch 00024: val_loss did not improve from 0.12880\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0894 - acc: 0.9736 - val_loss: 0.1301 - val_acc: 0.9597\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9752\n",
      "Epoch 00025: val_loss improved from 0.12880 to 0.12550, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/025-0.1255.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0842 - acc: 0.9752 - val_loss: 0.1255 - val_acc: 0.9616\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9755\n",
      "Epoch 00026: val_loss did not improve from 0.12550\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0819 - acc: 0.9755 - val_loss: 0.1320 - val_acc: 0.9588\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9768\n",
      "Epoch 00027: val_loss improved from 0.12550 to 0.12470, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/027-0.1247.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0770 - acc: 0.9769 - val_loss: 0.1247 - val_acc: 0.9625\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9786\n",
      "Epoch 00028: val_loss improved from 0.12470 to 0.12150, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_9_conv_checkpoint/028-0.1215.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0721 - acc: 0.9786 - val_loss: 0.1215 - val_acc: 0.9634\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9790\n",
      "Epoch 00029: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0711 - acc: 0.9791 - val_loss: 0.1314 - val_acc: 0.9567\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9818\n",
      "Epoch 00030: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0645 - acc: 0.9818 - val_loss: 0.1329 - val_acc: 0.9623\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9817\n",
      "Epoch 00031: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0622 - acc: 0.9817 - val_loss: 0.1313 - val_acc: 0.9611\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9830\n",
      "Epoch 00032: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0598 - acc: 0.9830 - val_loss: 0.1279 - val_acc: 0.9634\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9833\n",
      "Epoch 00033: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0582 - acc: 0.9833 - val_loss: 0.1309 - val_acc: 0.9606\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9848\n",
      "Epoch 00034: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0527 - acc: 0.9848 - val_loss: 0.1381 - val_acc: 0.9592\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9861\n",
      "Epoch 00035: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0501 - acc: 0.9861 - val_loss: 0.1255 - val_acc: 0.9641\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9860\n",
      "Epoch 00036: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0478 - acc: 0.9860 - val_loss: 0.1439 - val_acc: 0.9597\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9843\n",
      "Epoch 00037: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0533 - acc: 0.9843 - val_loss: 0.1320 - val_acc: 0.9625\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9882\n",
      "Epoch 00038: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0425 - acc: 0.9882 - val_loss: 0.1299 - val_acc: 0.9625\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9890\n",
      "Epoch 00039: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0396 - acc: 0.9890 - val_loss: 0.1319 - val_acc: 0.9644\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9876\n",
      "Epoch 00040: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0417 - acc: 0.9876 - val_loss: 0.1288 - val_acc: 0.9623\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9901\n",
      "Epoch 00041: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0365 - acc: 0.9901 - val_loss: 0.1274 - val_acc: 0.9632\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9910\n",
      "Epoch 00042: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0339 - acc: 0.9910 - val_loss: 0.1363 - val_acc: 0.9620\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9909\n",
      "Epoch 00043: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0331 - acc: 0.9909 - val_loss: 0.1389 - val_acc: 0.9641\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9915\n",
      "Epoch 00044: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0317 - acc: 0.9915 - val_loss: 0.1320 - val_acc: 0.9609\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9924\n",
      "Epoch 00045: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0297 - acc: 0.9924 - val_loss: 0.1502 - val_acc: 0.9597\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9918\n",
      "Epoch 00046: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0298 - acc: 0.9918 - val_loss: 0.1387 - val_acc: 0.9623\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9926\n",
      "Epoch 00047: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0279 - acc: 0.9926 - val_loss: 0.1357 - val_acc: 0.9616\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9931\n",
      "Epoch 00048: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0264 - acc: 0.9931 - val_loss: 0.1389 - val_acc: 0.9623\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9930\n",
      "Epoch 00049: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0258 - acc: 0.9930 - val_loss: 0.1412 - val_acc: 0.9592\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9942\n",
      "Epoch 00050: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0231 - acc: 0.9942 - val_loss: 0.1350 - val_acc: 0.9641\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9949\n",
      "Epoch 00051: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0207 - acc: 0.9949 - val_loss: 0.1373 - val_acc: 0.9627\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9948\n",
      "Epoch 00052: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0208 - acc: 0.9948 - val_loss: 0.1501 - val_acc: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9952\n",
      "Epoch 00053: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0205 - acc: 0.9952 - val_loss: 0.1423 - val_acc: 0.9625\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9945\n",
      "Epoch 00054: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0212 - acc: 0.9945 - val_loss: 0.1504 - val_acc: 0.9604\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9944\n",
      "Epoch 00055: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0207 - acc: 0.9944 - val_loss: 0.1436 - val_acc: 0.9632\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9957\n",
      "Epoch 00056: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0164 - acc: 0.9957 - val_loss: 0.1451 - val_acc: 0.9627\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9951\n",
      "Epoch 00057: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0178 - acc: 0.9951 - val_loss: 0.1372 - val_acc: 0.9641\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9958\n",
      "Epoch 00058: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0163 - acc: 0.9958 - val_loss: 0.1471 - val_acc: 0.9623\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9962\n",
      "Epoch 00059: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0160 - acc: 0.9962 - val_loss: 0.1629 - val_acc: 0.9602\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9963\n",
      "Epoch 00060: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0138 - acc: 0.9963 - val_loss: 0.1613 - val_acc: 0.9585\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9961\n",
      "Epoch 00061: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0143 - acc: 0.9961 - val_loss: 0.1581 - val_acc: 0.9597\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9962\n",
      "Epoch 00062: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0143 - acc: 0.9962 - val_loss: 0.1408 - val_acc: 0.9648\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9953\n",
      "Epoch 00063: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0177 - acc: 0.9953 - val_loss: 0.1603 - val_acc: 0.9590\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9966\n",
      "Epoch 00064: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0134 - acc: 0.9966 - val_loss: 0.1560 - val_acc: 0.9616\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9975\n",
      "Epoch 00065: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0111 - acc: 0.9975 - val_loss: 0.1488 - val_acc: 0.9630\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9970\n",
      "Epoch 00066: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0123 - acc: 0.9970 - val_loss: 0.1580 - val_acc: 0.9609\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9955\n",
      "Epoch 00067: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0155 - acc: 0.9955 - val_loss: 0.1662 - val_acc: 0.9590\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9971\n",
      "Epoch 00068: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0106 - acc: 0.9971 - val_loss: 0.1495 - val_acc: 0.9644\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9976\n",
      "Epoch 00069: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0103 - acc: 0.9976 - val_loss: 0.1668 - val_acc: 0.9602\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9970\n",
      "Epoch 00070: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0113 - acc: 0.9970 - val_loss: 0.1584 - val_acc: 0.9637\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9977\n",
      "Epoch 00071: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0098 - acc: 0.9977 - val_loss: 0.1527 - val_acc: 0.9634\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9973\n",
      "Epoch 00072: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0114 - acc: 0.9973 - val_loss: 0.1543 - val_acc: 0.9613\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9985\n",
      "Epoch 00073: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0079 - acc: 0.9985 - val_loss: 0.1638 - val_acc: 0.9632\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9975\n",
      "Epoch 00074: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0094 - acc: 0.9975 - val_loss: 0.1579 - val_acc: 0.9641\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9974\n",
      "Epoch 00075: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0100 - acc: 0.9974 - val_loss: 0.1776 - val_acc: 0.9609\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9965\n",
      "Epoch 00076: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0128 - acc: 0.9965 - val_loss: 0.1818 - val_acc: 0.9592\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9977\n",
      "Epoch 00077: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0093 - acc: 0.9977 - val_loss: 0.1500 - val_acc: 0.9648\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9982\n",
      "Epoch 00078: val_loss did not improve from 0.12150\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0071 - acc: 0.9982 - val_loss: 0.1701 - val_acc: 0.9620\n",
      "\n",
      "1D_CNN_custom_tanh_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPd/ZMdpIQkEWCIPselhYVrUtRK9r6KFp9XFr1sbWLtfVXutjaWlvb2qc+ttY+aK3aWpfiXm2pC4j6iAqICiKyQ1iykUzWyWzn98eZmSSQhAAZEpjv+/W6r2Rmztz7vTPJ+d5zzr3nijEGpZRSCsDR2wEopZTqOzQpKKWUStKkoJRSKkmTglJKqSRNCkoppZI0KSillErSpKCUUipJk4JSSqkkTQpKKaWSXL0dwMEqLCw0w4YN6+0wlFLqqLJy5coqY0zRgcoddUlh2LBhrFixorfDUEqpo4qIbOtOOe0+UkoplaRJQSmlVJImBaWUUkkpG1MQkQeAzwEVxpjxnZQ5FbgLcANVxpg5h7KtcDhMWVkZwWDwUMNNez6fj8GDB+N2u3s7FKVUL0rlQPODwO+Bhzt6UUTygD8Ac40x20Wk/6FuqKysjOzsbIYNG4aIHOpq0pYxhurqasrKyigpKentcJRSvShl3UfGmGXA3i6KfBF4yhizPV6+4lC3FQwGKSgo0IRwiESEgoICbWkppXp1TOFEIF9ElorIShG54nBWpgnh8Ojnp5SC3r1OwQVMA04HMoC3RGS5MeaTfQuKyHXAdQBDhw49pI1Fo81EIntxu/vjcGi/uVJKdaQ3WwplwGJjTKMxpgpYBkzqqKAxZqExptQYU1pUdMAL8joUiwUJhXZjTPjQI+5EbW0tf/jDHw7pveeccw61tbXdLn/rrbdy5513HtK2lFLqQHozKTwLnCQiLhHxAzOBdanamIjdVWNiPb7urpJCJBLp8r0vvvgieXl5PR6TUkodipQlBRF5FHgLGCUiZSLyZRG5XkSuBzDGrAP+BXwAvAPcb4xZk6p4Wne155PCggUL2LRpE5MnT+bmm29m6dKlnHzyycybN4+xY8cCcMEFFzBt2jTGjRvHwoULk+8dNmwYVVVVbN26lTFjxnDttdcybtw4zjrrLJqbm7vc7urVq5k1axYTJ07k85//PDU1NQDcfffdjB07lokTJ3LJJZcA8NprrzF58mQmT57MlClTqK+v7/HPQSl19EvZmIIx5tJulPk18Oue3O6GDTfS0LC6g1eiRKNNOBwZiBzcbmdlTWbkyLs6ff2OO+5gzZo1rF5tt7t06VJWrVrFmjVrkqd4PvDAA/Tr14/m5mamT5/OhRdeSEFBwT6xb+DRRx/lvvvu4+KLL+bJJ5/k8ssv73S7V1xxBb/73e+YM2cOP/rRj/jJT37CXXfdxR133MGWLVvwer3Jrqk777yTe+65h9mzZ9PQ0IDP5zuoz0AplR7S6IrmxNk15ohsbcaMGe3O+b/77ruZNGkSs2bNYseOHWzYsGG/95SUlDB58mQApk2bxtatWztdfyAQoLa2ljlz7PV+V155JcuWLQNg4sSJXHbZZfz1r3/F5bIJcPbs2dx0003cfffd1NbWJp9XSqm2jrmaobMj+lgsRGPjB3i9x+PxHNpg9cHIzMxM/r506VJefvll3nrrLfx+P6eeemqH1wR4vd7k706n84DdR5154YUXWLZsGc8//zy33347H374IQsWLODcc8/lxRdfZPbs2SxevJjRo0cf0vqVUseuNGoppG5MITs7u8s++kAgQH5+Pn6/n48//pjly5cf9jZzc3PJz8/n9ddfB+Avf/kLc+bMIRaLsWPHDk477TR++ctfEggEaGhoYNOmTUyYMIHvfve7TJ8+nY8//viwY1BKHXuOuZZCZ1rPPor2+LoLCgqYPXs248eP5+yzz+bcc89t9/rcuXP54x//yJgxYxg1ahSzZs3qke0+9NBDXH/99TQ1NTF8+HD+/Oc/E41GufzyywkEAhhj+MY3vkFeXh633HILS5YsweFwMG7cOM4+++weiUEpdWwRY45MH3tPKS0tNfveZGfdunWMGTPmgO+tr1+Jx1OM1zs4VeEd1br7OSqljj4istIYU3qgcmnUfQTgSMl1CkopdaxIq6Qg4tSkoJRSXUirpGB3t+fHFJRS6liRVklBRLuPlFKqK2mXFFJxSqpSSh0r0iop6ECzUkp1La2SgoiTvjKmkJWVdVDPK6XUkZBWSUFbCkop1bW0SgqpGmhesGAB99xzT/Jx4kY4DQ0NnH766UydOpUJEybw7LPPdnudxhhuvvlmxo8fz4QJE3j88ccB2L17N6eccgqTJ09m/PjxvP7660SjUa666qpk2d/+9rc9vo9KqfRw7E1zceONsLqjqbPBE2vBZULgzD64dU6eDHd1PnX2/PnzufHGG7nhhhsAeOKJJ1i8eDE+n4+nn36anJwcqqqqmDVrFvPmzevW/ZCfeuopVq9ezfvvv09VVRXTp0/nlFNO4W9/+xuf/exn+cEPfkA0GqWpqYnVq1ezc+dO1qyxt6M4mDu5KaVUW8deUuiSrYwNrRNp94QpU6ZQUVHBrl27qKysJD8/nyFDhhAOh/n+97/PsmXLcDgc7Ny5k/LycgYMGHDAdb7xxhtceumlOJ1OiouLmTNnDu+++y7Tp0/nS1/6EuFwmAsuuIDJkyczfPhwNm/ezNe//nXOPfdczjrrrB7cO6VUOklZUhCRB4DPARXGmPFdlJuOvUPbJcaYRYe94S6O6MMtewiFysjKmgLiPOxNtXXRRRexaNEi9uzZw/z58wF45JFHqKysZOXKlbjdboYNG9bhlNkH45RTTmHZsmW88MILXHXVVdx0001cccUVvP/++yxevJg//vGPPPHEEzzwwAM9sVtKqTSTyjGFB4G5XRUQezrQL4F/pzCONttL3X2a58+fz2OPPcaiRYu46KKLADtldv/+/XG73SxZsoRt27Z1e30nn3wyjz/+ONFolMrKSpYtW8aMGTPYtm0bxcXFXHvttVxzzTWsWrWKqqoqYrEYF154IT/72c9YtWpVj++fUio9pPJ2nMtEZNgBin0deBKYnqo42pJk6yAKuHt03ePGjaO+vp5BgwYxcOBAAC677DLOO+88JkyYQGlp6UHd1Obzn/88b731FpMmTUJE+NWvfsWAAQN46KGH+PWvf43b7SYrK4uHH36YnTt3cvXVVxOL2WT3i1/8okf3TSmVPlI6dXY8Kfyjo+4jERkE/A04DXggXu6A3UeHM3V2OFxDMLgJv38sTqe/W/uQTnTqbKWOXUfD1Nl3Ad813ejLEZHrRGSFiKyorKw85A2msvtIKaWOBb159lEp8Fj89MxC4BwRiRhjntm3oDFmIbAQbEvh0DeZultyKqXUsaDXkoIxpiTxu4g8iO0+2i8h9KTEmEIqbsmplFLHglSekvoocCpQKCJlwI+Jj+4aY/6Yqu12TVsKSinVlVSefXTpQZS9KlVxtKVjCkop1bW0m/vI0u4jpZTqSFolBUiMKfRsS6G2tpY//OEPh/Tec845R+cqUkr1GWmVFOyZTnJEk0IkEunyvS+++CJ5eXk9Go9SSh2qtEoKVs/fknPBggVs2rSJyZMnc/PNN7N06VJOPvlk5s2bx9ixYwG44IILmDZtGuPGjWPhwoXJ9w4bNoyqqiq2bt3KmDFjuPbaaxk3bhxnnXUWzc3N+23r+eefZ+bMmUyZMoUzzjiD8vJyABoaGrj66quZMGECEydO5MknnwTgX//6F1OnTmXSpEmcfvrpPbrfSqljzzE3S2oXM2cDEI2ORMSF4yDS4QFmzuaOO+5gzZo1rI5veOnSpaxatYo1a9ZQUmLPvH3ggQfo168fzc3NTJ8+nQsvvJCCgoJ269mwYQOPPvoo9913HxdffDFPPvkkl19+ebsyJ510EsuXL0dEuP/++/nVr37Fb37zG2677TZyc3P58MMPAaipqaGyspJrr72WZcuWUVJSwt69e7u/00qptHTMJYUDE+zk2ak1Y8aMZEIAuPvuu3n66acB2LFjBxs2bNgvKZSUlDB58mQApk2bxtatW/dbb1lZGfPnz2f37t2EQqHkNl5++WUee+yxZLn8/Hyef/55TjnllGSZfv369eg+KqWOPcdcUujqiB6gsXEbIm78/pEpjSMzMzP5+9KlS3n55Zd566238Pv9nHrqqR1Ooe31epO/O53ODruPvv71r3PTTTcxb948li5dyq233pqS+JVS6SntxhTsaak9O6aQnZ1NfX19p68HAgHy8/Px+/18/PHHLF++/JC3FQgEGDRoEAAPPfRQ8vkzzzyz3S1Ba2pqmDVrFsuWLWPLli0A2n2klDqgtEsK4OzxaS4KCgqYPXs248eP5+abb97v9blz5xKJRBgzZgwLFixg1qxZh7ytW2+9lYsuuohp06ZRWFiYfP6HP/whNTU1jB8/nkmTJrFkyRKKiopYuHAhX/jCF5g0aVLy5j9KKdWZlE6dnQqHM3U2QHPzJmKxZjIzO70ZXNrSqbOVOnYdDVNn9xKHTnOhlFKdSLukIOLUpKCUUp1Iu6Rgd1nnPlJKqY6kXVKwZx8ZjraxFKWUOhLSNCmA3lNBKaX2l3ZJIbHLevc1pZTaX8qSgog8ICIVIrKmk9cvE5EPRORDEfk/EZmUqljabzc102cfrKysrF7dvlJKdSSVLYUHgbldvL4FmGOMmQDcBizsomwP0u4jpZTqTMqSgjFmGdDpvArGmP8zxtTEHy4HBqcqlrZScUvOBQsWtJti4tZbb+XOO++koaGB008/nalTpzJhwgSeffbZA66rsym2O5oCu7PpspVS6lD1lQnxvgz8sydWdOO/bmT1ns7nzjYmSizWhMORgUj3dn/ygMncNbfzmfbmz5/PjTfeyA033ADAE088weLFi/H5fDz99NPk5ORQVVXFrFmzmDdvXvxmPx3raIrtWCzW4RTYHU2XrZRSh6PXk4KInIZNCid1UeY64DqAoUOHHu4WD/P9+5syZQoVFRXs2rWLyspK8vPzGTJkCOFwmO9///ssW7YMh8PBzp07KS8vZ8CAAZ2uq6MptisrKzucAruj6bKVUupw9GpSEJGJwP3A2caY6s7KGWMWEh9zKC0t7fICg66O6AGi0SBNTWvw+Upwuwu6LHswLrroIhYtWsSePXuSE8898sgjVFZWsnLlStxuN8OGDetwyuyE7k6xrZRSqdJrp6SKyFDgKeA/jTGfHLntpuaU1Pnz5/PYY4+xaNEiLrroIsBOc92/f3/cbjdLlixh27ZtXa6jsym2O5sCu6PpspVS6nCk8pTUR4G3gFEiUiYiXxaR60Xk+niRHwEFwB9EZLWIrOh0ZT0alzP+W8+efTRu3Djq6+sZNGgQAwcOBOCyyy5jxYoVTJgwgYcffpjRo0d3uY7OptjubArsjqbLVkqpw5F2U2cbY2hoWInHcxxe73GpCPGopVNnK3Xs0qmzO2HP/JFev3hNKaX6orRLCpYTvXhNKaX2d8wkhYPpBhNx6NxH+zjauhGVUqlxTCQFn89HdXV1tys2ewaSthQSjDFUV1fj8/l6OxSlVC/r9YvXesLgwYMpKyujsrKyW+VDoQrAiccTSm1gRxGfz8fgwUdkphGlVB92TCQFt9udvNq3O9577yuAYcyY11IXlFJKHYWOie6jg+V0ZhKNNvZ2GEop1eekbVKIxTQpKKXUvtI2KWhLQSml9pemSSGLaLSht8NQSqk+Jy2TgsOhLQWllOpIWiYFpzMTY0LEYpHeDkUppfqUtE0KgA42K6XUPtI6KWgXklJKtZeWScHhSCQFHWxWSqm20jIpOJ1ZgLYUlFJqX6m889oDIlIhIms6eV1E5G4R2SgiH4jI1FTFsi/tPlJKqY6lsqXwIDC3i9fPBkbGl+uAe1MYSzs60KyUUh1LWVIwxiwD9nZR5HzgYWMtB/JEZGCq4mlLWwpKKdWx3pwldRCwo83jsvhzu1O94daBZk0KKn2Ew9DUBMEgtLRAKGSXaBREWheASMQ+H42C0wluN3g8dnE6bZlE2ZYWaGy0S1MTGGPLud12iUbtdsJh+7PtumMxuxhjlwQRcDhat9H2Na/XLj6fXX9zs91uYvuJ/Wtpsdt0OsHlao0nK6t1cbuhoQHq66Guzr4/EmmN0ZjW97ndNp7EfiQ+O6ezdYnFWuNpbrbv9/laF5er/WfnctnnE/sUjba+v6nJbisRTyQCZ54J55+fur8ROEqmzhaR67BdTAwdOvSw19faUtCzj9JdoqKMxm/El6gY/X5bsSUYA3v3wtatsH27rYBCodaKx+ez78nMhIwMCASgqgqqq+37mpttZZWosKB9pReJtFY24XBrpd3SYl9LVGwu1/4VU6JySlQcsVj7fYlG2++j6jkOR+vnneB0tv4diNjvMPHdH8wNDkXs32Die3c6oX//Yzsp7ASGtHk8OP7cfowxC4GFAKWlpYd930g9+6hvMsZWtoFA6xFborJrbISKitalsc1Xl6gk2x4tJirKWMz+DAbt0WDiqDBxJBbp4qJ2nw9ycuwRZWWlfd+hyMmxCSNxtJhINsa0Vihtj2Q9HsjNtT+93taj7baVfuKoPVFpuN2tlYfD0f7I2+ForaQScXi9re9vWz7xnkQllDj6bZusEkf2iX3weu36MzPt+tsmrXC4Nb5E66Htuh2O1uSYWNrGEou1Js7Ea6FQ+9ZOYr8S20/sX6IlEY22HnG3bdU0NNjH2dl2SXxPbndrfIl9SSyJVlDic0/ElGj5OBz2/Z39fbf9jI1pjSmxP06njSEjw+5DYt+PpN5MCs8BXxORx4CZQMAYk/KuIwCn0w/oQHNPCAZthVlRYX8mlupq2xyvr2+thNtW0uFwawWe+Aetr9//qKszHk/rPyTYf9BEpeD3238oh6O14vH5YNCg9l0HibIZGfb9bSui5mYbf2IfCgth2DC7DB1qK5BEpe1y2c8hUdk0N0NeHhQUQL9+nVcS6shwOtu3+g5WIgl0RqQ1IXelbfdcQqL7KDf30OPraSlLCiLyKHAqUCgiZcCPATeAMeaPwIvAOcBGoAm4OlWx7B+bE4fDl9YthUjEHpEHAlBba3/W1NiujsRSW9u61NW19geHw7YSrKqyz3fE6bQVZ3Z2vALONLickqykvV5bYfr94Mmqx5FdiS8riC8riDezhbyMLIb6x+BxuXC5bLniYtt8zs5rIWjq8Lv9ZLgzcIiDmIkRCAaobq5mb/NenOKkOKuYIn8RXpe3wxijsShba7eyYe8G/G4/I/qNYGDWQKTNf25LpIWqpirCsXDyuZiJ0RBqYHcwQCAQoCHUgN/tJ9+XT35ePoXeXKImSm2khT3VQcKxMB6nx8brysDj9FDXUkdNsIaa5hqawk0UZRYxKHsQA7MH4nF2XgM1hZuob6nH7XTjcXpwO9w4xEE4FiYcDROKhnA5XOT58trthzGGisYKPq76mJpgDZnuTDI9mWS6M8nx5pCfkU+ONweHOJLlm8JNBFoCOMRBvi+/3ecYiUWoaqqiorEiuU2nOHE5XLREW2gMNdIQaqAp3ITX5SXXm0uuL5dcby55vjyyPFnt4uuKMYY9DXv4qPIjtgW2UZJXwqQBk+iX0a/D77QxbLfdGGokFA0l9y/TndnpNqubqllbuZbKxkqOzzue4fnDyfflIyLETIyKxgrK6sqoaa7BIY7k4nV57feekZ8sX9dSRyAYINASoDHUSEu0hWAkSEukhQx3BoOyB3Fc9nH0y+jXYTzBSJCPKj9ibcVaRIT+mf0pziymOKuYQn8hLkdqj+VTtnZjzKUHeN0AN6Rq+wdyLMyU2hJpYVPNJjZUb2Bd5QZ2VtZTtTdMVU2I2row4YhJHpknjn5bj2YNOKLgiNhFYhB1Q9QLER8OnHj67UHyyzBDyoj4ynEYD85YBi7jx00mec4CSjxFFPmL6JeVTcxTS4ujmiaqqQvvJdASoDZYS3kwQDgWpshfRHFWMcWZxXhdXrYHdrA9sJ2aYE3rTgXjC+B3+5k2cBozB80k15fLhxs/5MM3P+ST6k+ImtYOcq/TSyQWafdcW3m+vGRFlKgMyxvK2bh3Iy3RlnZl/W4/w/OHE46GKW8spzZY29Nf2wH1y+hHpjuTDHcGGa4MnA4n1U3VVDZV0hRu6tY6PE4PA7IGMCBrAADrq9YTaAl0+R6HOMj15uIQB4GWAJF9JozMcGWQn5FPKBqiuqkaw6H35CYSV77PJqMcbw7Z3myyPFmEo2Gawk00R5qpa6ljQ/WGDmMfkjOE0YWjaQg1UNlUSVVTVZffl8vhIt+Xn0xOub5cjDGsq1rHnoY9+5VPlNldv7vdQUFP8Tq9FPgLyPZkk+PNIcuTxe6G3XxS/Qkx03GT+aZZN/Gbz/6mx2NpS8zBjHz0AaWlpWbFihWHvZ633jqevLzTGDPmwcMP6iDtbd7Lq1tepb6lnuH5wxmeP5zjso+jqqmKpVuX8uqWV1m6bSnRWJQhmSPIi43A03gCDcEgVZHNVMc2U202EzDbMbLPH0/UbZeYG2lzxnGi6Zrsw3WAS1z2CM/hxOlwECNMhBZCUXt0OyBrAINzBjMkdwjFmcVEYpHkP2tDqIGqpioqGyupbKqkIdRArjeXAn8BBRkF9MvoR54vL/mP5XK4qGqqoryxnPKGclqiLQzJGcLQ3KEMzR1KcWYxGe4MvE4vPpePqqYq3tn5Du/seodVu1cRioYoySthQvEEJvafSHFWMc3hZprCTTSFm3A5XMltF/gLiMaiyW2VN5Ynj9oaw400hhop8BcwumA0owpHcWLBiTSFm9i4dyMb925kc81mPE5P8uiso9ZGlicruW/Znmwaw43UNNdQE6whEAzgcrjwuXx4XV48Tg8tkRaaIzbeUDREtic7eXTpd/upaKxgV/0udtbvpLyhnKZIU3L/IrEIhf5CivxFFPoLyfXlEolFCEVDhKIhYiaG22FbDh6nh1A0RHljObsbdrO7fjcxE2NUwShGFY5idOFoCv2FNIWbkkfTbVstNcEajDHtKs6YiSVfq2musZ9NVnHyCNbrskk5sXid3mQrxO/20xJtIRAMUNdSR22wltpgbXJde4N7qW+ppz5UT31LPQ2hBtxOd7JVlenJ5IT8ExhTOIaxRWM5Pu94Nu3dxPvl7/N++fts3LuRHG9O8vMpyCgg25udTP4ep4dAMEBNsMZut7mGQIs9ig8EA0RNlNGFoxlfNJ5x/cfRP7M/2wPb2Vyzmc01mwm0BBicPZjBOXYp8BdgjCFmYsRMjOZIMzXNNcl9isai7T67THdm8u/A6/TSFG5iZ/1O+13X7aQmWJPc97qWOgr9hUwsnsiE/hMY3388LoeL8sZyKhorKG8oZ2LxRGYPnX1I9Y6IrDTGlB6wXLomhXfeGYffP4bx4xcd9rpiJsbz65/nw4oPKckrYUS/EYzoN4JMTyZ7Gvawp2EPu+t3s3L3Sv696d+s2LViv6Msl7iJGHs04ormkFExh6ZABtGcTdBvA/ji/TQNxVAzHGpK8DWP5DjvSEbkn8iEQSMYNzyP4cOFkhLbf544dfBIiJlYsuuhpyUqvyxPVkrWr1Q66G5SOCpOSe0Rr7wCt94Kjz4Kgwf3yH2ao7Eof//o79z++u2sqehwNo92nOJkQv5MvlDwY3IqzmLH+iLW7trC7ubNRPI3Q3M/nNs/Q4l/CieOcDFyJIwcCSNGGAqH7KW40EduRmby7I2+NICZqoQAJI+AlVKplz5JobkZ3ngDdu1KJoWDGVN49MNHeWztYzjFaQf4nG7e2fkOn1R/wpjCMfz183/lgtEXsD2wnY17N7Jm9wY2bmuiYc9AKjYNZOvaAWx77wRWN+eyGtuFM2IEnDRpBBMnwoQJMHasPbtl/8pegIKe+yyUUqoT6ZMU+ve3PysqADvQHAod+AzYQDDAV1/8Kn/78G8MyxuWHAgLRUMUZxWz6KJFfH7M5wmHHCx7DZYuHcOSJWN4993Wc+AHDYKpU+Hyb8O4cbbyP/FEeyqaUkr1JWmbFLrTUnhz+5tc/vTl7Ajs4LbTbuN7J30Pp6N9R31ZGdzyQ1i40J6i6XTC9Onwne/AySfDtGn2VEqllDoapE9SKCqyPysrgURS6Hiai1A0xE9f+ym/eOMXDMsbxhtfeoNZg2e1K/POO3DnnfDUU/Z0z3nz4JprYM4ce26+UkodjdInKSQud022FLI6HGhevWc1Vz5zJR+Uf8BVk6/if+b+DznenOTrq1bBj38M//gH5OfDt74FX/0qlJQcsT1RSqmUSZ+kALYLqZPuo0gswh1v3MFPXvsJBRkFPHfJc5w36rzk6xs3ws03wzPP2GTw85/D176mrQKl1LElbZOCw5GJMWFisTAOh5uvvvBV7lt1H5eMv4Tfn/17CvytZ/s88wxceaX9/Sc/gW9+s2/NVaKUUj0lvZJCUZE9JZX2N9r503t/575V9/Hd2d/ljjPuSBaPRuGHP4Q77oDSUnjySTsZmlJKHatSeTvOvqdd95G9Onb5jtf52j+/xlknnMXtn7k9WbS6GubOtQnhuuvg9dc1ISiljn3p1VJIJAVj8HgGUBOCrz99DcdlH8ffvvC35Omm0ShcdBG8+Sb86U/wpS/1ctxKKXWEdKulICLfFJEcsf4kIqtE5KxUB9fj+ve38z7X1eHyDOYnH0F1cw1PXfxUuzGEn/4UliyBP/5RE4JSKr10t/voS8aYOuAsIB/4T+COrt/SByWuVaioYNGG5bwfgJ9/+gKmDJySLPLSS3DbbXZg+eojdocHpZTqG7rbfZS4E8Q5wF+MMWulu3fI6EvaXNW8eNer9Pc6OGdQ6406du2Cyy6DMWPgnnt6KUallOpF3W0prBSRf2OTwmIRyQYOeONEEZkrIutFZKOILOjg9aEiskRE3hORD0TknIML/yDFk0K0fDevbH6FmUX5tLRsA+w8RV/8or0Bzd//bq91U0qpdNPdlsKXgcnAZmNMk4j04wC3zxQRJ3APcCZQBrwrIs8ZYz5qU+yHwBPGmHtFZCz2Fp3DDnIfui+eFFbuWklNsIaTjptBMLgVsOMHr70GDz1kJ6xTSql01N2WwqewZq1sAAAgAElEQVSA9caYWhG5HFuZd31vP5gBbDTGbDbGhIDHgPP3KWOAxBwSucCubsZzaAoLAXhp77sAzBkyjWBwK7GY4Z57YOZMuOKKlEaglFJ9WneTwr1Ak4hMAr4NbAIePsB7BgE72jwuiz/X1q3A5SJShm0lfL2b8Rwarxdyc/l3aB1TBkxhUN5YYrEgL79cw8cfw1e+ktKtK6VUn9fdpBAx9r6d5wO/N8bcA/TErD+XAg8aYwYTH8QW2f8WXiJynYisEJEVlfFZTg9V/XGFvOXaw1knnIXPNwyAe+815OfDxRcf1qqVUuqo192kUC8i38OeivpCvOI+0M0gdwJD2jweHH+urS8DTwAYY94CfEDhvisyxiw0xpQaY0qLEqeVHqLXTvQQlhhnDj8Tn28Y1dUD+Mc/8rn6asjIOKxVK6XUUa+7SWE+0IK9XmEPtoL/9QHe8y4wUkRKRMQDXAI8t0+Z7cDpACIyBpsUDq8pcAAvDW4hIyLMHjobn+94Xnzxy0QiDq6/PpVbVUqpo0O3kkI8ETwC5IrI54CgMabLMQVjTAT4GrAYWIc9y2itiPxURObFi30buFZE3gceBa6Kd1OlzL/z9nLKLjc+lw+RbP7xj+v59KfXMXJkKreqlFJHh+5Oc3Ex8A5wEXAx8LaI/MeB3meMedEYc6Ix5gRjzO3x535kjHku/vtHxpjZxphJxpjJxph/H/quHNiOwA4+dtdy1sdhiMV44QWoqBjMhRcuSuVmlVLqqNHd6xR+AEw3xlQAiEgR8DJwVNWmL21+CYAzNxrYu5d77y2kf/9qZs16HLild4NTSqk+oLtjCo5EQoirPoj39hkvbX6JAc48xlfA5pU1LF4M8+evJBLZRIp7rZRS6qjQ3Yr9XyKyWESuEpGrgBew1xUcNWImxsubX+bMgukI8K9/GoyBSy7ZTSwWJByuOOA6lFLqWNfdgeabgYXAxPiy0Bjz3VQG1tNW71lNVVMVZw49DYDNmww+H4wcaafMTkx3oZRS6azbN9kxxjwJPJnCWFKqrK6M/pn9OWPs54Dvs2WHi2HDICNjGADNzVvIyZnZmyEqpVSv6zIpiEg9dn6i/V4CjDEmp4PX+qR5o+Zx3onnIbEYiLClPIOSKeDzHQ9oS0EppeAAScEY0xNTWfQZIgJOJxQUsGVvHp8qAZcrG5erQJOCUkpxFJ5B1BNqC06gNuRn+HD72OcbpklBKaVI06SwJWsCACUl9nFGRokmBaWUIl2Tgnc00JoUfL5htLRs02sVlFJpLy2TwmZsv1HbpBCLBQmFynsxKqWU6n1pmRS2hAeTRw15mWGA5H0VtAtJKZXu0jMpNBVTwhaoqgI0KSilVEJ6JoXaPJsUKuzUFl6vXquglFKQhknBGNhamclwNieTgsuVhdtdqElBKZX20i4p7NkDwZDTthTa3O9Zr1VQSqkUJwURmSsi60Vko4gs6KTMxSLykYisFZG/pTIegM2b7c+23UegSUEppeAgJsQ7WCLiBO4BzgTKgHdF5DljzEdtyowEvgfMNsbUiEj/VMWTsGWL/Vni3LFfUqiu/gfGGDsdhlJKpaFUthRmABuNMZuNMSHgMeD8fcpcC9xjjKkB2OdGPimRSArDihrbdR9lZIwgFgtqa0EpldZSmRQGATvaPC6LP9fWicCJIvKmiCwXkbkpjAewSWHgQPAV57ZrKeTkzAIgEHgz1SEopVSf1dsDzS5gJHAqcClwn4jk7VtIRK4TkRUisqKyzdH9odiyJX4lc//+7ZJCZuZ4nM4c6uo0KSil0lcqk8JOYEibx4Pjz7VVBjxnjAkbY7YAn2CTRDvGmIXGmFJjTGlRUdFhBdVZUhBxkpPzKQKBNw5r/UopdTRLZVJ4FxgpIiUi4gEuAZ7bp8wz2FYCIlKI7U7anKqAwmHYsQM7ZXZRUbsxBYDc3JNobFxLOFyTqhCUUqpPS1lSMMZEgK8Bi4F1wBPGmLUi8lMRmRcvthioFpGPgCXAzcaY6lTFtH07xGJtWgr19dDcnHw9N3c2YKireytVISilVJ+WslNSAYwxLwIv7vPcj9r8boCb4kvKJU9HLQFMsX2wcyeMGAFATs4MRFwEAm9SUHDOkQhJKaX6lN4eaD6i2iWFSZPsg1Wrkq87nZlkZU3RcQWlVNpKu6TgcsHgwcDEieD1wttvtyuTm3sS9fXvEIuFeidIpZTqRWmXFIYOBacTcLth6lR45512ZXJzZxOLBWloeK93glRKqV6Udkkhcbc1AGbOhJUr7WlJcTk5swG0C0kplZbSKils3rxPUpgxw559tHZt8imvdwA+3wl6ZbNSKi2lTVJoaLCXJQwf3ubJmTPtz/3GFWYTCLyBPTlKKaXSR9okha1b7c92LYWSEigs7GBc4STC4UqamzcesfiUUqovSJuk0O501AQR24XUQUsBdFxBKZV+0iYpDBoEN9wAI/edWWnGDPjoI3t1c5zfPxqXq5+OKyil0k5Kr2juS6ZOtct+Zs60N25esQJOOw0AEQe5uZ/WloJSKu2kTUuhU9On25/7jSvMobl5Pc3NKZufTyml+hxNCgUFdu6jfcYV+vefDwh79jzUO3EppVQv0KQAdlxhn5aCzzeE/Pwz2LPnIYyJ9VJgSil1ZGlSADuusHOnXdoYMOBqWlq2UVu7pJcCU0qpI0uTAtiWAuzXWigsvACnM5fdu//cC0EppdSRp0kBYPJkO0HePknB6cyguPhSqqqeJBIJ9FJwSil15KQ0KYjIXBFZLyIbRWRBF+UuFBEjIqWpjKdTPp+9v8I+g81gu5BisSAVFY/3QmBKKXVkpSwpiIgTuAc4GxgLXCoiYzsolw18E9i/Rj6SZs6Ed9+FaLTd09nZ0/H7x7Jnj3YhKaWOfalsKcwANhpjNhtjQsBjwPkdlLsN+CUQTGEsB3bqqXbWvFdeafe0iDBgwNXU1S2nsXFd78SmlFJHSCqTwiBgR5vHZfHnkkRkKjDEGPNCCuPonvPOs9cs3Hfffi8VF18OONmz58EjHpZSSh1JvTbQLCIO4L+Bb3ej7HUiskJEVlRWVqYmIK8XrrwSnn0WKir2eWkABQXnsGfPQ0SjTanZvlJK9QGpTAo7gSFtHg+OP5eQDYwHlorIVmAW8FxHg83GmIXGmFJjTGlRUVHqIr7mGnsXtof2v4p5yJDvEA6Xs2PHnanbvlJK9bJUJoV3gZEiUiIiHuAS4LnEi8aYgDGm0BgzzBgzDFgOzDPGrEhhTF0bMwZmz4b777eT5LWRl3cKRUX/wfbtdxAM7uhkBUopdXRLWVIwxkSArwGLgXXAE8aYtSLyUxGZl6rtHrZrr4VPPoFly/Z7afjwX2NMjM2bOz27VimljmpytN1ysrS01KxYkcLGRFMTHHccfO5z8Ne/7vfy5s0/ZPv225ky5U1ycz+dujiUUqoHichKY8wBrwXTK5r35ffDZZfBokVQU7Pfy0OHLsDjOY6NG7+pE+UppY45mhQ6cu210NLSYUvB5cpi+PA7qK9fQXn5X3ohOKWUSh1NCh2ZPBlKS+01Cx10rxUXX0Z29kw2bfouoVCKTpFVSqleoEmhMzfcAB9+CE8+ud9LIg5GjfpfIpEa1q+/lqNtXEYppTqjSaEz//mfMHEi3HwzBPefgSMraxLDh/+c6upn2b37/l4IUCmlep4mhc44nfDb38LWrXDXXR0WGTz4W+Tlnc7GjTfS1LThyManlFIpoEmhK5/5DJx/Ptx+O+zZs9/LIg5Gj34Qh8PLunWXEYuFeyFIpZTqOZoUDuTXv7ZnIt1yS4cv+3yDOfHEhdTXv8vWrbce2diUUqqHaVI4kJEj4etfhz/9CVav7rBI//7/wYABX2L79p+zc+cfjnCASinVczQpdMctt0C/fvDVr8LevR0WOfHEeykoOI8NG25g9+4HjnCASinVMzQpdEdenh1sfucdO2ne44/vd/2Cw+Fh3Li/k5//Wdavv4by8kd6KVillDp0mhS66/LL7e06hwyBSy6xN+XZvr1dEYfDy/jxT5GXdyrr1l2hiUEpddTRpHAwpkyB5cvhN7+BJUtg2rT9EoPT6Wf8+OfIzf0069ZdzgcfnEtT0ye9FLBSSh0cTQoHy+WCm26yrYaWFrjoIvuzXZEsJk16heHDf00g8DrvvjueTZtuJhKp66WglVKqezQpHKqxY+HBB+04w7e+td/LDoeHoUO/w4wZn1BcfDk7dtzJu++Oo6Zm6REPVSmlukuTwuH4whfgO9+Be++Fv3Q8Y6rXO4DRox9g6tTlOBx+3n//M2zatIBYLHSEg1VKqQNLaVIQkbkisl5ENorIfrcrE5GbROQjEflARF4RkeNTGU9K/OIXcMop8F//BR980GmxnJyZlJauYuDAa9ix45esWvUpGhvXHcFAlVLqwFKWFETECdwDnA2MBS4VkbH7FHsPKDXGTAQWAb9KVTwp43LZU1Tz8uzA88iR8NnPwvXX2/sxxFpvxON0ZjJq1ELGjXuKYHArK1ZMZMOGr+v020qpPiOVLYUZwEZjzGZjTAh4DDi/bQFjzBJjTFP84XJgcArjSZ0BA+CVV+Db34apU+0d2xYtsjOtzpgB//d/7YoXFX2eGTM+YuDAa9m5817efvsEtm37OdFoUycbUEqpIyOVSWEQsKPN47L4c535MvDPFMaTWmPGwB132FbDO+9AZSU88oidSG/2bHudw86dyeIeTzEnnvgHpk9fQ17eZ9iy5Qe8/fYJ7NjxG6LRxl7cEaVUOusTA80icjlQCvy6k9evE5EVIrKisvIo6WoRgS9+ET7+GH7wA/j732HECPh//w+qq5PFMjNHM2HCM0ye/DqZmePZtOk7LF8+jG3bfkEkEujFHVBKpSNJ1V3DRORTwK3GmM/GH38PwBjzi33KnQH8DphjjKk40HpLS0vNihUrUhBxim3ZAj/+sR1nyM62XU1f+QoUFbUrFgi8xbZtt7F37z9xODIoLPwCAwZcRb53JpKRCY4+kceVUoersRHWroXNm+2ybZudKeFzn0vJ5kRkpTGm9IDlUpgUXMAnwOnATuBd4IvGmLVtykzBDjDPNcZ06y41R21SSFi71k6w9/TT9nG/fnDiiXaAetQoGD0aRo+mvrieqhX/A88/Q/4bQXLXQvj4fBrv+Cr+eTfg9Q7s3f1QSh0aY+Dhh+2BYZteAzIzobnZHjheeun+7/vwQxg4EAoLD2mzvZ4U4kGcA9wFOIEHjDG3i8hPgRXGmOdE5GVgArA7/pbtxph5Xa3zqE8KCatWwdKl8MknsGEDrF/fbswBkeSke+GxQ6ktdZL1ylYydhoq5sDObx1P7oQvMmjQDXi9XQ3VKNVHGQNvv20rwuxsu+Tk2ErP7U7ddpub4a234NVXoaICvvENGD++fZmGBvjv/4YVK+Ckk+DMM2HSpO631FtaYNkyu55Jk2DYMPvejz+2PQRLl8KnPmVv9ztyJJSU2Pedey68/rodj7zkEvtcMAi33Qa/+hVcey384dCm5+9uUsAYc1Qt06ZNM8es+npjVq405pFHjLnlFmN+/3tjtm5NvhxtrDPNP/wvE/W5TdTnNNsuxfzfE06zdu0XTSDwbi8GrtRBiMWMeeEFY6ZPN8amhvaLiDH9+xszebIx551nzPPP2/e0FQwa87OfGTNggDHjxtly3/ymMb/7nTFvv21MKNR+ex9+aMwvf2nMaacZ4/Xa7Tidxvj9dnuXX27Mpk32fffea0xxsS1TUtIaV0GBMRdcYMyttxrz1FO2fFOTMRUV9vfVq43585+N+cIXjMnKar9P2dnGzJpljMdjTF6eMf/7v8ZEo/t/Ng0NxpxyijEOhzGPPWbMkiXGjBxp13HllcZUVR3yx449GD9gHZvSlkIqHDMthcOxbRssWIB54glwQOWpDnZcGCE2dSL9iy+hf//5ZGQMP/T1x2L2vhGH2ExVPcQYe4LC4sUwd67tb/b5Ure9DRvszaTq6uwFmXPm2O4KY2yf96uv2iNctxtmzbLL+PH2Wh2AaNT2k2dldXxEHQrBP/9pb2/77rv26Pl737Pdp/X1dgkEoLwcdu+GXbvsBaHbt8Opp9oj5enT7WSUX/mKbV3PnQteb2u/fGP8zL2MDFu2pMSWT0xcOXEinHGGvdXuySdDJGLXe/fdEA7DoEH2/+vkk+3zs2bZWF55BV56yU6IuWHDflPntzNokB0XOO886N8f3n/f3qDr/fdtq+AXv4Di4s7f39AA55wDb75p/xeHD4f//V8b92HoE91HqaBJoY2tW+F3v8Pcfz9SV4cRiHkh6gPj8+Bw+nDgwWGcSEzse0Ts4nTaacBHjLDL4MG2abtiBaxcaf85586Fn//czg57rDAGduyAggLbh7vva88/Dz/7ma0Mr7rKNuc9np6PoaHBnrZcWQlNTbayKihoLbNqFXzzm/DGGzYRBIOQmwsXXwwXXgjHH29jzMmx32dLi628du+2p0FXV7cuLpf9DqdOtRWMSGscgQD8+9+20nn1Vft3kZFh4wNbibW0tFaqAwbYyj9xFqDfb7t96uvtfoA9mJgzx1bkJ51kK+9nnoEXX7QJZ9gw+OEP4YorDtxNFA7DfffBrbfabZaW2r/R4cPh97+Hs89u/7mWldmK+8037fVBmzfbCv7cc23ZQZ10te7ebZPVmjV2wsvzzmv9nPaVGCB+/32oqmrt+srOtnFNmtT5e7uroQGuucYmtVtusZ/zYdKkkE7q6uCxx6CsjEjdHpqr1xCq2UA4Uo0RAw4Qlx+v9zh8nqF4PYNwxBz2iGjjRvuPBPYfdNIke4TVr5/tu6ypsYNeP/oR5OfbSqSuzrYkyspsZbF9u308ZIj9Ix4+3C4jRuz/x7xnj/2nrq62F/aNHt31P5AxtvyaNfafsaGhtcLKympdCgpsYhs0yFZKIjamPXvsP/zq1baSeOst+1xWFlx2GVx3na0s162DG2+0FeTIkXYbu3fb9X7xizB5sr1qPT/fVs7BINTW2s+jvt6+NnCgrTQHDLDrb1v5rl8P//qXXV5/vbUCbWvECHtkCrZPubDQVlRXXgmvvWbn13rqqdajYbAJIyPDfk8d8XptJR6J2Me5uTa+vXvtEo3a548/3vZXf+lL9oy4996z21y2zCbFz3zGLqNG2fJbt9rKNzEmkJVlK0W/Hz76qP3ROdgj5vPPt8tZZx38mEFdnb1f+iOP2O/t+9+3+626TZOCIhoN0tj4PnV171BX9xZ79y4mEtkLOMnNnU1W1iR8vuFkmMH4azLJGDUH8bWpxGtr7T/iXXd1XImBrfgGDrSV5Y4d9p+3rUGDbPdAdrY9+k0koIR+/eDTn7ZThAweDMcdZ5fKSnt0+eyz7QfgobVros0UIu14PLYiDofbP3/CCXZbM2bY1tDjj9sKbdw4W2lnZtoj0htusPv10kt2JtxnnrFdHwdDxK4vK8vGUl5unx81ynYDHH+8rXyLimwF+d57tpJdvtwmzG98wx4h5ua2X29Dg704cs+e1qWx0X4Hic+uuNgms4ICW0mHQvbIdtUqu99VVa2vFxTY/T/jDNtK6Elbt9oj9pISmDmz59evDoomBbUfY6LU1S2nuvoF9u79N83N64lGG5KvO53Z5OTMIjd3Nrm5J5GTMwunM9MeMT/xhK28cnNtl0Venm0ZHHdca/eKMfaIdfNm2LTJ9r1u2GDPsAoE7NH29Om2C6CgoLWZ/+abtlLeV0aG7cK64AJ7VXhOjq1kE/3qwaCtJOvrbUVXVmYTSCLxtD1yHz3a/myrttYeef7tb7Zv/Lbb7BHtvpqb7VkqNTV2CQRsbHl5dsnKss8num/Ky21MiVZNKGTPNPnsZ23XSddfkj2qT+XZNyotaVJQB2SMIRyuIhjcTFPTeurqlhMIvElj44eAQcRFdnYpubmnkJs7G693MG53EW53EU5nDw94hkL2qHfXLrt4vXDaaT3Sl6qU0qSgDkMkEiAQeItAYBm1tcuor38HY9p3xbhceeTmnkx+/hnk55+O3z8WOdzBNaVUynQ3KbiORDDq6OJy5VJQMJeCgrkARKNNNDS8TyhUTjhcSThcSTC4hZqaJVRXPw+A211MTs50srNLycqaRnb2VDyegZoolDrKaFJQB+R0+snN/VSHrzU3b6W29hVqa1+jvn4l1dUvALb16XLl4/ePJTNzLBkZI3G7C3G7C3C5+uHxFOPzleBw6J+gUn2J/keqw5KRMYyMjC8zcOCXAYhEGmhoeI+GhvdoalpHY+NaKiufjJ/11J6Im4yMEfj9o/D7RyeXjIxRuN15R3pXlFJoUlA9zOXKIi/vZPLyTk4+Z4whGq0jHN5LJLKXcLialpZdNDevp6npY5qa1lNd/UK7cQunMweHIwOHw4fD4cPjKSY7u5ScnBlkZ0/H5yvRrimlUkCTgko5EcHlysXlygVKOiwTi4UJBrckk0RLy05isSDGtBCLBQkGt7Jz5z2Ulf13fJ1e3O5+uFz9cLv74XYXxLun7OJy5eN0ZsWXbLzewfh8xyOiU48r1RVNCqpPcDjc+P0n4vef2GmZWCxMY+OH1NW9QzC4Jd7qsC2P5uaN1NUtJxyuwphIh+93OrPw+8eRmTken28YHk9R8hTb1vGOfBwON+FwLY2Na2hsXENz83oyM8dTWPgF3O78VH0ESvUJmhTUUcPhcJOdPZXs7Kmdlkl0VUUiAaLReqLRBiKROoLBLclKvrr6OcLhzu/g53RmtbuoT8SDMSE++eQr5OefRf/+88nKmojHMwC3uxARe6VuLBYiEqkhEqnH6x3c89dyKHUEaFJQx5T2XVWdi8VaCIerCIUqCYcrCIerCYerk60Pj2cAWVkTyMycgNc7mPr6lVRUPE5l5RN8/PELbdbkwO0uIBZrbpdIwEFGxgn4/WPx+0fj9Q6Mt0j643YXxMdLvDgcnvi4SSYOh1fHSVSv04vXlDoIxsRoaHiPYHArodCe+FKJ0+nH5crH7e6Hw5EZb5mspanpI5qbN3TapdWeIzkO4nLlJRenMyux9fjixOXKib+ei9OZG39PNk5nFg6HHxE3Doc7/tOPx1OE05mjSSeN9YmL10RkLvA/2Duv3W+MuWOf173Aw8A0oBqYb4zZmsqYlDocIg6ys6eRnT2t2+8xJko4XJO88C8criYWCxKLtSQH0qPRJqLRRmKxRiKReqLRAJFILeFwBc3Nm+LbFkAwJhrvHgsQiwUPInYPbndRfCylEJfLDs47nZkkEk7iIFHEmVyMiWFMiFgshDGh+Ouu+OLG7e5PRsZwMjJOwOc7AYfDTTTaQDTaSDTaiIi7zaC/v0cG+203YT3hcDUez0DtqutBKUsKYjta7wHOBMqAd0XkOWPMR22KfRmoMcaMEJFLgF8C81MVk1K9QcSJx1OIx1MIjOnRdcdiLfEE0dBmacKYcHKJRhsJhysJhSraJaZgcBvhcHW84rYJxy42kUEUY6Lx5ODB4fAg4sYmpjDGRDAmfFCJyX4eLiCRdFzxVk8/XK58XK48RCS+7sSSaCGBMRFCoT3xs9NapxD3eAbFk1IJHk9R/Ky0ApzOHIxpSSbdaLSOUGg3LS27CIV2EY02xq+VGY3fPwqv9/h4snO2GStqJhptju+naZPgshBxxJO4XUTceL2D8XoH4/EMxOFwYUw0vu2G+Bl19rOLxcLxxF9OKFROKFSBw+GLnyk3BK93CG53f1yunCN61lwqWwozgI3GmM0AIvIYcD7QNimcD9wa/30R8HsREXO09Wkp1UscDi8eT3+gg9ldj5BwuJZgcBPNzZsJBjdjTAynMzPZlWVMpF3SspWiTTqxWLjdNSzB4CZAkq0QWzHbCtEmLgdZWZPo1+9svN5BuN39aGnZSXPzJpqbN1FT8zKRSHUXiUrweIrxeI7D6x2Mw+GjuXkjtbWvEYt1Mj38IXPgcHiJxZq7XR46mg7eEe8qzGfQoK8wZMi3ezDG/aUyKQwCdrR5XAbM7KyMMSYiIgGgAKhKYVxKqR7kdufhdh9cl1qqRaNNhMN7iUbrcDgycDr9OBz+ePfV/vd1MCZGS0sZLS1lGBNNJi0gflKAXQBiscb4WW31QAynMxunMxuXK5tYLERLyw5aWnYQDO4gFgsmx3psksxok/BsK8ntLo6fydYv/v6y5DrC4SoikRrC4RoiEXsCRKodFWcfich1wHUAQ4cO7eVolFJ9ndNpE0B3iTjw+Ybi8x1+/ZKVNeGQ3+t0+vD7R+D3jzjsOA5VKjuqdgJD2jweHH+uwzJiOxpzsQPO7RhjFhpjSo0xpUVFRSkKVymlVCqTwrvASBEpEREPcAnw3D5lngOujP/+H8CrOp6glFK9J2XdR/Exgq8Bi7GnpD5gjFkrIj8FVhhjngP+BPxFRDYCe7GJQymlVC9J6ZiCMeZF4MV9nvtRm9+DwEWpjEEppVT36ZSRSimlkjQpKKWUStKkoJRSKkmTglJKqaSjbpZUEakEth3i2wvp21dL9+X4+nJsoPEdjr4cG/Tt+PpybNA+vuONMQe80OuoSwqHQ0RWdGfq2N7Sl+Pry7GBxnc4+nJs0Lfj68uxwaHFp91HSimlkjQpKKWUSkq3pLCwtwM4gL4cX1+ODTS+w9GXY4O+HV9fjg0OIb60GlNQSinVtXRrKSillOpC2iQFEZkrIutFZKOILOgD8TwgIhUisqbNc/1E5CUR2RD/md9LsQ0RkSUi8pGIrBWRb/aV+ETEJyLviMj78dh+En++RETejn+/j8dn5u01IuIUkfdE5B99LT4R2SoiH4rIahFZEX+u17/beBx5IrJIRD4WkXUi8qk+FNuo+GeWWOpE5MY+FN+34v8Ta0Tk0fj/ykH/3aVFUmhzv+izgbHApSIytnej4kFg7j7PLQBeMcaMBF6JP+4NEeDbxpixwCzghvjn1RfiawE+Y4yZBEwG5orILOz9vX9rjBkB1GDv/92bvgmsa/O4r8V3mjFmcuDjtmgAAAUJSURBVJvTFfvCdwvwP8C/jDGjgUnYz7BPxGaMWR//zCYD04Am4Om+EJ+IDAK+AZQaY8ZjZ6ZO3Pf+4P7ujDHH/AJ8Cljc5vH3gO/1gbiGAWvaPF4PDIz/PhBY39sxxmN5Fjizr8UH+IFV2Nu8VgGujr7vXohrMLZy+AzwD0D6WHxbgcJ9nuv17xZ7k60txMc6+1JsHcR6FvBmX4mP1lsb98POfv0P4LOH8neXFi0FOr5f9KBeiqUrxcaY3fHf9wDFvRkMgIgMA6YAb9NH4ot3zawGKoCXgE1ArTEmEi/S29/vXcD/o/Uu7AX0rfgM8G8RWRm/1S30je+2BKgE/hzvertfRDL7SGz7ugR4NP57r8dnjNkJ3AlsB3YDAWAlh/B3ly5J4ahjbGrv1VPDRCQLeBL+f3v381pHFYZx/PtINbSpNAoV1IJSBRWh1C6C2CqFurFIdVERrUXEZTfdSfEX+gcoLkS7rBpUKqkLl40S6EJrrbHWVlRUNAsbEX9VUEp8XJxzx+tNsWmgmYE8H7jk3jOT4b05M3lnzjDnZbft3/qXtRmf7VmXS/g1wChwYxtxnI2ku4EZ2x+1Hcv/2GR7A2U4dZekO/oXtti3y4ANwEu2bwH+YGAopiPHxSXANmD/4LK24qv3Me6hJNargGHmDk/Py1JJCvOpF90FpyRdCVB/zrQViKSLKQlhzPZ41+IDsP0L8B7lsnik1vmGdvt3I7BN0rfAG5QhpBfoTny9s0psz1DGxEfpRt9OA9O2P6if36IkiS7E1u8u4KjtU/VzF+K7E/jG9o+2zwDjlH3xvPe7pZIU5lMvugv6a1Y/TBnLX3SSRCmVetL2c32LWo9P0mpJI/X9csq9jpOU5LC9zdgAbO+xvcb2tZT97F3bO7oSn6RhSZf23lPGxo/Tgb61/QPwvaQbatMW4EQXYhvwAP8OHUE34vsOuFXSinr89v5257/ftX3DZhFvxGwFvqCMPz/egXhep4z9naGcIT1KGXueAL4EDgKXtxTbJsol8DFgqr62diE+YB3wcY3tOPBUbV8LHAa+olzWD3WgjzcD73QpvhrHJ/X1We9Y6ELf1jjWA0dq/74NXNaV2Gp8w8BPwKq+tk7EBzwDfF6Pi1eBoYXsd3miOSIiGktl+CgiIuYhSSEiIhpJChER0UhSiIiIRpJCREQ0khQiFpGkzb2ZUyO6KEkhIiIaSQoRZyHpoVq3YUrS3joJ32lJz9c56yckra7rrpf0vqRjkg705tOXdL2kg7X2w1FJ19XNr+yrGTBWn0CN6IQkhYgBkm4C7gc2uky8NwvsoDzNesT2zcAk8HT9lVeAx2yvAz7tax8DXnSp/XAb5Ql2KLPO7qbU9lhLmaMmohOWnXuViCVnC6WIyof1JH45ZZKzv4E36zqvAeOSVgEjtidr+z5gf51f6GrbBwBs/wlQt3fY9nT9PEWpq3Hown+tiHNLUoiYS8A+23v+0yg9ObDeQueI+avv/Sw5DqNDMnwUMdcEsF3SFdDUL76Gcrz0Zpx8EDhk+1fgZ0m31/adwKTt34FpSffWbQxJWrGo3yJiAXKGEjHA9glJT1Cqk11Emcl2F6Xoy2hdNkO57wBlSuKX6z/9r4FHavtOYK+kZ+s27lvErxGxIJklNWKeJJ22vbLtOCIupAwfRUREI1cKERHRyJVCREQ0khQiIqKRpBAREY0khYiIaCQpREREI0khIiIa/wC1mM8TAxr54wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 668us/sample - loss: 0.1681 - acc: 0.9477\n",
      "Loss: 0.16811249554776206 Accuracy: 0.94766355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_custom_tanh_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_tanh_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 374us/sample - loss: 2.7252 - acc: 0.0953\n",
      "Loss: 2.7251594565243984 Accuracy: 0.0953271\n",
      "\n",
      "1D_CNN_custom_tanh_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 546us/sample - loss: 2.0949 - acc: 0.3421\n",
      "Loss: 2.0949046964833546 Accuracy: 0.34205607\n",
      "\n",
      "1D_CNN_custom_tanh_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 620us/sample - loss: 1.6062 - acc: 0.5001\n",
      "Loss: 1.6061557545716394 Accuracy: 0.50010383\n",
      "\n",
      "1D_CNN_custom_tanh_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 640us/sample - loss: 1.2182 - acc: 0.6370\n",
      "Loss: 1.2181774709826318 Accuracy: 0.63696784\n",
      "\n",
      "1D_CNN_custom_tanh_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 703us/sample - loss: 0.9044 - acc: 0.7423\n",
      "Loss: 0.90441693653075 Accuracy: 0.74226373\n",
      "\n",
      "1D_CNN_custom_tanh_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 680us/sample - loss: 0.5394 - acc: 0.8455\n",
      "Loss: 0.539416510850841 Accuracy: 0.8454829\n",
      "\n",
      "1D_CNN_custom_tanh_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 709us/sample - loss: 0.2755 - acc: 0.9192\n",
      "Loss: 0.2754717957317396 Accuracy: 0.9192108\n",
      "\n",
      "1D_CNN_custom_tanh_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 730us/sample - loss: 0.1873 - acc: 0.9433\n",
      "Loss: 0.18725951010069247 Accuracy: 0.94330215\n",
      "\n",
      "1D_CNN_custom_tanh_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 730us/sample - loss: 0.1681 - acc: 0.9477\n",
      "Loss: 0.16811249554776206 Accuracy: 0.94766355\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_tanh_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_tanh_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 417us/sample - loss: 8.1461 - acc: 0.0987\n",
      "Loss: 8.146089713769044 Accuracy: 0.09865005\n",
      "\n",
      "1D_CNN_custom_tanh_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 615us/sample - loss: 5.4396 - acc: 0.3423\n",
      "Loss: 5.43957172861228 Accuracy: 0.34226376\n",
      "\n",
      "1D_CNN_custom_tanh_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 717us/sample - loss: 2.7503 - acc: 0.5774\n",
      "Loss: 2.7502910405056995 Accuracy: 0.5773624\n",
      "\n",
      "1D_CNN_custom_tanh_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 708us/sample - loss: 1.5140 - acc: 0.6982\n",
      "Loss: 1.5140112312411964 Accuracy: 0.6982347\n",
      "\n",
      "1D_CNN_custom_tanh_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 758us/sample - loss: 1.1235 - acc: 0.7433\n",
      "Loss: 1.1234925141339485 Accuracy: 0.74330217\n",
      "\n",
      "1D_CNN_custom_tanh_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 728us/sample - loss: 0.5808 - acc: 0.8586\n",
      "Loss: 0.5807907643347886 Accuracy: 0.858567\n",
      "\n",
      "1D_CNN_custom_tanh_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 766us/sample - loss: 0.2972 - acc: 0.9207\n",
      "Loss: 0.2972372277132076 Accuracy: 0.9206646\n",
      "\n",
      "1D_CNN_custom_tanh_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 787us/sample - loss: 0.2142 - acc: 0.9410\n",
      "Loss: 0.21423647905838206 Accuracy: 0.9410176\n",
      "\n",
      "1D_CNN_custom_tanh_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 843us/sample - loss: 0.2026 - acc: 0.9543\n",
      "Loss: 0.202633896684453 Accuracy: 0.95430946\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_tanh_DO'\n",
    "\n",
    "with open(path.join(log_dir, base+'_last'), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
